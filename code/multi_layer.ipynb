{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-4473aca54be6>:86: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "[step: 0] loss: 0.20791023969650269\n",
      "[nstep: 0] loss: 0.2396225482225418\n",
      "[step: 1] loss: 0.08533928543329239\n",
      "[nstep: 1] loss: 0.14774669706821442\n",
      "[step: 2] loss: 0.04344991594552994\n",
      "[nstep: 2] loss: 0.1016237810254097\n",
      "[step: 3] loss: 0.03496082127094269\n",
      "[nstep: 3] loss: 0.0704653188586235\n",
      "[step: 4] loss: 0.02889198623597622\n",
      "[nstep: 4] loss: 0.049962226301431656\n",
      "[step: 5] loss: 0.025131380185484886\n",
      "[nstep: 5] loss: 0.03616470843553543\n",
      "[step: 6] loss: 0.02470528520643711\n",
      "[nstep: 6] loss: 0.02802380733191967\n",
      "[step: 7] loss: 0.023841656744480133\n",
      "[nstep: 7] loss: 0.02445714734494686\n",
      "[step: 8] loss: 0.02278904616832733\n",
      "[nstep: 8] loss: 0.023938314989209175\n",
      "[step: 9] loss: 0.022407419979572296\n",
      "[nstep: 9] loss: 0.02528502605855465\n",
      "[step: 10] loss: 0.021640190854668617\n",
      "[nstep: 10] loss: 0.026163239032030106\n",
      "[step: 11] loss: 0.020679976791143417\n",
      "[nstep: 11] loss: 0.026001760736107826\n",
      "[step: 12] loss: 0.01959877274930477\n",
      "[nstep: 12] loss: 0.02514655329287052\n",
      "[step: 13] loss: 0.018411003053188324\n",
      "[nstep: 13] loss: 0.023906921967864037\n",
      "[step: 14] loss: 0.017522485926747322\n",
      "[nstep: 14] loss: 0.022495193406939507\n",
      "[step: 15] loss: 0.016605740413069725\n",
      "[nstep: 15] loss: 0.0212456826120615\n",
      "[step: 16] loss: 0.016011636704206467\n",
      "[nstep: 16] loss: 0.020360246300697327\n",
      "[step: 17] loss: 0.015352494083344936\n",
      "[nstep: 17] loss: 0.01972007006406784\n",
      "[step: 18] loss: 0.014812665060162544\n",
      "[nstep: 18] loss: 0.019129589200019836\n",
      "[step: 19] loss: 0.014374063350260258\n",
      "[nstep: 19] loss: 0.018554532900452614\n",
      "[step: 20] loss: 0.013778644613921642\n",
      "[nstep: 20] loss: 0.018232962116599083\n",
      "[step: 21] loss: 0.013291988521814346\n",
      "[nstep: 21] loss: 0.018060635775327682\n",
      "[step: 22] loss: 0.012586175464093685\n",
      "[nstep: 22] loss: 0.017903167754411697\n",
      "[step: 23] loss: 0.012060385197401047\n",
      "[nstep: 23] loss: 0.017953796312212944\n",
      "[step: 24] loss: 0.011412794701755047\n",
      "[nstep: 24] loss: 0.017994701862335205\n",
      "[step: 25] loss: 0.010884910821914673\n",
      "[nstep: 25] loss: 0.01791398599743843\n",
      "[step: 26] loss: 0.010338691994547844\n",
      "[nstep: 26] loss: 0.017752064391970634\n",
      "[step: 27] loss: 0.009714762680232525\n",
      "[nstep: 27] loss: 0.01756245642900467\n",
      "[step: 28] loss: 0.009167196229100227\n",
      "[nstep: 28] loss: 0.017744965851306915\n",
      "[step: 29] loss: 0.008750800974667072\n",
      "[nstep: 29] loss: 0.0170680470764637\n",
      "[step: 30] loss: 0.008605323731899261\n",
      "[nstep: 30] loss: 0.01693759486079216\n",
      "[step: 31] loss: 0.008637740276753902\n",
      "[nstep: 31] loss: 0.01678803563117981\n",
      "[step: 32] loss: 0.008080055937170982\n",
      "[nstep: 32] loss: 0.016560083255171776\n",
      "[step: 33] loss: 0.007266472093760967\n",
      "[nstep: 33] loss: 0.016320575028657913\n",
      "[step: 34] loss: 0.007222819607704878\n",
      "[nstep: 34] loss: 0.016159504652023315\n",
      "[step: 35] loss: 0.006980095058679581\n",
      "[nstep: 35] loss: 0.01606929861009121\n",
      "[step: 36] loss: 0.006289161741733551\n",
      "[nstep: 36] loss: 0.015972226858139038\n",
      "[step: 37] loss: 0.006321819499135017\n",
      "[nstep: 37] loss: 0.015857232734560966\n",
      "[step: 38] loss: 0.00628802552819252\n",
      "[nstep: 38] loss: 0.015781119465827942\n",
      "[step: 39] loss: 0.005691015161573887\n",
      "[nstep: 39] loss: 0.015722937881946564\n",
      "[step: 40] loss: 0.005562083795666695\n",
      "[nstep: 40] loss: 0.015601461753249168\n",
      "[step: 41] loss: 0.005556223448365927\n",
      "[nstep: 41] loss: 0.015421446412801743\n",
      "[step: 42] loss: 0.005224018823355436\n",
      "[nstep: 42] loss: 0.015244308859109879\n",
      "[step: 43] loss: 0.00504502234980464\n",
      "[nstep: 43] loss: 0.015030387789011002\n",
      "[step: 44] loss: 0.004993424750864506\n",
      "[nstep: 44] loss: 0.01475294679403305\n",
      "[step: 45] loss: 0.004739492200314999\n",
      "[nstep: 45] loss: 0.014500858262181282\n",
      "[step: 46] loss: 0.004644365981221199\n",
      "[nstep: 46] loss: 0.014392493292689323\n",
      "[step: 47] loss: 0.004588132258504629\n",
      "[nstep: 47] loss: 0.014209364540874958\n",
      "[step: 48] loss: 0.004348594695329666\n",
      "[nstep: 48] loss: 0.01396979857236147\n",
      "[step: 49] loss: 0.004243041854351759\n",
      "[nstep: 49] loss: 0.013773077167570591\n",
      "[step: 50] loss: 0.004245708230882883\n",
      "[nstep: 50] loss: 0.01358405128121376\n",
      "[step: 51] loss: 0.004110062960535288\n",
      "[nstep: 51] loss: 0.013358619064092636\n",
      "[step: 52] loss: 0.0039747292175889015\n",
      "[nstep: 52] loss: 0.013173017650842667\n",
      "[step: 53] loss: 0.003940062131732702\n",
      "[nstep: 53] loss: 0.013008740730583668\n",
      "[step: 54] loss: 0.003866213373839855\n",
      "[nstep: 54] loss: 0.012826221063733101\n",
      "[step: 55] loss: 0.003746473230421543\n",
      "[nstep: 55] loss: 0.01258996780961752\n",
      "[step: 56] loss: 0.0036810224410146475\n",
      "[nstep: 56] loss: 0.012324578128755093\n",
      "[step: 57] loss: 0.0036508161574602127\n",
      "[nstep: 57] loss: 0.012072245590388775\n",
      "[step: 58] loss: 0.0035854196175932884\n",
      "[nstep: 58] loss: 0.011820799671113491\n",
      "[step: 59] loss: 0.003501936560496688\n",
      "[nstep: 59] loss: 0.011558160185813904\n",
      "[step: 60] loss: 0.0034562700893729925\n",
      "[nstep: 60] loss: 0.01129970047622919\n",
      "[step: 61] loss: 0.0034311984200030565\n",
      "[nstep: 61] loss: 0.011045705527067184\n",
      "[step: 62] loss: 0.003382527269423008\n",
      "[nstep: 62] loss: 0.010775415226817131\n",
      "[step: 63] loss: 0.0033167428337037563\n",
      "[nstep: 63] loss: 0.010529517196118832\n",
      "[step: 64] loss: 0.0032706938218325377\n",
      "[nstep: 64] loss: 0.01028777752071619\n",
      "[step: 65] loss: 0.003251992166042328\n",
      "[nstep: 65] loss: 0.010053622536361217\n",
      "[step: 66] loss: 0.0032284404151141644\n",
      "[nstep: 66] loss: 0.009844996966421604\n",
      "[step: 67] loss: 0.003180548083037138\n",
      "[nstep: 67] loss: 0.009672639891505241\n",
      "[step: 68] loss: 0.003128791693598032\n",
      "[nstep: 68] loss: 0.009818701073527336\n",
      "[step: 69] loss: 0.003085704054683447\n",
      "[nstep: 69] loss: 0.010922383517026901\n",
      "[step: 70] loss: 0.0030501217115670443\n",
      "[nstep: 70] loss: 0.009483981877565384\n",
      "[step: 71] loss: 0.0030236325692385435\n",
      "[nstep: 71] loss: 0.009691925719380379\n",
      "[step: 72] loss: 0.0030003665015101433\n",
      "[nstep: 72] loss: 0.009981051087379456\n",
      "[step: 73] loss: 0.002979459473863244\n",
      "[nstep: 73] loss: 0.008948215283453465\n",
      "[step: 74] loss: 0.002957792254164815\n",
      "[nstep: 74] loss: 0.010013159364461899\n",
      "[step: 75] loss: 0.0029345634393393993\n",
      "[nstep: 75] loss: 0.008789612911641598\n",
      "[step: 76] loss: 0.002910617273300886\n",
      "[nstep: 76] loss: 0.00940863136202097\n",
      "[step: 77] loss: 0.0028949377592653036\n",
      "[nstep: 77] loss: 0.008755465038120747\n",
      "[step: 78] loss: 0.0028774715028703213\n",
      "[nstep: 78] loss: 0.008984840475022793\n",
      "[step: 79] loss: 0.002863503061234951\n",
      "[nstep: 79] loss: 0.008795289322733879\n",
      "[step: 80] loss: 0.002854452468454838\n",
      "[nstep: 80] loss: 0.008616680279374123\n",
      "[step: 81] loss: 0.002850062446668744\n",
      "[nstep: 81] loss: 0.008770270273089409\n",
      "[step: 82] loss: 0.002837456064298749\n",
      "[nstep: 82] loss: 0.008350611664354801\n",
      "[step: 83] loss: 0.002819607499986887\n",
      "[nstep: 83] loss: 0.008635514415800571\n",
      "[step: 84] loss: 0.00278379931114614\n",
      "[nstep: 84] loss: 0.00824198592454195\n",
      "[step: 85] loss: 0.002740265103057027\n",
      "[nstep: 85] loss: 0.008511096239089966\n",
      "[step: 86] loss: 0.002686506137251854\n",
      "[nstep: 86] loss: 0.008112912997603416\n",
      "[step: 87] loss: 0.0026403660885989666\n",
      "[nstep: 87] loss: 0.008379773236811161\n",
      "[step: 88] loss: 0.002610117197036743\n",
      "[nstep: 88] loss: 0.008075167424976826\n",
      "[step: 89] loss: 0.0025938162580132484\n",
      "[nstep: 89] loss: 0.008204082027077675\n",
      "[step: 90] loss: 0.002587601076811552\n",
      "[nstep: 90] loss: 0.008047704584896564\n",
      "[step: 91] loss: 0.002589272568002343\n",
      "[nstep: 91] loss: 0.008066111244261265\n",
      "[step: 92] loss: 0.0026004903484135866\n",
      "[nstep: 92] loss: 0.008035536855459213\n",
      "[step: 93] loss: 0.0026209489442408085\n",
      "[nstep: 93] loss: 0.007922534830868244\n",
      "[step: 94] loss: 0.0026644603349268436\n",
      "[nstep: 94] loss: 0.008010043762624264\n",
      "[step: 95] loss: 0.0027162963524460793\n",
      "[nstep: 95] loss: 0.007850928232073784\n",
      "[step: 96] loss: 0.002782361814752221\n",
      "[nstep: 96] loss: 0.007934389635920525\n",
      "[step: 97] loss: 0.0027594587299972773\n",
      "[nstep: 97] loss: 0.007840611971914768\n",
      "[step: 98] loss: 0.0026575084775686264\n",
      "[nstep: 98] loss: 0.007823728024959564\n",
      "[step: 99] loss: 0.0024927558843046427\n",
      "[nstep: 99] loss: 0.007837085984647274\n",
      "[step: 100] loss: 0.002425981918349862\n",
      "[nstep: 100] loss: 0.007739292457699776\n",
      "[step: 101] loss: 0.002482771873474121\n",
      "[nstep: 101] loss: 0.0077967955730855465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 102] loss: 0.0025484671350568533\n",
      "[nstep: 102] loss: 0.0077240075916051865\n",
      "[step: 103] loss: 0.0025343764573335648\n",
      "[nstep: 103] loss: 0.007706062868237495\n",
      "[step: 104] loss: 0.002432374283671379\n",
      "[nstep: 104] loss: 0.007722881622612476\n",
      "[step: 105] loss: 0.0023611823562532663\n",
      "[nstep: 105] loss: 0.007659499533474445\n",
      "[step: 106] loss: 0.0023746087681502104\n",
      "[nstep: 106] loss: 0.007676725275814533\n",
      "[step: 107] loss: 0.0024224568624049425\n",
      "[nstep: 107] loss: 0.0076533351093530655\n",
      "[step: 108] loss: 0.0024379410315304995\n",
      "[nstep: 108] loss: 0.0076171960681676865\n",
      "[step: 109] loss: 0.0023866405244916677\n",
      "[nstep: 109] loss: 0.007637395989149809\n",
      "[step: 110] loss: 0.00232197274453938\n",
      "[nstep: 110] loss: 0.007605643477290869\n",
      "[step: 111] loss: 0.002291165292263031\n",
      "[nstep: 111] loss: 0.007578135468065739\n",
      "[step: 112] loss: 0.002303970977663994\n",
      "[nstep: 112] loss: 0.007594050373882055\n",
      "[step: 113] loss: 0.0023334852885454893\n",
      "[nstep: 113] loss: 0.00756797706708312\n",
      "[step: 114] loss: 0.0023433775641024113\n",
      "[nstep: 114] loss: 0.007541906088590622\n",
      "[step: 115] loss: 0.0023301842156797647\n",
      "[nstep: 115] loss: 0.007551734335720539\n",
      "[step: 116] loss: 0.002288926625624299\n",
      "[nstep: 116] loss: 0.007542641367763281\n",
      "[step: 117] loss: 0.0022490075789391994\n",
      "[nstep: 117] loss: 0.0075127133168280125\n",
      "[step: 118] loss: 0.0022223086562007666\n",
      "[nstep: 118] loss: 0.0075062839314341545\n",
      "[step: 119] loss: 0.002213247586041689\n",
      "[nstep: 119] loss: 0.007511461619287729\n",
      "[step: 120] loss: 0.0022189663723111153\n",
      "[nstep: 120] loss: 0.007496499922126532\n",
      "[step: 121] loss: 0.00223171547986567\n",
      "[nstep: 121] loss: 0.007473963778465986\n",
      "[step: 122] loss: 0.00225184322334826\n",
      "[nstep: 122] loss: 0.007466488052159548\n",
      "[step: 123] loss: 0.0022737111430615187\n",
      "[nstep: 123] loss: 0.007469759788364172\n",
      "[step: 124] loss: 0.0023129472974687815\n",
      "[nstep: 124] loss: 0.007464869413524866\n",
      "[step: 125] loss: 0.0023430909495800734\n",
      "[nstep: 125] loss: 0.007446832489222288\n",
      "[step: 126] loss: 0.0023831732105463743\n",
      "[nstep: 126] loss: 0.007431962992995977\n",
      "[step: 127] loss: 0.0023579990956932306\n",
      "[nstep: 127] loss: 0.007426017429679632\n",
      "[step: 128] loss: 0.0023076613433659077\n",
      "[nstep: 128] loss: 0.007425500545650721\n",
      "[step: 129] loss: 0.002210199134424329\n",
      "[nstep: 129] loss: 0.007423953153192997\n",
      "[step: 130] loss: 0.0021387275774031878\n",
      "[nstep: 130] loss: 0.0074170781299471855\n",
      "[step: 131] loss: 0.002121499739587307\n",
      "[nstep: 131] loss: 0.00740601634606719\n",
      "[step: 132] loss: 0.002151787979528308\n",
      "[nstep: 132] loss: 0.0073937224224209785\n",
      "[step: 133] loss: 0.0021968658547848463\n",
      "[nstep: 133] loss: 0.007383918855339289\n",
      "[step: 134] loss: 0.0022142231464385986\n",
      "[nstep: 134] loss: 0.007376327645033598\n",
      "[step: 135] loss: 0.0022035136353224516\n",
      "[nstep: 135] loss: 0.0073716407641768456\n",
      "[step: 136] loss: 0.002153947949409485\n",
      "[nstep: 136] loss: 0.007368959952145815\n",
      "[step: 137] loss: 0.0021047042682766914\n",
      "[nstep: 137] loss: 0.007368115708231926\n",
      "[step: 138] loss: 0.0020746486261487007\n",
      "[nstep: 138] loss: 0.007371767424046993\n",
      "[step: 139] loss: 0.0020726537331938744\n",
      "[nstep: 139] loss: 0.007381543982774019\n",
      "[step: 140] loss: 0.0020901705138385296\n",
      "[nstep: 140] loss: 0.007409846410155296\n",
      "[step: 141] loss: 0.0021110724192112684\n",
      "[nstep: 141] loss: 0.007458846550434828\n",
      "[step: 142] loss: 0.0021275116596370935\n",
      "[nstep: 142] loss: 0.0075602238066494465\n",
      "[step: 143] loss: 0.0021254317834973335\n",
      "[nstep: 143] loss: 0.00760787446051836\n",
      "[step: 144] loss: 0.002114301547408104\n",
      "[nstep: 144] loss: 0.00757521390914917\n",
      "[step: 145] loss: 0.0020876985508948565\n",
      "[nstep: 145] loss: 0.00739365303888917\n",
      "[step: 146] loss: 0.002060096012428403\n",
      "[nstep: 146] loss: 0.007326877675950527\n",
      "[step: 147] loss: 0.002035029698163271\n",
      "[nstep: 147] loss: 0.0074253748171031475\n",
      "[step: 148] loss: 0.00201969500631094\n",
      "[nstep: 148] loss: 0.007448701653629541\n",
      "[step: 149] loss: 0.0020141059067100286\n",
      "[nstep: 149] loss: 0.007347346283495426\n",
      "[step: 150] loss: 0.0020159222185611725\n",
      "[nstep: 150] loss: 0.007313411217182875\n",
      "[step: 151] loss: 0.0020229825749993324\n",
      "[nstep: 151] loss: 0.00738114956766367\n",
      "[step: 152] loss: 0.0020337719470262527\n",
      "[nstep: 152] loss: 0.007379178889095783\n",
      "[step: 153] loss: 0.0020520577672868967\n",
      "[nstep: 153] loss: 0.007303584832698107\n",
      "[step: 154] loss: 0.002076429547742009\n",
      "[nstep: 154] loss: 0.007301026489585638\n",
      "[step: 155] loss: 0.002119864569976926\n",
      "[nstep: 155] loss: 0.007346646394580603\n",
      "[step: 156] loss: 0.002168233273550868\n",
      "[nstep: 156] loss: 0.007326131220906973\n",
      "[step: 157] loss: 0.0022437809966504574\n",
      "[nstep: 157] loss: 0.007277670782059431\n",
      "[step: 158] loss: 0.0022715413942933083\n",
      "[nstep: 158] loss: 0.007283828686922789\n",
      "[step: 159] loss: 0.002269595395773649\n",
      "[nstep: 159] loss: 0.00731385312974453\n",
      "[step: 160] loss: 0.0021456603426486254\n",
      "[nstep: 160] loss: 0.007304581813514233\n",
      "[step: 161] loss: 0.002015706617385149\n",
      "[nstep: 161] loss: 0.007267470937222242\n",
      "[step: 162] loss: 0.0019618344958871603\n",
      "[nstep: 162] loss: 0.00725606270134449\n",
      "[step: 163] loss: 0.0020109256729483604\n",
      "[nstep: 163] loss: 0.007273871451616287\n",
      "[step: 164] loss: 0.0020846943370997906\n",
      "[nstep: 164] loss: 0.007284280378371477\n",
      "[step: 165] loss: 0.002082296647131443\n",
      "[nstep: 165] loss: 0.007269802503287792\n",
      "[step: 166] loss: 0.0020174779929220676\n",
      "[nstep: 166] loss: 0.007244963198900223\n",
      "[step: 167] loss: 0.0019499085610732436\n",
      "[nstep: 167] loss: 0.007235787343233824\n",
      "[step: 168] loss: 0.001945852767676115\n",
      "[nstep: 168] loss: 0.007243611384183168\n",
      "[step: 169] loss: 0.001988257747143507\n",
      "[nstep: 169] loss: 0.007252056151628494\n",
      "[step: 170] loss: 0.0020152158103883266\n",
      "[nstep: 170] loss: 0.007249405607581139\n",
      "[step: 171] loss: 0.001999275293201208\n",
      "[nstep: 171] loss: 0.007235367316752672\n",
      "[step: 172] loss: 0.0019507152028381824\n",
      "[nstep: 172] loss: 0.00722159631550312\n",
      "[step: 173] loss: 0.0019209138117730618\n",
      "[nstep: 173] loss: 0.00721505843102932\n",
      "[step: 174] loss: 0.001928005600348115\n",
      "[nstep: 174] loss: 0.007216213271021843\n",
      "[step: 175] loss: 0.0019514885498210788\n",
      "[nstep: 175] loss: 0.0072212438099086285\n",
      "[step: 176] loss: 0.0019621532410383224\n",
      "[nstep: 176] loss: 0.007225488778203726\n",
      "[step: 177] loss: 0.0019438739400357008\n",
      "[nstep: 177] loss: 0.00722822081297636\n",
      "[step: 178] loss: 0.0019161475356668234\n",
      "[nstep: 178] loss: 0.007227455265820026\n",
      "[step: 179] loss: 0.0018999646417796612\n",
      "[nstep: 179] loss: 0.007225509267300367\n",
      "[step: 180] loss: 0.0019025951623916626\n",
      "[nstep: 180] loss: 0.007221371866762638\n",
      "[step: 181] loss: 0.0019148155115544796\n",
      "[nstep: 181] loss: 0.007218250073492527\n",
      "[step: 182] loss: 0.001920871902257204\n",
      "[nstep: 182] loss: 0.007214501965790987\n",
      "[step: 183] loss: 0.0019153831526637077\n",
      "[nstep: 183] loss: 0.007212702184915543\n",
      "[step: 184] loss: 0.0019006440415978432\n",
      "[nstep: 184] loss: 0.007211041171103716\n",
      "[step: 185] loss: 0.0018859824631363153\n",
      "[nstep: 185] loss: 0.007212560623884201\n",
      "[step: 186] loss: 0.0018772946204990149\n",
      "[nstep: 186] loss: 0.007216060068458319\n",
      "[step: 187] loss: 0.0018759038066491485\n",
      "[nstep: 187] loss: 0.007226136978715658\n",
      "[step: 188] loss: 0.0018792800838127732\n",
      "[nstep: 188] loss: 0.007240010891109705\n",
      "[step: 189] loss: 0.001884188619442284\n",
      "[nstep: 189] loss: 0.00726395845413208\n",
      "[step: 190] loss: 0.001888931030407548\n",
      "[nstep: 190] loss: 0.00728563591837883\n",
      "[step: 191] loss: 0.0018918378045782447\n",
      "[nstep: 191] loss: 0.0073085264302790165\n",
      "[step: 192] loss: 0.0018943604081869125\n",
      "[nstep: 192] loss: 0.007301183883100748\n",
      "[step: 193] loss: 0.0018947835778817534\n",
      "[nstep: 193] loss: 0.0072668613865971565\n",
      "[step: 194] loss: 0.0018964229384437203\n",
      "[nstep: 194] loss: 0.007208302151411772\n",
      "[step: 195] loss: 0.001897025154903531\n",
      "[nstep: 195] loss: 0.007165181450545788\n",
      "[step: 196] loss: 0.0019003170309588313\n",
      "[nstep: 196] loss: 0.007160710636526346\n",
      "[step: 197] loss: 0.0019026860827580094\n",
      "[nstep: 197] loss: 0.007184724789112806\n",
      "[step: 198] loss: 0.0019090361893177032\n",
      "[nstep: 198] loss: 0.0072104958817362785\n",
      "[step: 199] loss: 0.0019145384430885315\n",
      "[nstep: 199] loss: 0.007215825375169516\n",
      "[step: 200] loss: 0.0019256771774962544\n",
      "[nstep: 200] loss: 0.007201134692877531\n",
      "[step: 201] loss: 0.0019337047124281526\n",
      "[nstep: 201] loss: 0.007175126578658819\n",
      "[step: 202] loss: 0.0019465048098936677\n",
      "[nstep: 202] loss: 0.007152946665883064\n",
      "[step: 203] loss: 0.001949178520590067\n",
      "[nstep: 203] loss: 0.00714349839836359\n",
      "[step: 204] loss: 0.0019517568871378899\n",
      "[nstep: 204] loss: 0.007146930787712336\n",
      "[step: 205] loss: 0.0019360576989129186\n",
      "[nstep: 205] loss: 0.00715857045724988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 206] loss: 0.0019146472914144397\n",
      "[nstep: 206] loss: 0.007170995231717825\n",
      "[step: 207] loss: 0.001879936084151268\n",
      "[nstep: 207] loss: 0.007179704029113054\n",
      "[step: 208] loss: 0.001847988460212946\n",
      "[nstep: 208] loss: 0.007180118001997471\n",
      "[step: 209] loss: 0.001823566621169448\n",
      "[nstep: 209] loss: 0.0071730040945112705\n",
      "[step: 210] loss: 0.001812262344174087\n",
      "[nstep: 210] loss: 0.007159905042499304\n",
      "[step: 211] loss: 0.001813019160181284\n",
      "[nstep: 211] loss: 0.007147574797272682\n",
      "[step: 212] loss: 0.0018219668418169022\n",
      "[nstep: 212] loss: 0.007139165885746479\n",
      "[step: 213] loss: 0.0018355248030275106\n",
      "[nstep: 213] loss: 0.007136365864425898\n",
      "[step: 214] loss: 0.0018496853299438953\n",
      "[nstep: 214] loss: 0.007134030107408762\n",
      "[step: 215] loss: 0.0018653167644515634\n",
      "[nstep: 215] loss: 0.007130867335945368\n",
      "[step: 216] loss: 0.0018763156840577722\n",
      "[nstep: 216] loss: 0.007122465409338474\n",
      "[step: 217] loss: 0.0018866374157369137\n",
      "[nstep: 217] loss: 0.007113934960216284\n",
      "[step: 218] loss: 0.001885475474409759\n",
      "[nstep: 218] loss: 0.007108192890882492\n",
      "[step: 219] loss: 0.0018798671662807465\n",
      "[nstep: 219] loss: 0.007107389625161886\n",
      "[step: 220] loss: 0.0018606736557558179\n",
      "[nstep: 220] loss: 0.007110190112143755\n",
      "[step: 221] loss: 0.0018384389113634825\n",
      "[nstep: 221] loss: 0.007114753592759371\n",
      "[step: 222] loss: 0.0018132372060790658\n",
      "[nstep: 222] loss: 0.00712144048884511\n",
      "[step: 223] loss: 0.001793550094589591\n",
      "[nstep: 223] loss: 0.007135344203561544\n",
      "[step: 224] loss: 0.001781282713636756\n",
      "[nstep: 224] loss: 0.007168942596763372\n",
      "[step: 225] loss: 0.0017768967663869262\n",
      "[nstep: 225] loss: 0.00725514255464077\n",
      "[step: 226] loss: 0.0017787080723792315\n",
      "[nstep: 226] loss: 0.0074249571189284325\n",
      "[step: 227] loss: 0.001784816849976778\n",
      "[nstep: 227] loss: 0.007730951998382807\n",
      "[step: 228] loss: 0.001794640440493822\n",
      "[nstep: 228] loss: 0.007874168455600739\n",
      "[step: 229] loss: 0.001807557069696486\n",
      "[nstep: 229] loss: 0.00755523145198822\n",
      "[step: 230] loss: 0.0018269404536113143\n",
      "[nstep: 230] loss: 0.007128775119781494\n",
      "[step: 231] loss: 0.0018509374931454659\n",
      "[nstep: 231] loss: 0.007287945132702589\n",
      "[step: 232] loss: 0.0018882862059399486\n",
      "[nstep: 232] loss: 0.0074568940326571465\n",
      "[step: 233] loss: 0.001926890341565013\n",
      "[nstep: 233] loss: 0.007189778611063957\n",
      "[step: 234] loss: 0.0019792765378952026\n",
      "[nstep: 234] loss: 0.0072198170237243176\n",
      "[step: 235] loss: 0.0019993556197732687\n",
      "[nstep: 235] loss: 0.0073064640164375305\n",
      "[step: 236] loss: 0.0019990878645330667\n",
      "[nstep: 236] loss: 0.00714391702786088\n",
      "[step: 237] loss: 0.0019241241971030831\n",
      "[nstep: 237] loss: 0.007261499762535095\n",
      "[step: 238] loss: 0.0018318122019991279\n",
      "[nstep: 238] loss: 0.0071856179274618626\n",
      "[step: 239] loss: 0.001760257058776915\n",
      "[nstep: 239] loss: 0.0071273986250162125\n",
      "[step: 240] loss: 0.0017532157944515347\n",
      "[nstep: 240] loss: 0.007226727437227964\n",
      "[step: 241] loss: 0.0017964369617402554\n",
      "[nstep: 241] loss: 0.007094085216522217\n",
      "[step: 242] loss: 0.0018406349699944258\n",
      "[nstep: 242] loss: 0.007168786134570837\n",
      "[step: 243] loss: 0.0018545460188761353\n",
      "[nstep: 243] loss: 0.007114578038454056\n",
      "[step: 244] loss: 0.0018198498291894794\n",
      "[nstep: 244] loss: 0.007108351215720177\n",
      "[step: 245] loss: 0.0017712283879518509\n",
      "[nstep: 245] loss: 0.007139876950532198\n",
      "[step: 246] loss: 0.0017369640991091728\n",
      "[nstep: 246] loss: 0.007079928182065487\n",
      "[step: 247] loss: 0.0017358206678181887\n",
      "[nstep: 247] loss: 0.0071334997192025185\n",
      "[step: 248] loss: 0.0017582611180841923\n",
      "[nstep: 248] loss: 0.00707580242305994\n",
      "[step: 249] loss: 0.001779648824594915\n",
      "[nstep: 249] loss: 0.007101519964635372\n",
      "[step: 250] loss: 0.0017845500260591507\n",
      "[nstep: 250] loss: 0.007087388541549444\n",
      "[step: 251] loss: 0.0017662792233750224\n",
      "[nstep: 251] loss: 0.007069761864840984\n",
      "[step: 252] loss: 0.0017407103441655636\n",
      "[nstep: 252] loss: 0.007089613936841488\n",
      "[step: 253] loss: 0.0017214289400726557\n",
      "[nstep: 253] loss: 0.007055596448481083\n",
      "[step: 254] loss: 0.001717542763799429\n",
      "[nstep: 254] loss: 0.007081066258251667\n",
      "[step: 255] loss: 0.0017260803142562509\n",
      "[nstep: 255] loss: 0.007057255133986473\n",
      "[step: 256] loss: 0.001737233018502593\n",
      "[nstep: 256] loss: 0.007060859352350235\n",
      "[step: 257] loss: 0.0017432182794436812\n",
      "[nstep: 257] loss: 0.007061518728733063\n",
      "[step: 258] loss: 0.001739170984365046\n",
      "[nstep: 258] loss: 0.0070442138239741325\n",
      "[step: 259] loss: 0.0017290825489908457\n",
      "[nstep: 259] loss: 0.007058207876980305\n",
      "[step: 260] loss: 0.001716144266538322\n",
      "[nstep: 260] loss: 0.0070405881851911545\n",
      "[step: 261] loss: 0.001705619040876627\n",
      "[nstep: 261] loss: 0.007045346312224865\n",
      "[step: 262] loss: 0.0016995205078274012\n",
      "[nstep: 262] loss: 0.007043881807476282\n",
      "[step: 263] loss: 0.001698007108643651\n",
      "[nstep: 263] loss: 0.007032552268356085\n",
      "[step: 264] loss: 0.001699785585515201\n",
      "[nstep: 264] loss: 0.007041118107736111\n",
      "[step: 265] loss: 0.0017033966723829508\n",
      "[nstep: 265] loss: 0.0070307282730937\n",
      "[step: 266] loss: 0.0017084144055843353\n",
      "[nstep: 266] loss: 0.007029315922409296\n",
      "[step: 267] loss: 0.0017142471624538302\n",
      "[nstep: 267] loss: 0.0070322295650839806\n",
      "[step: 268] loss: 0.0017223989125341177\n",
      "[nstep: 268] loss: 0.007022233214229345\n",
      "[step: 269] loss: 0.0017320463666692376\n",
      "[nstep: 269] loss: 0.007024757098406553\n",
      "[step: 270] loss: 0.001747548347339034\n",
      "[nstep: 270] loss: 0.007023213896900415\n",
      "[step: 271] loss: 0.0017664855113252997\n",
      "[nstep: 271] loss: 0.007016653660684824\n",
      "[step: 272] loss: 0.001797739416360855\n",
      "[nstep: 272] loss: 0.007019686978310347\n",
      "[step: 273] loss: 0.0018311934545636177\n",
      "[nstep: 273] loss: 0.007017250172793865\n",
      "[step: 274] loss: 0.0018805305007845163\n",
      "[nstep: 274] loss: 0.007015250623226166\n",
      "[step: 275] loss: 0.001909488346427679\n",
      "[nstep: 275] loss: 0.007022629491984844\n",
      "[step: 276] loss: 0.001931996550410986\n",
      "[nstep: 276] loss: 0.007033115718513727\n",
      "[step: 277] loss: 0.00188681751023978\n",
      "[nstep: 277] loss: 0.007067338563501835\n",
      "[step: 278] loss: 0.001813489943742752\n",
      "[nstep: 278] loss: 0.007160529028624296\n",
      "[step: 279] loss: 0.001720578409731388\n",
      "[nstep: 279] loss: 0.007387669291347265\n",
      "[step: 280] loss: 0.001669719466008246\n",
      "[nstep: 280] loss: 0.007753250654786825\n",
      "[step: 281] loss: 0.001677572843618691\n",
      "[nstep: 281] loss: 0.008025340735912323\n",
      "[step: 282] loss: 0.0017215514089912176\n",
      "[nstep: 282] loss: 0.007661142386496067\n",
      "[step: 283] loss: 0.0017646761843934655\n",
      "[nstep: 283] loss: 0.007054667919874191\n",
      "[step: 284] loss: 0.001769081805832684\n",
      "[nstep: 284] loss: 0.007419676519930363\n",
      "[step: 285] loss: 0.0017419473733752966\n",
      "[nstep: 285] loss: 0.007612093351781368\n",
      "[step: 286] loss: 0.0016933773877099156\n",
      "[nstep: 286] loss: 0.0070757451467216015\n",
      "[step: 287] loss: 0.0016589885344728827\n",
      "[nstep: 287] loss: 0.0072837951593101025\n",
      "[step: 288] loss: 0.001653881510719657\n",
      "[nstep: 288] loss: 0.007439472246915102\n",
      "[step: 289] loss: 0.0016721670981496572\n",
      "[nstep: 289] loss: 0.007044544909149408\n",
      "[step: 290] loss: 0.001694475649856031\n",
      "[nstep: 290] loss: 0.007284713443368673\n",
      "[step: 291] loss: 0.0017006932757794857\n",
      "[nstep: 291] loss: 0.0072730774991214275\n",
      "[step: 292] loss: 0.0016897849272936583\n",
      "[nstep: 292] loss: 0.007035513874143362\n",
      "[step: 293] loss: 0.0016662690322846174\n",
      "[nstep: 293] loss: 0.007275312673300505\n",
      "[step: 294] loss: 0.001646246644668281\n",
      "[nstep: 294] loss: 0.0071279387921094894\n",
      "[step: 295] loss: 0.0016382141038775444\n",
      "[nstep: 295] loss: 0.007059621158987284\n",
      "[step: 296] loss: 0.001642546965740621\n",
      "[nstep: 296] loss: 0.007215654943138361\n",
      "[step: 297] loss: 0.0016528082778677344\n",
      "[nstep: 297] loss: 0.007047465071082115\n",
      "[step: 298] loss: 0.0016608716687187552\n",
      "[nstep: 298] loss: 0.00706790667027235\n",
      "[step: 299] loss: 0.0016635068459436297\n",
      "[nstep: 299] loss: 0.007148806005716324\n",
      "[step: 300] loss: 0.0016583935357630253\n",
      "[nstep: 300] loss: 0.0070130485109984875\n",
      "[step: 301] loss: 0.0016495325835421681\n",
      "[nstep: 301] loss: 0.007061654236167669\n",
      "[step: 302] loss: 0.0016387117793783545\n",
      "[nstep: 302] loss: 0.007095320150256157\n",
      "[step: 303] loss: 0.0016295835375785828\n",
      "[nstep: 303] loss: 0.006995967123657465\n",
      "[step: 304] loss: 0.0016230609035119414\n",
      "[nstep: 304] loss: 0.007047906517982483\n",
      "[step: 305] loss: 0.001619316404685378\n",
      "[nstep: 305] loss: 0.0070573980920016766\n",
      "[step: 306] loss: 0.001617849338799715\n",
      "[nstep: 306] loss: 0.00698607275262475\n",
      "[step: 307] loss: 0.0016181241953745484\n",
      "[nstep: 307] loss: 0.007032343186438084\n",
      "[step: 308] loss: 0.0016197782242670655\n",
      "[nstep: 308] loss: 0.007029477972537279\n",
      "[step: 309] loss: 0.0016226980369538069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 309] loss: 0.006980824749916792\n",
      "[step: 310] loss: 0.001627815654501319\n",
      "[nstep: 310] loss: 0.007014139089733362\n",
      "[step: 311] loss: 0.0016360152512788773\n",
      "[nstep: 311] loss: 0.007010189816355705\n",
      "[step: 312] loss: 0.0016512639122083783\n",
      "[nstep: 312] loss: 0.006976197008043528\n",
      "[step: 313] loss: 0.0016760913422331214\n",
      "[nstep: 313] loss: 0.006996003910899162\n",
      "[step: 314] loss: 0.0017238034633919597\n",
      "[nstep: 314] loss: 0.006997117772698402\n",
      "[step: 315] loss: 0.0017961455741897225\n",
      "[nstep: 315] loss: 0.006971574388444424\n",
      "[step: 316] loss: 0.0019275175873190165\n",
      "[nstep: 316] loss: 0.006979130674153566\n",
      "[step: 317] loss: 0.0020652771927416325\n",
      "[nstep: 317] loss: 0.006987287662923336\n",
      "[step: 318] loss: 0.0022324619349092245\n",
      "[nstep: 318] loss: 0.0069688791409134865\n",
      "[step: 319] loss: 0.0021575940772891045\n",
      "[nstep: 319] loss: 0.0069657377898693085\n",
      "[step: 320] loss: 0.0019325498724356294\n",
      "[nstep: 320] loss: 0.0069768899120390415\n",
      "[step: 321] loss: 0.0016503656515851617\n",
      "[nstep: 321] loss: 0.006967455148696899\n",
      "[step: 322] loss: 0.0016418395098298788\n",
      "[nstep: 322] loss: 0.006956006865948439\n",
      "[step: 323] loss: 0.0018301551463082433\n",
      "[nstep: 323] loss: 0.0069637238048017025\n",
      "[step: 324] loss: 0.0018700152868404984\n",
      "[nstep: 324] loss: 0.006964844651520252\n",
      "[step: 325] loss: 0.0017243047477677464\n",
      "[nstep: 325] loss: 0.006952614989131689\n",
      "[step: 326] loss: 0.0015955045819282532\n",
      "[nstep: 326] loss: 0.00695106340572238\n",
      "[step: 327] loss: 0.001660656533204019\n",
      "[nstep: 327] loss: 0.006957331672310829\n",
      "[step: 328] loss: 0.001766373636201024\n",
      "[nstep: 328] loss: 0.006953743286430836\n",
      "[step: 329] loss: 0.0017123835859820247\n",
      "[nstep: 329] loss: 0.006945463363081217\n",
      "[step: 330] loss: 0.0016043868381530046\n",
      "[nstep: 330] loss: 0.006944651249796152\n",
      "[step: 331] loss: 0.0016040336340665817\n",
      "[nstep: 331] loss: 0.006947540678083897\n",
      "[step: 332] loss: 0.0016774168470874429\n",
      "[nstep: 332] loss: 0.006945877801626921\n",
      "[step: 333] loss: 0.0016796443378552794\n",
      "[nstep: 333] loss: 0.006940167397260666\n",
      "[step: 334] loss: 0.0016034460859373212\n",
      "[nstep: 334] loss: 0.006936636753380299\n",
      "[step: 335] loss: 0.00158357759937644\n",
      "[nstep: 335] loss: 0.006937801837921143\n",
      "[step: 336] loss: 0.0016304380260407925\n",
      "[nstep: 336] loss: 0.006938839331269264\n",
      "[step: 337] loss: 0.0016396734863519669\n",
      "[nstep: 337] loss: 0.006935912184417248\n",
      "[step: 338] loss: 0.001595129957422614\n",
      "[nstep: 338] loss: 0.006931562442332506\n",
      "[step: 339] loss: 0.0015723410760983825\n",
      "[nstep: 339] loss: 0.006929112132638693\n",
      "[step: 340] loss: 0.0015988280065357685\n",
      "[nstep: 340] loss: 0.006928689312189817\n",
      "[step: 341] loss: 0.0016139858635142446\n",
      "[nstep: 341] loss: 0.006928789895027876\n",
      "[step: 342] loss: 0.0015868806513026357\n",
      "[nstep: 342] loss: 0.006928085349500179\n",
      "[step: 343] loss: 0.0015645954990759492\n",
      "[nstep: 343] loss: 0.006926087662577629\n",
      "[step: 344] loss: 0.0015756780048832297\n",
      "[nstep: 344] loss: 0.0069230953231453896\n",
      "[step: 345] loss: 0.0015911419177427888\n",
      "[nstep: 345] loss: 0.006920471787452698\n",
      "[step: 346] loss: 0.0015825165901333094\n",
      "[nstep: 346] loss: 0.006918658968061209\n",
      "[step: 347] loss: 0.0015619001351296902\n",
      "[nstep: 347] loss: 0.0069173783995211124\n",
      "[step: 348] loss: 0.0015576757723465562\n",
      "[nstep: 348] loss: 0.006916521117091179\n",
      "[step: 349] loss: 0.0015685245161876082\n",
      "[nstep: 349] loss: 0.006916063372045755\n",
      "[step: 350] loss: 0.0015727350255474448\n",
      "[nstep: 350] loss: 0.006915869191288948\n",
      "[step: 351] loss: 0.0015627766260877252\n",
      "[nstep: 351] loss: 0.006915837060660124\n",
      "[step: 352] loss: 0.0015505575574934483\n",
      "[nstep: 352] loss: 0.00691670598462224\n",
      "[step: 353] loss: 0.0015492558013647795\n",
      "[nstep: 353] loss: 0.006918939296156168\n",
      "[step: 354] loss: 0.0015554634155705571\n",
      "[nstep: 354] loss: 0.006925272289663553\n",
      "[step: 355] loss: 0.0015576237346976995\n",
      "[nstep: 355] loss: 0.006937072612345219\n",
      "[step: 356] loss: 0.001551792141981423\n",
      "[nstep: 356] loss: 0.006966800894588232\n",
      "[step: 357] loss: 0.001543156336992979\n",
      "[nstep: 357] loss: 0.007012699265033007\n",
      "[step: 358] loss: 0.0015391918132081628\n",
      "[nstep: 358] loss: 0.0071151359006762505\n",
      "[step: 359] loss: 0.0015407453756779432\n",
      "[nstep: 359] loss: 0.007182688917964697\n",
      "[step: 360] loss: 0.0015433856751769781\n",
      "[nstep: 360] loss: 0.007219858933240175\n",
      "[step: 361] loss: 0.0015429349150508642\n",
      "[nstep: 361] loss: 0.0070731802843511105\n",
      "[step: 362] loss: 0.001538590993732214\n",
      "[nstep: 362] loss: 0.006923998240381479\n",
      "[step: 363] loss: 0.0015332094626501203\n",
      "[nstep: 363] loss: 0.00695037143304944\n",
      "[step: 364] loss: 0.0015293934848159552\n",
      "[nstep: 364] loss: 0.0070743681862950325\n",
      "[step: 365] loss: 0.0015281616942957044\n",
      "[nstep: 365] loss: 0.00714105274528265\n",
      "[step: 366] loss: 0.0015286114066839218\n",
      "[nstep: 366] loss: 0.007021455094218254\n",
      "[step: 367] loss: 0.0015291612362489104\n",
      "[nstep: 367] loss: 0.006908813025802374\n",
      "[step: 368] loss: 0.0015288128051906824\n",
      "[nstep: 368] loss: 0.007002389524132013\n",
      "[step: 369] loss: 0.0015271151205524802\n",
      "[nstep: 369] loss: 0.007071678526699543\n",
      "[step: 370] loss: 0.001524544321000576\n",
      "[nstep: 370] loss: 0.006990482565015554\n",
      "[step: 371] loss: 0.0015214869054034352\n",
      "[nstep: 371] loss: 0.006919071078300476\n",
      "[step: 372] loss: 0.0015185389202088118\n",
      "[nstep: 372] loss: 0.006956957746297121\n",
      "[step: 373] loss: 0.0015159026952460408\n",
      "[nstep: 373] loss: 0.006998478900641203\n",
      "[step: 374] loss: 0.0015136732254177332\n",
      "[nstep: 374] loss: 0.0069501157850027084\n",
      "[step: 375] loss: 0.0015118190785869956\n",
      "[nstep: 375] loss: 0.006894811987876892\n",
      "[step: 376] loss: 0.00151022057980299\n",
      "[nstep: 376] loss: 0.006929437629878521\n",
      "[step: 377] loss: 0.0015088232466951013\n",
      "[nstep: 377] loss: 0.006946931593120098\n",
      "[step: 378] loss: 0.0015076061245054007\n",
      "[nstep: 378] loss: 0.006911930628120899\n",
      "[step: 379] loss: 0.001506635220721364\n",
      "[nstep: 379] loss: 0.006897368002682924\n",
      "[step: 380] loss: 0.0015060951700434089\n",
      "[nstep: 380] loss: 0.006903907284140587\n",
      "[step: 381] loss: 0.0015064150793477893\n",
      "[nstep: 381] loss: 0.006923281587660313\n",
      "[step: 382] loss: 0.0015084592159837484\n",
      "[nstep: 382] loss: 0.006914815399795771\n",
      "[step: 383] loss: 0.0015144763747230172\n",
      "[nstep: 383] loss: 0.006884585600346327\n",
      "[step: 384] loss: 0.001528813154436648\n",
      "[nstep: 384] loss: 0.006889503449201584\n",
      "[step: 385] loss: 0.0015640618512406945\n",
      "[nstep: 385] loss: 0.006902163848280907\n",
      "[step: 386] loss: 0.0016418895684182644\n",
      "[nstep: 386] loss: 0.006906865630298853\n",
      "[step: 387] loss: 0.0018331101164221764\n",
      "[nstep: 387] loss: 0.006900997366756201\n",
      "[step: 388] loss: 0.0021863000001758337\n",
      "[nstep: 388] loss: 0.006878481712192297\n",
      "[step: 389] loss: 0.002896134043112397\n",
      "[nstep: 389] loss: 0.006877983454614878\n",
      "[step: 390] loss: 0.0031696856021881104\n",
      "[nstep: 390] loss: 0.006886654067784548\n",
      "[step: 391] loss: 0.002811630256474018\n",
      "[nstep: 391] loss: 0.006894240155816078\n",
      "[step: 392] loss: 0.0016586732817813754\n",
      "[nstep: 392] loss: 0.006898803636431694\n",
      "[step: 393] loss: 0.0019683334976434708\n",
      "[nstep: 393] loss: 0.006882979068905115\n",
      "[step: 394] loss: 0.002535019302740693\n",
      "[nstep: 394] loss: 0.006873143836855888\n",
      "[step: 395] loss: 0.0017050819005817175\n",
      "[nstep: 395] loss: 0.006867638789117336\n",
      "[step: 396] loss: 0.0017863927641883492\n",
      "[nstep: 396] loss: 0.006867021322250366\n",
      "[step: 397] loss: 0.0022175763733685017\n",
      "[nstep: 397] loss: 0.0068739173002541065\n",
      "[step: 398] loss: 0.001599387265741825\n",
      "[nstep: 398] loss: 0.00687514478340745\n",
      "[step: 399] loss: 0.0017798927146941423\n",
      "[nstep: 399] loss: 0.00687905540689826\n",
      "[step: 400] loss: 0.001994106685742736\n",
      "[nstep: 400] loss: 0.0068822093307971954\n",
      "[step: 401] loss: 0.0015290442388504744\n",
      "[nstep: 401] loss: 0.006885693874210119\n",
      "[step: 402] loss: 0.0018236221512779593\n",
      "[nstep: 402] loss: 0.006888910196721554\n",
      "[step: 403] loss: 0.0018058522837236524\n",
      "[nstep: 403] loss: 0.0068842279724776745\n",
      "[step: 404] loss: 0.0015248211566358805\n",
      "[nstep: 404] loss: 0.006883879192173481\n",
      "[step: 405] loss: 0.0018205350497737527\n",
      "[nstep: 405] loss: 0.006883819587528706\n",
      "[step: 406] loss: 0.0016255987575277686\n",
      "[nstep: 406] loss: 0.006891021504998207\n",
      "[step: 407] loss: 0.0015817669918760657\n",
      "[nstep: 407] loss: 0.006896488834172487\n",
      "[step: 408] loss: 0.0017329967813566327\n",
      "[nstep: 408] loss: 0.006905000191181898\n",
      "[step: 409] loss: 0.0015147735830396414\n",
      "[nstep: 409] loss: 0.0069146850146353245\n",
      "[step: 410] loss: 0.0016387993237003684\n",
      "[nstep: 410] loss: 0.0069413078017532825\n",
      "[step: 411] loss: 0.0015925681218504906\n",
      "[nstep: 411] loss: 0.00695792306214571\n",
      "[step: 412] loss: 0.0015226214891299605\n",
      "[nstep: 412] loss: 0.006968171335756779\n",
      "[step: 413] loss: 0.0016235808143392205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 413] loss: 0.006944021675735712\n",
      "[step: 414] loss: 0.001505897962488234\n",
      "[nstep: 414] loss: 0.006918531842529774\n",
      "[step: 415] loss: 0.001565877697430551\n",
      "[nstep: 415] loss: 0.006882225163280964\n",
      "[step: 416] loss: 0.0015545657370239496\n",
      "[nstep: 416] loss: 0.0068547590635716915\n",
      "[step: 417] loss: 0.001498222816735506\n",
      "[nstep: 417] loss: 0.006842921953648329\n",
      "[step: 418] loss: 0.0015681893564760685\n",
      "[nstep: 418] loss: 0.006848203483968973\n",
      "[step: 419] loss: 0.0014987995382398367\n",
      "[nstep: 419] loss: 0.0068642329424619675\n",
      "[step: 420] loss: 0.001518694101832807\n",
      "[nstep: 420] loss: 0.00688175018876791\n",
      "[step: 421] loss: 0.0015302777756005526\n",
      "[nstep: 421] loss: 0.006895259488373995\n",
      "[step: 422] loss: 0.001481629558838904\n",
      "[nstep: 422] loss: 0.006892537232488394\n",
      "[step: 423] loss: 0.0015236218459904194\n",
      "[nstep: 423] loss: 0.006883166264742613\n",
      "[step: 424] loss: 0.0014915767824277282\n",
      "[nstep: 424] loss: 0.006865212693810463\n",
      "[step: 425] loss: 0.0014880847884342074\n",
      "[nstep: 425] loss: 0.006850242614746094\n",
      "[step: 426] loss: 0.0015072956448420882\n",
      "[nstep: 426] loss: 0.00683869794011116\n",
      "[step: 427] loss: 0.0014737372985109687\n",
      "[nstep: 427] loss: 0.006832730490714312\n",
      "[step: 428] loss: 0.0014925373252481222\n",
      "[nstep: 428] loss: 0.006831067148596048\n",
      "[step: 429] loss: 0.0014855529880151153\n",
      "[nstep: 429] loss: 0.006832627579569817\n",
      "[step: 430] loss: 0.0014697269070893526\n",
      "[nstep: 430] loss: 0.006837416440248489\n",
      "[step: 431] loss: 0.001486738445237279\n",
      "[nstep: 431] loss: 0.006845259573310614\n",
      "[step: 432] loss: 0.0014699267921969295\n",
      "[nstep: 432] loss: 0.006860376335680485\n",
      "[step: 433] loss: 0.0014689610106870532\n",
      "[nstep: 433] loss: 0.0068841371685266495\n",
      "[step: 434] loss: 0.001476128469221294\n",
      "[nstep: 434] loss: 0.006937806960195303\n",
      "[step: 435] loss: 0.0014609050704166293\n",
      "[nstep: 435] loss: 0.007000982295721769\n",
      "[step: 436] loss: 0.0014657725114375353\n",
      "[nstep: 436] loss: 0.007111256010830402\n",
      "[step: 437] loss: 0.0014657118590548635\n",
      "[nstep: 437] loss: 0.007132750004529953\n",
      "[step: 438] loss: 0.001455173478461802\n",
      "[nstep: 438] loss: 0.007075221743434668\n",
      "[step: 439] loss: 0.0014608632773160934\n",
      "[nstep: 439] loss: 0.006942246109247208\n",
      "[step: 440] loss: 0.0014571489300578833\n",
      "[nstep: 440] loss: 0.00686135096475482\n",
      "[step: 441] loss: 0.001450583804398775\n",
      "[nstep: 441] loss: 0.006879964377731085\n",
      "[step: 442] loss: 0.001455273013561964\n",
      "[nstep: 442] loss: 0.006947792135179043\n",
      "[step: 443] loss: 0.0014500987017527223\n",
      "[nstep: 443] loss: 0.006999651435762644\n",
      "[step: 444] loss: 0.0014464040286839008\n",
      "[nstep: 444] loss: 0.006942260079085827\n",
      "[step: 445] loss: 0.0014493551570922136\n",
      "[nstep: 445] loss: 0.006864790339022875\n",
      "[step: 446] loss: 0.0014442773535847664\n",
      "[nstep: 446] loss: 0.006872900296002626\n",
      "[step: 447] loss: 0.001442140550352633\n",
      "[nstep: 447] loss: 0.0069162193685770035\n",
      "[step: 448] loss: 0.0014436023775488138\n",
      "[nstep: 448] loss: 0.006941139232367277\n",
      "[step: 449] loss: 0.0014395067701116204\n",
      "[nstep: 449] loss: 0.006885130424052477\n",
      "[step: 450] loss: 0.0014376189792528749\n",
      "[nstep: 450] loss: 0.006838144268840551\n",
      "[step: 451] loss: 0.0014381954679265618\n",
      "[nstep: 451] loss: 0.006867590360343456\n",
      "[step: 452] loss: 0.0014351933496072888\n",
      "[nstep: 452] loss: 0.006905539892613888\n",
      "[step: 453] loss: 0.0014330000849440694\n",
      "[nstep: 453] loss: 0.006906983442604542\n",
      "[step: 454] loss: 0.0014331680722534657\n",
      "[nstep: 454] loss: 0.0068475292064249516\n",
      "[step: 455] loss: 0.0014310074038803577\n",
      "[nstep: 455] loss: 0.006822079885751009\n",
      "[step: 456] loss: 0.0014285027282312512\n",
      "[nstep: 456] loss: 0.006859917193651199\n",
      "[step: 457] loss: 0.0014283397467806935\n",
      "[nstep: 457] loss: 0.0068901036866009235\n",
      "[step: 458] loss: 0.0014268604572862387\n",
      "[nstep: 458] loss: 0.0068851495161652565\n",
      "[step: 459] loss: 0.0014243253972381353\n",
      "[nstep: 459] loss: 0.006829880643635988\n",
      "[step: 460] loss: 0.0014236194547265768\n",
      "[nstep: 460] loss: 0.00681111728772521\n",
      "[step: 461] loss: 0.0014226456405594945\n",
      "[nstep: 461] loss: 0.006838676054030657\n",
      "[step: 462] loss: 0.001420438289642334\n",
      "[nstep: 462] loss: 0.006860530935227871\n",
      "[step: 463] loss: 0.0014190879883244634\n",
      "[nstep: 463] loss: 0.006864691618829966\n",
      "[step: 464] loss: 0.0014182992745190859\n",
      "[nstep: 464] loss: 0.006827912759035826\n",
      "[step: 465] loss: 0.0014166728360578418\n",
      "[nstep: 465] loss: 0.006810103077441454\n",
      "[step: 466] loss: 0.0014148843474686146\n",
      "[nstep: 466] loss: 0.0068122209049761295\n",
      "[step: 467] loss: 0.0014138618716970086\n",
      "[nstep: 467] loss: 0.006819672882556915\n",
      "[step: 468] loss: 0.0014127717586234212\n",
      "[nstep: 468] loss: 0.006831109989434481\n",
      "[step: 469] loss: 0.0014110381016507745\n",
      "[nstep: 469] loss: 0.0068242186680436134\n",
      "[step: 470] loss: 0.0014095811638981104\n",
      "[nstep: 470] loss: 0.006820484064519405\n",
      "[step: 471] loss: 0.0014085890725255013\n",
      "[nstep: 471] loss: 0.00680897431448102\n",
      "[step: 472] loss: 0.0014072718331590295\n",
      "[nstep: 472] loss: 0.006800456438213587\n",
      "[step: 473] loss: 0.0014056832296773791\n",
      "[nstep: 473] loss: 0.006793859414756298\n",
      "[step: 474] loss: 0.0014043641276657581\n",
      "[nstep: 474] loss: 0.006794050335884094\n",
      "[step: 475] loss: 0.0014032498002052307\n",
      "[nstep: 475] loss: 0.006800305563956499\n",
      "[step: 476] loss: 0.001401950023137033\n",
      "[nstep: 476] loss: 0.006803251337260008\n",
      "[step: 477] loss: 0.0014004642143845558\n",
      "[nstep: 477] loss: 0.006810921244323254\n",
      "[step: 478] loss: 0.0013991305604577065\n",
      "[nstep: 478] loss: 0.006811188068240881\n",
      "[step: 479] loss: 0.0013979811919853091\n",
      "[nstep: 479] loss: 0.0068173641338944435\n",
      "[step: 480] loss: 0.0013967176200821996\n",
      "[nstep: 480] loss: 0.006822199560701847\n",
      "[step: 481] loss: 0.0013953089946880937\n",
      "[nstep: 481] loss: 0.006833411753177643\n",
      "[step: 482] loss: 0.0013939664931967854\n",
      "[nstep: 482] loss: 0.006837358698248863\n",
      "[step: 483] loss: 0.001392743200995028\n",
      "[nstep: 483] loss: 0.006843669340014458\n",
      "[step: 484] loss: 0.001391522935591638\n",
      "[nstep: 484] loss: 0.0068492647260427475\n",
      "[step: 485] loss: 0.0013902123318985105\n",
      "[nstep: 485] loss: 0.006874429993331432\n",
      "[step: 486] loss: 0.0013888548128306866\n",
      "[nstep: 486] loss: 0.006869380362331867\n",
      "[step: 487] loss: 0.0013875651638954878\n",
      "[nstep: 487] loss: 0.006844626273959875\n",
      "[step: 488] loss: 0.0013863437343388796\n",
      "[nstep: 488] loss: 0.006825554650276899\n",
      "[step: 489] loss: 0.001385098323225975\n",
      "[nstep: 489] loss: 0.0068083577789366245\n",
      "[step: 490] loss: 0.0013838022714480758\n",
      "[nstep: 490] loss: 0.006797031499445438\n",
      "[step: 491] loss: 0.001382486429065466\n",
      "[nstep: 491] loss: 0.006780260242521763\n",
      "[step: 492] loss: 0.0013811989920213819\n",
      "[nstep: 492] loss: 0.006779803428798914\n",
      "[step: 493] loss: 0.0013799539301544428\n",
      "[nstep: 493] loss: 0.006777351256459951\n",
      "[step: 494] loss: 0.0013787152711302042\n",
      "[nstep: 494] loss: 0.0067839110270142555\n",
      "[step: 495] loss: 0.0013774525141343474\n",
      "[nstep: 495] loss: 0.0067842500284314156\n",
      "[step: 496] loss: 0.0013761654263362288\n",
      "[nstep: 496] loss: 0.006787484046071768\n",
      "[step: 497] loss: 0.0013748779892921448\n",
      "[nstep: 497] loss: 0.006790092680603266\n",
      "[step: 498] loss: 0.0013736033579334617\n",
      "[nstep: 498] loss: 0.006800341885536909\n",
      "[step: 499] loss: 0.0013723488664254546\n",
      "[nstep: 499] loss: 0.006804350297898054\n",
      "[step: 500] loss: 0.0013711045030504465\n",
      "[nstep: 500] loss: 0.006806282792240381\n",
      "[step: 501] loss: 0.001369853620417416\n",
      "[nstep: 501] loss: 0.006813972722738981\n",
      "[step: 502] loss: 0.0013685916783288121\n",
      "[nstep: 502] loss: 0.006838252302259207\n",
      "[step: 503] loss: 0.0013673220528289676\n",
      "[nstep: 503] loss: 0.0068582287058234215\n",
      "[step: 504] loss: 0.001366050448268652\n",
      "[nstep: 504] loss: 0.006867679301649332\n",
      "[step: 505] loss: 0.00136478035710752\n",
      "[nstep: 505] loss: 0.006884556729346514\n",
      "[step: 506] loss: 0.0013635180657729506\n",
      "[nstep: 506] loss: 0.006921339780092239\n",
      "[step: 507] loss: 0.0013622598489746451\n",
      "[nstep: 507] loss: 0.006917939521372318\n",
      "[step: 508] loss: 0.0013610044261440635\n",
      "[nstep: 508] loss: 0.0068457345478236675\n",
      "[step: 509] loss: 0.0013597534270957112\n",
      "[nstep: 509] loss: 0.006807936821132898\n",
      "[step: 510] loss: 0.0013585027772933245\n",
      "[nstep: 510] loss: 0.006769709289073944\n",
      "[step: 511] loss: 0.0013572530588135123\n",
      "[nstep: 511] loss: 0.006774066481739283\n",
      "[step: 512] loss: 0.001356005435809493\n",
      "[nstep: 512] loss: 0.00676976190879941\n",
      "[step: 513] loss: 0.0013547613052651286\n",
      "[nstep: 513] loss: 0.006790007930248976\n",
      "[step: 514] loss: 0.001353521947748959\n",
      "[nstep: 514] loss: 0.006787050515413284\n",
      "[step: 515] loss: 0.001352294348180294\n",
      "[nstep: 515] loss: 0.006792034022510052\n",
      "[step: 516] loss: 0.0013510879362002015\n",
      "[nstep: 516] loss: 0.0067763603292405605\n",
      "[step: 517] loss: 0.001349923200905323\n",
      "[nstep: 517] loss: 0.006763688754290342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 518] loss: 0.001348836231045425\n",
      "[nstep: 518] loss: 0.006762426812201738\n",
      "[step: 519] loss: 0.0013479138724505901\n",
      "[nstep: 519] loss: 0.006759814918041229\n",
      "[step: 520] loss: 0.001347349607385695\n",
      "[nstep: 520] loss: 0.006767703220248222\n",
      "[step: 521] loss: 0.001347556710243225\n",
      "[nstep: 521] loss: 0.00676352484151721\n",
      "[step: 522] loss: 0.0013495873427018523\n",
      "[nstep: 522] loss: 0.006770421285182238\n",
      "[step: 523] loss: 0.001355750602670014\n",
      "[nstep: 523] loss: 0.006767727434635162\n",
      "[step: 524] loss: 0.0013725993921980262\n",
      "[nstep: 524] loss: 0.006776754278689623\n",
      "[step: 525] loss: 0.0014137333491817117\n",
      "[nstep: 525] loss: 0.006770569831132889\n",
      "[step: 526] loss: 0.0015232007717713714\n",
      "[nstep: 526] loss: 0.006772890221327543\n",
      "[step: 527] loss: 0.0017635870026424527\n",
      "[nstep: 527] loss: 0.0067741782404482365\n",
      "[step: 528] loss: 0.0023653011303395033\n",
      "[nstep: 528] loss: 0.0067899515852332115\n",
      "[step: 529] loss: 0.0030251832213252783\n",
      "[nstep: 529] loss: 0.006794719956815243\n",
      "[step: 530] loss: 0.003681695321574807\n",
      "[nstep: 530] loss: 0.006799676921218634\n",
      "[step: 531] loss: 0.0021908015478402376\n",
      "[nstep: 531] loss: 0.006814972963184118\n",
      "[step: 532] loss: 0.001550166169181466\n",
      "[nstep: 532] loss: 0.006857918109744787\n",
      "[step: 533] loss: 0.0022935380693525076\n",
      "[nstep: 533] loss: 0.006877031642943621\n",
      "[step: 534] loss: 0.0019091896247118711\n",
      "[nstep: 534] loss: 0.006854838691651821\n",
      "[step: 535] loss: 0.0016041846247389913\n",
      "[nstep: 535] loss: 0.006851124111562967\n",
      "[step: 536] loss: 0.001828787731938064\n",
      "[nstep: 536] loss: 0.00684483302757144\n",
      "[step: 537] loss: 0.0017275403952226043\n",
      "[nstep: 537] loss: 0.006831432692706585\n",
      "[step: 538] loss: 0.0015432639047503471\n",
      "[nstep: 538] loss: 0.006768547464162111\n",
      "[step: 539] loss: 0.001637630513869226\n",
      "[nstep: 539] loss: 0.006767901126295328\n",
      "[step: 540] loss: 0.001607660437002778\n",
      "[nstep: 540] loss: 0.0067412350326776505\n",
      "[step: 541] loss: 0.001423720852471888\n",
      "[nstep: 541] loss: 0.006765611004084349\n",
      "[step: 542] loss: 0.00164723617490381\n",
      "[nstep: 542] loss: 0.006752889137715101\n",
      "[step: 543] loss: 0.0013968358980491757\n",
      "[nstep: 543] loss: 0.006768949329853058\n",
      "[step: 544] loss: 0.0015364161226898432\n",
      "[nstep: 544] loss: 0.006750606000423431\n",
      "[step: 545] loss: 0.0014664994087070227\n",
      "[nstep: 545] loss: 0.0067513794638216496\n",
      "[step: 546] loss: 0.0014557285467162728\n",
      "[nstep: 546] loss: 0.006739357020705938\n",
      "[step: 547] loss: 0.0014495218638330698\n",
      "[nstep: 547] loss: 0.006740091368556023\n",
      "[step: 548] loss: 0.0014534586807712913\n",
      "[nstep: 548] loss: 0.0067410701885819435\n",
      "[step: 549] loss: 0.0013895700685679913\n",
      "[nstep: 549] loss: 0.006745428312569857\n",
      "[step: 550] loss: 0.0014698593877255917\n",
      "[nstep: 550] loss: 0.006750492844730616\n",
      "[step: 551] loss: 0.0013647526502609253\n",
      "[nstep: 551] loss: 0.006749281659722328\n",
      "[step: 552] loss: 0.0014373669400811195\n",
      "[nstep: 552] loss: 0.00675190519541502\n",
      "[step: 553] loss: 0.0013759343419224024\n",
      "[nstep: 553] loss: 0.006749533116817474\n",
      "[step: 554] loss: 0.0013952782610431314\n",
      "[nstep: 554] loss: 0.006755207199603319\n",
      "[step: 555] loss: 0.0013856375589966774\n",
      "[nstep: 555] loss: 0.006753246299922466\n",
      "[step: 556] loss: 0.0013790461234748363\n",
      "[nstep: 556] loss: 0.006764348596334457\n",
      "[step: 557] loss: 0.001364298746921122\n",
      "[nstep: 557] loss: 0.006777684669941664\n",
      "[step: 558] loss: 0.001385729294270277\n",
      "[nstep: 558] loss: 0.006815651897341013\n",
      "[step: 559] loss: 0.0013444077922031283\n",
      "[nstep: 559] loss: 0.006819847039878368\n",
      "[step: 560] loss: 0.001378576853312552\n",
      "[nstep: 560] loss: 0.006827845703810453\n",
      "[step: 561] loss: 0.0013454497093334794\n",
      "[nstep: 561] loss: 0.006828922778367996\n",
      "[step: 562] loss: 0.0013569057919085026\n",
      "[nstep: 562] loss: 0.006851919461041689\n",
      "[step: 563] loss: 0.0013496187748387456\n",
      "[nstep: 563] loss: 0.006843235343694687\n",
      "[step: 564] loss: 0.0013468884862959385\n",
      "[nstep: 564] loss: 0.0068087452091276646\n",
      "[step: 565] loss: 0.0013401085743680596\n",
      "[nstep: 565] loss: 0.006789301056414843\n",
      "[step: 566] loss: 0.0013485264498740435\n",
      "[nstep: 566] loss: 0.0067471652291715145\n",
      "[step: 567] loss: 0.0013288820628076792\n",
      "[nstep: 567] loss: 0.006730471737682819\n",
      "[step: 568] loss: 0.001342486939392984\n",
      "[nstep: 568] loss: 0.006721756886690855\n",
      "[step: 569] loss: 0.0013288486516103148\n",
      "[nstep: 569] loss: 0.006743669044226408\n",
      "[step: 570] loss: 0.001331599080003798\n",
      "[nstep: 570] loss: 0.0067449901252985\n",
      "[step: 571] loss: 0.0013284219894558191\n",
      "[nstep: 571] loss: 0.0067468807101249695\n",
      "[step: 572] loss: 0.001327021629549563\n",
      "[nstep: 572] loss: 0.006730998866260052\n",
      "[step: 573] loss: 0.0013204991118982434\n",
      "[nstep: 573] loss: 0.006729147396981716\n",
      "[step: 574] loss: 0.0013263559667393565\n",
      "[nstep: 574] loss: 0.00672581372782588\n",
      "[step: 575] loss: 0.0013163960538804531\n",
      "[nstep: 575] loss: 0.006732183042913675\n",
      "[step: 576] loss: 0.0013199582463130355\n",
      "[nstep: 576] loss: 0.006722074467688799\n",
      "[step: 577] loss: 0.0013160441303625703\n",
      "[nstep: 577] loss: 0.006714962422847748\n",
      "[step: 578] loss: 0.0013149031437933445\n",
      "[nstep: 578] loss: 0.006711638066917658\n",
      "[step: 579] loss: 0.0013117616763338447\n",
      "[nstep: 579] loss: 0.006715974770486355\n",
      "[step: 580] loss: 0.0013133905595168471\n",
      "[nstep: 580] loss: 0.006721110548824072\n",
      "[step: 581] loss: 0.001307376311160624\n",
      "[nstep: 581] loss: 0.006721389014273882\n",
      "[step: 582] loss: 0.0013090204447507858\n",
      "[nstep: 582] loss: 0.006726101040840149\n",
      "[step: 583] loss: 0.0013065009843558073\n",
      "[nstep: 583] loss: 0.00673506548628211\n",
      "[step: 584] loss: 0.001304707140661776\n",
      "[nstep: 584] loss: 0.006755389738827944\n",
      "[step: 585] loss: 0.0013029022375121713\n",
      "[nstep: 585] loss: 0.006782565265893936\n",
      "[step: 586] loss: 0.0013029291294515133\n",
      "[nstep: 586] loss: 0.006837026681751013\n",
      "[step: 587] loss: 0.0012995778815820813\n",
      "[nstep: 587] loss: 0.006913299672305584\n",
      "[step: 588] loss: 0.0012990584364160895\n",
      "[nstep: 588] loss: 0.007022315636277199\n",
      "[step: 589] loss: 0.0012979417806491256\n",
      "[nstep: 589] loss: 0.007022952660918236\n",
      "[step: 590] loss: 0.0012959787854924798\n",
      "[nstep: 590] loss: 0.006912778597325087\n",
      "[step: 591] loss: 0.0012944479240104556\n",
      "[nstep: 591] loss: 0.006795782130211592\n",
      "[step: 592] loss: 0.001293701003305614\n",
      "[nstep: 592] loss: 0.006717459298670292\n",
      "[step: 593] loss: 0.0012921163579449058\n",
      "[nstep: 593] loss: 0.00672269519418478\n",
      "[step: 594] loss: 0.0012902111047878861\n",
      "[nstep: 594] loss: 0.006770807784050703\n",
      "[step: 595] loss: 0.0012896633706986904\n",
      "[nstep: 595] loss: 0.006801733747124672\n",
      "[step: 596] loss: 0.0012879990972578526\n",
      "[nstep: 596] loss: 0.006771976128220558\n",
      "[step: 597] loss: 0.0012866545002907515\n",
      "[nstep: 597] loss: 0.006719417404383421\n",
      "[step: 598] loss: 0.0012851286446675658\n",
      "[nstep: 598] loss: 0.006702237296849489\n",
      "[step: 599] loss: 0.0012844490120187402\n",
      "[nstep: 599] loss: 0.006735008675605059\n",
      "[step: 600] loss: 0.0012825506273657084\n",
      "[nstep: 600] loss: 0.006763552315533161\n",
      "[step: 601] loss: 0.001281447010114789\n",
      "[nstep: 601] loss: 0.0067599122412502766\n",
      "[step: 602] loss: 0.001280156895518303\n",
      "[nstep: 602] loss: 0.00672257412225008\n",
      "[step: 603] loss: 0.0012791093904525042\n",
      "[nstep: 603] loss: 0.006697258446365595\n",
      "[step: 604] loss: 0.0012775046052411199\n",
      "[nstep: 604] loss: 0.006702037062495947\n",
      "[step: 605] loss: 0.0012763049453496933\n",
      "[nstep: 605] loss: 0.006724874954670668\n",
      "[step: 606] loss: 0.001275210757739842\n",
      "[nstep: 606] loss: 0.006739098113030195\n",
      "[step: 607] loss: 0.0012738800141960382\n",
      "[nstep: 607] loss: 0.0067315492779016495\n",
      "[step: 608] loss: 0.001272564404644072\n",
      "[nstep: 608] loss: 0.006711849011480808\n",
      "[step: 609] loss: 0.0012712398311123252\n",
      "[nstep: 609] loss: 0.00669583585113287\n",
      "[step: 610] loss: 0.00127021421212703\n",
      "[nstep: 610] loss: 0.006690655834972858\n",
      "[step: 611] loss: 0.0012688348069787025\n",
      "[nstep: 611] loss: 0.006696076598018408\n",
      "[step: 612] loss: 0.001267594168893993\n",
      "[nstep: 612] loss: 0.006705417763441801\n",
      "[step: 613] loss: 0.0012662805384024978\n",
      "[nstep: 613] loss: 0.006711299531161785\n",
      "[step: 614] loss: 0.001265165163204074\n",
      "[nstep: 614] loss: 0.006713199894875288\n",
      "[step: 615] loss: 0.001263909274712205\n",
      "[nstep: 615] loss: 0.006706178188323975\n",
      "[step: 616] loss: 0.0012626047246158123\n",
      "[nstep: 616] loss: 0.006700184661895037\n",
      "[step: 617] loss: 0.00126136455219239\n",
      "[nstep: 617] loss: 0.006691935937851667\n",
      "[step: 618] loss: 0.001260126824490726\n",
      "[nstep: 618] loss: 0.0066862343810498714\n",
      "[step: 619] loss: 0.0012589690741151571\n",
      "[nstep: 619] loss: 0.006681802216917276\n",
      "[step: 620] loss: 0.001257668249309063\n",
      "[nstep: 620] loss: 0.006681264843791723\n",
      "[step: 621] loss: 0.001256429823115468\n",
      "[nstep: 621] loss: 0.0066842809319496155\n",
      "[step: 622] loss: 0.0012551633408293128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 622] loss: 0.006689310073852539\n",
      "[step: 623] loss: 0.0012539628660306334\n",
      "[nstep: 623] loss: 0.006694287993013859\n",
      "[step: 624] loss: 0.0012527485378086567\n",
      "[nstep: 624] loss: 0.006696015130728483\n",
      "[step: 625] loss: 0.0012514857808128\n",
      "[nstep: 625] loss: 0.006700778380036354\n",
      "[step: 626] loss: 0.0012502443278208375\n",
      "[nstep: 626] loss: 0.006702221930027008\n",
      "[step: 627] loss: 0.0012489737709984183\n",
      "[nstep: 627] loss: 0.006707433145493269\n",
      "[step: 628] loss: 0.0012477646814659238\n",
      "[nstep: 628] loss: 0.006711528170853853\n",
      "[step: 629] loss: 0.001246526837348938\n",
      "[nstep: 629] loss: 0.0067200125195086\n",
      "[step: 630] loss: 0.001245295163244009\n",
      "[nstep: 630] loss: 0.006722533609718084\n",
      "[step: 631] loss: 0.001244048005901277\n",
      "[nstep: 631] loss: 0.006725884508341551\n",
      "[step: 632] loss: 0.0012427822221070528\n",
      "[nstep: 632] loss: 0.00672439020127058\n",
      "[step: 633] loss: 0.001241543097421527\n",
      "[nstep: 633] loss: 0.006729337386786938\n",
      "[step: 634] loss: 0.0012402903521433473\n",
      "[nstep: 634] loss: 0.006719102617353201\n",
      "[step: 635] loss: 0.0012390551855787635\n",
      "[nstep: 635] loss: 0.006707194726914167\n",
      "[step: 636] loss: 0.0012378140818327665\n",
      "[nstep: 636] loss: 0.006692244205623865\n",
      "[step: 637] loss: 0.001236565294675529\n",
      "[nstep: 637] loss: 0.0066823409870266914\n",
      "[step: 638] loss: 0.0012353196507319808\n",
      "[nstep: 638] loss: 0.006673389580100775\n",
      "[step: 639] loss: 0.0012340577086433768\n",
      "[nstep: 639] loss: 0.006668373476713896\n",
      "[step: 640] loss: 0.0012328035663813353\n",
      "[nstep: 640] loss: 0.006666979286819696\n",
      "[step: 641] loss: 0.001231541158631444\n",
      "[nstep: 641] loss: 0.00666842283681035\n",
      "[step: 642] loss: 0.0012302803806960583\n",
      "[nstep: 642] loss: 0.006671189330518246\n",
      "[step: 643] loss: 0.0012290228623896837\n",
      "[nstep: 643] loss: 0.006674346048384905\n",
      "[step: 644] loss: 0.0012277566129341722\n",
      "[nstep: 644] loss: 0.006679030600935221\n",
      "[step: 645] loss: 0.0012264971155673265\n",
      "[nstep: 645] loss: 0.006684539839625359\n",
      "[step: 646] loss: 0.001225233543664217\n",
      "[nstep: 646] loss: 0.006690948735922575\n",
      "[step: 647] loss: 0.0012239670613780618\n",
      "[nstep: 647] loss: 0.006697051227092743\n",
      "[step: 648] loss: 0.0012227038387209177\n",
      "[nstep: 648] loss: 0.006707365158945322\n",
      "[step: 649] loss: 0.001221437705680728\n",
      "[nstep: 649] loss: 0.00671754265204072\n",
      "[step: 650] loss: 0.001220177044160664\n",
      "[nstep: 650] loss: 0.00673190550878644\n",
      "[step: 651] loss: 0.001218922552652657\n",
      "[nstep: 651] loss: 0.006741029676049948\n",
      "[step: 652] loss: 0.0012176854070276022\n",
      "[nstep: 652] loss: 0.006756470538675785\n",
      "[step: 653] loss: 0.0012164963409304619\n",
      "[nstep: 653] loss: 0.006753941532224417\n",
      "[step: 654] loss: 0.0012154141440987587\n",
      "[nstep: 654] loss: 0.006744250189512968\n",
      "[step: 655] loss: 0.0012146063381806016\n",
      "[nstep: 655] loss: 0.0067221131175756454\n",
      "[step: 656] loss: 0.0012144912034273148\n",
      "[nstep: 656] loss: 0.006699231453239918\n",
      "[step: 657] loss: 0.0012162330094724894\n",
      "[nstep: 657] loss: 0.006676157470792532\n",
      "[step: 658] loss: 0.001222936436533928\n",
      "[nstep: 658] loss: 0.006660471670329571\n",
      "[step: 659] loss: 0.00124409853015095\n",
      "[nstep: 659] loss: 0.006655366159975529\n",
      "[step: 660] loss: 0.001304382225498557\n",
      "[nstep: 660] loss: 0.006659489590674639\n",
      "[step: 661] loss: 0.0014848039718344808\n",
      "[nstep: 661] loss: 0.006668784189969301\n",
      "[step: 662] loss: 0.0019189509330317378\n",
      "[nstep: 662] loss: 0.0066774990409612656\n",
      "[step: 663] loss: 0.0030249268747866154\n",
      "[nstep: 663] loss: 0.006684147287160158\n",
      "[step: 664] loss: 0.003616678062826395\n",
      "[nstep: 664] loss: 0.00668638339266181\n",
      "[step: 665] loss: 0.0034985244274139404\n",
      "[nstep: 665] loss: 0.006683953106403351\n",
      "[step: 666] loss: 0.0014496257063001394\n",
      "[nstep: 666] loss: 0.006678374949842691\n",
      "[step: 667] loss: 0.0024075901601463556\n",
      "[nstep: 667] loss: 0.0066713690757751465\n",
      "[step: 668] loss: 0.002930165734142065\n",
      "[nstep: 668] loss: 0.006664289627224207\n",
      "[step: 669] loss: 0.0013307678746059537\n",
      "[nstep: 669] loss: 0.006657659076154232\n",
      "[step: 670] loss: 0.002691451460123062\n",
      "[nstep: 670] loss: 0.006652466021478176\n",
      "[step: 671] loss: 0.0021115692798048258\n",
      "[nstep: 671] loss: 0.006648977752774954\n",
      "[step: 672] loss: 0.0016313735395669937\n",
      "[nstep: 672] loss: 0.00664690462872386\n",
      "[step: 673] loss: 0.0024244484957307577\n",
      "[nstep: 673] loss: 0.0066459570080041885\n",
      "[step: 674] loss: 0.0013083785306662321\n",
      "[nstep: 674] loss: 0.006645644083619118\n",
      "[step: 675] loss: 0.002102011814713478\n",
      "[nstep: 675] loss: 0.006645615678280592\n",
      "[step: 676] loss: 0.0013641852419823408\n",
      "[nstep: 676] loss: 0.006645459681749344\n",
      "[step: 677] loss: 0.0018273269524797797\n",
      "[nstep: 677] loss: 0.006645105313509703\n",
      "[step: 678] loss: 0.001512531191110611\n",
      "[nstep: 678] loss: 0.0066444058902561665\n",
      "[step: 679] loss: 0.001583816367201507\n",
      "[nstep: 679] loss: 0.006643642205744982\n",
      "[step: 680] loss: 0.001537082833237946\n",
      "[nstep: 680] loss: 0.0066428217105567455\n",
      "[step: 681] loss: 0.0014455532655119896\n",
      "[nstep: 681] loss: 0.0066427066922187805\n",
      "[step: 682] loss: 0.00152266351506114\n",
      "[nstep: 682] loss: 0.006643712520599365\n",
      "[step: 683] loss: 0.0013877105666324496\n",
      "[nstep: 683] loss: 0.006648488342761993\n",
      "[step: 684] loss: 0.0014828023267909884\n",
      "[nstep: 684] loss: 0.006658617407083511\n",
      "[step: 685] loss: 0.0013505462557077408\n",
      "[nstep: 685] loss: 0.006687559653073549\n",
      "[step: 686] loss: 0.0014374279417097569\n",
      "[nstep: 686] loss: 0.006736312992870808\n",
      "[step: 687] loss: 0.0013265180168673396\n",
      "[nstep: 687] loss: 0.006862519308924675\n",
      "[step: 688] loss: 0.0013949028216302395\n",
      "[nstep: 688] loss: 0.006950201001018286\n",
      "[step: 689] loss: 0.0013155549531802535\n",
      "[nstep: 689] loss: 0.007042767480015755\n",
      "[step: 690] loss: 0.001357284840196371\n",
      "[nstep: 690] loss: 0.00705723837018013\n",
      "[step: 691] loss: 0.0013032054994255304\n",
      "[nstep: 691] loss: 0.007005857769399881\n",
      "[step: 692] loss: 0.0013230525655671954\n",
      "[nstep: 692] loss: 0.0069305747747421265\n",
      "[step: 693] loss: 0.0012988690286874771\n",
      "[nstep: 693] loss: 0.00675629498437047\n",
      "[step: 694] loss: 0.0012959751766175032\n",
      "[nstep: 694] loss: 0.00667106918990612\n",
      "[step: 695] loss: 0.0012889779172837734\n",
      "[nstep: 695] loss: 0.006798247341066599\n",
      "[step: 696] loss: 0.0012725564884021878\n",
      "[nstep: 696] loss: 0.006858555134385824\n",
      "[step: 697] loss: 0.0012825358426198363\n",
      "[nstep: 697] loss: 0.006723680533468723\n",
      "[step: 698] loss: 0.0012572648702189326\n",
      "[nstep: 698] loss: 0.006662415806204081\n",
      "[step: 699] loss: 0.0012714631156995893\n",
      "[nstep: 699] loss: 0.006748713552951813\n",
      "[step: 700] loss: 0.0012443943414837122\n",
      "[nstep: 700] loss: 0.006737821269780397\n",
      "[step: 701] loss: 0.0012620538473129272\n",
      "[nstep: 701] loss: 0.0067064352333545685\n",
      "[step: 702] loss: 0.0012363315327093005\n",
      "[nstep: 702] loss: 0.006699307821691036\n",
      "[step: 703] loss: 0.00125110091175884\n",
      "[nstep: 703] loss: 0.006667450536042452\n",
      "[step: 704] loss: 0.0012298995861783624\n",
      "[nstep: 704] loss: 0.006666800007224083\n",
      "[step: 705] loss: 0.0012412079377099872\n",
      "[nstep: 705] loss: 0.006721388548612595\n",
      "[step: 706] loss: 0.0012251095613464713\n",
      "[nstep: 706] loss: 0.006707719527184963\n",
      "[step: 707] loss: 0.0012302347458899021\n",
      "[nstep: 707] loss: 0.006660818587988615\n",
      "[step: 708] loss: 0.0012217931216582656\n",
      "[nstep: 708] loss: 0.0066799744963645935\n",
      "[step: 709] loss: 0.0012206364190205932\n",
      "[nstep: 709] loss: 0.006684110965579748\n",
      "[step: 710] loss: 0.0012191906571388245\n",
      "[nstep: 710] loss: 0.006675699260085821\n",
      "[step: 711] loss: 0.001211510389111936\n",
      "[nstep: 711] loss: 0.006708900444209576\n",
      "[step: 712] loss: 0.0012158907484263182\n",
      "[nstep: 712] loss: 0.006661387160420418\n",
      "[step: 713] loss: 0.0012044730829074979\n",
      "[nstep: 713] loss: 0.006635254714637995\n",
      "[step: 714] loss: 0.001212071510963142\n",
      "[nstep: 714] loss: 0.006656784098595381\n",
      "[step: 715] loss: 0.0011987759498879313\n",
      "[nstep: 715] loss: 0.0066603487357497215\n",
      "[step: 716] loss: 0.0012068208307027817\n",
      "[nstep: 716] loss: 0.00664930185303092\n",
      "[step: 717] loss: 0.001195102813653648\n",
      "[nstep: 717] loss: 0.006661453749984503\n",
      "[step: 718] loss: 0.0012012680526822805\n",
      "[nstep: 718] loss: 0.006645650137215853\n",
      "[step: 719] loss: 0.0011925974395126104\n",
      "[nstep: 719] loss: 0.0066282982006669044\n",
      "[step: 720] loss: 0.0011950029293075204\n",
      "[nstep: 720] loss: 0.006639775354415178\n",
      "[step: 721] loss: 0.0011904941638931632\n",
      "[nstep: 721] loss: 0.006644667591899633\n",
      "[step: 722] loss: 0.0011895116185769439\n",
      "[nstep: 722] loss: 0.006637897342443466\n",
      "[step: 723] loss: 0.0011878613149747252\n",
      "[nstep: 723] loss: 0.006650103256106377\n",
      "[step: 724] loss: 0.0011850178707391024\n",
      "[nstep: 724] loss: 0.0066360607743263245\n",
      "[step: 725] loss: 0.0011843591928482056\n",
      "[nstep: 725] loss: 0.006620958913117647\n",
      "[step: 726] loss: 0.0011818441562354565\n",
      "[nstep: 726] loss: 0.00662493659183383\n",
      "[step: 727] loss: 0.0011801959481090307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 727] loss: 0.006627862341701984\n",
      "[step: 728] loss: 0.001179158454760909\n",
      "[nstep: 728] loss: 0.006622412707656622\n",
      "[step: 729] loss: 0.0011761849746108055\n",
      "[nstep: 729] loss: 0.006632938515394926\n",
      "[step: 730] loss: 0.0011761768255382776\n",
      "[nstep: 730] loss: 0.006639110390096903\n",
      "[step: 731] loss: 0.0011728104436770082\n",
      "[nstep: 731] loss: 0.006625941954553127\n",
      "[step: 732] loss: 0.0011726308148354292\n",
      "[nstep: 732] loss: 0.006621720269322395\n",
      "[step: 733] loss: 0.0011701621115207672\n",
      "[nstep: 733] loss: 0.006621664855629206\n",
      "[step: 734] loss: 0.0011688306694850326\n",
      "[nstep: 734] loss: 0.0066126431338489056\n",
      "[step: 735] loss: 0.0011675192508846521\n",
      "[nstep: 735] loss: 0.006618532817810774\n",
      "[step: 736] loss: 0.0011654157424345613\n",
      "[nstep: 736] loss: 0.0066248346120119095\n",
      "[step: 737] loss: 0.0011644981568679214\n",
      "[nstep: 737] loss: 0.006621195934712887\n",
      "[step: 738] loss: 0.0011625596089288592\n",
      "[nstep: 738] loss: 0.006621106993407011\n",
      "[step: 739] loss: 0.0011611244408413768\n",
      "[nstep: 739] loss: 0.006624273955821991\n",
      "[step: 740] loss: 0.0011598740238696337\n",
      "[nstep: 740] loss: 0.006613985635340214\n",
      "[step: 741] loss: 0.001157962018623948\n",
      "[nstep: 741] loss: 0.006609944626688957\n",
      "[step: 742] loss: 0.001156902639195323\n",
      "[nstep: 742] loss: 0.00660975556820631\n",
      "[step: 743] loss: 0.0011551689822226763\n",
      "[nstep: 743] loss: 0.006606291513890028\n",
      "[step: 744] loss: 0.0011537001701071858\n",
      "[nstep: 744] loss: 0.0066038817167282104\n",
      "[step: 745] loss: 0.0011524937581270933\n",
      "[nstep: 745] loss: 0.006605267059057951\n",
      "[step: 746] loss: 0.0011506698792800307\n",
      "[nstep: 746] loss: 0.006605423521250486\n",
      "[step: 747] loss: 0.0011495578801259398\n",
      "[nstep: 747] loss: 0.006603654939681292\n",
      "[step: 748] loss: 0.0011479150271043181\n",
      "[nstep: 748] loss: 0.006605224683880806\n",
      "[step: 749] loss: 0.0011465282877907157\n",
      "[nstep: 749] loss: 0.006606835871934891\n",
      "[step: 750] loss: 0.001145132351666689\n",
      "[nstep: 750] loss: 0.006608821917325258\n",
      "[step: 751] loss: 0.0011436680797487497\n",
      "[nstep: 751] loss: 0.006610484793782234\n",
      "[step: 752] loss: 0.0011421876261010766\n",
      "[nstep: 752] loss: 0.006616664584726095\n",
      "[step: 753] loss: 0.0011409278959035873\n",
      "[nstep: 753] loss: 0.006624246947467327\n",
      "[step: 754] loss: 0.0011392893502488732\n",
      "[nstep: 754] loss: 0.006638218183070421\n",
      "[step: 755] loss: 0.0011380588402971625\n",
      "[nstep: 755] loss: 0.0066536543890833855\n",
      "[step: 756] loss: 0.0011365528916940093\n",
      "[nstep: 756] loss: 0.006685879547148943\n",
      "[step: 757] loss: 0.0011351339053362608\n",
      "[nstep: 757] loss: 0.006710471119731665\n",
      "[step: 758] loss: 0.0011337760370224714\n",
      "[nstep: 758] loss: 0.006751114036887884\n",
      "[step: 759] loss: 0.0011323293438181281\n",
      "[nstep: 759] loss: 0.006760780233889818\n",
      "[step: 760] loss: 0.0011309095425531268\n",
      "[nstep: 760] loss: 0.006744083482772112\n",
      "[step: 761] loss: 0.0011295528383925557\n",
      "[nstep: 761] loss: 0.006704014725983143\n",
      "[step: 762] loss: 0.0011281064944341779\n",
      "[nstep: 762] loss: 0.006659628823399544\n",
      "[step: 763] loss: 0.0011266972869634628\n",
      "[nstep: 763] loss: 0.006610658951103687\n",
      "[step: 764] loss: 0.0011253462871536613\n",
      "[nstep: 764] loss: 0.006597026251256466\n",
      "[step: 765] loss: 0.0011238808510825038\n",
      "[nstep: 765] loss: 0.006619964260607958\n",
      "[step: 766] loss: 0.0011225104099139571\n",
      "[nstep: 766] loss: 0.006651912350207567\n",
      "[step: 767] loss: 0.0011211176170036197\n",
      "[nstep: 767] loss: 0.006660635583102703\n",
      "[step: 768] loss: 0.0011196917621418834\n",
      "[nstep: 768] loss: 0.006639888975769281\n",
      "[step: 769] loss: 0.0011183025781065226\n",
      "[nstep: 769] loss: 0.006611487828195095\n",
      "[step: 770] loss: 0.0011169109493494034\n",
      "[nstep: 770] loss: 0.006592176854610443\n",
      "[step: 771] loss: 0.0011154947569593787\n",
      "[nstep: 771] loss: 0.006596767343580723\n",
      "[step: 772] loss: 0.001114094746299088\n",
      "[nstep: 772] loss: 0.006614205427467823\n",
      "[step: 773] loss: 0.0011127111501991749\n",
      "[nstep: 773] loss: 0.006626768968999386\n",
      "[step: 774] loss: 0.001111289020627737\n",
      "[nstep: 774] loss: 0.006625764537602663\n",
      "[step: 775] loss: 0.0011098913382738829\n",
      "[nstep: 775] loss: 0.006614086218178272\n",
      "[step: 776] loss: 0.001108503551222384\n",
      "[nstep: 776] loss: 0.006597812287509441\n",
      "[step: 777] loss: 0.0011070853797718883\n",
      "[nstep: 777] loss: 0.006587496493011713\n",
      "[step: 778] loss: 0.0011056872317567468\n",
      "[nstep: 778] loss: 0.006586593575775623\n",
      "[step: 779] loss: 0.001104285242035985\n",
      "[nstep: 779] loss: 0.0065924376249313354\n",
      "[step: 780] loss: 0.0011028817389160395\n",
      "[nstep: 780] loss: 0.00659897131845355\n",
      "[step: 781] loss: 0.0011014650808647275\n",
      "[nstep: 781] loss: 0.006603849120438099\n",
      "[step: 782] loss: 0.0011000687954947352\n",
      "[nstep: 782] loss: 0.006605115253478289\n",
      "[step: 783] loss: 0.0010986622655764222\n",
      "[nstep: 783] loss: 0.0066011641174554825\n",
      "[step: 784] loss: 0.0010972442105412483\n",
      "[nstep: 784] loss: 0.006595401093363762\n",
      "[step: 785] loss: 0.0010958407074213028\n",
      "[nstep: 785] loss: 0.006589587777853012\n",
      "[step: 786] loss: 0.001094429288059473\n",
      "[nstep: 786] loss: 0.006584861781448126\n",
      "[step: 787] loss: 0.001093016006052494\n",
      "[nstep: 787] loss: 0.006580953486263752\n",
      "[step: 788] loss: 0.0010915985330939293\n",
      "[nstep: 788] loss: 0.006578915752470493\n",
      "[step: 789] loss: 0.0010901879286393523\n",
      "[nstep: 789] loss: 0.0065783848986029625\n",
      "[step: 790] loss: 0.001088771503418684\n",
      "[nstep: 790] loss: 0.006578924134373665\n",
      "[step: 791] loss: 0.0010873506544157863\n",
      "[nstep: 791] loss: 0.00658017722889781\n",
      "[step: 792] loss: 0.0010859309695661068\n",
      "[nstep: 792] loss: 0.006581810303032398\n",
      "[step: 793] loss: 0.0010845105862244964\n",
      "[nstep: 793] loss: 0.006584423128515482\n",
      "[step: 794] loss: 0.0010830867104232311\n",
      "[nstep: 794] loss: 0.006587592884898186\n",
      "[step: 795] loss: 0.0010816608555614948\n",
      "[nstep: 795] loss: 0.006591985002160072\n",
      "[step: 796] loss: 0.001080233952961862\n",
      "[nstep: 796] loss: 0.0065996055491268635\n",
      "[step: 797] loss: 0.0010788061190396547\n",
      "[nstep: 797] loss: 0.006608034484088421\n",
      "[step: 798] loss: 0.0010773753747344017\n",
      "[nstep: 798] loss: 0.006622001063078642\n",
      "[step: 799] loss: 0.0010759407887235284\n",
      "[nstep: 799] loss: 0.006636449601501226\n",
      "[step: 800] loss: 0.0010745074832811952\n",
      "[nstep: 800] loss: 0.006662585306912661\n",
      "[step: 801] loss: 0.0010730695212259889\n",
      "[nstep: 801] loss: 0.0066753351129591465\n",
      "[step: 802] loss: 0.0010716321412473917\n",
      "[nstep: 802] loss: 0.006685551721602678\n",
      "[step: 803] loss: 0.0010701896389946342\n",
      "[nstep: 803] loss: 0.006684781052172184\n",
      "[step: 804] loss: 0.0010687450412660837\n",
      "[nstep: 804] loss: 0.00667499890550971\n",
      "[step: 805] loss: 0.0010673004435375333\n",
      "[nstep: 805] loss: 0.006637392099946737\n",
      "[step: 806] loss: 0.0010658525861799717\n",
      "[nstep: 806] loss: 0.006598778534680605\n",
      "[step: 807] loss: 0.0010644008871167898\n",
      "[nstep: 807] loss: 0.006574179977178574\n",
      "[step: 808] loss: 0.0010629475582391024\n",
      "[nstep: 808] loss: 0.006571647245436907\n",
      "[step: 809] loss: 0.0010614913189783692\n",
      "[nstep: 809] loss: 0.006581592373549938\n",
      "[step: 810] loss: 0.0010600335663184524\n",
      "[nstep: 810] loss: 0.0065991515293717384\n",
      "[step: 811] loss: 0.0010585723211988807\n",
      "[nstep: 811] loss: 0.0066098435781896114\n",
      "[step: 812] loss: 0.0010571082821115851\n",
      "[nstep: 812] loss: 0.006610663142055273\n",
      "[step: 813] loss: 0.0010556415654718876\n",
      "[nstep: 813] loss: 0.006595545448362827\n",
      "[step: 814] loss: 0.0010541722876951098\n",
      "[nstep: 814] loss: 0.006581291556358337\n",
      "[step: 815] loss: 0.0010526995174586773\n",
      "[nstep: 815] loss: 0.006570053286850452\n",
      "[step: 816] loss: 0.0010512247681617737\n",
      "[nstep: 816] loss: 0.006564280483871698\n",
      "[step: 817] loss: 0.0010497457114979625\n",
      "[nstep: 817] loss: 0.006566393654793501\n",
      "[step: 818] loss: 0.0010482657235115767\n",
      "[nstep: 818] loss: 0.00657074386253953\n",
      "[step: 819] loss: 0.001046780846081674\n",
      "[nstep: 819] loss: 0.006579365115612745\n",
      "[step: 820] loss: 0.0010452942224219441\n",
      "[nstep: 820] loss: 0.00658333208411932\n",
      "[step: 821] loss: 0.001043803058564663\n",
      "[nstep: 821] loss: 0.006587385665625334\n",
      "[step: 822] loss: 0.001042309682816267\n",
      "[nstep: 822] loss: 0.00658935634419322\n",
      "[step: 823] loss: 0.0010408125817775726\n",
      "[nstep: 823] loss: 0.006587408948689699\n",
      "[step: 824] loss: 0.0010393128031864762\n",
      "[nstep: 824] loss: 0.006582131143659353\n",
      "[step: 825] loss: 0.0010378099977970123\n",
      "[nstep: 825] loss: 0.006576785817742348\n",
      "[step: 826] loss: 0.0010363021865487099\n",
      "[nstep: 826] loss: 0.006574236787855625\n",
      "[step: 827] loss: 0.0010347922798246145\n",
      "[nstep: 827] loss: 0.006567309144884348\n",
      "[step: 828] loss: 0.0010332789970561862\n",
      "[nstep: 828] loss: 0.006564268842339516\n",
      "[step: 829] loss: 0.001031760941259563\n",
      "[nstep: 829] loss: 0.006559089291840792\n",
      "[step: 830] loss: 0.001030240673571825\n",
      "[nstep: 830] loss: 0.006558298598974943\n",
      "[step: 831] loss: 0.001028717844747007\n",
      "[nstep: 831] loss: 0.006554987747222185\n",
      "[step: 832] loss: 0.0010271900100633502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 832] loss: 0.006554704159498215\n",
      "[step: 833] loss: 0.0010256585665047169\n",
      "[nstep: 833] loss: 0.0065535674802958965\n",
      "[step: 834] loss: 0.0010241237469017506\n",
      "[nstep: 834] loss: 0.006553130690008402\n",
      "[step: 835] loss: 0.0010225861333310604\n",
      "[nstep: 835] loss: 0.006553581450134516\n",
      "[step: 836] loss: 0.0010210431646555662\n",
      "[nstep: 836] loss: 0.006553176324814558\n",
      "[step: 837] loss: 0.0010194986825808883\n",
      "[nstep: 837] loss: 0.006555027328431606\n",
      "[step: 838] loss: 0.0010179495438933372\n",
      "[nstep: 838] loss: 0.006555837579071522\n",
      "[step: 839] loss: 0.0010163964470848441\n",
      "[nstep: 839] loss: 0.0065598031505942345\n",
      "[step: 840] loss: 0.0010148398578166962\n",
      "[nstep: 840] loss: 0.0065660555846989155\n",
      "[step: 841] loss: 0.001013280125334859\n",
      "[nstep: 841] loss: 0.006579176522791386\n",
      "[step: 842] loss: 0.0010117179481312633\n",
      "[nstep: 842] loss: 0.006597478408366442\n",
      "[step: 843] loss: 0.001010152860544622\n",
      "[nstep: 843] loss: 0.006636958569288254\n",
      "[step: 844] loss: 0.001008585561066866\n",
      "[nstep: 844] loss: 0.006689417641609907\n",
      "[step: 845] loss: 0.0010070219868794084\n",
      "[nstep: 845] loss: 0.006768725346773863\n",
      "[step: 846] loss: 0.0010054752929136157\n",
      "[nstep: 846] loss: 0.00684158131480217\n",
      "[step: 847] loss: 0.0010039826156571507\n",
      "[nstep: 847] loss: 0.006880978588014841\n",
      "[step: 848] loss: 0.0010026622330769897\n",
      "[nstep: 848] loss: 0.006858981214463711\n",
      "[step: 849] loss: 0.0010018865577876568\n",
      "[nstep: 849] loss: 0.006722979247570038\n",
      "[step: 850] loss: 0.001002850360237062\n",
      "[nstep: 850] loss: 0.006597462575882673\n",
      "[step: 851] loss: 0.0010095980251207948\n",
      "[nstep: 851] loss: 0.006552472710609436\n",
      "[step: 852] loss: 0.001035061664879322\n",
      "[nstep: 852] loss: 0.006608233787119389\n",
      "[step: 853] loss: 0.0011216581333428621\n",
      "[nstep: 853] loss: 0.006675251759588718\n",
      "[step: 854] loss: 0.0013711875071749091\n",
      "[nstep: 854] loss: 0.006654678378254175\n",
      "[step: 855] loss: 0.0018763424595817924\n",
      "[nstep: 855] loss: 0.006590649019926786\n",
      "[step: 856] loss: 0.002745161997154355\n",
      "[nstep: 856] loss: 0.006549951154738665\n",
      "[step: 857] loss: 0.002200534101575613\n",
      "[nstep: 857] loss: 0.0065720053389668465\n",
      "[step: 858] loss: 0.0013778534485027194\n",
      "[nstep: 858] loss: 0.00661200238391757\n",
      "[step: 859] loss: 0.0012082618195563555\n",
      "[nstep: 859] loss: 0.006616851780563593\n",
      "[step: 860] loss: 0.0016428657108917832\n",
      "[nstep: 860] loss: 0.006582311354577541\n",
      "[step: 861] loss: 0.001641714945435524\n",
      "[nstep: 861] loss: 0.00654812203720212\n",
      "[step: 862] loss: 0.0011240142630413175\n",
      "[nstep: 862] loss: 0.006545273121446371\n",
      "[step: 863] loss: 0.0013384799240157008\n",
      "[nstep: 863] loss: 0.006567924749106169\n",
      "[step: 864] loss: 0.001562324701808393\n",
      "[nstep: 864] loss: 0.0065855528227984905\n",
      "[step: 865] loss: 0.0010395399294793606\n",
      "[nstep: 865] loss: 0.006581541150808334\n",
      "[step: 866] loss: 0.001397180836647749\n",
      "[nstep: 866] loss: 0.0065626115538179874\n",
      "[step: 867] loss: 0.0013654453214257956\n",
      "[nstep: 867] loss: 0.006541508715599775\n",
      "[step: 868] loss: 0.0010763532482087612\n",
      "[nstep: 868] loss: 0.0065374672412872314\n",
      "[step: 869] loss: 0.0013549156719818711\n",
      "[nstep: 869] loss: 0.006547983270138502\n",
      "[step: 870] loss: 0.0012045346666127443\n",
      "[nstep: 870] loss: 0.006561563350260258\n",
      "[step: 871] loss: 0.0010901284404098988\n",
      "[nstep: 871] loss: 0.006565639283508062\n",
      "[step: 872] loss: 0.001278234994970262\n",
      "[nstep: 872] loss: 0.006556116044521332\n",
      "[step: 873] loss: 0.0010744330938905478\n",
      "[nstep: 873] loss: 0.006543263327330351\n",
      "[step: 874] loss: 0.001133185112848878\n",
      "[nstep: 874] loss: 0.006533417850732803\n",
      "[step: 875] loss: 0.0011609895154833794\n",
      "[nstep: 875] loss: 0.006532789207994938\n",
      "[step: 876] loss: 0.0010211052140221\n",
      "[nstep: 876] loss: 0.006538608577102423\n",
      "[step: 877] loss: 0.0011451409664005041\n",
      "[nstep: 877] loss: 0.006545509677380323\n",
      "[step: 878] loss: 0.0010413321433588862\n",
      "[nstep: 878] loss: 0.006548388861119747\n",
      "[step: 879] loss: 0.0010515237227082253\n",
      "[nstep: 879] loss: 0.006544737610965967\n",
      "[step: 880] loss: 0.0010863974457606673\n",
      "[nstep: 880] loss: 0.006538955494761467\n",
      "[step: 881] loss: 0.0010068990522995591\n",
      "[nstep: 881] loss: 0.006531823892146349\n",
      "[step: 882] loss: 0.0010610301978886127\n",
      "[nstep: 882] loss: 0.006527662742882967\n",
      "[step: 883] loss: 0.00103483977727592\n",
      "[nstep: 883] loss: 0.006526583340018988\n",
      "[step: 884] loss: 0.0010015119332820177\n",
      "[nstep: 884] loss: 0.006528420373797417\n",
      "[step: 885] loss: 0.0010536436457186937\n",
      "[nstep: 885] loss: 0.00653104530647397\n",
      "[step: 886] loss: 0.0009859042475000024\n",
      "[nstep: 886] loss: 0.006533012259751558\n",
      "[step: 887] loss: 0.0010194738861173391\n",
      "[nstep: 887] loss: 0.0065343803726136684\n",
      "[step: 888] loss: 0.0010056175524368882\n",
      "[nstep: 888] loss: 0.006532862316817045\n",
      "[step: 889] loss: 0.0009838261175900698\n",
      "[nstep: 889] loss: 0.006530876271426678\n",
      "[step: 890] loss: 0.0010062971850857139\n",
      "[nstep: 890] loss: 0.006527847610414028\n",
      "[step: 891] loss: 0.0009822694119066\n",
      "[nstep: 891] loss: 0.006525125354528427\n",
      "[step: 892] loss: 0.0009809561306610703\n",
      "[nstep: 892] loss: 0.006522582843899727\n",
      "[step: 893] loss: 0.0009921404998749495\n",
      "[nstep: 893] loss: 0.006520763039588928\n",
      "[step: 894] loss: 0.0009657322079874575\n",
      "[nstep: 894] loss: 0.006519646383821964\n",
      "[step: 895] loss: 0.000980487558990717\n",
      "[nstep: 895] loss: 0.006518964655697346\n",
      "[step: 896] loss: 0.0009727442520670593\n",
      "[nstep: 896] loss: 0.006518767215311527\n",
      "[step: 897] loss: 0.0009603498037904501\n",
      "[nstep: 897] loss: 0.0065187918953597546\n",
      "[step: 898] loss: 0.0009734887280501425\n",
      "[nstep: 898] loss: 0.0065190671011805534\n",
      "[step: 899] loss: 0.000957723765168339\n",
      "[nstep: 899] loss: 0.006519511807709932\n",
      "[step: 900] loss: 0.00095852225786075\n",
      "[nstep: 900] loss: 0.006520468275994062\n",
      "[step: 901] loss: 0.0009613707661628723\n",
      "[nstep: 901] loss: 0.006521593313664198\n",
      "[step: 902] loss: 0.0009502133470959961\n",
      "[nstep: 902] loss: 0.006523850839585066\n",
      "[step: 903] loss: 0.0009528036462143064\n",
      "[nstep: 903] loss: 0.006526778917759657\n",
      "[step: 904] loss: 0.0009522940963506699\n",
      "[nstep: 904] loss: 0.0065329489298164845\n",
      "[step: 905] loss: 0.0009429067722521722\n",
      "[nstep: 905] loss: 0.006539323832839727\n",
      "[step: 906] loss: 0.0009478784631937742\n",
      "[nstep: 906] loss: 0.006552916951477528\n",
      "[step: 907] loss: 0.0009425856987945735\n",
      "[nstep: 907] loss: 0.006566762924194336\n",
      "[step: 908] loss: 0.0009384569129906595\n",
      "[nstep: 908] loss: 0.00659215310588479\n",
      "[step: 909] loss: 0.0009409122867509723\n",
      "[nstep: 909] loss: 0.00660945288836956\n",
      "[step: 910] loss: 0.000935788790229708\n",
      "[nstep: 910] loss: 0.0066330935806035995\n",
      "[step: 911] loss: 0.0009336016373708844\n",
      "[nstep: 911] loss: 0.0066464743576943874\n",
      "[step: 912] loss: 0.0009344159625470638\n",
      "[nstep: 912] loss: 0.006646072957664728\n",
      "[step: 913] loss: 0.000930159178096801\n",
      "[nstep: 913] loss: 0.006635380443185568\n",
      "[step: 914] loss: 0.0009282329119741917\n",
      "[nstep: 914] loss: 0.006599085405468941\n",
      "[step: 915] loss: 0.000928812543861568\n",
      "[nstep: 915] loss: 0.006560187321156263\n",
      "[step: 916] loss: 0.0009245173423551023\n",
      "[nstep: 916] loss: 0.00652418052777648\n",
      "[step: 917] loss: 0.0009235143661499023\n",
      "[nstep: 917] loss: 0.006509325932711363\n",
      "[step: 918] loss: 0.0009231118601746857\n",
      "[nstep: 918] loss: 0.006517358589917421\n",
      "[step: 919] loss: 0.0009196634637191892\n",
      "[nstep: 919] loss: 0.0065371086820960045\n",
      "[step: 920] loss: 0.0009186259121634066\n",
      "[nstep: 920] loss: 0.006554801482707262\n",
      "[step: 921] loss: 0.0009177902829833329\n",
      "[nstep: 921] loss: 0.006556269712746143\n",
      "[step: 922] loss: 0.0009151791455224156\n",
      "[nstep: 922] loss: 0.006546677555888891\n",
      "[step: 923] loss: 0.0009136375738307834\n",
      "[nstep: 923] loss: 0.006528359372168779\n",
      "[step: 924] loss: 0.0009130645776167512\n",
      "[nstep: 924] loss: 0.00651326822116971\n",
      "[step: 925] loss: 0.0009106017532758415\n",
      "[nstep: 925] loss: 0.00650579622015357\n",
      "[step: 926] loss: 0.0009090652456507087\n",
      "[nstep: 926] loss: 0.006505686789751053\n",
      "[step: 927] loss: 0.0009083528420887887\n",
      "[nstep: 927] loss: 0.0065097385086119175\n",
      "[step: 928] loss: 0.0009062218014150858\n",
      "[nstep: 928] loss: 0.006513862404972315\n",
      "[step: 929] loss: 0.0009046952472999692\n",
      "[nstep: 929] loss: 0.006516988854855299\n",
      "[step: 930] loss: 0.0009036698029376566\n",
      "[nstep: 930] loss: 0.006519134156405926\n",
      "[step: 931] loss: 0.0009021367295645177\n",
      "[nstep: 931] loss: 0.006522172596305609\n",
      "[step: 932] loss: 0.0009003442828543484\n",
      "[nstep: 932] loss: 0.006528925150632858\n",
      "[step: 933] loss: 0.0008992990478873253\n",
      "[nstep: 933] loss: 0.006531355436891317\n",
      "[step: 934] loss: 0.0008979961858130991\n",
      "[nstep: 934] loss: 0.006539642345160246\n",
      "[step: 935] loss: 0.0008962505962699652\n",
      "[nstep: 935] loss: 0.006532697007060051\n",
      "[step: 936] loss: 0.0008950316696427763\n",
      "[nstep: 936] loss: 0.006530434358865023\n",
      "[step: 937] loss: 0.0008938887622207403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 937] loss: 0.006520143710076809\n",
      "[step: 938] loss: 0.0008923101704567671\n",
      "[nstep: 938] loss: 0.006514483597129583\n",
      "[step: 939] loss: 0.0008909413008950651\n",
      "[nstep: 939] loss: 0.006510184612125158\n",
      "[step: 940] loss: 0.0008897992083802819\n",
      "[nstep: 940] loss: 0.006507136393338442\n",
      "[step: 941] loss: 0.0008884578128345311\n",
      "[nstep: 941] loss: 0.0065044863149523735\n",
      "[step: 942] loss: 0.0008870167075656354\n",
      "[nstep: 942] loss: 0.006500899326056242\n",
      "[step: 943] loss: 0.0008857711800374091\n",
      "[nstep: 943] loss: 0.006497059483081102\n",
      "[step: 944] loss: 0.0008846154087223113\n",
      "[nstep: 944] loss: 0.006493529304862022\n",
      "[step: 945] loss: 0.0008832087041810155\n",
      "[nstep: 945] loss: 0.0064911190420389175\n",
      "[step: 946] loss: 0.0008818992646411061\n",
      "[nstep: 946] loss: 0.006490091793239117\n",
      "[step: 947] loss: 0.0008807335398159921\n",
      "[nstep: 947] loss: 0.006490196101367474\n",
      "[step: 948] loss: 0.0008794774767011404\n",
      "[nstep: 948] loss: 0.0064908722415566444\n",
      "[step: 949] loss: 0.0008781580254435539\n",
      "[nstep: 949] loss: 0.006491605658084154\n",
      "[step: 950] loss: 0.0008769116830080748\n",
      "[nstep: 950] loss: 0.0064921630546450615\n",
      "[step: 951] loss: 0.0008757310570217669\n",
      "[nstep: 951] loss: 0.006492596119642258\n",
      "[step: 952] loss: 0.0008744957158342004\n",
      "[nstep: 952] loss: 0.006493560504168272\n",
      "[step: 953] loss: 0.0008732143323868513\n",
      "[nstep: 953] loss: 0.00649570906534791\n",
      "[step: 954] loss: 0.0008719977922737598\n",
      "[nstep: 954] loss: 0.006500734947621822\n",
      "[step: 955] loss: 0.0008708329405635595\n",
      "[nstep: 955] loss: 0.0065113515593111515\n",
      "[step: 956] loss: 0.0008696014410816133\n",
      "[nstep: 956] loss: 0.0065292539075016975\n",
      "[step: 957] loss: 0.0008683609194122255\n",
      "[nstep: 957] loss: 0.0065666744485497475\n",
      "[step: 958] loss: 0.0008671692339703441\n",
      "[nstep: 958] loss: 0.006607541814446449\n",
      "[step: 959] loss: 0.0008659940795041621\n",
      "[nstep: 959] loss: 0.0066786776296794415\n",
      "[step: 960] loss: 0.0008648008806630969\n",
      "[nstep: 960] loss: 0.006743190810084343\n",
      "[step: 961] loss: 0.0008635863196104765\n",
      "[nstep: 961] loss: 0.006792121101170778\n",
      "[step: 962] loss: 0.000862397369928658\n",
      "[nstep: 962] loss: 0.006856544874608517\n",
      "[step: 963] loss: 0.0008612353121861815\n",
      "[nstep: 963] loss: 0.006802418734878302\n",
      "[step: 964] loss: 0.0008600630098953843\n",
      "[nstep: 964] loss: 0.006701468490064144\n",
      "[step: 965] loss: 0.0008588708587922156\n",
      "[nstep: 965] loss: 0.006548881530761719\n",
      "[step: 966] loss: 0.0008576932013966143\n",
      "[nstep: 966] loss: 0.0065013025887310505\n",
      "[step: 967] loss: 0.0008565369644202292\n",
      "[nstep: 967] loss: 0.006574780214577913\n",
      "[step: 968] loss: 0.0008553785155527294\n",
      "[nstep: 968] loss: 0.006638451479375362\n",
      "[step: 969] loss: 0.0008542143041267991\n",
      "[nstep: 969] loss: 0.006592081394046545\n",
      "[step: 970] loss: 0.0008530475315637887\n",
      "[nstep: 970] loss: 0.006502120289951563\n",
      "[step: 971] loss: 0.0008518903632648289\n",
      "[nstep: 971] loss: 0.006503654643893242\n",
      "[step: 972] loss: 0.0008507455349899828\n",
      "[nstep: 972] loss: 0.006554014980792999\n",
      "[step: 973] loss: 0.0008496009395457804\n",
      "[nstep: 973] loss: 0.0065568541176617146\n",
      "[step: 974] loss: 0.0008484504651278257\n",
      "[nstep: 974] loss: 0.006525799632072449\n",
      "[step: 975] loss: 0.0008473005145788193\n",
      "[nstep: 975] loss: 0.006523570977151394\n",
      "[step: 976] loss: 0.0008461608085781336\n",
      "[nstep: 976] loss: 0.006516766734421253\n",
      "[step: 977] loss: 0.0008450253517366946\n",
      "[nstep: 977] loss: 0.0065006837248802185\n",
      "[step: 978] loss: 0.0008438923396170139\n",
      "[nstep: 978] loss: 0.00648852251470089\n",
      "[step: 979] loss: 0.000842757523059845\n",
      "[nstep: 979] loss: 0.006506516132503748\n",
      "[step: 980] loss: 0.0008416231139563024\n",
      "[nstep: 980] loss: 0.006535875145345926\n",
      "[step: 981] loss: 0.0008404923137277365\n",
      "[nstep: 981] loss: 0.0065023223869502544\n",
      "[step: 982] loss: 0.0008393668103963137\n",
      "[nstep: 982] loss: 0.006480732932686806\n",
      "[step: 983] loss: 0.0008382450323551893\n",
      "[nstep: 983] loss: 0.006481166463345289\n",
      "[step: 984] loss: 0.0008371233125217259\n",
      "[nstep: 984] loss: 0.006488654296845198\n",
      "[step: 985] loss: 0.0008360016508959234\n",
      "[nstep: 985] loss: 0.006490586791187525\n",
      "[step: 986] loss: 0.0008348824921995401\n",
      "[nstep: 986] loss: 0.006487139966338873\n",
      "[step: 987] loss: 0.0008337642066180706\n",
      "[nstep: 987] loss: 0.006493425462394953\n",
      "[step: 988] loss: 0.0008326492388732731\n",
      "[nstep: 988] loss: 0.006495600566267967\n",
      "[step: 989] loss: 0.0008315383456647396\n",
      "[nstep: 989] loss: 0.0064839813858270645\n",
      "[step: 990] loss: 0.0008304279181174934\n",
      "[nstep: 990] loss: 0.006472148001194\n",
      "[step: 991] loss: 0.0008293184218928218\n",
      "[nstep: 991] loss: 0.0064717549830675125\n",
      "[step: 992] loss: 0.0008282103226520121\n",
      "[nstep: 992] loss: 0.006475319154560566\n",
      "[step: 993] loss: 0.0008271043770946562\n",
      "[nstep: 993] loss: 0.00647409912198782\n",
      "[step: 994] loss: 0.0008259991300292313\n",
      "[nstep: 994] loss: 0.006471364293247461\n",
      "[step: 995] loss: 0.0008248953381553292\n",
      "[nstep: 995] loss: 0.006473315879702568\n",
      "[step: 996] loss: 0.0008237938163802028\n",
      "[nstep: 996] loss: 0.006479395087808371\n",
      "[step: 997] loss: 0.0008226943900808692\n",
      "[nstep: 997] loss: 0.006476834882050753\n",
      "[step: 998] loss: 0.0008215971174649894\n",
      "[nstep: 998] loss: 0.0064727975986897945\n",
      "[step: 999] loss: 0.0008205004269257188\n",
      "[nstep: 999] loss: 0.006468632258474827\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    mpl.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = np.float32)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 27\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "epsilon = 1\n",
    "e = math.exp(epsilon)\n",
    "q = (1/(e+1))\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "\n",
    "\n",
    "train_set = xy[:train_size]\n",
    "test_set = xy[train_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "test_set_with_noise = xy_with_noise[train_size:]\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    multi_cell1 = tf.contrib.rnn.MultiRNNCell([cell1]*2)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(multi_cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_mean(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    multi_cell2 =  tf.contrib.rnn.MultiRNNCell([cell2]*2)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(multi_cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RMSE \n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])*2\n",
    "x2 = x1+0.33*2\n",
    "x3 = x2+0.33*2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[nstep: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:testX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:testX_with_noise})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation(real&predict) :0.9800392012114896\n",
      "correlation(real&noise) :0.9425082676587109\n"
     ]
    }
   ],
   "source": [
    "testY_mean = np.mean(testY)\n",
    "test_predict_mean = np.mean(test_predict)\n",
    "test_predict_with_noise_mean = np.mean(test_predict_with_noise)\n",
    "\n",
    "X_1 = testY-testY_mean\n",
    "Y_1 = test_predict-test_predict_mean\n",
    "Y_2 = test_predict_with_noise-test_predict_with_noise_mean\n",
    "#\n",
    "a_1 = np.sum(np.multiply(X_1,Y_1))\n",
    "a_2 = np.sum(np.multiply(X_1,Y_2))\n",
    "\n",
    "#\n",
    "b_1 = np.sqrt(np.sum(np.square(X_1)))\n",
    "b_2 = b_1\n",
    "c_1 = np.sqrt(np.sum(np.square(Y_1)))\n",
    "c_2 = np.sqrt(np.sum(np.square(Y_2)))\n",
    "#\n",
    "r_1 = (a_1/(b_1*c_1))\n",
    "r_2 = (a_2/(b_2*c_2))\n",
    "print(\"correlation(real&predict) :{}\".format(r_1))\n",
    "print(\"correlation(real&noise) :{}\".format(r_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03586297 0.05377722 0.08003187 ... 0.02401246 0.03321214 0.03538657] \n",
      " 0.03192697810770819 \n",
      " 38.44008164168067\n"
     ]
    }
   ],
   "source": [
    "#Euclidean Distance\n",
    "ED = abs(testY[:,0]-test_predict[:,0])\n",
    "print(ED,\"\\n\",np.mean(ED),\"\\n\",sum(ED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5451346335281584\n"
     ]
    }
   ],
   "source": [
    "rmp = testY[:,0] - test_predict[:,0]\n",
    "rmp_2 = np.square(rmp)\n",
    "ED_2 = np.sqrt(sum(rmp_2))\n",
    "print(ED_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
