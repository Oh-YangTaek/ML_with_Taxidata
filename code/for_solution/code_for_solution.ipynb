{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.21923425793647766\n",
      "[step: 500] loss: 0.0017684613121673465\n",
      "[step: 1000] loss: 0.0015603110659867525\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "#load data\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/taxi_data/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data preprocessing\n",
    "train_size = int(len(xy)*0.7)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "test_set = xy[train_size:]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "data_max = np.max(b,1)\n",
    "data_min = np.min(b,1)\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "train_set = xy[:train_size]\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 27\n",
    "output_dim = 25\n",
    "learning_rate = 0.1\n",
    "iterations = 1001\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#make train_set&test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,data_dim],name = \"X\")\n",
    "Y = tf.placeholder(tf.float32,[None,25], name = \"Y\")\n",
    "\n",
    "\n",
    "\n",
    "#make LSTM CELL\n",
    "with tf.variable_scope(\"rnn\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    #multi_cell1 = tf.contrib.rnn.MultiRNNCell([cell1]*5)\n",
    "    outputs,_states = tf.nn.dynamic_rnn(cell,X,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim,activation_fn = None)\n",
    "    \n",
    "    loss =tf.reduce_mean(tf.square(Y_pred-Y), name = \"loss\")\n",
    "    train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# make saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train,loss],feed_dict={X:trainX, Y:trainY})\n",
    "        if i%500 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i,step_loss))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/Users/yeseo/ML_with_Taxidata/code/for_solution/model/my_model.ckpt\")\n",
    "    \n",
    "    #7th value from initial data\n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X:[testX[0,:,:]]})\n",
    "    \n",
    "    #initial data shape = 6,27\n",
    "    initial_data = testX[0,:,:]\n",
    "    \n",
    "    \n",
    "    #day&time for 7th data\n",
    "    if initial_data[5,1] != 47:\n",
    "        day_time = np.array([[initial_data[5,0],(initial_data[5,1]+1)]])\n",
    "    else:\n",
    "        if initial_data[5,0] == 7:\n",
    "            day_time = np.array([[1,0]])\n",
    "        else:\n",
    "            day_time = np.array([[(initial_data[5,0]+1),0]])\n",
    "    \n",
    "    time_array = day_time\n",
    "    \n",
    "    #make 7th data\n",
    "    seventh_data = np.hstack((day_time,test_predict))\n",
    "    \n",
    "    #1~5 data + 7th data\n",
    "    new_testX = np.vstack((initial_data[1:,:],seventh_data))\n",
    "    \n",
    "    #select range to predict\n",
    "    for j in range(48):\n",
    "        #8th value from second data\n",
    "        temp = sess.run(Y_pred,feed_dict = {X:[new_testX]})\n",
    "        \n",
    "        test_predict = np.vstack((test_predict,temp))\n",
    "        \n",
    "        if new_testX[5,1] != 47:\n",
    "            day_time = np.array([[new_testX[5,0],(new_testX[5,1]+1)]])\n",
    "        else:\n",
    "            if new_testX[5,0] == 7:\n",
    "                day_time = np.array([[1,0]])\n",
    "            else:\n",
    "                day_time = np.array([[(new_testX[5,0]+1),0]])\n",
    "        time_array = np.vstack((time_array,day_time))\n",
    "        seventh_data = np.hstack((day_time,test_predict[[-1],:]))\n",
    "        new_testX = np.vstack((new_testX[1:,:],seventh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1204, 6, 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data to original\n",
    "def ReturnToOriginal(data,data_max,data_min):\n",
    "    data_max = data_max+(1e-7)\n",
    "    for i in range(0,len(data)):\n",
    "        data[i,:]= (data[i,:]*data_max[i])-(data[i,:]-1)*data_min[i]\n",
    "    return data\n",
    "\n",
    "test_predict_to_original = ReturnToOriginal(test_predict,data_max[train_size+seq_length:],data_min[train_size+seq_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = np.array(test_predict_to_original,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = np.hstack((time_array,test_predict_to_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = np.array(test_predict_to_original,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = np.array(test_predict_to_original,str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3', '44', '8096', ..., '9398', '10709', '10677'],\n",
       "       ['3', '45', '8195', ..., '9729', '11194', '11197'],\n",
       "       ['3', '46', '8337', ..., '9233', '10526', '10647'],\n",
       "       ...,\n",
       "       ['4', '42', '9055', ..., '10182', '11782', '11714'],\n",
       "       ['4', '43', '8796', ..., '9641', '11313', '11233'],\n",
       "       ['4', '44', '9837', ..., '11072', '13217', '13193']], dtype='<U11')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(test_predict_to_original)):    \n",
    "    with open(\"/Users/yeseo/desktop/prediction_12.txt\",'a') as fa:\n",
    "        for j in range(27):\n",
    "            fa.write(test_predict_to_original[i,j]+\",\")\n",
    "        fa.write('\\n')\n",
    "        fa.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data to json format\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "file_data = OrderedDict()\n",
    "\n",
    "day_time = 0\n",
    "\n",
    "file_data[\"sigungu\"] = {\"Seoul_Jongno-gu\":str(test_predict_to_original[:,0]),\n",
    "                            \"Seoul_Jung-gu\":str(test_predict_to_original[:,1]),\n",
    "                            \"Seoul_Yongsan-gu\":str(test_predict_to_original[:,2]),\n",
    "                            \"Seoul_Seongdong-gu\":str(test_predict_to_original[:,3]),\n",
    "                            \"Seoul_Gwangjin-gu\":str(test_predict_to_original[:,4]),\n",
    "                            \"Seoul_Dongdaemun-gu\":str(test_predict_to_original[:,5]),\n",
    "                            \"Seoul_Jungnang-gu\":str(test_predict_to_original[:,6]),\n",
    "                            \"Seoul_Seongbuk-gu\":str(test_predict_to_original[:,7]),\n",
    "                            \"Seoul_Gangbuk-gu\":str(test_predict_to_original[:,8]),\n",
    "                            \"Seoul_Dobong-gu\":str(test_predict_to_original[:,9]),\n",
    "                            \"Seoul_Nowon-gu\":str(test_predict_to_original[:,10]),\n",
    "                            \"Seoul_Eunpyeong-gu\":str(test_predict_to_original[:,11]),\n",
    "                            \"Seoul_Seodaemun-gu\":str(test_predict_to_original[:,12]),\n",
    "                            \"Seoul_Mapo-gu\":str(test_predict_to_original[:,13]),\n",
    "                            \"Seoul_Yangcheon-gu\":str(test_predict_to_original[:,14]),\n",
    "                            \"Seoul_Gangseo-gu\":str(test_predict_to_original[:,15]),\n",
    "                            \"Seoul_Guro-gu\":str(test_predict_to_original[:,16]),\n",
    "                            \"Seoul_Geumcheon-gu\":str(test_predict_to_original[:,17]),\n",
    "                            \"Seoul_Yeongdeungpo-gu\":str(test_predict_to_original[:,18]),\n",
    "                            \"Seoul_Dongjak-gu\":str(test_predict_to_original[:,19]),\n",
    "                            \"Seoul_Gwanak-gu\":str(test_predict_to_original[:,20]),\n",
    "                            \"Seoul_Seocho-gu\":str(test_predict_to_original[:,21]),\n",
    "                            \"Seoul_Gangnam-gu\":str(test_predict_to_original[:,22]),\n",
    "                            \"Seoul_Songpa-gu\":str(test_predict_to_original[:,23]),\n",
    "                            \"Seoul_Gangdong-gu\":str(test_predict_to_original[:,24])}\n",
    "\n",
    "#Output to a file\n",
    "with open(\"/Users/yeseo/Desktop/predict_\"+str(2) +\".json\",'w',encoding = \"utf-8\") as make_file:\n",
    "    json.dump(file_data,make_file,ensure_ascii =False,indent = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_predict_to_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
