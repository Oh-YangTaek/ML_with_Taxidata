{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3d5e169ec5ad>:88: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 0.33690133690834045\n",
      "[nstep: 0] loss: 0.27755868434906006\n",
      "[step: 1] loss: 0.2272791713476181\n",
      "[nstep: 1] loss: 0.17710557579994202\n",
      "[step: 2] loss: 0.16273143887519836\n",
      "[nstep: 2] loss: 0.11819535493850708\n",
      "[step: 3] loss: 0.11887208372354507\n",
      "[nstep: 3] loss: 0.0813722237944603\n",
      "[step: 4] loss: 0.08529619127511978\n",
      "[nstep: 4] loss: 0.058974817395210266\n",
      "[step: 5] loss: 0.06094425544142723\n",
      "[nstep: 5] loss: 0.046768128871917725\n",
      "[step: 6] loss: 0.04634390398859978\n",
      "[nstep: 6] loss: 0.04079196974635124\n",
      "[step: 7] loss: 0.03922262787818909\n",
      "[nstep: 7] loss: 0.037346355617046356\n",
      "[step: 8] loss: 0.03647897019982338\n",
      "[nstep: 8] loss: 0.03554962947964668\n",
      "[step: 9] loss: 0.0348692461848259\n",
      "[nstep: 9] loss: 0.03402063250541687\n",
      "[step: 10] loss: 0.03386818990111351\n",
      "[nstep: 10] loss: 0.03225020319223404\n",
      "[step: 11] loss: 0.03327495977282524\n",
      "[nstep: 11] loss: 0.030386967584490776\n",
      "[step: 12] loss: 0.03232228755950928\n",
      "[nstep: 12] loss: 0.028536435216665268\n",
      "[step: 13] loss: 0.0309264175593853\n",
      "[nstep: 13] loss: 0.026798328384757042\n",
      "[step: 14] loss: 0.02934412844479084\n",
      "[nstep: 14] loss: 0.02503441460430622\n",
      "[step: 15] loss: 0.027721278369426727\n",
      "[nstep: 15] loss: 0.023657092824578285\n",
      "[step: 16] loss: 0.0260779932141304\n",
      "[nstep: 16] loss: 0.022575432434678078\n",
      "[step: 17] loss: 0.024501651525497437\n",
      "[nstep: 17] loss: 0.02180754765868187\n",
      "[step: 18] loss: 0.023043759167194366\n",
      "[nstep: 18] loss: 0.021397538483142853\n",
      "[step: 19] loss: 0.021639423444867134\n",
      "[nstep: 19] loss: 0.021246306598186493\n",
      "[step: 20] loss: 0.02031640149652958\n",
      "[nstep: 20] loss: 0.021047979593276978\n",
      "[step: 21] loss: 0.019269201904535294\n",
      "[nstep: 21] loss: 0.02066151797771454\n",
      "[step: 22] loss: 0.018367866054177284\n",
      "[nstep: 22] loss: 0.02010556124150753\n",
      "[step: 23] loss: 0.017382508143782616\n",
      "[nstep: 23] loss: 0.01960250735282898\n",
      "[step: 24] loss: 0.01649327762424946\n",
      "[nstep: 24] loss: 0.019216910004615784\n",
      "[step: 25] loss: 0.015659214928746223\n",
      "[nstep: 25] loss: 0.018815018236637115\n",
      "[step: 26] loss: 0.015191493555903435\n",
      "[nstep: 26] loss: 0.01834031753242016\n",
      "[step: 27] loss: 0.014828904531896114\n",
      "[nstep: 27] loss: 0.017769204452633858\n",
      "[step: 28] loss: 0.014325764030218124\n",
      "[nstep: 28] loss: 0.01724972389638424\n",
      "[step: 29] loss: 0.013904434628784657\n",
      "[nstep: 29] loss: 0.01688961684703827\n",
      "[step: 30] loss: 0.013315198943018913\n",
      "[nstep: 30] loss: 0.016568826511502266\n",
      "[step: 31] loss: 0.01292921882122755\n",
      "[nstep: 31] loss: 0.016252659261226654\n",
      "[step: 32] loss: 0.012429577298462391\n",
      "[nstep: 32] loss: 0.016002262011170387\n",
      "[step: 33] loss: 0.011890172027051449\n",
      "[nstep: 33] loss: 0.015723256394267082\n",
      "[step: 34] loss: 0.01149946078658104\n",
      "[nstep: 34] loss: 0.015506244264543056\n",
      "[step: 35] loss: 0.011098447255790234\n",
      "[nstep: 35] loss: 0.015374399721622467\n",
      "[step: 36] loss: 0.010835293680429459\n",
      "[nstep: 36] loss: 0.01576906628906727\n",
      "[step: 37] loss: 0.01038940716534853\n",
      "[nstep: 37] loss: 0.015110387466847897\n",
      "[step: 38] loss: 0.00992375984787941\n",
      "[nstep: 38] loss: 0.01515199989080429\n",
      "[step: 39] loss: 0.009667426347732544\n",
      "[nstep: 39] loss: 0.014953987672924995\n",
      "[step: 40] loss: 0.009307866916060448\n",
      "[nstep: 40] loss: 0.014680730178952217\n",
      "[step: 41] loss: 0.00897776149213314\n",
      "[nstep: 41] loss: 0.014314965344965458\n",
      "[step: 42] loss: 0.008699645288288593\n",
      "[nstep: 42] loss: 0.014238912612199783\n",
      "[step: 43] loss: 0.008380227722227573\n",
      "[nstep: 43] loss: 0.014092735946178436\n",
      "[step: 44] loss: 0.00812056940048933\n",
      "[nstep: 44] loss: 0.013992680236697197\n",
      "[step: 45] loss: 0.0078621506690979\n",
      "[nstep: 45] loss: 0.013829896226525307\n",
      "[step: 46] loss: 0.0075792889110744\n",
      "[nstep: 46] loss: 0.013778327964246273\n",
      "[step: 47] loss: 0.0073638795875012875\n",
      "[nstep: 47] loss: 0.013645668514072895\n",
      "[step: 48] loss: 0.007207443471997976\n",
      "[nstep: 48] loss: 0.013438085094094276\n",
      "[step: 49] loss: 0.006948418915271759\n",
      "[nstep: 49] loss: 0.013356066308915615\n",
      "[step: 50] loss: 0.006683667656034231\n",
      "[nstep: 50] loss: 0.013198122382164001\n",
      "[step: 51] loss: 0.006500390823930502\n",
      "[nstep: 51] loss: 0.013151922263205051\n",
      "[step: 52] loss: 0.006291918456554413\n",
      "[nstep: 52] loss: 0.013028832152485847\n",
      "[step: 53] loss: 0.006064587272703648\n",
      "[nstep: 53] loss: 0.012873804196715355\n",
      "[step: 54] loss: 0.005880322773009539\n",
      "[nstep: 54] loss: 0.012792487628757954\n",
      "[step: 55] loss: 0.005740941036492586\n",
      "[nstep: 55] loss: 0.012705600820481777\n",
      "[step: 56] loss: 0.005565688479691744\n",
      "[nstep: 56] loss: 0.012660657055675983\n",
      "[step: 57] loss: 0.005372873041778803\n",
      "[nstep: 57] loss: 0.012586851604282856\n",
      "[step: 58] loss: 0.005220749881118536\n",
      "[nstep: 58] loss: 0.012532182969152927\n",
      "[step: 59] loss: 0.005105722229927778\n",
      "[nstep: 59] loss: 0.012454650364816189\n",
      "[step: 60] loss: 0.004998482763767242\n",
      "[nstep: 60] loss: 0.012369505129754543\n",
      "[step: 61] loss: 0.004872737918049097\n",
      "[nstep: 61] loss: 0.012313233688473701\n",
      "[step: 62] loss: 0.004755335859954357\n",
      "[nstep: 62] loss: 0.012240474112331867\n",
      "[step: 63] loss: 0.004639053717255592\n",
      "[nstep: 63] loss: 0.012203721329569817\n",
      "[step: 64] loss: 0.004519924055784941\n",
      "[nstep: 64] loss: 0.012116015888750553\n",
      "[step: 65] loss: 0.004409332759678364\n",
      "[nstep: 65] loss: 0.012069371528923512\n",
      "[step: 66] loss: 0.004312112927436829\n",
      "[nstep: 66] loss: 0.012011164799332619\n",
      "[step: 67] loss: 0.004218243062496185\n",
      "[nstep: 67] loss: 0.01195189356803894\n",
      "[step: 68] loss: 0.004123491235077381\n",
      "[nstep: 68] loss: 0.011907605454325676\n",
      "[step: 69] loss: 0.004047281574457884\n",
      "[nstep: 69] loss: 0.011882140301167965\n",
      "[step: 70] loss: 0.003996828570961952\n",
      "[nstep: 70] loss: 0.011829239316284657\n",
      "[step: 71] loss: 0.004011050797998905\n",
      "[nstep: 71] loss: 0.01178803388029337\n",
      "[step: 72] loss: 0.004174904432147741\n",
      "[nstep: 72] loss: 0.011749493889510632\n",
      "[step: 73] loss: 0.004349047783762217\n",
      "[nstep: 73] loss: 0.011705138720571995\n",
      "[step: 74] loss: 0.00397889968007803\n",
      "[nstep: 74] loss: 0.011665217578411102\n",
      "[step: 75] loss: 0.003641346003860235\n",
      "[nstep: 75] loss: 0.011635984294116497\n",
      "[step: 76] loss: 0.003910555504262447\n",
      "[nstep: 76] loss: 0.011606056243181229\n",
      "[step: 77] loss: 0.0037304849829524755\n",
      "[nstep: 77] loss: 0.011563355103135109\n",
      "[step: 78] loss: 0.0034841401502490044\n",
      "[nstep: 78] loss: 0.011530092917382717\n",
      "[step: 79] loss: 0.00367998075671494\n",
      "[nstep: 79] loss: 0.011495846323668957\n",
      "[step: 80] loss: 0.003451902884989977\n",
      "[nstep: 80] loss: 0.011468805372714996\n",
      "[step: 81] loss: 0.0033927473705261946\n",
      "[nstep: 81] loss: 0.011441601440310478\n",
      "[step: 82] loss: 0.0034752434585243464\n",
      "[nstep: 82] loss: 0.011417502537369728\n",
      "[step: 83] loss: 0.0032446938566863537\n",
      "[nstep: 83] loss: 0.01139170303940773\n",
      "[step: 84] loss: 0.0033141407184302807\n",
      "[nstep: 84] loss: 0.011376122944056988\n",
      "[step: 85] loss: 0.003260241821408272\n",
      "[nstep: 85] loss: 0.011357737705111504\n",
      "[step: 86] loss: 0.003134143305942416\n",
      "[nstep: 86] loss: 0.01135415118187666\n",
      "[step: 87] loss: 0.003205656073987484\n",
      "[nstep: 87] loss: 0.011334064416587353\n",
      "[step: 88] loss: 0.0030909734778106213\n",
      "[nstep: 88] loss: 0.011301970109343529\n",
      "[step: 89] loss: 0.003052248852327466\n",
      "[nstep: 89] loss: 0.011241802014410496\n",
      "[step: 90] loss: 0.003084399038925767\n",
      "[nstep: 90] loss: 0.011200644075870514\n",
      "[step: 91] loss: 0.0029765150975435972\n",
      "[nstep: 91] loss: 0.011184664443135262\n",
      "[step: 92] loss: 0.002972359536215663\n",
      "[nstep: 92] loss: 0.0111780297011137\n",
      "[step: 93] loss: 0.002992797875776887\n",
      "[nstep: 93] loss: 0.011168099008500576\n",
      "[step: 94] loss: 0.0028961931820958853\n",
      "[nstep: 94] loss: 0.01113496720790863\n",
      "[step: 95] loss: 0.0028800484724342823\n",
      "[nstep: 95] loss: 0.011098799295723438\n",
      "[step: 96] loss: 0.002911841031163931\n",
      "[nstep: 96] loss: 0.011067655868828297\n",
      "[step: 97] loss: 0.002843283349648118\n",
      "[nstep: 97] loss: 0.0110460901632905\n",
      "[step: 98] loss: 0.002787840785458684\n",
      "[nstep: 98] loss: 0.011033546179533005\n",
      "[step: 99] loss: 0.002795459469780326\n",
      "[nstep: 99] loss: 0.011026587337255478\n",
      "[step: 100] loss: 0.0028002969920635223\n",
      "[nstep: 100] loss: 0.01103302463889122\n",
      "[step: 101] loss: 0.002779503585770726\n",
      "[nstep: 101] loss: 0.01104346290230751\n",
      "[step: 102] loss: 0.0027138073928654194\n",
      "[nstep: 102] loss: 0.011082923971116543\n",
      "[step: 103] loss: 0.0026959292590618134\n",
      "[nstep: 103] loss: 0.011036419309675694\n",
      "[step: 104] loss: 0.0027258163318037987\n",
      "[nstep: 104] loss: 0.01096761878579855\n",
      "[step: 105] loss: 0.0027396122459322214\n",
      "[nstep: 105] loss: 0.010899405926465988\n",
      "[step: 106] loss: 0.002710816916078329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 106] loss: 0.010906473733484745\n",
      "[step: 107] loss: 0.002630656585097313\n",
      "[nstep: 107] loss: 0.010934528894722462\n",
      "[step: 108] loss: 0.00260296743363142\n",
      "[nstep: 108] loss: 0.010889244265854359\n",
      "[step: 109] loss: 0.002622848143801093\n",
      "[nstep: 109] loss: 0.010834441520273685\n",
      "[step: 110] loss: 0.0026493894401937723\n",
      "[nstep: 110] loss: 0.010825123637914658\n",
      "[step: 111] loss: 0.002651866525411606\n",
      "[nstep: 111] loss: 0.010839611291885376\n",
      "[step: 112] loss: 0.0026002952363342047\n",
      "[nstep: 112] loss: 0.010836876928806305\n",
      "[step: 113] loss: 0.0025471332482993603\n",
      "[nstep: 113] loss: 0.010789221152663231\n",
      "[step: 114] loss: 0.002511565573513508\n",
      "[nstep: 114] loss: 0.010752665810286999\n",
      "[step: 115] loss: 0.00250511197373271\n",
      "[nstep: 115] loss: 0.010743997991085052\n",
      "[step: 116] loss: 0.002520818030461669\n",
      "[nstep: 116] loss: 0.010749725624918938\n",
      "[step: 117] loss: 0.002539469860494137\n",
      "[nstep: 117] loss: 0.010749060660600662\n",
      "[step: 118] loss: 0.0025512734428048134\n",
      "[nstep: 118] loss: 0.010723426938056946\n",
      "[step: 119] loss: 0.0025232923217117786\n",
      "[nstep: 119] loss: 0.010693551041185856\n",
      "[step: 120] loss: 0.0024912201333791018\n",
      "[nstep: 120] loss: 0.010666790418326855\n",
      "[step: 121] loss: 0.002449625637382269\n",
      "[nstep: 121] loss: 0.010652135126292706\n",
      "[step: 122] loss: 0.0024175746366381645\n",
      "[nstep: 122] loss: 0.01064780168235302\n",
      "[step: 123] loss: 0.0023974566720426083\n",
      "[nstep: 123] loss: 0.010648529045283794\n",
      "[step: 124] loss: 0.002390954876318574\n",
      "[nstep: 124] loss: 0.010656392201781273\n",
      "[step: 125] loss: 0.0023944179993122816\n",
      "[nstep: 125] loss: 0.010657697916030884\n",
      "[step: 126] loss: 0.002406678395345807\n",
      "[nstep: 126] loss: 0.01066714059561491\n",
      "[step: 127] loss: 0.0024325342383235693\n",
      "[nstep: 127] loss: 0.010642440058290958\n",
      "[step: 128] loss: 0.002462289296090603\n",
      "[nstep: 128] loss: 0.010609769262373447\n",
      "[step: 129] loss: 0.002522799652069807\n",
      "[nstep: 129] loss: 0.010562852956354618\n",
      "[step: 130] loss: 0.0025422496255487204\n",
      "[nstep: 130] loss: 0.010537693277001381\n",
      "[step: 131] loss: 0.002520433161407709\n",
      "[nstep: 131] loss: 0.01053803600370884\n",
      "[step: 132] loss: 0.002403334714472294\n",
      "[nstep: 132] loss: 0.010546631179749966\n",
      "[step: 133] loss: 0.0023074497003108263\n",
      "[nstep: 133] loss: 0.010552247986197472\n",
      "[step: 134] loss: 0.0023001451045274734\n",
      "[nstep: 134] loss: 0.01053479965776205\n",
      "[step: 135] loss: 0.002354992087930441\n",
      "[nstep: 135] loss: 0.010511969216167927\n",
      "[step: 136] loss: 0.0023985118605196476\n",
      "[nstep: 136] loss: 0.010481330566108227\n",
      "[step: 137] loss: 0.0023539618123322725\n",
      "[nstep: 137] loss: 0.010458351112902164\n",
      "[step: 138] loss: 0.0022744829766452312\n",
      "[nstep: 138] loss: 0.01044393703341484\n",
      "[step: 139] loss: 0.0022450173273682594\n",
      "[nstep: 139] loss: 0.010437036864459515\n",
      "[step: 140] loss: 0.0022702659480273724\n",
      "[nstep: 140] loss: 0.010436233133077621\n",
      "[step: 141] loss: 0.0023050298914313316\n",
      "[nstep: 141] loss: 0.010441052727401257\n",
      "[step: 142] loss: 0.0022877054288983345\n",
      "[nstep: 142] loss: 0.010462590493261814\n",
      "[step: 143] loss: 0.002235070103779435\n",
      "[nstep: 143] loss: 0.010492886416614056\n",
      "[step: 144] loss: 0.0022015718277543783\n",
      "[nstep: 144] loss: 0.010569747537374496\n",
      "[step: 145] loss: 0.002202761359512806\n",
      "[nstep: 145] loss: 0.010568900033831596\n",
      "[step: 146] loss: 0.0022229538299143314\n",
      "[nstep: 146] loss: 0.010530591011047363\n",
      "[step: 147] loss: 0.0022300526034086943\n",
      "[nstep: 147] loss: 0.010387091897428036\n",
      "[step: 148] loss: 0.002213881816715002\n",
      "[nstep: 148] loss: 0.010359353385865688\n",
      "[step: 149] loss: 0.0021850450430065393\n",
      "[nstep: 149] loss: 0.010435234755277634\n",
      "[step: 150] loss: 0.00215857638977468\n",
      "[nstep: 150] loss: 0.010425102896988392\n",
      "[step: 151] loss: 0.0021434668451547623\n",
      "[nstep: 151] loss: 0.01034429669380188\n",
      "[step: 152] loss: 0.0021409313194453716\n",
      "[nstep: 152] loss: 0.010310530662536621\n",
      "[step: 153] loss: 0.0021449048072099686\n",
      "[nstep: 153] loss: 0.010355522856116295\n",
      "[step: 154] loss: 0.002151695778593421\n",
      "[nstep: 154] loss: 0.010384381748735905\n",
      "[step: 155] loss: 0.002157019218429923\n",
      "[nstep: 155] loss: 0.010318560525774956\n",
      "[step: 156] loss: 0.00215923017822206\n",
      "[nstep: 156] loss: 0.010266807861626148\n",
      "[step: 157] loss: 0.002159374300390482\n",
      "[nstep: 157] loss: 0.010282767005264759\n",
      "[step: 158] loss: 0.0021532869432121515\n",
      "[nstep: 158] loss: 0.010306931100785732\n",
      "[step: 159] loss: 0.002146434271708131\n",
      "[nstep: 159] loss: 0.010294691659510136\n",
      "[step: 160] loss: 0.002134826499968767\n",
      "[nstep: 160] loss: 0.010241086594760418\n",
      "[step: 161] loss: 0.0021237158216536045\n",
      "[nstep: 161] loss: 0.010222074575722218\n",
      "[step: 162] loss: 0.0021072416566312313\n",
      "[nstep: 162] loss: 0.010237805545330048\n",
      "[step: 163] loss: 0.002090826630592346\n",
      "[nstep: 163] loss: 0.010251318104565144\n",
      "[step: 164] loss: 0.0020745890215039253\n",
      "[nstep: 164] loss: 0.010239320807158947\n",
      "[step: 165] loss: 0.002061076695099473\n",
      "[nstep: 165] loss: 0.010203194804489613\n",
      "[step: 166] loss: 0.002049014437943697\n",
      "[nstep: 166] loss: 0.010175354778766632\n",
      "[step: 167] loss: 0.0020383151713758707\n",
      "[nstep: 167] loss: 0.010169372893869877\n",
      "[step: 168] loss: 0.002029665745794773\n",
      "[nstep: 168] loss: 0.010174574330449104\n",
      "[step: 169] loss: 0.002021974651142955\n",
      "[nstep: 169] loss: 0.010178781114518642\n",
      "[step: 170] loss: 0.002015753649175167\n",
      "[nstep: 170] loss: 0.010166391730308533\n",
      "[step: 171] loss: 0.002010183408856392\n",
      "[nstep: 171] loss: 0.010147377848625183\n",
      "[step: 172] loss: 0.0020070152822881937\n",
      "[nstep: 172] loss: 0.01012589130550623\n",
      "[step: 173] loss: 0.0020083868876099586\n",
      "[nstep: 173] loss: 0.010113592259585857\n",
      "[step: 174] loss: 0.002020533662289381\n",
      "[nstep: 174] loss: 0.010109142400324345\n",
      "[step: 175] loss: 0.002059758873656392\n",
      "[nstep: 175] loss: 0.010109756141901016\n",
      "[step: 176] loss: 0.0021583610214293003\n",
      "[nstep: 176] loss: 0.010111290961503983\n",
      "[step: 177] loss: 0.0024035254027694464\n",
      "[nstep: 177] loss: 0.010110625997185707\n",
      "[step: 178] loss: 0.0027291574515402317\n",
      "[nstep: 178] loss: 0.010112622752785683\n",
      "[step: 179] loss: 0.002844196977093816\n",
      "[nstep: 179] loss: 0.010109379887580872\n",
      "[step: 180] loss: 0.0023033791221678257\n",
      "[nstep: 180] loss: 0.010109642520546913\n",
      "[step: 181] loss: 0.0019816558342427015\n",
      "[nstep: 181] loss: 0.010101234540343285\n",
      "[step: 182] loss: 0.0024024415761232376\n",
      "[nstep: 182] loss: 0.010096755810081959\n",
      "[step: 183] loss: 0.002418922260403633\n",
      "[nstep: 183] loss: 0.010079322382807732\n",
      "[step: 184] loss: 0.0020048178266733885\n",
      "[nstep: 184] loss: 0.01006243284791708\n",
      "[step: 185] loss: 0.0021798545494675636\n",
      "[nstep: 185] loss: 0.010040178894996643\n",
      "[step: 186] loss: 0.0022669092286378145\n",
      "[nstep: 186] loss: 0.010021742433309555\n",
      "[step: 187] loss: 0.0019656033255159855\n",
      "[nstep: 187] loss: 0.010005239397287369\n",
      "[step: 188] loss: 0.0021056097466498613\n",
      "[nstep: 188] loss: 0.009992786683142185\n",
      "[step: 189] loss: 0.0021317878272384405\n",
      "[nstep: 189] loss: 0.009983360767364502\n",
      "[step: 190] loss: 0.0019340193830430508\n",
      "[nstep: 190] loss: 0.00997624546289444\n",
      "[step: 191] loss: 0.0020835595205426216\n",
      "[nstep: 191] loss: 0.009971041232347488\n",
      "[step: 192] loss: 0.0020258817821741104\n",
      "[nstep: 192] loss: 0.009969471953809261\n",
      "[step: 193] loss: 0.001919326139613986\n",
      "[nstep: 193] loss: 0.00997675210237503\n",
      "[step: 194] loss: 0.002035175682976842\n",
      "[nstep: 194] loss: 0.010003534145653248\n",
      "[step: 195] loss: 0.0019515857566148043\n",
      "[nstep: 195] loss: 0.010091478936374187\n",
      "[step: 196] loss: 0.0019091528374701738\n",
      "[nstep: 196] loss: 0.010249927639961243\n",
      "[step: 197] loss: 0.0019936878234148026\n",
      "[nstep: 197] loss: 0.010595567524433136\n",
      "[step: 198] loss: 0.001905333949252963\n",
      "[nstep: 198] loss: 0.010432861745357513\n",
      "[step: 199] loss: 0.001895420835353434\n",
      "[nstep: 199] loss: 0.010048693977296352\n",
      "[step: 200] loss: 0.0019494181033223867\n",
      "[nstep: 200] loss: 0.009989027865231037\n",
      "[step: 201] loss: 0.001874162582680583\n",
      "[nstep: 201] loss: 0.01023226510733366\n",
      "[step: 202] loss: 0.0018828840693458915\n",
      "[nstep: 202] loss: 0.010111761279404163\n",
      "[step: 203] loss: 0.0019122244557365775\n",
      "[nstep: 203] loss: 0.009911458939313889\n",
      "[step: 204] loss: 0.001854135887697339\n",
      "[nstep: 204] loss: 0.01013127714395523\n",
      "[step: 205] loss: 0.0018610683036968112\n",
      "[nstep: 205] loss: 0.010115409269928932\n",
      "[step: 206] loss: 0.0018801018595695496\n",
      "[nstep: 206] loss: 0.009881752543151379\n",
      "[step: 207] loss: 0.001836841576732695\n",
      "[nstep: 207] loss: 0.01004253700375557\n",
      "[step: 208] loss: 0.0018395635997876525\n",
      "[nstep: 208] loss: 0.010057135485112667\n",
      "[step: 209] loss: 0.001855208887718618\n",
      "[nstep: 209] loss: 0.00985712744295597\n",
      "[step: 210] loss: 0.0018235427560284734\n",
      "[nstep: 210] loss: 0.009994092397391796\n",
      "[step: 211] loss: 0.0018182015046477318\n",
      "[nstep: 211] loss: 0.010005050338804722\n",
      "[step: 212] loss: 0.0018330056918784976\n",
      "[nstep: 212] loss: 0.009834236465394497\n",
      "[step: 213] loss: 0.001810366753488779\n",
      "[nstep: 213] loss: 0.009952803142368793\n",
      "[step: 214] loss: 0.0017986512975767255\n",
      "[nstep: 214] loss: 0.009964602999389172\n",
      "[step: 215] loss: 0.0018099695444107056\n",
      "[nstep: 215] loss: 0.0098129753023386\n",
      "[step: 216] loss: 0.0017975401133298874\n",
      "[nstep: 216] loss: 0.009905788116157055\n",
      "[step: 217] loss: 0.0017829707358032465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 217] loss: 0.009921672753989697\n",
      "[step: 218] loss: 0.0017872316529974341\n",
      "[nstep: 218] loss: 0.009792046621441841\n",
      "[step: 219] loss: 0.0017854206962510943\n",
      "[nstep: 219] loss: 0.009864702820777893\n",
      "[step: 220] loss: 0.0017703853081911802\n",
      "[nstep: 220] loss: 0.009876362048089504\n",
      "[step: 221] loss: 0.001766537199728191\n",
      "[nstep: 221] loss: 0.009771715849637985\n",
      "[step: 222] loss: 0.0017693564295768738\n",
      "[nstep: 222] loss: 0.00983146857470274\n",
      "[step: 223] loss: 0.0017609840724617243\n",
      "[nstep: 223] loss: 0.009836344048380852\n",
      "[step: 224] loss: 0.0017505524447187781\n",
      "[nstep: 224] loss: 0.00975063256919384\n",
      "[step: 225] loss: 0.0017493994673714042\n",
      "[nstep: 225] loss: 0.00979625154286623\n",
      "[step: 226] loss: 0.0017490718746557832\n",
      "[nstep: 226] loss: 0.009803978726267815\n",
      "[step: 227] loss: 0.0017405395628884435\n",
      "[nstep: 227] loss: 0.009731252677738667\n",
      "[step: 228] loss: 0.0017325564986094832\n",
      "[nstep: 228] loss: 0.00975851435214281\n",
      "[step: 229] loss: 0.0017307507805526257\n",
      "[nstep: 229] loss: 0.009775524958968163\n",
      "[step: 230] loss: 0.0017290131654590368\n",
      "[nstep: 230] loss: 0.009715343825519085\n",
      "[step: 231] loss: 0.0017225549090653658\n",
      "[nstep: 231] loss: 0.009721322916448116\n",
      "[step: 232] loss: 0.0017151436768472195\n",
      "[nstep: 232] loss: 0.00974575337022543\n",
      "[step: 233] loss: 0.0017116045346483588\n",
      "[nstep: 233] loss: 0.009702855721116066\n",
      "[step: 234] loss: 0.0017096609808504581\n",
      "[nstep: 234] loss: 0.009686616249382496\n",
      "[step: 235] loss: 0.0017052345210686326\n",
      "[nstep: 235] loss: 0.009709726087749004\n",
      "[step: 236] loss: 0.0016987797571346164\n",
      "[nstep: 236] loss: 0.009691884741187096\n",
      "[step: 237] loss: 0.0016934409504756331\n",
      "[nstep: 237] loss: 0.009662500582635403\n",
      "[step: 238] loss: 0.0016902688657864928\n",
      "[nstep: 238] loss: 0.009669396094977856\n",
      "[step: 239] loss: 0.001687340554781258\n",
      "[nstep: 239] loss: 0.009673558175563812\n",
      "[step: 240] loss: 0.0016829378437250853\n",
      "[nstep: 240] loss: 0.009650789201259613\n",
      "[step: 241] loss: 0.0016773589886724949\n",
      "[nstep: 241] loss: 0.009636595845222473\n",
      "[step: 242] loss: 0.001672353595495224\n",
      "[nstep: 242] loss: 0.009643100202083588\n",
      "[step: 243] loss: 0.0016683872090652585\n",
      "[nstep: 243] loss: 0.009639783762395382\n",
      "[step: 244] loss: 0.0016651690239086747\n",
      "[nstep: 244] loss: 0.00962025299668312\n",
      "[step: 245] loss: 0.0016614656196907163\n",
      "[nstep: 245] loss: 0.009610206820070744\n",
      "[step: 246] loss: 0.0016571281012147665\n",
      "[nstep: 246] loss: 0.009612813591957092\n",
      "[step: 247] loss: 0.001652254955843091\n",
      "[nstep: 247] loss: 0.009608837775886059\n",
      "[step: 248] loss: 0.0016475538723170757\n",
      "[nstep: 248] loss: 0.009594643488526344\n",
      "[step: 249] loss: 0.0016432319534942508\n",
      "[nstep: 249] loss: 0.009582900442183018\n",
      "[step: 250] loss: 0.0016393850091844797\n",
      "[nstep: 250] loss: 0.009580380283296108\n",
      "[step: 251] loss: 0.0016357067506760359\n",
      "[nstep: 251] loss: 0.009579076431691647\n",
      "[step: 252] loss: 0.0016320467693731189\n",
      "[nstep: 252] loss: 0.009570966474711895\n",
      "[step: 253] loss: 0.0016282302094623446\n",
      "[nstep: 253] loss: 0.009559071622788906\n",
      "[step: 254] loss: 0.0016242822166532278\n",
      "[nstep: 254] loss: 0.009549913927912712\n",
      "[step: 255] loss: 0.001620229217223823\n",
      "[nstep: 255] loss: 0.0095455851405859\n",
      "[step: 256] loss: 0.001616144087165594\n",
      "[nstep: 256] loss: 0.009542662650346756\n",
      "[step: 257] loss: 0.0016120950458571315\n",
      "[nstep: 257] loss: 0.00953715294599533\n",
      "[step: 258] loss: 0.001608125283382833\n",
      "[nstep: 258] loss: 0.009528794325888157\n",
      "[step: 259] loss: 0.001604330725967884\n",
      "[nstep: 259] loss: 0.009519169107079506\n",
      "[step: 260] loss: 0.0016007514204829931\n",
      "[nstep: 260] loss: 0.00951061025261879\n",
      "[step: 261] loss: 0.0015976480208337307\n",
      "[nstep: 261] loss: 0.009503900073468685\n",
      "[step: 262] loss: 0.001595355337485671\n",
      "[nstep: 262] loss: 0.00949863251298666\n",
      "[step: 263] loss: 0.001594843459315598\n",
      "[nstep: 263] loss: 0.009494059719145298\n",
      "[step: 264] loss: 0.0015977616421878338\n",
      "[nstep: 264] loss: 0.009489564225077629\n",
      "[step: 265] loss: 0.0016089151613414288\n",
      "[nstep: 265] loss: 0.009485242888331413\n",
      "[step: 266] loss: 0.0016365937190130353\n",
      "[nstep: 266] loss: 0.009481078945100307\n",
      "[step: 267] loss: 0.0017050682799890637\n",
      "[nstep: 267] loss: 0.009478306397795677\n",
      "[step: 268] loss: 0.0018402080750092864\n",
      "[nstep: 268] loss: 0.00947726983577013\n",
      "[step: 269] loss: 0.002113392809405923\n",
      "[nstep: 269] loss: 0.009481596760451794\n",
      "[step: 270] loss: 0.002385309198871255\n",
      "[nstep: 270] loss: 0.00949271023273468\n",
      "[step: 271] loss: 0.0024329491425305605\n",
      "[nstep: 271] loss: 0.009523226879537106\n",
      "[step: 272] loss: 0.0019521667854860425\n",
      "[nstep: 272] loss: 0.009572437033057213\n",
      "[step: 273] loss: 0.0015754414489492774\n",
      "[nstep: 273] loss: 0.009673050604760647\n",
      "[step: 274] loss: 0.001831505447626114\n",
      "[nstep: 274] loss: 0.009747401811182499\n",
      "[step: 275] loss: 0.0020473587792366743\n",
      "[nstep: 275] loss: 0.009800810366868973\n",
      "[step: 276] loss: 0.0017301201587542892\n",
      "[nstep: 276] loss: 0.009631641209125519\n",
      "[step: 277] loss: 0.0015852124197408557\n",
      "[nstep: 277] loss: 0.009447883814573288\n",
      "[step: 278] loss: 0.0018496951088309288\n",
      "[nstep: 278] loss: 0.009424666874110699\n",
      "[step: 279] loss: 0.0017882224638015032\n",
      "[nstep: 279] loss: 0.009537730365991592\n",
      "[step: 280] loss: 0.0015668425476178527\n",
      "[nstep: 280] loss: 0.009589990600943565\n",
      "[step: 281] loss: 0.0016716243699193\n",
      "[nstep: 281] loss: 0.009471233934164047\n",
      "[step: 282] loss: 0.001732368255034089\n",
      "[nstep: 282] loss: 0.00938054546713829\n",
      "[step: 283] loss: 0.0015810774639248848\n",
      "[nstep: 283] loss: 0.009418467991054058\n",
      "[step: 284] loss: 0.0015827205497771502\n",
      "[nstep: 284] loss: 0.009477687999606133\n",
      "[step: 285] loss: 0.0016477737808600068\n",
      "[nstep: 285] loss: 0.009450748562812805\n",
      "[step: 286] loss: 0.0015955655835568905\n",
      "[nstep: 286] loss: 0.009368919767439365\n",
      "[step: 287] loss: 0.001550606801174581\n",
      "[nstep: 287] loss: 0.009356164373457432\n",
      "[step: 288] loss: 0.0015707947313785553\n",
      "[nstep: 288] loss: 0.009403157979249954\n",
      "[step: 289] loss: 0.0015830224147066474\n",
      "[nstep: 289] loss: 0.009413872845470905\n",
      "[step: 290] loss: 0.001547482912428677\n",
      "[nstep: 290] loss: 0.009370538406074047\n",
      "[step: 291] loss: 0.0015196878230199218\n",
      "[nstep: 291] loss: 0.009324479848146439\n",
      "[step: 292] loss: 0.0015587031375616789\n",
      "[nstep: 292] loss: 0.009326419793069363\n",
      "[step: 293] loss: 0.0015396903036162257\n",
      "[nstep: 293] loss: 0.009353135712444782\n",
      "[step: 294] loss: 0.001494470750913024\n",
      "[nstep: 294] loss: 0.009351675398647785\n",
      "[step: 295] loss: 0.0015259122010320425\n",
      "[nstep: 295] loss: 0.009320815093815327\n",
      "[step: 296] loss: 0.0015276254853233695\n",
      "[nstep: 296] loss: 0.009291846305131912\n",
      "[step: 297] loss: 0.0014861853560432792\n",
      "[nstep: 297] loss: 0.009290323592722416\n",
      "[step: 298] loss: 0.0014970346819609404\n",
      "[nstep: 298] loss: 0.009304207749664783\n",
      "[step: 299] loss: 0.0015039010904729366\n",
      "[nstep: 299] loss: 0.009307018481194973\n",
      "[step: 300] loss: 0.001484813285060227\n",
      "[nstep: 300] loss: 0.009292572736740112\n",
      "[step: 301] loss: 0.001480810809880495\n",
      "[nstep: 301] loss: 0.009269044734537601\n",
      "[step: 302] loss: 0.0014763175277039409\n",
      "[nstep: 302] loss: 0.009252835065126419\n",
      "[step: 303] loss: 0.0014766949461773038\n",
      "[nstep: 303] loss: 0.009249342605471611\n",
      "[step: 304] loss: 0.001473135664127767\n",
      "[nstep: 304] loss: 0.009253810159862041\n",
      "[step: 305] loss: 0.0014572838554158807\n",
      "[nstep: 305] loss: 0.009258968755602837\n",
      "[step: 306] loss: 0.0014584854943677783\n",
      "[nstep: 306] loss: 0.00925848912447691\n",
      "[step: 307] loss: 0.0014643431641161442\n",
      "[nstep: 307] loss: 0.009253359399735928\n",
      "[step: 308] loss: 0.0014496863586828113\n",
      "[nstep: 308] loss: 0.009242122992873192\n",
      "[step: 309] loss: 0.0014420514926314354\n",
      "[nstep: 309] loss: 0.00922935176640749\n",
      "[step: 310] loss: 0.0014465461717918515\n",
      "[nstep: 310] loss: 0.00921567715704441\n",
      "[step: 311] loss: 0.0014421975938603282\n",
      "[nstep: 311] loss: 0.009203745052218437\n",
      "[step: 312] loss: 0.0014355909079313278\n",
      "[nstep: 312] loss: 0.009193535894155502\n",
      "[step: 313] loss: 0.0014310766709968448\n",
      "[nstep: 313] loss: 0.009185101836919785\n",
      "[step: 314] loss: 0.0014264718629419804\n",
      "[nstep: 314] loss: 0.009177811443805695\n",
      "[step: 315] loss: 0.001425576047040522\n",
      "[nstep: 315] loss: 0.009171216748654842\n",
      "[step: 316] loss: 0.001424505957402289\n",
      "[nstep: 316] loss: 0.009165145456790924\n",
      "[step: 317] loss: 0.0014161638682708144\n",
      "[nstep: 317] loss: 0.009159565903246403\n",
      "[step: 318] loss: 0.0014104251749813557\n",
      "[nstep: 318] loss: 0.009154717437922955\n",
      "[step: 319] loss: 0.0014102058485150337\n",
      "[nstep: 319] loss: 0.009151599369943142\n",
      "[step: 320] loss: 0.0014084386639297009\n",
      "[nstep: 320] loss: 0.009153103455901146\n",
      "[step: 321] loss: 0.001403019530698657\n",
      "[nstep: 321] loss: 0.009166422300040722\n",
      "[step: 322] loss: 0.0013988835271447897\n",
      "[nstep: 322] loss: 0.009215394034981728\n",
      "[step: 323] loss: 0.001394937396980822\n",
      "[nstep: 323] loss: 0.009351028129458427\n",
      "[step: 324] loss: 0.0013909324770793319\n",
      "[nstep: 324] loss: 0.009733511134982109\n",
      "[step: 325] loss: 0.0013885829830542207\n",
      "[nstep: 325] loss: 0.010271255858242512\n",
      "[step: 326] loss: 0.0013867788948118687\n",
      "[nstep: 326] loss: 0.010729222558438778\n",
      "[step: 327] loss: 0.0013826475478708744\n",
      "[nstep: 327] loss: 0.009622727520763874\n",
      "[step: 328] loss: 0.0013774667168036103\n",
      "[nstep: 328] loss: 0.009281461127102375\n",
      "[step: 329] loss: 0.0013737367698922753\n",
      "[nstep: 329] loss: 0.009962273761630058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 330] loss: 0.001370992511510849\n",
      "[nstep: 330] loss: 0.00941670686006546\n",
      "[step: 331] loss: 0.0013678792165592313\n",
      "[nstep: 331] loss: 0.0092858225107193\n",
      "[step: 332] loss: 0.001364513416774571\n",
      "[nstep: 332] loss: 0.009682351723313332\n",
      "[step: 333] loss: 0.0013617262011393905\n",
      "[nstep: 333] loss: 0.009189569391310215\n",
      "[step: 334] loss: 0.0013589703012257814\n",
      "[nstep: 334] loss: 0.009365515783429146\n",
      "[step: 335] loss: 0.00135550438426435\n",
      "[nstep: 335] loss: 0.009427991695702076\n",
      "[step: 336] loss: 0.0013515102909877896\n",
      "[nstep: 336] loss: 0.009119225665926933\n",
      "[step: 337] loss: 0.0013477103784680367\n",
      "[nstep: 337] loss: 0.009421502240002155\n",
      "[step: 338] loss: 0.0013443687930703163\n",
      "[nstep: 338] loss: 0.009228816255927086\n",
      "[step: 339] loss: 0.0013410791289061308\n",
      "[nstep: 339] loss: 0.009163249284029007\n",
      "[step: 340] loss: 0.0013375976122915745\n",
      "[nstep: 340] loss: 0.009333423338830471\n",
      "[step: 341] loss: 0.0013339727884158492\n",
      "[nstep: 341] loss: 0.00909474492073059\n",
      "[step: 342] loss: 0.0013305004686117172\n",
      "[nstep: 342] loss: 0.009197329171001911\n",
      "[step: 343] loss: 0.0013272614451125264\n",
      "[nstep: 343] loss: 0.00918151531368494\n",
      "[step: 344] loss: 0.0013241937849670649\n",
      "[nstep: 344] loss: 0.009068355895578861\n",
      "[step: 345] loss: 0.0013211796758696437\n",
      "[nstep: 345] loss: 0.009184809401631355\n",
      "[step: 346] loss: 0.0013181989779695868\n",
      "[nstep: 346] loss: 0.009065650403499603\n",
      "[step: 347] loss: 0.0013154614716768265\n",
      "[nstep: 347] loss: 0.00910084880888462\n",
      "[step: 348] loss: 0.0013134564505890012\n",
      "[nstep: 348] loss: 0.00910975132137537\n",
      "[step: 349] loss: 0.0013133394531905651\n",
      "[nstep: 349] loss: 0.00903148390352726\n",
      "[step: 350] loss: 0.0013181730173528194\n",
      "[nstep: 350] loss: 0.009099168702960014\n",
      "[step: 351] loss: 0.0013356488198041916\n",
      "[nstep: 351] loss: 0.00903935544192791\n",
      "[step: 352] loss: 0.0013886397937312722\n",
      "[nstep: 352] loss: 0.00903064664453268\n",
      "[step: 353] loss: 0.001528149121440947\n",
      "[nstep: 353] loss: 0.00906379520893097\n",
      "[step: 354] loss: 0.0018933621468022466\n",
      "[nstep: 354] loss: 0.008999921381473541\n",
      "[step: 355] loss: 0.00251537817530334\n",
      "[nstep: 355] loss: 0.00902819074690342\n",
      "[step: 356] loss: 0.0031654853373765945\n",
      "[nstep: 356] loss: 0.009019208140671253\n",
      "[step: 357] loss: 0.0027516400441527367\n",
      "[nstep: 357] loss: 0.008981212973594666\n",
      "[step: 358] loss: 0.001796639640815556\n",
      "[nstep: 358] loss: 0.00901026464998722\n",
      "[step: 359] loss: 0.0017152109649032354\n",
      "[nstep: 359] loss: 0.008983014151453972\n",
      "[step: 360] loss: 0.0018143977504223585\n",
      "[nstep: 360] loss: 0.008969117887318134\n",
      "[step: 361] loss: 0.0019746439065784216\n",
      "[nstep: 361] loss: 0.00898539274930954\n",
      "[step: 362] loss: 0.0014580414863303304\n",
      "[nstep: 362] loss: 0.008958701975643635\n",
      "[step: 363] loss: 0.0016116327606141567\n",
      "[nstep: 363] loss: 0.008955694735050201\n",
      "[step: 364] loss: 0.0016759015852585435\n",
      "[nstep: 364] loss: 0.008962106890976429\n",
      "[step: 365] loss: 0.0014201030135154724\n",
      "[nstep: 365] loss: 0.008939305320382118\n",
      "[step: 366] loss: 0.0014957536477595568\n",
      "[nstep: 366] loss: 0.0089393500238657\n",
      "[step: 367] loss: 0.001485964166931808\n",
      "[nstep: 367] loss: 0.008941137231886387\n",
      "[step: 368] loss: 0.0014411405427381396\n",
      "[nstep: 368] loss: 0.008921898901462555\n",
      "[step: 369] loss: 0.001358676585368812\n",
      "[nstep: 369] loss: 0.008921792730689049\n",
      "[step: 370] loss: 0.0014826941769570112\n",
      "[nstep: 370] loss: 0.00892226118594408\n",
      "[step: 371] loss: 0.0013220589607954025\n",
      "[nstep: 371] loss: 0.008906286209821701\n",
      "[step: 372] loss: 0.0013922370271757245\n",
      "[nstep: 372] loss: 0.008903836831450462\n",
      "[step: 373] loss: 0.0013612847542390227\n",
      "[nstep: 373] loss: 0.008904368616640568\n",
      "[step: 374] loss: 0.0013635832583531737\n",
      "[nstep: 374] loss: 0.008891645818948746\n",
      "[step: 375] loss: 0.001300503616221249\n",
      "[nstep: 375] loss: 0.008885861374437809\n",
      "[step: 376] loss: 0.001385074807330966\n",
      "[nstep: 376] loss: 0.008886339142918587\n",
      "[step: 377] loss: 0.0012828035978600383\n",
      "[nstep: 377] loss: 0.008877594955265522\n",
      "[step: 378] loss: 0.0013236692175269127\n",
      "[nstep: 378] loss: 0.008869352750480175\n",
      "[step: 379] loss: 0.0012979644816368818\n",
      "[nstep: 379] loss: 0.008867745287716389\n",
      "[step: 380] loss: 0.0013085498940199614\n",
      "[nstep: 380] loss: 0.008863477036356926\n",
      "[step: 381] loss: 0.0012621016940101981\n",
      "[nstep: 381] loss: 0.008854840882122517\n",
      "[step: 382] loss: 0.0013088396517559886\n",
      "[nstep: 382] loss: 0.008849581703543663\n",
      "[step: 383] loss: 0.0012598378816619515\n",
      "[nstep: 383] loss: 0.008847320452332497\n",
      "[step: 384] loss: 0.0012751120375469327\n",
      "[nstep: 384] loss: 0.008841869421303272\n",
      "[step: 385] loss: 0.0012632168363779783\n",
      "[nstep: 385] loss: 0.008834202773869038\n",
      "[step: 386] loss: 0.001269476255401969\n",
      "[nstep: 386] loss: 0.00882942508906126\n",
      "[step: 387] loss: 0.0012390606570988894\n",
      "[nstep: 387] loss: 0.008826597593724728\n",
      "[step: 388] loss: 0.0012624632799997926\n",
      "[nstep: 388] loss: 0.008821520023047924\n",
      "[step: 389] loss: 0.0012434108648449183\n",
      "[nstep: 389] loss: 0.008814835920929909\n",
      "[step: 390] loss: 0.0012401466956362128\n",
      "[nstep: 390] loss: 0.008809277787804604\n",
      "[step: 391] loss: 0.001235917559824884\n",
      "[nstep: 391] loss: 0.008805399760603905\n",
      "[step: 392] loss: 0.0012399707920849323\n",
      "[nstep: 392] loss: 0.008801511488854885\n",
      "[step: 393] loss: 0.00122506741899997\n",
      "[nstep: 393] loss: 0.008796457201242447\n",
      "[step: 394] loss: 0.00122411223128438\n",
      "[nstep: 394] loss: 0.008790574036538601\n",
      "[step: 395] loss: 0.0012265846598893404\n",
      "[nstep: 395] loss: 0.00878498237580061\n",
      "[step: 396] loss: 0.0012159580364823341\n",
      "[nstep: 396] loss: 0.008780326694250107\n",
      "[step: 397] loss: 0.0012137480080127716\n",
      "[nstep: 397] loss: 0.00877627544105053\n",
      "[step: 398] loss: 0.0012104326160624623\n",
      "[nstep: 398] loss: 0.008772175759077072\n",
      "[step: 399] loss: 0.001212139381095767\n",
      "[nstep: 399] loss: 0.008767703548073769\n",
      "[step: 400] loss: 0.0011996850371360779\n",
      "[nstep: 400] loss: 0.00876295380294323\n",
      "[step: 401] loss: 0.001203196938149631\n",
      "[nstep: 401] loss: 0.008758042939007282\n",
      "[step: 402] loss: 0.0011974629014730453\n",
      "[nstep: 402] loss: 0.008753078058362007\n",
      "[step: 403] loss: 0.00119674950838089\n",
      "[nstep: 403] loss: 0.008748235180974007\n",
      "[step: 404] loss: 0.0011887411819770932\n",
      "[nstep: 404] loss: 0.008743605576455593\n",
      "[step: 405] loss: 0.0011896975338459015\n",
      "[nstep: 405] loss: 0.008739305660128593\n",
      "[step: 406] loss: 0.0011865498963743448\n",
      "[nstep: 406] loss: 0.00873555988073349\n",
      "[step: 407] loss: 0.0011819929350167513\n",
      "[nstep: 407] loss: 0.008733130060136318\n",
      "[step: 408] loss: 0.0011795200407505035\n",
      "[nstep: 408] loss: 0.00873382668942213\n",
      "[step: 409] loss: 0.0011762692593038082\n",
      "[nstep: 409] loss: 0.008743268437683582\n",
      "[step: 410] loss: 0.0011758238542824984\n",
      "[nstep: 410] loss: 0.00877444539219141\n",
      "[step: 411] loss: 0.001169922179542482\n",
      "[nstep: 411] loss: 0.008870980702340603\n",
      "[step: 412] loss: 0.0011684891069307923\n",
      "[nstep: 412] loss: 0.009081140160560608\n",
      "[step: 413] loss: 0.0011653864057734609\n",
      "[nstep: 413] loss: 0.009546066634356976\n",
      "[step: 414] loss: 0.0011635279515758157\n",
      "[nstep: 414] loss: 0.009663710370659828\n",
      "[step: 415] loss: 0.001160547137260437\n",
      "[nstep: 415] loss: 0.009413295425474644\n",
      "[step: 416] loss: 0.0011565022869035602\n",
      "[nstep: 416] loss: 0.008776923641562462\n",
      "[step: 417] loss: 0.0011551891220733523\n",
      "[nstep: 417] loss: 0.008992710150778294\n",
      "[step: 418] loss: 0.001152041251771152\n",
      "[nstep: 418] loss: 0.009335006587207317\n",
      "[step: 419] loss: 0.0011500621913000941\n",
      "[nstep: 419] loss: 0.008840874768793583\n",
      "[step: 420] loss: 0.0011467228177934885\n",
      "[nstep: 420] loss: 0.008842265233397484\n",
      "[step: 421] loss: 0.0011437688954174519\n",
      "[nstep: 421] loss: 0.00911818165332079\n",
      "[step: 422] loss: 0.0011419818038120866\n",
      "[nstep: 422] loss: 0.008774496614933014\n",
      "[step: 423] loss: 0.0011390821309760213\n",
      "[nstep: 423] loss: 0.008825603872537613\n",
      "[step: 424] loss: 0.0011369234416633844\n",
      "[nstep: 424] loss: 0.008989132009446621\n",
      "[step: 425] loss: 0.0011339812772348523\n",
      "[nstep: 425] loss: 0.008730782195925713\n",
      "[step: 426] loss: 0.0011311162961646914\n",
      "[nstep: 426] loss: 0.008805820718407631\n",
      "[step: 427] loss: 0.0011290883412584662\n",
      "[nstep: 427] loss: 0.008877099491655827\n",
      "[step: 428] loss: 0.0011264120694249868\n",
      "[nstep: 428] loss: 0.008700416423380375\n",
      "[step: 429] loss: 0.0011242141481488943\n",
      "[nstep: 429] loss: 0.008789002895355225\n",
      "[step: 430] loss: 0.0011217037681490183\n",
      "[nstep: 430] loss: 0.008794950321316719\n",
      "[step: 431] loss: 0.0011188553180545568\n",
      "[nstep: 431] loss: 0.008681856095790863\n",
      "[step: 432] loss: 0.0011165664764121175\n",
      "[nstep: 432] loss: 0.008763538673520088\n",
      "[step: 433] loss: 0.0011140103451907635\n",
      "[nstep: 433] loss: 0.00873655453324318\n",
      "[step: 434] loss: 0.00111161929089576\n",
      "[nstep: 434] loss: 0.008672072552144527\n",
      "[step: 435] loss: 0.0011094798101112247\n",
      "[nstep: 435] loss: 0.008730681613087654\n",
      "[step: 436] loss: 0.0011069702450186014\n",
      "[nstep: 436] loss: 0.008695474825799465\n",
      "[step: 437] loss: 0.0011046051513403654\n",
      "[nstep: 437] loss: 0.008655238896608353\n",
      "[step: 438] loss: 0.0011022608960047364\n",
      "[nstep: 438] loss: 0.008701374754309654\n",
      "[step: 439] loss: 0.0010996995260939002\n",
      "[nstep: 439] loss: 0.008668205700814724\n",
      "[step: 440] loss: 0.0010973329190164804\n",
      "[nstep: 440] loss: 0.008636826649308205\n",
      "[step: 441] loss: 0.001094999723136425\n",
      "[nstep: 441] loss: 0.008674482814967632\n",
      "[step: 442] loss: 0.001092539168894291\n",
      "[nstep: 442] loss: 0.008646239526569843\n",
      "[step: 443] loss: 0.0010902544017881155\n",
      "[nstep: 443] loss: 0.008618461899459362\n",
      "[step: 444] loss: 0.0010879820911213756\n",
      "[nstep: 444] loss: 0.008652345277369022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 445] loss: 0.0010856181615963578\n",
      "[nstep: 445] loss: 0.0086292065680027\n",
      "[step: 446] loss: 0.0010833796113729477\n",
      "[nstep: 446] loss: 0.008602794259786606\n",
      "[step: 447] loss: 0.0010811787797138095\n",
      "[nstep: 447] loss: 0.00862993486225605\n",
      "[step: 448] loss: 0.0010789355728775263\n",
      "[nstep: 448] loss: 0.008612634614109993\n",
      "[step: 449] loss: 0.001076831598766148\n",
      "[nstep: 449] loss: 0.00859070848673582\n",
      "[step: 450] loss: 0.0010749292559921741\n",
      "[nstep: 450] loss: 0.008607074618339539\n",
      "[step: 451] loss: 0.0010733017697930336\n",
      "[nstep: 451] loss: 0.008598385378718376\n",
      "[step: 452] loss: 0.0010724423918873072\n",
      "[nstep: 452] loss: 0.008580156601965427\n",
      "[step: 453] loss: 0.0010733570670709014\n",
      "[nstep: 453] loss: 0.00858445093035698\n",
      "[step: 454] loss: 0.0010783830657601357\n",
      "[nstep: 454] loss: 0.0085839768871665\n",
      "[step: 455] loss: 0.0010935148457065225\n",
      "[nstep: 455] loss: 0.008571616373956203\n",
      "[step: 456] loss: 0.0011346859391778708\n",
      "[nstep: 456] loss: 0.008565235882997513\n",
      "[step: 457] loss: 0.001237856806255877\n",
      "[nstep: 457] loss: 0.008567034266889095\n",
      "[step: 458] loss: 0.0014920324319973588\n",
      "[nstep: 458] loss: 0.008563011884689331\n",
      "[step: 459] loss: 0.0019927583634853363\n",
      "[nstep: 459] loss: 0.008551543578505516\n",
      "[step: 460] loss: 0.002688573207706213\n",
      "[nstep: 460] loss: 0.00854803528636694\n",
      "[step: 461] loss: 0.002974859206005931\n",
      "[nstep: 461] loss: 0.008550557307898998\n",
      "[step: 462] loss: 0.0018914822721853852\n",
      "[nstep: 462] loss: 0.008543409407138824\n",
      "[step: 463] loss: 0.0010867119999602437\n",
      "[nstep: 463] loss: 0.008533259853720665\n",
      "[step: 464] loss: 0.0016136103076860309\n",
      "[nstep: 464] loss: 0.008533209562301636\n",
      "[step: 465] loss: 0.0018953819526359439\n",
      "[nstep: 465] loss: 0.008533106185495853\n",
      "[step: 466] loss: 0.001258646254427731\n",
      "[nstep: 466] loss: 0.008525240235030651\n",
      "[step: 467] loss: 0.001204991596750915\n",
      "[nstep: 467] loss: 0.008518745191395283\n",
      "[step: 468] loss: 0.0016156734200194478\n",
      "[nstep: 468] loss: 0.008516105823218822\n",
      "[step: 469] loss: 0.001278152340091765\n",
      "[nstep: 469] loss: 0.008514049462974072\n",
      "[step: 470] loss: 0.0011206233175471425\n",
      "[nstep: 470] loss: 0.00851083267480135\n",
      "[step: 471] loss: 0.001438997918739915\n",
      "[nstep: 471] loss: 0.008504802361130714\n",
      "[step: 472] loss: 0.001220950623974204\n",
      "[nstep: 472] loss: 0.008498807437717915\n",
      "[step: 473] loss: 0.0011020679958164692\n",
      "[nstep: 473] loss: 0.008495992980897427\n",
      "[step: 474] loss: 0.0013243695721030235\n",
      "[nstep: 474] loss: 0.008494371548295021\n",
      "[step: 475] loss: 0.0011558779515326023\n",
      "[nstep: 475] loss: 0.008490806445479393\n",
      "[step: 476] loss: 0.0010974283795803785\n",
      "[nstep: 476] loss: 0.008485585451126099\n",
      "[step: 477] loss: 0.001239876844920218\n",
      "[nstep: 477] loss: 0.00848085805773735\n",
      "[step: 478] loss: 0.0011007592547684908\n",
      "[nstep: 478] loss: 0.008476519025862217\n",
      "[step: 479] loss: 0.0010955403558909893\n",
      "[nstep: 479] loss: 0.00847251620143652\n",
      "[step: 480] loss: 0.0011742687784135342\n",
      "[nstep: 480] loss: 0.00846959464251995\n",
      "[step: 481] loss: 0.0010660707484930754\n",
      "[nstep: 481] loss: 0.008467122912406921\n",
      "[step: 482] loss: 0.0010909681441262364\n",
      "[nstep: 482] loss: 0.008464151062071323\n",
      "[step: 483] loss: 0.0011223627952858806\n",
      "[nstep: 483] loss: 0.00846081878989935\n",
      "[step: 484] loss: 0.0010489195119589567\n",
      "[nstep: 484] loss: 0.008457840420305729\n",
      "[step: 485] loss: 0.0010822588810697198\n",
      "[nstep: 485] loss: 0.008455038070678711\n",
      "[step: 486] loss: 0.0010816784342750907\n",
      "[nstep: 486] loss: 0.00845277588814497\n",
      "[step: 487] loss: 0.0010396972065791488\n",
      "[nstep: 487] loss: 0.00845133513212204\n",
      "[step: 488] loss: 0.0010691593633964658\n",
      "[nstep: 488] loss: 0.008452949114143848\n",
      "[step: 489] loss: 0.001052398351021111\n",
      "[nstep: 489] loss: 0.008458948694169521\n",
      "[step: 490] loss: 0.0010329664219170809\n",
      "[nstep: 490] loss: 0.008477328345179558\n",
      "[step: 491] loss: 0.0010551853338256478\n",
      "[nstep: 491] loss: 0.008512316271662712\n",
      "[step: 492] loss: 0.0010328987846150994\n",
      "[nstep: 492] loss: 0.008601618930697441\n",
      "[step: 493] loss: 0.0010266723111271858\n",
      "[nstep: 493] loss: 0.00871193129569292\n",
      "[step: 494] loss: 0.0010421625338494778\n",
      "[nstep: 494] loss: 0.008923975750803947\n",
      "[step: 495] loss: 0.0010191688779741526\n",
      "[nstep: 495] loss: 0.008860877715051174\n",
      "[step: 496] loss: 0.0010189686436206102\n",
      "[nstep: 496] loss: 0.008689251728355885\n",
      "[step: 497] loss: 0.0010300795547664165\n",
      "[nstep: 497] loss: 0.008452109061181545\n",
      "[step: 498] loss: 0.001009385334327817\n",
      "[nstep: 498] loss: 0.008489223197102547\n",
      "[step: 499] loss: 0.0010101882508024573\n",
      "[nstep: 499] loss: 0.008659565821290016\n",
      "[step: 500] loss: 0.0010191203327849507\n",
      "[nstep: 500] loss: 0.00860105361789465\n",
      "[step: 501] loss: 0.0010021846974268556\n",
      "[nstep: 501] loss: 0.00844957958906889\n",
      "[step: 502] loss: 0.0010018597822636366\n",
      "[nstep: 502] loss: 0.008441298268735409\n",
      "[step: 503] loss: 0.0010091696167364717\n",
      "[nstep: 503] loss: 0.008533603511750698\n",
      "[step: 504] loss: 0.000996335525996983\n",
      "[nstep: 504] loss: 0.008536187931895256\n",
      "[step: 505] loss: 0.0009939263109117746\n",
      "[nstep: 505] loss: 0.008434636518359184\n",
      "[step: 506] loss: 0.0009996532462537289\n",
      "[nstep: 506] loss: 0.008413412608206272\n",
      "[step: 507] loss: 0.000990936066955328\n",
      "[nstep: 507] loss: 0.008470161817967892\n",
      "[step: 508] loss: 0.0009867793414741755\n",
      "[nstep: 508] loss: 0.008463583886623383\n",
      "[step: 509] loss: 0.000990628614090383\n",
      "[nstep: 509] loss: 0.008404349908232689\n",
      "[step: 510] loss: 0.00098553323186934\n",
      "[nstep: 510] loss: 0.008390706963837147\n",
      "[step: 511] loss: 0.0009806615998968482\n",
      "[nstep: 511] loss: 0.008425776846706867\n",
      "[step: 512] loss: 0.0009823681320995092\n",
      "[nstep: 512] loss: 0.008424457162618637\n",
      "[step: 513] loss: 0.0009796400554478168\n",
      "[nstep: 513] loss: 0.008378725498914719\n",
      "[step: 514] loss: 0.0009752103360369802\n",
      "[nstep: 514] loss: 0.008371024392545223\n",
      "[step: 515] loss: 0.0009751358302310109\n",
      "[nstep: 515] loss: 0.0083992388099432\n",
      "[step: 516] loss: 0.0009735831990838051\n",
      "[nstep: 516] loss: 0.008393029682338238\n",
      "[step: 517] loss: 0.0009696913184598088\n",
      "[nstep: 517] loss: 0.008358863182365894\n",
      "[step: 518] loss: 0.0009686804260127246\n",
      "[nstep: 518] loss: 0.008350878022611141\n",
      "[step: 519] loss: 0.0009677962516434491\n",
      "[nstep: 519] loss: 0.008368338458240032\n",
      "[step: 520] loss: 0.0009643682278692722\n",
      "[nstep: 520] loss: 0.00836949609220028\n",
      "[step: 521] loss: 0.0009623313671909273\n",
      "[nstep: 521] loss: 0.008349383249878883\n",
      "[step: 522] loss: 0.0009619442862458527\n",
      "[nstep: 522] loss: 0.008337614126503468\n",
      "[step: 523] loss: 0.00095961126498878\n",
      "[nstep: 523] loss: 0.008338400162756443\n",
      "[step: 524] loss: 0.0009565679938532412\n",
      "[nstep: 524] loss: 0.008339506573975086\n",
      "[step: 525] loss: 0.0009555037831887603\n",
      "[nstep: 525] loss: 0.008337493985891342\n",
      "[step: 526] loss: 0.0009545647772029042\n",
      "[nstep: 526] loss: 0.008332573808729649\n",
      "[step: 527] loss: 0.000951955618802458\n",
      "[nstep: 527] loss: 0.008324059657752514\n",
      "[step: 528] loss: 0.0009496093261986971\n",
      "[nstep: 528] loss: 0.008313261903822422\n",
      "[step: 529] loss: 0.0009485437185503542\n",
      "[nstep: 529] loss: 0.008309428580105305\n",
      "[step: 530] loss: 0.0009471665252931416\n",
      "[nstep: 530] loss: 0.008312806487083435\n",
      "[step: 531] loss: 0.0009448672644793987\n",
      "[nstep: 531] loss: 0.008314172737300396\n",
      "[step: 532] loss: 0.0009429117781110108\n",
      "[nstep: 532] loss: 0.008309918455779552\n",
      "[step: 533] loss: 0.0009416014654561877\n",
      "[nstep: 533] loss: 0.008301926776766777\n",
      "[step: 534] loss: 0.0009400026174262166\n",
      "[nstep: 534] loss: 0.008295204490423203\n",
      "[step: 535] loss: 0.0009379885159432888\n",
      "[nstep: 535] loss: 0.00828945729881525\n",
      "[step: 536] loss: 0.0009362514829263091\n",
      "[nstep: 536] loss: 0.00828386191278696\n",
      "[step: 537] loss: 0.0009348422172479331\n",
      "[nstep: 537] loss: 0.00827896036207676\n",
      "[step: 538] loss: 0.0009332089102827013\n",
      "[nstep: 538] loss: 0.008276346139609814\n",
      "[step: 539] loss: 0.0009312914335168898\n",
      "[nstep: 539] loss: 0.008275763131678104\n",
      "[step: 540] loss: 0.0009295584168285131\n",
      "[nstep: 540] loss: 0.008275541476905346\n",
      "[step: 541] loss: 0.0009281291859224439\n",
      "[nstep: 541] loss: 0.008276134729385376\n",
      "[step: 542] loss: 0.0009266430279240012\n",
      "[nstep: 542] loss: 0.008279182016849518\n",
      "[step: 543] loss: 0.0009248880087397993\n",
      "[nstep: 543] loss: 0.008291049860417843\n",
      "[step: 544] loss: 0.0009230895084328949\n",
      "[nstep: 544] loss: 0.008314273320138454\n",
      "[step: 545] loss: 0.0009214876918122172\n",
      "[nstep: 545] loss: 0.008373423479497433\n",
      "[step: 546] loss: 0.0009200231870636344\n",
      "[nstep: 546] loss: 0.008458676747977734\n",
      "[step: 547] loss: 0.0009184869704768062\n",
      "[nstep: 547] loss: 0.008651256561279297\n",
      "[step: 548] loss: 0.0009168318938463926\n",
      "[nstep: 548] loss: 0.008719413541257381\n",
      "[step: 549] loss: 0.0009151775157079101\n",
      "[nstep: 549] loss: 0.008768357336521149\n",
      "[step: 550] loss: 0.0009136184817180037\n",
      "[nstep: 550] loss: 0.008474381640553474\n",
      "[step: 551] loss: 0.0009121155017055571\n",
      "[nstep: 551] loss: 0.008293522521853447\n",
      "[step: 552] loss: 0.0009105772478505969\n",
      "[nstep: 552] loss: 0.008351613767445087\n",
      "[step: 553] loss: 0.0009089777013286948\n",
      "[nstep: 553] loss: 0.008458642289042473\n",
      "[step: 554] loss: 0.0009073674445971847\n",
      "[nstep: 554] loss: 0.008445451967418194\n",
      "[step: 555] loss: 0.0009058070136234164\n",
      "[nstep: 555] loss: 0.00831302534788847\n",
      "[step: 556] loss: 0.0009043057216331363\n",
      "[nstep: 556] loss: 0.008277017623186111\n",
      "[step: 557] loss: 0.0009028280037455261\n",
      "[nstep: 557] loss: 0.00833155121654272\n",
      "[step: 558] loss: 0.0009013351518660784\n",
      "[nstep: 558] loss: 0.008357096463441849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 559] loss: 0.000899816513992846\n",
      "[nstep: 559] loss: 0.008328796364367008\n",
      "[step: 560] loss: 0.0008982880972325802\n",
      "[nstep: 560] loss: 0.008246151730418205\n",
      "[step: 561] loss: 0.0008967738831415772\n",
      "[nstep: 561] loss: 0.008245603181421757\n",
      "[step: 562] loss: 0.0008952891221269965\n",
      "[nstep: 562] loss: 0.008316581137478352\n",
      "[step: 563] loss: 0.0008938381215557456\n",
      "[nstep: 563] loss: 0.008275524713099003\n",
      "[step: 564] loss: 0.0008924180874601007\n",
      "[nstep: 564] loss: 0.008206529542803764\n",
      "[step: 565] loss: 0.0008910312317311764\n",
      "[nstep: 565] loss: 0.00823259074240923\n",
      "[step: 566] loss: 0.0008896990329958498\n",
      "[nstep: 566] loss: 0.008255071938037872\n",
      "[step: 567] loss: 0.0008884681737981737\n",
      "[nstep: 567] loss: 0.008224990218877792\n",
      "[step: 568] loss: 0.0008874415070749819\n",
      "[nstep: 568] loss: 0.00820298120379448\n",
      "[step: 569] loss: 0.0008868210134096444\n",
      "[nstep: 569] loss: 0.008206544443964958\n",
      "[step: 570] loss: 0.0008870346937328577\n",
      "[nstep: 570] loss: 0.00820827390998602\n",
      "[step: 571] loss: 0.0008889727760106325\n",
      "[nstep: 571] loss: 0.008202442899346352\n",
      "[step: 572] loss: 0.0008945434237830341\n",
      "[nstep: 572] loss: 0.00820061657577753\n",
      "[step: 573] loss: 0.000907695444766432\n",
      "[nstep: 573] loss: 0.008187041617929935\n",
      "[step: 574] loss: 0.0009361481061205268\n",
      "[nstep: 574] loss: 0.008168989792466164\n",
      "[step: 575] loss: 0.0009918734431266785\n",
      "[nstep: 575] loss: 0.008173882029950619\n",
      "[step: 576] loss: 0.0010831321123987436\n",
      "[nstep: 576] loss: 0.008184692822396755\n",
      "[step: 577] loss: 0.0011787567054852843\n",
      "[nstep: 577] loss: 0.008175431750714779\n",
      "[step: 578] loss: 0.001188904163427651\n",
      "[nstep: 578] loss: 0.00816098228096962\n",
      "[step: 579] loss: 0.0010499918134883046\n",
      "[nstep: 579] loss: 0.008157756179571152\n",
      "[step: 580] loss: 0.0009332795161753893\n",
      "[nstep: 580] loss: 0.008150049485266209\n",
      "[step: 581] loss: 0.0010008944664150476\n",
      "[nstep: 581] loss: 0.008143076673150063\n",
      "[step: 582] loss: 0.001102080917917192\n",
      "[nstep: 582] loss: 0.008147441782057285\n",
      "[step: 583] loss: 0.001047173049300909\n",
      "[nstep: 583] loss: 0.008149858564138412\n",
      "[step: 584] loss: 0.0009529013186693192\n",
      "[nstep: 584] loss: 0.008143281564116478\n",
      "[step: 585] loss: 0.000998063711449504\n",
      "[nstep: 585] loss: 0.0081351762637496\n",
      "[step: 586] loss: 0.0009936128044500947\n",
      "[nstep: 586] loss: 0.00813225470483303\n",
      "[step: 587] loss: 0.0008901364635676146\n",
      "[nstep: 587] loss: 0.008127821609377861\n",
      "[step: 588] loss: 0.0008976520039141178\n",
      "[nstep: 588] loss: 0.008119783364236355\n",
      "[step: 589] loss: 0.0009344110148958862\n",
      "[nstep: 589] loss: 0.008112507872283459\n",
      "[step: 590] loss: 0.0008884241105988622\n",
      "[nstep: 590] loss: 0.008109202608466148\n",
      "[step: 591] loss: 0.0008971274946816266\n",
      "[nstep: 591] loss: 0.008106651715934277\n",
      "[step: 592] loss: 0.000939604768063873\n",
      "[nstep: 592] loss: 0.008102333173155785\n",
      "[step: 593] loss: 0.0009020059951581061\n",
      "[nstep: 593] loss: 0.008096867240965366\n",
      "[step: 594] loss: 0.0008855912019498646\n",
      "[nstep: 594] loss: 0.008092590607702732\n",
      "[step: 595] loss: 0.0009046360501088202\n",
      "[nstep: 595] loss: 0.008090353570878506\n",
      "[step: 596] loss: 0.0008741956553421915\n",
      "[nstep: 596] loss: 0.008088245987892151\n",
      "[step: 597] loss: 0.0008567522163502872\n",
      "[nstep: 597] loss: 0.008084808476269245\n",
      "[step: 598] loss: 0.0008760878699831665\n",
      "[nstep: 598] loss: 0.008081699721515179\n",
      "[step: 599] loss: 0.0008665004279464483\n",
      "[nstep: 599] loss: 0.008082768879830837\n",
      "[step: 600] loss: 0.0008580994908697903\n",
      "[nstep: 600] loss: 0.00809341948479414\n",
      "[step: 601] loss: 0.0008751836139708757\n",
      "[nstep: 601] loss: 0.008133635856211185\n",
      "[step: 602] loss: 0.0008733961149118841\n",
      "[nstep: 602] loss: 0.008244029246270657\n",
      "[step: 603] loss: 0.0008616212289780378\n",
      "[nstep: 603] loss: 0.008613030426204205\n",
      "[step: 604] loss: 0.0008679925231263041\n",
      "[nstep: 604] loss: 0.009096236899495125\n",
      "[step: 605] loss: 0.0008678998565301299\n",
      "[nstep: 605] loss: 0.009985213167965412\n",
      "[step: 606] loss: 0.00085466168820858\n",
      "[nstep: 606] loss: 0.008983032777905464\n",
      "[step: 607] loss: 0.0008537956164218485\n",
      "[nstep: 607] loss: 0.008587180636823177\n",
      "[step: 608] loss: 0.0008559240959584713\n",
      "[nstep: 608] loss: 0.008719178847968578\n",
      "[step: 609] loss: 0.0008460200042463839\n",
      "[nstep: 609] loss: 0.008851567283272743\n",
      "[step: 610] loss: 0.0008418245706707239\n",
      "[nstep: 610] loss: 0.008631536737084389\n",
      "[step: 611] loss: 0.0008451591711491346\n",
      "[nstep: 611] loss: 0.008330554701387882\n",
      "[step: 612] loss: 0.0008408791036345065\n",
      "[nstep: 612] loss: 0.008821853436529636\n",
      "[step: 613] loss: 0.0008355826721526682\n",
      "[nstep: 613] loss: 0.008194494061172009\n",
      "[step: 614] loss: 0.0008376751793548465\n",
      "[nstep: 614] loss: 0.008527659811079502\n",
      "[step: 615] loss: 0.0008381161605939269\n",
      "[nstep: 615] loss: 0.008302330039441586\n",
      "[step: 616] loss: 0.0008344107773154974\n",
      "[nstep: 616] loss: 0.008349305018782616\n",
      "[step: 617] loss: 0.0008358915220014751\n",
      "[nstep: 617] loss: 0.008259586058557034\n",
      "[step: 618] loss: 0.0008421162492595613\n",
      "[nstep: 618] loss: 0.008326728828251362\n",
      "[step: 619] loss: 0.0008481819531880319\n",
      "[nstep: 619] loss: 0.008218428120017052\n",
      "[step: 620] loss: 0.0008622942259535193\n",
      "[nstep: 620] loss: 0.008229468949139118\n",
      "[step: 621] loss: 0.0008966528112068772\n",
      "[nstep: 621] loss: 0.00822154339402914\n",
      "[step: 622] loss: 0.0009608218097127974\n",
      "[nstep: 622] loss: 0.00815059244632721\n",
      "[step: 623] loss: 0.001084508141502738\n",
      "[nstep: 623] loss: 0.00817731861025095\n",
      "[step: 624] loss: 0.0012976967263966799\n",
      "[nstep: 624] loss: 0.008142562583088875\n",
      "[step: 625] loss: 0.001612483523786068\n",
      "[nstep: 625] loss: 0.008144496940076351\n",
      "[step: 626] loss: 0.001841865829192102\n",
      "[nstep: 626] loss: 0.008114016614854336\n",
      "[step: 627] loss: 0.0017980915727093816\n",
      "[nstep: 627] loss: 0.008135420270264149\n",
      "[step: 628] loss: 0.0012598555767908692\n",
      "[nstep: 628] loss: 0.008068693801760674\n",
      "[step: 629] loss: 0.0008521671989001334\n",
      "[nstep: 629] loss: 0.00811742153018713\n",
      "[step: 630] loss: 0.0009839151753112674\n",
      "[nstep: 630] loss: 0.008059985935688019\n",
      "[step: 631] loss: 0.0012763766571879387\n",
      "[nstep: 631] loss: 0.008080643601715565\n",
      "[step: 632] loss: 0.0011799229541793466\n",
      "[nstep: 632] loss: 0.008049920201301575\n",
      "[step: 633] loss: 0.0008584635797888041\n",
      "[nstep: 633] loss: 0.008073464967310429\n",
      "[step: 634] loss: 0.0009183358051814139\n",
      "[nstep: 634] loss: 0.008020933717489243\n",
      "[step: 635] loss: 0.0011271376861259341\n",
      "[nstep: 635] loss: 0.00805486086755991\n",
      "[step: 636] loss: 0.000989673426374793\n",
      "[nstep: 636] loss: 0.008014270104467869\n",
      "[step: 637] loss: 0.0008241155301220715\n",
      "[nstep: 637] loss: 0.008031115867197514\n",
      "[step: 638] loss: 0.000937244389206171\n",
      "[nstep: 638] loss: 0.007999105378985405\n",
      "[step: 639] loss: 0.0010013649007305503\n",
      "[nstep: 639] loss: 0.00802278146147728\n",
      "[step: 640] loss: 0.0008651021053083241\n",
      "[nstep: 640] loss: 0.00799049437046051\n",
      "[step: 641] loss: 0.0008351295837201178\n",
      "[nstep: 641] loss: 0.007998218759894371\n",
      "[step: 642] loss: 0.0009322180412709713\n",
      "[nstep: 642] loss: 0.007992835715413094\n",
      "[step: 643] loss: 0.0009002673905342817\n",
      "[nstep: 643] loss: 0.007980559021234512\n",
      "[step: 644] loss: 0.0008172130328603089\n",
      "[nstep: 644] loss: 0.007980703376233578\n",
      "[step: 645] loss: 0.0008607316412962973\n",
      "[nstep: 645] loss: 0.007968979887664318\n",
      "[step: 646] loss: 0.0008951039053499699\n",
      "[nstep: 646] loss: 0.007975403219461441\n",
      "[step: 647] loss: 0.0008315296145156026\n",
      "[nstep: 647] loss: 0.00795737188309431\n",
      "[step: 648] loss: 0.0008196343551389873\n",
      "[nstep: 648] loss: 0.007962150499224663\n",
      "[step: 649] loss: 0.00086268997984007\n",
      "[nstep: 649] loss: 0.007954214699566364\n",
      "[step: 650] loss: 0.0008439022931270301\n",
      "[nstep: 650] loss: 0.007951603271067142\n",
      "[step: 651] loss: 0.0008075006771832705\n",
      "[nstep: 651] loss: 0.007945227436721325\n",
      "[step: 652] loss: 0.00082531722728163\n",
      "[nstep: 652] loss: 0.007937801070511341\n",
      "[step: 653] loss: 0.0008393143652938306\n",
      "[nstep: 653] loss: 0.007941351272165775\n",
      "[step: 654] loss: 0.0008121885475702584\n",
      "[nstep: 654] loss: 0.007930215448141098\n",
      "[step: 655] loss: 0.000802277703769505\n",
      "[nstep: 655] loss: 0.007929042913019657\n",
      "[step: 656] loss: 0.0008204408222809434\n",
      "[nstep: 656] loss: 0.007923905737698078\n",
      "[step: 657] loss: 0.0008173089590854943\n",
      "[nstep: 657] loss: 0.007918242365121841\n",
      "[step: 658] loss: 0.0007969174766913056\n",
      "[nstep: 658] loss: 0.007918915711343288\n",
      "[step: 659] loss: 0.0007994663319550455\n",
      "[nstep: 659] loss: 0.007911001332104206\n",
      "[step: 660] loss: 0.0008109803311526775\n",
      "[nstep: 660] loss: 0.007908548228442669\n",
      "[step: 661] loss: 0.0008008303120732307\n",
      "[nstep: 661] loss: 0.007903723046183586\n",
      "[step: 662] loss: 0.0007886831881478429\n",
      "[nstep: 662] loss: 0.007898963056504726\n",
      "[step: 663] loss: 0.000795432657469064\n",
      "[nstep: 663] loss: 0.007898222655057907\n",
      "[step: 664] loss: 0.0007999902591109276\n",
      "[nstep: 664] loss: 0.00789288803935051\n",
      "[step: 665] loss: 0.0007896181778050959\n",
      "[nstep: 665] loss: 0.007889490574598312\n",
      "[step: 666] loss: 0.0007834211573936045\n",
      "[nstep: 666] loss: 0.007886430248618126\n",
      "[step: 667] loss: 0.0007894942536950111\n",
      "[nstep: 667] loss: 0.007880414836108685\n",
      "[step: 668] loss: 0.000790399790275842\n",
      "[nstep: 668] loss: 0.007878128439188004\n",
      "[step: 669] loss: 0.0007819258025847375\n",
      "[nstep: 669] loss: 0.007874485105276108\n",
      "[step: 670] loss: 0.0007786997593939304\n",
      "[nstep: 670] loss: 0.007870342582464218\n",
      "[step: 671] loss: 0.0007825865759514272\n",
      "[nstep: 671] loss: 0.007868201471865177\n",
      "[step: 672] loss: 0.0007823465275578201\n",
      "[nstep: 672] loss: 0.007864410988986492\n",
      "[step: 673] loss: 0.0007764491601847112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 673] loss: 0.007860967889428139\n",
      "[step: 674] loss: 0.0007737959967926145\n",
      "[nstep: 674] loss: 0.007858372293412685\n",
      "[step: 675] loss: 0.0007757262792438269\n",
      "[nstep: 675] loss: 0.007854428142309189\n",
      "[step: 676] loss: 0.0007756563136354089\n",
      "[nstep: 676] loss: 0.007851638831198215\n",
      "[step: 677] loss: 0.0007719587301835418\n",
      "[nstep: 677] loss: 0.007849069312214851\n",
      "[step: 678] loss: 0.0007691369974054396\n",
      "[nstep: 678] loss: 0.007846196182072163\n",
      "[step: 679] loss: 0.0007691806531511247\n",
      "[nstep: 679] loss: 0.00784494448453188\n",
      "[step: 680] loss: 0.0007694101077504456\n",
      "[nstep: 680] loss: 0.007845403626561165\n",
      "[step: 681] loss: 0.0007676345994696021\n",
      "[nstep: 681] loss: 0.007847772911190987\n",
      "[step: 682] loss: 0.0007650199695490301\n",
      "[nstep: 682] loss: 0.007855708710849285\n",
      "[step: 683] loss: 0.0007635560468770564\n",
      "[nstep: 683] loss: 0.007870812900364399\n",
      "[step: 684] loss: 0.0007633026107214391\n",
      "[nstep: 684] loss: 0.007895288988947868\n",
      "[step: 685] loss: 0.0007628431776538491\n",
      "[nstep: 685] loss: 0.007934505119919777\n",
      "[step: 686] loss: 0.0007612366462126374\n",
      "[nstep: 686] loss: 0.00797011237591505\n",
      "[step: 687] loss: 0.0007591748726554215\n",
      "[nstep: 687] loss: 0.008024649694561958\n",
      "[step: 688] loss: 0.0007578452350571752\n",
      "[nstep: 688] loss: 0.008019703440368176\n",
      "[step: 689] loss: 0.0007573762559331954\n",
      "[nstep: 689] loss: 0.008008304983377457\n",
      "[step: 690] loss: 0.0007568436558358371\n",
      "[nstep: 690] loss: 0.007924230769276619\n",
      "[step: 691] loss: 0.0007555136107839644\n",
      "[nstep: 691] loss: 0.007854713127017021\n",
      "[step: 692] loss: 0.0007537403143942356\n",
      "[nstep: 692] loss: 0.007814569398760796\n",
      "[step: 693] loss: 0.0007523656822741032\n",
      "[nstep: 693] loss: 0.00782203208655119\n",
      "[step: 694] loss: 0.0007516196346841753\n",
      "[nstep: 694] loss: 0.007861167192459106\n",
      "[step: 695] loss: 0.0007510127616114914\n",
      "[nstep: 695] loss: 0.00788729079067707\n",
      "[step: 696] loss: 0.000750033010262996\n",
      "[nstep: 696] loss: 0.007891022600233555\n",
      "[step: 697] loss: 0.0007486696704290807\n",
      "[nstep: 697] loss: 0.007845045067369938\n",
      "[step: 698] loss: 0.0007472996949218214\n",
      "[nstep: 698] loss: 0.00779894320294261\n",
      "[step: 699] loss: 0.000746174540836364\n",
      "[nstep: 699] loss: 0.007778414059430361\n",
      "[step: 700] loss: 0.000745279889088124\n",
      "[nstep: 700] loss: 0.007791122421622276\n",
      "[step: 701] loss: 0.000744449149351567\n",
      "[nstep: 701] loss: 0.007816579192876816\n",
      "[step: 702] loss: 0.0007435333682224154\n",
      "[nstep: 702] loss: 0.007823862135410309\n",
      "[step: 703] loss: 0.0007424919167533517\n",
      "[nstep: 703] loss: 0.00781171303242445\n",
      "[step: 704] loss: 0.0007413469138555229\n",
      "[nstep: 704] loss: 0.0077819377183914185\n",
      "[step: 705] loss: 0.0007401755428873003\n",
      "[nstep: 705] loss: 0.007760325446724892\n",
      "[step: 706] loss: 0.0007390712853521109\n",
      "[nstep: 706] loss: 0.0077558765187859535\n",
      "[step: 707] loss: 0.0007380845490843058\n",
      "[nstep: 707] loss: 0.007764499634504318\n",
      "[step: 708] loss: 0.0007371868123300374\n",
      "[nstep: 708] loss: 0.007775392383337021\n",
      "[step: 709] loss: 0.0007363104377873242\n",
      "[nstep: 709] loss: 0.007777761202305555\n",
      "[step: 710] loss: 0.0007353919208981097\n",
      "[nstep: 710] loss: 0.007774716708809137\n",
      "[step: 711] loss: 0.0007344164187088609\n",
      "[nstep: 711] loss: 0.007764081470668316\n",
      "[step: 712] loss: 0.0007334073889069259\n",
      "[nstep: 712] loss: 0.007754277903586626\n",
      "[step: 713] loss: 0.0007323856698349118\n",
      "[nstep: 713] loss: 0.007745746988803148\n",
      "[step: 714] loss: 0.0007313649985007942\n",
      "[nstep: 714] loss: 0.007740851026028395\n",
      "[step: 715] loss: 0.0007303477614186704\n",
      "[nstep: 715] loss: 0.0077379546128213406\n",
      "[step: 716] loss: 0.0007293377420864999\n",
      "[nstep: 716] loss: 0.007735584396868944\n",
      "[step: 717] loss: 0.0007283422164618969\n",
      "[nstep: 717] loss: 0.007732908241450787\n",
      "[step: 718] loss: 0.0007273686351254582\n",
      "[nstep: 718] loss: 0.00772864930331707\n",
      "[step: 719] loss: 0.0007264116429723799\n",
      "[nstep: 719] loss: 0.007723845541477203\n",
      "[step: 720] loss: 0.0007254672236740589\n",
      "[nstep: 720] loss: 0.0077184331603348255\n",
      "[step: 721] loss: 0.0007245278684422374\n",
      "[nstep: 721] loss: 0.007714089937508106\n",
      "[step: 722] loss: 0.000723591772839427\n",
      "[nstep: 722] loss: 0.007710885256528854\n",
      "[step: 723] loss: 0.0007226640591397882\n",
      "[nstep: 723] loss: 0.007711025886237621\n",
      "[step: 724] loss: 0.0007217503734864295\n",
      "[nstep: 724] loss: 0.0077149090357124805\n",
      "[step: 725] loss: 0.0007208578172139823\n",
      "[nstep: 725] loss: 0.007728151511400938\n",
      "[step: 726] loss: 0.0007199975661933422\n",
      "[nstep: 726] loss: 0.007752097677439451\n",
      "[step: 727] loss: 0.0007191919721662998\n",
      "[nstep: 727] loss: 0.0078096454963088036\n",
      "[step: 728] loss: 0.0007184863206930459\n",
      "[nstep: 728] loss: 0.007891004905104637\n",
      "[step: 729] loss: 0.0007179801468737423\n",
      "[nstep: 729] loss: 0.0080660879611969\n",
      "[step: 730] loss: 0.0007178724044933915\n",
      "[nstep: 730] loss: 0.008157949894666672\n",
      "[step: 731] loss: 0.0007185928407125175\n",
      "[nstep: 731] loss: 0.008256676606833935\n",
      "[step: 732] loss: 0.0007210438488982618\n",
      "[nstep: 732] loss: 0.008000378496944904\n",
      "[step: 733] loss: 0.0007272810908034444\n",
      "[nstep: 733] loss: 0.007767761591821909\n",
      "[step: 734] loss: 0.0007417138549499214\n",
      "[nstep: 734] loss: 0.007733714766800404\n",
      "[step: 735] loss: 0.0007747680065222085\n",
      "[nstep: 735] loss: 0.007859488017857075\n",
      "[step: 736] loss: 0.0008477683295495808\n",
      "[nstep: 736] loss: 0.00790863111615181\n",
      "[step: 737] loss: 0.0010094293393194675\n",
      "[nstep: 737] loss: 0.0077666789293289185\n",
      "[step: 738] loss: 0.0013244522269815207\n",
      "[nstep: 738] loss: 0.007705076597630978\n",
      "[step: 739] loss: 0.0018751254538074136\n",
      "[nstep: 739] loss: 0.007788335904479027\n",
      "[step: 740] loss: 0.0023408615961670876\n",
      "[nstep: 740] loss: 0.007805824279785156\n",
      "[step: 741] loss: 0.0023109656758606434\n",
      "[nstep: 741] loss: 0.007701802998781204\n",
      "[step: 742] loss: 0.0012662067310884595\n",
      "[nstep: 742] loss: 0.007661969866603613\n",
      "[step: 743] loss: 0.000748569262214005\n",
      "[nstep: 743] loss: 0.007726133801043034\n",
      "[step: 744] loss: 0.0012920423178002238\n",
      "[nstep: 744] loss: 0.007728321012109518\n",
      "[step: 745] loss: 0.0014402740634977818\n",
      "[nstep: 745] loss: 0.0076641966588795185\n",
      "[step: 746] loss: 0.0008682866464368999\n",
      "[nstep: 746] loss: 0.00766146183013916\n",
      "[step: 747] loss: 0.0008590776706114411\n",
      "[nstep: 747] loss: 0.0076894559897482395\n",
      "[step: 748] loss: 0.0012079825391992927\n",
      "[nstep: 748] loss: 0.007668434176594019\n",
      "[step: 749] loss: 0.000930202251765877\n",
      "[nstep: 749] loss: 0.007639642804861069\n",
      "[step: 750] loss: 0.0007753684767521918\n",
      "[nstep: 750] loss: 0.007640609983354807\n",
      "[step: 751] loss: 0.0010503963567316532\n",
      "[nstep: 751] loss: 0.007654368411749601\n",
      "[step: 752] loss: 0.0008951352210715413\n",
      "[nstep: 752] loss: 0.007648827973753214\n",
      "[step: 753] loss: 0.0007563348626717925\n",
      "[nstep: 753] loss: 0.0076237148605287075\n",
      "[step: 754] loss: 0.0009672724409028888\n",
      "[nstep: 754] loss: 0.007622423581779003\n",
      "[step: 755] loss: 0.0008354432065971196\n",
      "[nstep: 755] loss: 0.007641313131898642\n",
      "[step: 756] loss: 0.0007493350422009826\n",
      "[nstep: 756] loss: 0.0076376828365027905\n",
      "[step: 757] loss: 0.0009104543132707477\n",
      "[nstep: 757] loss: 0.007615394424647093\n",
      "[step: 758] loss: 0.0007870567496865988\n",
      "[nstep: 758] loss: 0.007620769087225199\n",
      "[step: 759] loss: 0.0007471864228136837\n",
      "[nstep: 759] loss: 0.007643088232725859\n",
      "[step: 760] loss: 0.0008569727651774883\n",
      "[nstep: 760] loss: 0.007659540977329016\n",
      "[step: 761] loss: 0.0007537025376223028\n",
      "[nstep: 761] loss: 0.007665390148758888\n",
      "[step: 762] loss: 0.0007467006798833609\n",
      "[nstep: 762] loss: 0.007708596996963024\n",
      "[step: 763] loss: 0.0008130494388751686\n",
      "[nstep: 763] loss: 0.00776285445317626\n",
      "[step: 764] loss: 0.0007303136517293751\n",
      "[nstep: 764] loss: 0.007827037945389748\n",
      "[step: 765] loss: 0.0007438288303092122\n",
      "[nstep: 765] loss: 0.007809826172888279\n",
      "[step: 766] loss: 0.0007786240894347429\n",
      "[nstep: 766] loss: 0.007758189924061298\n",
      "[step: 767] loss: 0.000713389425072819\n",
      "[nstep: 767] loss: 0.0076613654382526875\n",
      "[step: 768] loss: 0.0007385617936961353\n",
      "[nstep: 768] loss: 0.007610451430082321\n",
      "[step: 769] loss: 0.0007531767478212714\n",
      "[nstep: 769] loss: 0.007590020075440407\n",
      "[step: 770] loss: 0.0007054657908156514\n",
      "[nstep: 770] loss: 0.0076088192872703075\n",
      "[step: 771] loss: 0.0007302429294213653\n",
      "[nstep: 771] loss: 0.007658489979803562\n",
      "[step: 772] loss: 0.0007331937085837126\n",
      "[nstep: 772] loss: 0.007663799449801445\n",
      "[step: 773] loss: 0.000702084566000849\n",
      "[nstep: 773] loss: 0.0076189166866242886\n",
      "[step: 774] loss: 0.0007209003088064492\n",
      "[nstep: 774] loss: 0.007560033816844225\n",
      "[step: 775] loss: 0.0007187975570559502\n",
      "[nstep: 775] loss: 0.007548411842435598\n",
      "[step: 776] loss: 0.0006986959488131106\n",
      "[nstep: 776] loss: 0.007573082111775875\n",
      "[step: 777] loss: 0.0007129788864403963\n",
      "[nstep: 777] loss: 0.007593491580337286\n",
      "[step: 778] loss: 0.000709360174369067\n",
      "[nstep: 778] loss: 0.007596002891659737\n",
      "[step: 779] loss: 0.0006942996988072991\n",
      "[nstep: 779] loss: 0.0075693936087191105\n",
      "[step: 780] loss: 0.0007059621275402606\n",
      "[nstep: 780] loss: 0.007545337080955505\n",
      "[step: 781] loss: 0.0007023645448498428\n",
      "[nstep: 781] loss: 0.007532026153057814\n",
      "[step: 782] loss: 0.0006905722548253834\n",
      "[nstep: 782] loss: 0.007528797257691622\n",
      "[step: 783] loss: 0.0006991567206569016\n",
      "[nstep: 783] loss: 0.007533104158937931\n",
      "[step: 784] loss: 0.0006966484943404794\n",
      "[nstep: 784] loss: 0.007541820872575045\n",
      "[step: 785] loss: 0.0006879987195134163\n",
      "[nstep: 785] loss: 0.007549131754785776\n",
      "[step: 786] loss: 0.0006929629016667604\n",
      "[nstep: 786] loss: 0.007542985957115889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 787] loss: 0.0006918180733919144\n",
      "[nstep: 787] loss: 0.007529277354478836\n",
      "[step: 788] loss: 0.0006851570797152817\n",
      "[nstep: 788] loss: 0.007512989919632673\n",
      "[step: 789] loss: 0.0006877685664221644\n",
      "[nstep: 789] loss: 0.007500489242374897\n",
      "[step: 790] loss: 0.0006878685671836138\n",
      "[nstep: 790] loss: 0.0074928863905370235\n",
      "[step: 791] loss: 0.0006820707931183279\n",
      "[nstep: 791] loss: 0.007491079159080982\n",
      "[step: 792] loss: 0.0006831229547969997\n",
      "[nstep: 792] loss: 0.00749216927215457\n",
      "[step: 793] loss: 0.000684195663779974\n",
      "[nstep: 793] loss: 0.007492645177990198\n",
      "[step: 794] loss: 0.0006795864901505411\n",
      "[nstep: 794] loss: 0.007493751123547554\n",
      "[step: 795] loss: 0.0006789896870031953\n",
      "[nstep: 795] loss: 0.00749692739918828\n",
      "[step: 796] loss: 0.0006802294519729912\n",
      "[nstep: 796] loss: 0.007504208479076624\n",
      "[step: 797] loss: 0.0006773778004571795\n",
      "[nstep: 797] loss: 0.007513420190662146\n",
      "[step: 798] loss: 0.0006756591610610485\n",
      "[nstep: 798] loss: 0.007532837800681591\n",
      "[step: 799] loss: 0.0006764737772755325\n",
      "[nstep: 799] loss: 0.007556064985692501\n",
      "[step: 800] loss: 0.0006749733001925051\n",
      "[nstep: 800] loss: 0.007606478873640299\n",
      "[step: 801] loss: 0.0006727091385982931\n",
      "[nstep: 801] loss: 0.007648158818483353\n",
      "[step: 802] loss: 0.0006729763699695468\n",
      "[nstep: 802] loss: 0.00772472657263279\n",
      "[step: 803] loss: 0.0006724761333316565\n",
      "[nstep: 803] loss: 0.0077175418846309185\n",
      "[step: 804] loss: 0.0006702681421302259\n",
      "[nstep: 804] loss: 0.007682258728891611\n",
      "[step: 805] loss: 0.0006696080672554672\n",
      "[nstep: 805] loss: 0.0075574410147964954\n",
      "[step: 806] loss: 0.0006695886258967221\n",
      "[nstep: 806] loss: 0.007474728859961033\n",
      "[step: 807] loss: 0.0006681320955976844\n",
      "[nstep: 807] loss: 0.007471059449017048\n",
      "[step: 808] loss: 0.0006667868583463132\n",
      "[nstep: 808] loss: 0.007509076502174139\n",
      "[step: 809] loss: 0.0006665309192612767\n",
      "[nstep: 809] loss: 0.007534375414252281\n",
      "[step: 810] loss: 0.0006658077472820878\n",
      "[nstep: 810] loss: 0.007514820899814367\n",
      "[step: 811] loss: 0.0006643796223215759\n",
      "[nstep: 811] loss: 0.007495906203985214\n",
      "[step: 812] loss: 0.0006636323523707688\n",
      "[nstep: 812] loss: 0.007480165455490351\n",
      "[step: 813] loss: 0.0006632644799537957\n",
      "[nstep: 813] loss: 0.007461798842996359\n",
      "[step: 814] loss: 0.0006622013170272112\n",
      "[nstep: 814] loss: 0.007440620567649603\n",
      "[step: 815] loss: 0.0006610632408410311\n",
      "[nstep: 815] loss: 0.007435149513185024\n",
      "[step: 816] loss: 0.000660496938508004\n",
      "[nstep: 816] loss: 0.007455125916749239\n",
      "[step: 817] loss: 0.0006598844192922115\n",
      "[nstep: 817] loss: 0.007469320669770241\n",
      "[step: 818] loss: 0.0006588579853996634\n",
      "[nstep: 818] loss: 0.0074567776173353195\n",
      "[step: 819] loss: 0.0006579326000064611\n",
      "[nstep: 819] loss: 0.007424143608659506\n",
      "[step: 820] loss: 0.0006573464488610625\n",
      "[nstep: 820] loss: 0.00740513950586319\n",
      "[step: 821] loss: 0.000656621647067368\n",
      "[nstep: 821] loss: 0.007411684840917587\n",
      "[step: 822] loss: 0.0006556498701684177\n",
      "[nstep: 822] loss: 0.007427832577377558\n",
      "[step: 823] loss: 0.00065484584774822\n",
      "[nstep: 823] loss: 0.007435992360115051\n",
      "[step: 824] loss: 0.0006542230257764459\n",
      "[nstep: 824] loss: 0.007441626861691475\n",
      "[step: 825] loss: 0.000653474940918386\n",
      "[nstep: 825] loss: 0.0074670445173978806\n",
      "[step: 826] loss: 0.0006525834905914962\n",
      "[nstep: 826] loss: 0.00752995116636157\n",
      "[step: 827] loss: 0.000651794602163136\n",
      "[nstep: 827] loss: 0.007658136542886496\n",
      "[step: 828] loss: 0.0006511370302177966\n",
      "[nstep: 828] loss: 0.007816378958523273\n",
      "[step: 829] loss: 0.0006504031480289996\n",
      "[nstep: 829] loss: 0.00793182197958231\n",
      "[step: 830] loss: 0.0006495697889477015\n",
      "[nstep: 830] loss: 0.007823963649570942\n",
      "[step: 831] loss: 0.0006488017970696092\n",
      "[nstep: 831] loss: 0.007564992643892765\n",
      "[step: 832] loss: 0.0006481173331849277\n",
      "[nstep: 832] loss: 0.007464543450623751\n",
      "[step: 833] loss: 0.0006474010879173875\n",
      "[nstep: 833] loss: 0.00753451231867075\n",
      "[step: 834] loss: 0.000646614411380142\n",
      "[nstep: 834] loss: 0.007562217302620411\n",
      "[step: 835] loss: 0.0006458425777964294\n",
      "[nstep: 835] loss: 0.00749123515561223\n",
      "[step: 836] loss: 0.0006451400695368648\n",
      "[nstep: 836] loss: 0.007461379282176495\n",
      "[step: 837] loss: 0.0006444426835514605\n",
      "[nstep: 837] loss: 0.007478334475308657\n",
      "[step: 838] loss: 0.00064369838219136\n",
      "[nstep: 838] loss: 0.0074493661522865295\n",
      "[step: 839] loss: 0.0006429427303373814\n",
      "[nstep: 839] loss: 0.0074106729589402676\n",
      "[step: 840] loss: 0.0006422218284569681\n",
      "[nstep: 840] loss: 0.007436885964125395\n",
      "[step: 841] loss: 0.0006415271782316267\n",
      "[nstep: 841] loss: 0.007429939694702625\n",
      "[step: 842] loss: 0.0006408147164620459\n",
      "[nstep: 842] loss: 0.007382705342024565\n",
      "[step: 843] loss: 0.0006400776910595596\n",
      "[nstep: 843] loss: 0.007394028827548027\n",
      "[step: 844] loss: 0.0006393499206751585\n",
      "[nstep: 844] loss: 0.007417178247123957\n",
      "[step: 845] loss: 0.0006386478198692203\n",
      "[nstep: 845] loss: 0.007379820104688406\n",
      "[step: 846] loss: 0.0006379532278515399\n",
      "[nstep: 846] loss: 0.007346966303884983\n",
      "[step: 847] loss: 0.000637246121186763\n",
      "[nstep: 847] loss: 0.007375376299023628\n",
      "[step: 848] loss: 0.0006365280714817345\n",
      "[nstep: 848] loss: 0.007376858964562416\n",
      "[step: 849] loss: 0.0006358186365105212\n",
      "[nstep: 849] loss: 0.007332929410040379\n",
      "[step: 850] loss: 0.0006351248011924326\n",
      "[nstep: 850] loss: 0.007330795284360647\n",
      "[step: 851] loss: 0.0006344346911646426\n",
      "[nstep: 851] loss: 0.0073546236380934715\n",
      "[step: 852] loss: 0.0006337369559332728\n",
      "[nstep: 852] loss: 0.007340339943766594\n",
      "[step: 853] loss: 0.0006330346222966909\n",
      "[nstep: 853] loss: 0.007315450813621283\n",
      "[step: 854] loss: 0.0006323360721580684\n",
      "[nstep: 854] loss: 0.007322816178202629\n",
      "[step: 855] loss: 0.000631646893452853\n",
      "[nstep: 855] loss: 0.007331317290663719\n",
      "[step: 856] loss: 0.0006309630116447806\n",
      "[nstep: 856] loss: 0.007320123724639416\n",
      "[step: 857] loss: 0.000630276626907289\n",
      "[nstep: 857] loss: 0.007312658708542585\n",
      "[step: 858] loss: 0.0006295878556557\n",
      "[nstep: 858] loss: 0.007327748462557793\n",
      "[step: 859] loss: 0.0006288987933658063\n",
      "[nstep: 859] loss: 0.007339033298194408\n",
      "[step: 860] loss: 0.0006282152025960386\n",
      "[nstep: 860] loss: 0.007354786153882742\n",
      "[step: 861] loss: 0.0006275370833463967\n",
      "[nstep: 861] loss: 0.007389935664832592\n",
      "[step: 862] loss: 0.0006268610013648868\n",
      "[nstep: 862] loss: 0.007480043452233076\n",
      "[step: 863] loss: 0.0006261842790991068\n",
      "[nstep: 863] loss: 0.007582193706184626\n",
      "[step: 864] loss: 0.000625506741926074\n",
      "[nstep: 864] loss: 0.007767699658870697\n",
      "[step: 865] loss: 0.0006248308927752078\n",
      "[nstep: 865] loss: 0.007800287567079067\n",
      "[step: 866] loss: 0.000624158710706979\n",
      "[nstep: 866] loss: 0.007798071950674057\n",
      "[step: 867] loss: 0.000623490137513727\n",
      "[nstep: 867] loss: 0.007478110026568174\n",
      "[step: 868] loss: 0.0006228246493265033\n",
      "[nstep: 868] loss: 0.007295252289623022\n",
      "[step: 869] loss: 0.0006221616640686989\n",
      "[nstep: 869] loss: 0.007366552017629147\n",
      "[step: 870] loss: 0.0006215018220245838\n",
      "[nstep: 870] loss: 0.007493272889405489\n",
      "[step: 871] loss: 0.000620847858954221\n",
      "[nstep: 871] loss: 0.007501423358917236\n",
      "[step: 872] loss: 0.0006202053045853972\n",
      "[nstep: 872] loss: 0.007350326515734196\n",
      "[step: 873] loss: 0.0006195830646902323\n",
      "[nstep: 873] loss: 0.007280103396624327\n",
      "[step: 874] loss: 0.0006189967389218509\n",
      "[nstep: 874] loss: 0.007347454782575369\n",
      "[step: 875] loss: 0.0006184775847941637\n",
      "[nstep: 875] loss: 0.007387095596641302\n",
      "[step: 876] loss: 0.0006180884665809572\n",
      "[nstep: 876] loss: 0.007331993896514177\n",
      "[step: 877] loss: 0.00061795674264431\n",
      "[nstep: 877] loss: 0.007277395110577345\n",
      "[step: 878] loss: 0.0006183505756780505\n",
      "[nstep: 878] loss: 0.007302603684365749\n",
      "[step: 879] loss: 0.0006198004120960832\n",
      "[nstep: 879] loss: 0.0073096570558846\n",
      "[step: 880] loss: 0.0006234688917174935\n",
      "[nstep: 880] loss: 0.007269567809998989\n",
      "[step: 881] loss: 0.0006315088248811662\n",
      "[nstep: 881] loss: 0.0072656236588954926\n",
      "[step: 882] loss: 0.0006485707708634436\n",
      "[nstep: 882] loss: 0.007284525316208601\n",
      "[step: 883] loss: 0.0006807795725762844\n",
      "[nstep: 883] loss: 0.007266503758728504\n",
      "[step: 884] loss: 0.0007373223197646439\n",
      "[nstep: 884] loss: 0.007239592261612415\n",
      "[step: 885] loss: 0.0008058636449277401\n",
      "[nstep: 885] loss: 0.007237465586513281\n",
      "[step: 886] loss: 0.0008473088964819908\n",
      "[nstep: 886] loss: 0.007245247717946768\n",
      "[step: 887] loss: 0.0007922675576992333\n",
      "[nstep: 887] loss: 0.007245778106153011\n",
      "[step: 888] loss: 0.0006763892015442252\n",
      "[nstep: 888] loss: 0.007234995253384113\n",
      "[step: 889] loss: 0.0006406871252693236\n",
      "[nstep: 889] loss: 0.00721500301733613\n",
      "[step: 890] loss: 0.000704890233464539\n",
      "[nstep: 890] loss: 0.007208467926830053\n",
      "[step: 891] loss: 0.0007237858953885734\n",
      "[nstep: 891] loss: 0.007218820508569479\n",
      "[step: 892] loss: 0.0006469198851846159\n",
      "[nstep: 892] loss: 0.007217547856271267\n",
      "[step: 893] loss: 0.0006270214798860252\n",
      "[nstep: 893] loss: 0.007200540509074926\n",
      "[step: 894] loss: 0.0006706816493533552\n",
      "[nstep: 894] loss: 0.007192935328930616\n",
      "[step: 895] loss: 0.0006502405158244073\n",
      "[nstep: 895] loss: 0.007198087405413389\n",
      "[step: 896] loss: 0.0006107171066105366\n",
      "[nstep: 896] loss: 0.007196845952421427\n",
      "[step: 897] loss: 0.0006378352409228683\n",
      "[nstep: 897] loss: 0.007187116425484419\n",
      "[step: 898] loss: 0.0006501043098978698\n",
      "[nstep: 898] loss: 0.007180012762546539\n",
      "[step: 899] loss: 0.0006170328706502914\n",
      "[nstep: 899] loss: 0.00717686302959919\n",
      "[step: 900] loss: 0.0006234886823222041\n",
      "[nstep: 900] loss: 0.007175297476351261\n",
      "[step: 901] loss: 0.0006410495261661708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 901] loss: 0.007174004800617695\n",
      "[step: 902] loss: 0.0006202742806635797\n",
      "[nstep: 902] loss: 0.007171496283262968\n",
      "[step: 903] loss: 0.0006133790011517704\n",
      "[nstep: 903] loss: 0.007163562346249819\n",
      "[step: 904] loss: 0.0006287592113949358\n",
      "[nstep: 904] loss: 0.007155543193221092\n",
      "[step: 905] loss: 0.0006177592440508306\n",
      "[nstep: 905] loss: 0.007152894511818886\n",
      "[step: 906] loss: 0.0006056653801351786\n",
      "[nstep: 906] loss: 0.007153225131332874\n",
      "[step: 907] loss: 0.0006155953160487115\n",
      "[nstep: 907] loss: 0.007151838857680559\n",
      "[step: 908] loss: 0.0006137414602562785\n",
      "[nstep: 908] loss: 0.007149045355618\n",
      "[step: 909] loss: 0.0006017704727128148\n",
      "[nstep: 909] loss: 0.007148876786231995\n",
      "[step: 910] loss: 0.0006058824947103858\n",
      "[nstep: 910] loss: 0.0071525732055306435\n",
      "[step: 911] loss: 0.0006092640687711537\n",
      "[nstep: 911] loss: 0.007163806818425655\n",
      "[step: 912] loss: 0.0006002035224810243\n",
      "[nstep: 912] loss: 0.007185225374996662\n",
      "[step: 913] loss: 0.0005993100930936635\n",
      "[nstep: 913] loss: 0.007223629858344793\n",
      "[step: 914] loss: 0.0006041405722498894\n",
      "[nstep: 914] loss: 0.007246508728712797\n",
      "[step: 915] loss: 0.0005995947867631912\n",
      "[nstep: 915] loss: 0.007255838252604008\n",
      "[step: 916] loss: 0.000595636258367449\n",
      "[nstep: 916] loss: 0.007223737891763449\n",
      "[step: 917] loss: 0.000599284830968827\n",
      "[nstep: 917] loss: 0.007215628866106272\n",
      "[step: 918] loss: 0.0005986755131743848\n",
      "[nstep: 918] loss: 0.007245570886880159\n",
      "[step: 919] loss: 0.0005944475997239351\n",
      "[nstep: 919] loss: 0.0073090665973722935\n",
      "[step: 920] loss: 0.0005960215930826962\n",
      "[nstep: 920] loss: 0.007336436305195093\n",
      "[step: 921] loss: 0.0005983857554383576\n",
      "[nstep: 921] loss: 0.007314097136259079\n",
      "[step: 922] loss: 0.0005970419151708484\n",
      "[nstep: 922] loss: 0.007255855947732925\n",
      "[step: 923] loss: 0.0005990837817080319\n",
      "[nstep: 923] loss: 0.007206742651760578\n",
      "[step: 924] loss: 0.0006068737129680812\n",
      "[nstep: 924] loss: 0.00716487318277359\n",
      "[step: 925] loss: 0.0006167222745716572\n",
      "[nstep: 925] loss: 0.007129186764359474\n",
      "[step: 926] loss: 0.000635950593277812\n",
      "[nstep: 926] loss: 0.007103671785444021\n",
      "[step: 927] loss: 0.0006755047361366451\n",
      "[nstep: 927] loss: 0.007110934238880873\n",
      "[step: 928] loss: 0.0007505192770622671\n",
      "[nstep: 928] loss: 0.007145950570702553\n",
      "[step: 929] loss: 0.0008751247078180313\n",
      "[nstep: 929] loss: 0.0071699186228215694\n",
      "[step: 930] loss: 0.0010948742274194956\n",
      "[nstep: 930] loss: 0.007162963971495628\n",
      "[step: 931] loss: 0.0013434520224109292\n",
      "[nstep: 931] loss: 0.0071255420334637165\n",
      "[step: 932] loss: 0.0015940347220748663\n",
      "[nstep: 932] loss: 0.007095785811543465\n",
      "[step: 933] loss: 0.0014191195368766785\n",
      "[nstep: 933] loss: 0.007086844649165869\n",
      "[step: 934] loss: 0.0009993771091103554\n",
      "[nstep: 934] loss: 0.007091724779456854\n",
      "[step: 935] loss: 0.0006216033361852169\n",
      "[nstep: 935] loss: 0.00709568290039897\n",
      "[step: 936] loss: 0.0007212699856609106\n",
      "[nstep: 936] loss: 0.007092954125255346\n",
      "[step: 937] loss: 0.0010031394194811583\n",
      "[nstep: 937] loss: 0.00708388676866889\n",
      "[step: 938] loss: 0.0009156041196547449\n",
      "[nstep: 938] loss: 0.0070720696821808815\n",
      "[step: 939] loss: 0.0006603859947063029\n",
      "[nstep: 939] loss: 0.007064455654472113\n",
      "[step: 940] loss: 0.0006432860973291099\n",
      "[nstep: 940] loss: 0.007062443066388369\n",
      "[step: 941] loss: 0.0008082560379989445\n",
      "[nstep: 941] loss: 0.007061820011585951\n",
      "[step: 942] loss: 0.0008001216920092702\n",
      "[nstep: 942] loss: 0.007059323601424694\n",
      "[step: 943] loss: 0.000628211535513401\n",
      "[nstep: 943] loss: 0.007054819725453854\n",
      "[step: 944] loss: 0.000638276687823236\n",
      "[nstep: 944] loss: 0.007052193395793438\n",
      "[step: 945] loss: 0.0007565220003016293\n",
      "[nstep: 945] loss: 0.007054315414279699\n",
      "[step: 946] loss: 0.0006945018540136516\n",
      "[nstep: 946] loss: 0.007061491720378399\n",
      "[step: 947] loss: 0.0005944892764091492\n",
      "[nstep: 947] loss: 0.0070784124545753\n",
      "[step: 948] loss: 0.0006474948022514582\n",
      "[nstep: 948] loss: 0.0071062687784433365\n",
      "[step: 949] loss: 0.000695105001796037\n",
      "[nstep: 949] loss: 0.007158097345381975\n",
      "[step: 950] loss: 0.0006230495637282729\n",
      "[nstep: 950] loss: 0.007216774392873049\n",
      "[step: 951] loss: 0.0005920293624512851\n",
      "[nstep: 951] loss: 0.007304086349904537\n",
      "[step: 952] loss: 0.0006482447497546673\n",
      "[nstep: 952] loss: 0.007331342436373234\n",
      "[step: 953] loss: 0.0006445777835324407\n",
      "[nstep: 953] loss: 0.007368571124970913\n",
      "[step: 954] loss: 0.0005925567238591611\n",
      "[nstep: 954] loss: 0.007289649453014135\n",
      "[step: 955] loss: 0.0006033916724845767\n",
      "[nstep: 955] loss: 0.007215072866529226\n",
      "[step: 956] loss: 0.0006301904213614762\n",
      "[nstep: 956] loss: 0.007132681552320719\n",
      "[step: 957] loss: 0.0006081326864659786\n",
      "[nstep: 957] loss: 0.007105185184627771\n",
      "[step: 958] loss: 0.0005882057594135404\n",
      "[nstep: 958] loss: 0.007128583267331123\n",
      "[step: 959] loss: 0.0005994280218146741\n",
      "[nstep: 959] loss: 0.007137266453355551\n",
      "[step: 960] loss: 0.000607694557402283\n",
      "[nstep: 960] loss: 0.007104602642357349\n",
      "[step: 961] loss: 0.0005949413171038032\n",
      "[nstep: 961] loss: 0.007043938618153334\n",
      "[step: 962] loss: 0.0005824457039125264\n",
      "[nstep: 962] loss: 0.007020054385066032\n",
      "[step: 963] loss: 0.0005908297025598586\n",
      "[nstep: 963] loss: 0.007053183391690254\n",
      "[step: 964] loss: 0.0005981670692563057\n",
      "[nstep: 964] loss: 0.007090411614626646\n",
      "[step: 965] loss: 0.00058446527691558\n",
      "[nstep: 965] loss: 0.0070772916078567505\n",
      "[step: 966] loss: 0.0005764506640844047\n",
      "[nstep: 966] loss: 0.007029242347925901\n",
      "[step: 967] loss: 0.0005874470807611942\n",
      "[nstep: 967] loss: 0.006999830715358257\n",
      "[step: 968] loss: 0.0005882462137378752\n",
      "[nstep: 968] loss: 0.0070112464018166065\n",
      "[step: 969] loss: 0.000575828889850527\n",
      "[nstep: 969] loss: 0.007023809012025595\n",
      "[step: 970] loss: 0.0005750777781940997\n",
      "[nstep: 970] loss: 0.007010174915194511\n",
      "[step: 971] loss: 0.0005820833030156791\n",
      "[nstep: 971] loss: 0.00697949668392539\n",
      "[step: 972] loss: 0.0005793541204184294\n",
      "[nstep: 972] loss: 0.006962959188967943\n",
      "[step: 973] loss: 0.0005726425442844629\n",
      "[nstep: 973] loss: 0.0069705708883702755\n",
      "[step: 974] loss: 0.0005726870731450617\n",
      "[nstep: 974] loss: 0.006984603591263294\n",
      "[step: 975] loss: 0.0005753206787630916\n",
      "[nstep: 975] loss: 0.006987597793340683\n",
      "[step: 976] loss: 0.0005743438377976418\n",
      "[nstep: 976] loss: 0.006976078264415264\n",
      "[step: 977] loss: 0.0005704350769519806\n",
      "[nstep: 977] loss: 0.006964958272874355\n",
      "[step: 978] loss: 0.0005685632931999862\n",
      "[nstep: 978] loss: 0.006964512635022402\n",
      "[step: 979] loss: 0.0005703297792933881\n",
      "[nstep: 979] loss: 0.006975097581744194\n",
      "[step: 980] loss: 0.0005709687829948962\n",
      "[nstep: 980] loss: 0.006987444125115871\n",
      "[step: 981] loss: 0.000567563169170171\n",
      "[nstep: 981] loss: 0.006996462121605873\n",
      "[step: 982] loss: 0.0005652480758726597\n",
      "[nstep: 982] loss: 0.007002574857324362\n",
      "[step: 983] loss: 0.0005666598444804549\n",
      "[nstep: 983] loss: 0.007017550524324179\n",
      "[step: 984] loss: 0.0005672464030794799\n",
      "[nstep: 984] loss: 0.007041077129542828\n",
      "[step: 985] loss: 0.0005649455124512315\n",
      "[nstep: 985] loss: 0.007085850462317467\n",
      "[step: 986] loss: 0.0005630434607155621\n",
      "[nstep: 986] loss: 0.007121254224330187\n",
      "[step: 987] loss: 0.0005632221582345665\n",
      "[nstep: 987] loss: 0.007162718567997217\n",
      "[step: 988] loss: 0.0005634905537590384\n",
      "[nstep: 988] loss: 0.0071472786366939545\n",
      "[step: 989] loss: 0.0005626355996355414\n",
      "[nstep: 989] loss: 0.007112727500498295\n",
      "[step: 990] loss: 0.0005612728418782353\n",
      "[nstep: 990] loss: 0.007038578856736422\n",
      "[step: 991] loss: 0.0005602623568847775\n",
      "[nstep: 991] loss: 0.006979088298976421\n",
      "[step: 992] loss: 0.0005600238800980151\n",
      "[nstep: 992] loss: 0.006960559170693159\n",
      "[step: 993] loss: 0.0005600775475613773\n",
      "[nstep: 993] loss: 0.006985528394579887\n",
      "[step: 994] loss: 0.0005593570531345904\n",
      "[nstep: 994] loss: 0.007033398374915123\n",
      "[step: 995] loss: 0.0005580284050665796\n",
      "[nstep: 995] loss: 0.007028681226074696\n",
      "[step: 996] loss: 0.0005572372465394437\n",
      "[nstep: 996] loss: 0.006984076928347349\n",
      "[step: 997] loss: 0.0005572005175054073\n",
      "[nstep: 997] loss: 0.006929489783942699\n",
      "[step: 998] loss: 0.0005569498753175139\n",
      "[nstep: 998] loss: 0.006923415698111057\n",
      "[step: 999] loss: 0.0005561055149883032\n",
      "[nstep: 999] loss: 0.0069435108453035355\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    mpl.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/201709.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/eliminated_1.txt',delimiter = ',',dtype = np.float32)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "data_max = np.max(b,1)\n",
    "data_min = np.min(b,1)\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "epsilon = 1\n",
    "e = math.exp(epsilon)\n",
    "q = (1/(e+1))\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "\n",
    "\n",
    "train_set = xy[:train_size]\n",
    "test_set = xy[train_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "test_set_with_noise = xy_with_noise[train_size:]\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL만들기\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "   # multi_cell1 = tf.contrib.rnn.MultiRNNCell([cell1]*2)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_mean(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    #multi_cell2 =  tf.contrib.rnn.MultiRNNCell([cell2]*2)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])*2\n",
    "x2 = x1+0.33*2\n",
    "x3 = x2+0.33*2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[nstep: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:testX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:testX_with_noise})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래데이터로 되돌리기\n",
    "def ReturnToOriginal(data,data_max,data_min):\n",
    "    #og = data*(data_max+1e-7)-data_min*(data-1)\n",
    "    data_max = data_max+(1e-7)\n",
    "    for i in range(0,len(data)):\n",
    "        data[i,:]= (data[i,:]*data_max[i])-(data[i,:]-1)*data_min[i]\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = ReturnToOriginal(test_predict,data_max[train_size+seq_length:],data_min[train_size+seq_length:])\n",
    "testY_to_original = ReturnToOriginal(testY,data_max[train_size+seq_length:],data_min[train_size+seq_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t\"sigungu\": {\n",
      "\t\t\"Seoul_Jongno-gu\": \"[ 8067.6895  8216.719   6043.035   5347.1055  3820.9094  3293.149\\n  2495.1     3212.4004  3295.1328  4205.0215  4524.338   7686.716\\n 11480.725  16184.938  16283.29   18204.732  17976.297  20918.969\\n 18204.979  18019.928  15685.555  17692.756  15741.231  15578.643\\n 14200.642  15789.244  14433.665  14706.007  12959.396  13065.931\\n 11506.864  11658.321  10233.797  10249.063   8922.2705  9720.244\\n  8863.919   9699.193   9208.809  11027.163  10449.341  12083.042\\n 10901.754  12564.617  11354.58   13418.322  12430.914  13762.162\\n 10422.312  10485.199   7969.461   6896.9805  5257.505   4671.9478\\n  3660.773   4035.007   3968.3933  4594.192   4353.2227  5780.353\\n  6722.915   9584.264  10934.264  14615.81   15582.412  17898.686\\n 15954.615  16241.091  13999.97   15842.749  14546.552  15106.494\\n 13344.145  13623.642  11926.414  12534.777  11166.13   11580.072\\n 10287.557  11540.177  10373.832  11046.9     9666.647  10022.588\\n  9064.313   9799.574   8941.341   9632.355   8830.592  10328.99\\n  9523.968  10840.754   9402.861  11085.371  10200.289 ]\",\n",
      "\t\t\"Seoul_Jung-gu\": \"[ 8062.6304  7467.723   5480.6514  4856.7153  3445.7227  3021.6738\\n  2377.4084  3572.663   3790.168   4910.5483  5099.7607  8655.649\\n 12583.036  16728.629  15894.998  17919.182  17218.727  20787.615\\n 17513.568  17331.719  14441.261  16489.213  14672.865  14728.306\\n 12890.838  14826.559  13446.668  13829.777  11911.737  12375.829\\n 10803.774  11268.524   9885.264  10219.019   8990.799  10037.343\\n  9142.804  10168.295   9888.344  12039.741  11049.781  12248.187\\n 10854.9375 12216.111  10874.883  12375.408  11364.748  12410.823\\n  9583.858   8806.264   6707.1797  6132.5547  4808.412   4368.3154\\n  3334.8274  3962.734   4088.6025  4859.3257  4507.2036  6224.583\\n  7284.999  10706.456  11694.303  15480.187  16049.23   18587.361\\n 15508.284  15703.796  12858.508  14907.654  13481.494  14320.685\\n 12198.601  12719.421  10948.285  11852.8    10573.117  11257.209\\n  9948.41   11419.571  10093.231  10872.93    9431.033  10009.202\\n  9152.291  10071.84    9198.163   9904.837   9073.018  10457.988\\n  9601.273  10575.309   9086.845  10474.646   9643.182 ]\",\n",
      "\t\t\"Seoul_Yongsan-gu\": \"[ 9161.691   9323.193   7102.3984  6107.005   4358.9355  3551.481\\n  2624.2227  3612.3374  3756.0847  4615.252   4661.0986  7177.8975\\n  9578.594  13003.337  12880.365  15205.503  15298.967  18648.783\\n 16284.821  16215.514  13822.237  15796.753  14176.221  14340.756\\n 13046.269  14863.41   13570.814  14147.0205 12503.403  13064.725\\n 11648.278  12220.015  11038.958  11446.707  10281.313  11499.209\\n 10642.491  11812.295  11430.707  13593.685  12859.458  14592.5625\\n 13285.298  15069.19   13488.719  15820.684  14429.627  15714.71\\n 11299.779  11422.201   9189.287   8170.274   6510.959   5848.218\\n  4500.715   4835.8584  4813.261   5361.081   4829.0176  5971.7607\\n  6338.7363  8592.045   9370.423  12725.194  13567.979  16454.54\\n 14675.584  15123.522  12623.489  14547.875  13344.29   14275.389\\n 12663.86   13259.741  11700.311  12667.628  11552.396  12342.124\\n 11210.764  12872.238  11720.022  12584.938  11268.044  11846.616\\n 10991.645  12014.857  11201.102  12122.848  11350.781  13089.471\\n 12151.132  13517.928  11935.973  13866.192  12793.273 ]\",\n",
      "\t\t\"Seoul_Seongdong-gu\": \"[ 9170.131   9663.402   7421.3794  6373.3535  4622.146   3881.9214\\n  2845.4492  3497.9565  3184.38    3682.4836  3460.0708  5006.621\\n  6484.283  10062.521  10822.8    12585.042  12839.319  14810.714\\n 12664.936  11849.645  10055.919  11295.389  10358.717  10373.145\\n  9336.451  10762.954   9968.785  10350.772   9055.1455  9561.709\\n  8466.966   9062.932   8209.272   8748.554   7831.298   9025.153\\n  8539.212   9733.558   9632.527  12267.524  12166.1875 14179.518\\n 12861.626  14830.347  13740.857  16105.503  14964.175  16327.698\\n 11074.348  11712.631   9879.204   8975.84    7372.8306  6618.297\\n  5135.714   5371.8267  4862.943   5131.007   4494.288   5126.9604\\n  4887.247   5832.406   6044.5527  8756.598  10052.37   12525.252\\n 10706.076  10323.572   8616.07   10185.4375  9708.715  10387.133\\n  9109.242   9452.881   8123.1714  8972.215   8360.875   9089.342\\n  8329.311   9787.99    9012.31    9988.678   8864.666   9608.932\\n  9136.927  10341.346   9808.198  10872.748  10274.439  12155.325\\n 11641.338  13135.975  11544.584  13415.047  12858.907 ]\",\n",
      "\t\t\"Seoul_Gwangjin-gu\": \"9677.093\",\n",
      "\t\t\"Seoul_Dongdaemun-gu\": \"9536.36\",\n",
      "\t\t\"Seoul_Jungnang-gu\": \"9618.825\",\n",
      "\t\t\"Seoul_Seongbuk-gu\": \"9645.693\",\n",
      "\t\t\"Seoul_Gangbuk-gu\": \"9817.928\",\n",
      "\t\t\"Seoul_Dobong-gu\": \"9708.716\",\n",
      "\t\t\"Seoul_Nowon-gu\": \"9298.5625\",\n",
      "\t\t\"Seoul_Eunpyeong-gu\": \"9198.199\",\n",
      "\t\t\"Seoul_Seodaemun-gu\": \"10082.229\",\n",
      "\t\t\"Seoul_Mapo-gu\": \"9789.997\",\n",
      "\t\t\"Seoul_Yangcheon-gu\": \"9610.391\",\n",
      "\t\t\"Seoul_Gangseo-gu\": \"9546.009\",\n",
      "\t\t\"Seoul_Guro-gu\": \"10437.904\",\n",
      "\t\t\"Seoul_Geumcheon-gu\": \"10239.505\",\n",
      "\t\t\"Seoul_Yeongdeungpo-gu\": \"8952.79\",\n",
      "\t\t\"Seoul_Dongjak-gu\": \"10104.699\",\n",
      "\t\t\"Seoul_Gwanak-gu\": \"10541.255\",\n",
      "\t\t\"Seoul_Seocho-gu\": \"9619.865\",\n",
      "\t\t\"Seoul_Gangnam-gu\": \"8776.719\",\n",
      "\t\t\"Seoul_Songpa-gu\": \"9337.787\",\n",
      "\t\t\"Seoul_Gangdong-gu\": \"9221.111\"\n",
      "\t}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#json 파일로 만들기\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "file_data = OrderedDict()\n",
    "\n",
    "day_time = 0\n",
    "\n",
    "file_data[\"sigungu\"] = {\"Seoul_Jongno-gu\":str(test_predict_to_original[:,0]),\n",
    "                            \"Seoul_Jung-gu\":str(test_predict_to_original[:,1]),\n",
    "                            \"Seoul_Yongsan-gu\":str(test_predict_to_original[:,2]),\n",
    "                            \"Seoul_Seongdong-gu\":str(test_predict_to_original[:,3]),\n",
    "                            \"Seoul_Gwangjin-gu\":str(test_predict_to_original[:,4]),\n",
    "                            \"Seoul_Dongdaemun-gu\":str(test_predict_to_original[:,5]),\n",
    "                            \"Seoul_Jungnang-gu\":str(test_predict_to_original[:,6]),\n",
    "                            \"Seoul_Seongbuk-gu\":str(test_predict_to_original[:,7]),\n",
    "                            \"Seoul_Gangbuk-gu\":str(test_predict_to_original[:,8]),\n",
    "                            \"Seoul_Dobong-gu\":str(test_predict_to_original[:,9]),\n",
    "                            \"Seoul_Nowon-gu\":str(test_predict_to_original[:,10]),\n",
    "                            \"Seoul_Eunpyeong-gu\":str(test_predict_to_original[:,11]),\n",
    "                            \"Seoul_Seodaemun-gu\":str(test_predict_to_original[:,12]),\n",
    "                            \"Seoul_Mapo-gu\":str(test_predict_to_original[:,13]),\n",
    "                            \"Seoul_Yangcheon-gu\":str(test_predict_to_original[:,14]),\n",
    "                            \"Seoul_Gangseo-gu\":str(test_predict_to_original[:,15]),\n",
    "                            \"Seoul_Guro-gu\":str(test_predict_to_original[:,16]),\n",
    "                            \"Seoul_Geumcheon-gu\":str(test_predict_to_original[:,17]),\n",
    "                            \"Seoul_Yeongdeungpo-gu\":str(test_predict_to_original[:,18]),\n",
    "                            \"Seoul_Dongjak-gu\":str(test_predict_to_original[:,19]),\n",
    "                            \"Seoul_Gwanak-gu\":str(test_predict_to_original[:,20]),\n",
    "                            \"Seoul_Seocho-gu\":str(test_predict_to_original[:,21]),\n",
    "                            \"Seoul_Gangnam-gu\":str(test_predict_to_original[:,22]),\n",
    "                            \"Seoul_Songpa-gu\":str(test_predict_to_original[:,23]),\n",
    "                            \"Seoul_Gangdong-gu\":str(test_predict_to_original[:,24])}\n",
    "\n",
    "\n",
    "print(json.dumps(file_data,ensure_ascii =False,indent = \"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yeseo/Desktop/predict_\"+str(2) +\".json\",'w',encoding = \"utf-8\") as make_file:\n",
    "    json.dump(file_data,make_file,ensure_ascii =False,indent = \"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
