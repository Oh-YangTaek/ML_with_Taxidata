{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-69b70e1a8d9c>:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "[step: 0] loss: 0.17290590703487396\n",
      "[step: 500] loss: 0.004770060535520315\n",
      "[step: 1000] loss: 0.0010872113052755594\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "#load data\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data preprocessing\n",
    "train_size = int(len(xy)*0.7)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "test_set = xy[train_size:]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "data_max = np.max(b,1)\n",
    "data_min = np.min(b,1)\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "train_set = xy[:train_size]\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 27\n",
    "output_dim = 25\n",
    "learning_rate = 0.1\n",
    "iterations = 1001\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#make train_set&test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#make LSTM CELL\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    multi_cell1 = tf.contrib.rnn.MultiRNNCell([cell1]*5)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(multi_cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_mean(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        if i%500 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "\n",
    "    \n",
    "    #7th value from initial data\n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:[testX[0,:,:]]})\n",
    "    \n",
    "    #initial data shape = 6,27\n",
    "    initial_data = testX[0,:,:]\n",
    "    \n",
    "    \n",
    "    #day&time for 7th data\n",
    "    if initial_data[5,1] != 47:\n",
    "        day_time = np.array([[initial_data[5,0],(initial_data[5,1]+1)]])\n",
    "    else:\n",
    "        if initial_data[5,0] == 7:\n",
    "            day_time = np.array([[1,0]])\n",
    "        else:\n",
    "            day_time = np.array([[(initial_data[5,0]+1),0]])\n",
    "            \n",
    "    #make 7th data\n",
    "    seventh_data = np.hstack((day_time,test_predict))\n",
    "    \n",
    "    #1~5 data + 7th data\n",
    "    new_testX = np.vstack((initial_data[1:,:],seventh_data))\n",
    "    \n",
    "    #select range to predict\n",
    "    for j in range(48):\n",
    "        #8th value from second data\n",
    "        temp = sess.run(Y_pred,feed_dict = {X1:[new_testX]})\n",
    "        \n",
    "        test_predict = np.vstack((test_predict,temp))\n",
    "        \n",
    "        if new_testX[5,1] != 47:\n",
    "            day_time = np.array([[new_testX[5,0],(new_testX[5,1]+1)]])\n",
    "        else:\n",
    "            if new_testX[5,0] == 7:\n",
    "                day_time = np.array([[1,0]])\n",
    "            else:\n",
    "                day_time = np.array([[(new_testX[5,0]+1),0]])\n",
    "                \n",
    "        seventh_data = np.hstack((day_time,test_predict[[-1],:]))\n",
    "        new_testX = np.vstack((new_testX[1:,:],seventh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data to original\n",
    "def ReturnToOriginal(data,data_max,data_min):\n",
    "    data_max = data_max+(1e-7)\n",
    "    for i in range(0,len(data)):\n",
    "        data[i,:]= (data[i,:]*data_max[i])-(data[i,:]-1)*data_min[i]\n",
    "    return data\n",
    "\n",
    "test_predict_to_original = ReturnToOriginal(test_predict,data_max[train_size+seq_length:],data_min[train_size+seq_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1204, 2), (49, 25))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(testY[:,:2]),np.shape(test_predict_to_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_to_original = np.hstack((testY[:49,:2],test_predict_to_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data to json format\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "file_data = OrderedDict()\n",
    "\n",
    "day_time = 0\n",
    "\n",
    "file_data[\"sigungu\"] = {\"Seoul_Jongno-gu\":str(test_predict_to_original[:,0]),\n",
    "                            \"Seoul_Jung-gu\":str(test_predict_to_original[:,1]),\n",
    "                            \"Seoul_Yongsan-gu\":str(test_predict_to_original[:,2]),\n",
    "                            \"Seoul_Seongdong-gu\":str(test_predict_to_original[:,3]),\n",
    "                            \"Seoul_Gwangjin-gu\":str(test_predict_to_original[:,4]),\n",
    "                            \"Seoul_Dongdaemun-gu\":str(test_predict_to_original[:,5]),\n",
    "                            \"Seoul_Jungnang-gu\":str(test_predict_to_original[:,6]),\n",
    "                            \"Seoul_Seongbuk-gu\":str(test_predict_to_original[:,7]),\n",
    "                            \"Seoul_Gangbuk-gu\":str(test_predict_to_original[:,8]),\n",
    "                            \"Seoul_Dobong-gu\":str(test_predict_to_original[:,9]),\n",
    "                            \"Seoul_Nowon-gu\":str(test_predict_to_original[:,10]),\n",
    "                            \"Seoul_Eunpyeong-gu\":str(test_predict_to_original[:,11]),\n",
    "                            \"Seoul_Seodaemun-gu\":str(test_predict_to_original[:,12]),\n",
    "                            \"Seoul_Mapo-gu\":str(test_predict_to_original[:,13]),\n",
    "                            \"Seoul_Yangcheon-gu\":str(test_predict_to_original[:,14]),\n",
    "                            \"Seoul_Gangseo-gu\":str(test_predict_to_original[:,15]),\n",
    "                            \"Seoul_Guro-gu\":str(test_predict_to_original[:,16]),\n",
    "                            \"Seoul_Geumcheon-gu\":str(test_predict_to_original[:,17]),\n",
    "                            \"Seoul_Yeongdeungpo-gu\":str(test_predict_to_original[:,18]),\n",
    "                            \"Seoul_Dongjak-gu\":str(test_predict_to_original[:,19]),\n",
    "                            \"Seoul_Gwanak-gu\":str(test_predict_to_original[:,20]),\n",
    "                            \"Seoul_Seocho-gu\":str(test_predict_to_original[:,21]),\n",
    "                            \"Seoul_Gangnam-gu\":str(test_predict_to_original[:,22]),\n",
    "                            \"Seoul_Songpa-gu\":str(test_predict_to_original[:,23]),\n",
    "                            \"Seoul_Gangdong-gu\":str(test_predict_to_original[:,24])}\n",
    "\n",
    "#Output to a file\n",
    "with open(\"/Users/yeseo/Desktop/predict_\"+str(2) +\".json\",'w',encoding = \"utf-8\") as make_file:\n",
    "    json.dump(file_data,make_file,ensure_ascii =False,indent = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_predict_to_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
