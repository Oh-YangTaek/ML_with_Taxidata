{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.2825731635093689\n",
      "[nstep: 0] loss: 0.2595043480396271\n",
      "[step: 1] loss: 0.1896384209394455\n",
      "[nstep: 1] loss: 0.15791241824626923\n",
      "[step: 2] loss: 0.13416717946529388\n",
      "[nstep: 2] loss: 0.10182338207960129\n",
      "[step: 3] loss: 0.09712933748960495\n",
      "[nstep: 3] loss: 0.06897313892841339\n",
      "[step: 4] loss: 0.07193728536367416\n",
      "[nstep: 4] loss: 0.04919050633907318\n",
      "[step: 5] loss: 0.0542481429874897\n",
      "[nstep: 5] loss: 0.038430724292993546\n",
      "[step: 6] loss: 0.042936116456985474\n",
      "[nstep: 6] loss: 0.03298943489789963\n",
      "[step: 7] loss: 0.03701147809624672\n",
      "[nstep: 7] loss: 0.03087751939892769\n",
      "[step: 8] loss: 0.03462768718600273\n",
      "[nstep: 8] loss: 0.02905426360666752\n",
      "[step: 9] loss: 0.033082686364650726\n",
      "[nstep: 9] loss: 0.027896417304873466\n",
      "[step: 10] loss: 0.0323486402630806\n",
      "[nstep: 10] loss: 0.02676728367805481\n",
      "[step: 11] loss: 0.03149361535906792\n",
      "[nstep: 11] loss: 0.025134265422821045\n",
      "[step: 12] loss: 0.030025651678442955\n",
      "[nstep: 12] loss: 0.02340037189424038\n",
      "[step: 13] loss: 0.028489528223872185\n",
      "[nstep: 13] loss: 0.021984679624438286\n",
      "[step: 14] loss: 0.02722685970366001\n",
      "[nstep: 14] loss: 0.02085292339324951\n",
      "[step: 15] loss: 0.02583296410739422\n",
      "[nstep: 15] loss: 0.01981799677014351\n",
      "[step: 16] loss: 0.024207117035984993\n",
      "[nstep: 16] loss: 0.018895097076892853\n",
      "[step: 17] loss: 0.02287271060049534\n",
      "[nstep: 17] loss: 0.018185259774327278\n",
      "[step: 18] loss: 0.021644994616508484\n",
      "[nstep: 18] loss: 0.017742116004228592\n",
      "[step: 19] loss: 0.020391594618558884\n",
      "[nstep: 19] loss: 0.017526887357234955\n",
      "[step: 20] loss: 0.019208494573831558\n",
      "[nstep: 20] loss: 0.0173865407705307\n",
      "[step: 21] loss: 0.01812063902616501\n",
      "[nstep: 21] loss: 0.01713973842561245\n",
      "[step: 22] loss: 0.017163340002298355\n",
      "[nstep: 22] loss: 0.01675848662853241\n",
      "[step: 23] loss: 0.016012104228138924\n",
      "[nstep: 23] loss: 0.01636539027094841\n",
      "[step: 24] loss: 0.015104436315596104\n",
      "[nstep: 24] loss: 0.01604532077908516\n",
      "[step: 25] loss: 0.014474364928901196\n",
      "[nstep: 25] loss: 0.015777789056301117\n",
      "[step: 26] loss: 0.013834134675562382\n",
      "[nstep: 26] loss: 0.015460229478776455\n",
      "[step: 27] loss: 0.013493310660123825\n",
      "[nstep: 27] loss: 0.015070164576172829\n",
      "[step: 28] loss: 0.012937563471496105\n",
      "[nstep: 28] loss: 0.014743052423000336\n",
      "[step: 29] loss: 0.012701407074928284\n",
      "[nstep: 29] loss: 0.014430326409637928\n",
      "[step: 30] loss: 0.012183244340121746\n",
      "[nstep: 30] loss: 0.014095541089773178\n",
      "[step: 31] loss: 0.011884148232638836\n",
      "[nstep: 31] loss: 0.013810102827847004\n",
      "[step: 32] loss: 0.011296171694993973\n",
      "[nstep: 32] loss: 0.013499405235052109\n",
      "[step: 33] loss: 0.010935831815004349\n",
      "[nstep: 33] loss: 0.013231383636593819\n",
      "[step: 34] loss: 0.010441363789141178\n",
      "[nstep: 34] loss: 0.012975119985640049\n",
      "[step: 35] loss: 0.010041055269539356\n",
      "[nstep: 35] loss: 0.012737243436276913\n",
      "[step: 36] loss: 0.009609459899365902\n",
      "[nstep: 36] loss: 0.012525836005806923\n",
      "[step: 37] loss: 0.009251278825104237\n",
      "[nstep: 37] loss: 0.012335564009845257\n",
      "[step: 38] loss: 0.008913295343518257\n",
      "[nstep: 38] loss: 0.012141121551394463\n",
      "[step: 39] loss: 0.00860163290053606\n",
      "[nstep: 39] loss: 0.01193370670080185\n",
      "[step: 40] loss: 0.00836372934281826\n",
      "[nstep: 40] loss: 0.011727842502295971\n",
      "[step: 41] loss: 0.008055053651332855\n",
      "[nstep: 41] loss: 0.011500781401991844\n",
      "[step: 42] loss: 0.007832923904061317\n",
      "[nstep: 42] loss: 0.011285359971225262\n",
      "[step: 43] loss: 0.0075289118103682995\n",
      "[nstep: 43] loss: 0.011087927035987377\n",
      "[step: 44] loss: 0.007276792544871569\n",
      "[nstep: 44] loss: 0.01088781002908945\n",
      "[step: 45] loss: 0.00698379660025239\n",
      "[nstep: 45] loss: 0.010694785043597221\n",
      "[step: 46] loss: 0.0066961729899048805\n",
      "[nstep: 46] loss: 0.01054612547159195\n",
      "[step: 47] loss: 0.006434014532715082\n",
      "[nstep: 47] loss: 0.010427556931972504\n",
      "[step: 48] loss: 0.006170038599520922\n",
      "[nstep: 48] loss: 0.010282126255333424\n",
      "[step: 49] loss: 0.00596265122294426\n",
      "[nstep: 49] loss: 0.010141176171600819\n",
      "[step: 50] loss: 0.005769883282482624\n",
      "[nstep: 50] loss: 0.010024403221905231\n",
      "[step: 51] loss: 0.005595071241259575\n",
      "[nstep: 51] loss: 0.009903374128043652\n",
      "[step: 52] loss: 0.0054236724972724915\n",
      "[nstep: 52] loss: 0.009770059026777744\n",
      "[step: 53] loss: 0.0052718548104166985\n",
      "[nstep: 53] loss: 0.009638442657887936\n",
      "[step: 54] loss: 0.005132884252816439\n",
      "[nstep: 54] loss: 0.009533800184726715\n",
      "[step: 55] loss: 0.004990778863430023\n",
      "[nstep: 55] loss: 0.009440697729587555\n",
      "[step: 56] loss: 0.004869325552135706\n",
      "[nstep: 56] loss: 0.009331065230071545\n",
      "[step: 57] loss: 0.004753571469336748\n",
      "[nstep: 57] loss: 0.009224019944667816\n",
      "[step: 58] loss: 0.004642011132091284\n",
      "[nstep: 58] loss: 0.009134139865636826\n",
      "[step: 59] loss: 0.004531476646661758\n",
      "[nstep: 59] loss: 0.009062054567039013\n",
      "[step: 60] loss: 0.004418403375893831\n",
      "[nstep: 60] loss: 0.008990817703306675\n",
      "[step: 61] loss: 0.004311508499085903\n",
      "[nstep: 61] loss: 0.008905291557312012\n",
      "[step: 62] loss: 0.0042101601138710976\n",
      "[nstep: 62] loss: 0.008825304917991161\n",
      "[step: 63] loss: 0.004118701443076134\n",
      "[nstep: 63] loss: 0.008757426403462887\n",
      "[step: 64] loss: 0.00403337087482214\n",
      "[nstep: 64] loss: 0.008699541911482811\n",
      "[step: 65] loss: 0.003961878828704357\n",
      "[nstep: 65] loss: 0.008646496571600437\n",
      "[step: 66] loss: 0.003891051048412919\n",
      "[nstep: 66] loss: 0.008588863536715508\n",
      "[step: 67] loss: 0.00384106975980103\n",
      "[nstep: 67] loss: 0.008529871702194214\n",
      "[step: 68] loss: 0.0038128255400806665\n",
      "[nstep: 68] loss: 0.008471820503473282\n",
      "[step: 69] loss: 0.0038412015419453382\n",
      "[nstep: 69] loss: 0.008421567268669605\n",
      "[step: 70] loss: 0.003876026254147291\n",
      "[nstep: 70] loss: 0.008380569517612457\n",
      "[step: 71] loss: 0.003855567891150713\n",
      "[nstep: 71] loss: 0.008348763920366764\n",
      "[step: 72] loss: 0.0035968353040516376\n",
      "[nstep: 72] loss: 0.008325147442519665\n",
      "[step: 73] loss: 0.003518044715747237\n",
      "[nstep: 73] loss: 0.008285908959805965\n",
      "[step: 74] loss: 0.003615334630012512\n",
      "[nstep: 74] loss: 0.008241243660449982\n",
      "[step: 75] loss: 0.003498972626402974\n",
      "[nstep: 75] loss: 0.008197528310120106\n",
      "[step: 76] loss: 0.00336255319416523\n",
      "[nstep: 76] loss: 0.00816609151661396\n",
      "[step: 77] loss: 0.0034064066130667925\n",
      "[nstep: 77] loss: 0.00814672652631998\n",
      "[step: 78] loss: 0.0033669339027255774\n",
      "[nstep: 78] loss: 0.008126293309032917\n",
      "[step: 79] loss: 0.0032571323681622744\n",
      "[nstep: 79] loss: 0.008106068708002567\n",
      "[step: 80] loss: 0.0032623140141367912\n",
      "[nstep: 80] loss: 0.008078558370471\n",
      "[step: 81] loss: 0.003258547279983759\n",
      "[nstep: 81] loss: 0.008051753975450993\n",
      "[step: 82] loss: 0.00318089104257524\n",
      "[nstep: 82] loss: 0.008022688329219818\n",
      "[step: 83] loss: 0.0031519834883511066\n",
      "[nstep: 83] loss: 0.00799975823611021\n",
      "[step: 84] loss: 0.0031638015061616898\n",
      "[nstep: 84] loss: 0.007975364103913307\n",
      "[step: 85] loss: 0.003118214663118124\n",
      "[nstep: 85] loss: 0.007955597713589668\n",
      "[step: 86] loss: 0.0030626135412603617\n",
      "[nstep: 86] loss: 0.007937153801321983\n",
      "[step: 87] loss: 0.0030674885492771864\n",
      "[nstep: 87] loss: 0.007918891496956348\n",
      "[step: 88] loss: 0.0030580086167901754\n",
      "[nstep: 88] loss: 0.007902086712419987\n",
      "[step: 89] loss: 0.0030017425306141376\n",
      "[nstep: 89] loss: 0.007889960892498493\n",
      "[step: 90] loss: 0.0029703264590352774\n",
      "[nstep: 90] loss: 0.00788482278585434\n",
      "[step: 91] loss: 0.0029709015507251024\n",
      "[nstep: 91] loss: 0.007901179604232311\n",
      "[step: 92] loss: 0.002961608348414302\n",
      "[nstep: 92] loss: 0.007959200069308281\n",
      "[step: 93] loss: 0.0029334663413465023\n",
      "[nstep: 93] loss: 0.007989169098436832\n",
      "[step: 94] loss: 0.0028910308610647917\n",
      "[nstep: 94] loss: 0.007939684204757214\n",
      "[step: 95] loss: 0.002865379210561514\n",
      "[nstep: 95] loss: 0.007822288200259209\n",
      "[step: 96] loss: 0.0028695682995021343\n",
      "[nstep: 96] loss: 0.00786165613681078\n",
      "[step: 97] loss: 0.002859679516404867\n",
      "[nstep: 97] loss: 0.007880279794335365\n",
      "[step: 98] loss: 0.002851479221135378\n",
      "[nstep: 98] loss: 0.007799901068210602\n",
      "[step: 99] loss: 0.002807689132168889\n",
      "[nstep: 99] loss: 0.007814936339855194\n",
      "[step: 100] loss: 0.002781322691589594\n",
      "[nstep: 100] loss: 0.007804203778505325\n",
      "[step: 101] loss: 0.002761750714853406\n",
      "[nstep: 101] loss: 0.007783086039125919\n",
      "[step: 102] loss: 0.0027678993064910173\n",
      "[nstep: 102] loss: 0.00778263621032238\n",
      "[step: 103] loss: 0.002767456229776144\n",
      "[nstep: 103] loss: 0.007745817303657532\n",
      "[step: 104] loss: 0.0027798472438007593\n",
      "[nstep: 104] loss: 0.00776804331690073\n",
      "[step: 105] loss: 0.0028215914499014616\n",
      "[nstep: 105] loss: 0.007712359074503183\n",
      "[step: 106] loss: 0.00281979120336473\n",
      "[nstep: 106] loss: 0.00773618882521987\n",
      "[step: 107] loss: 0.0027593469712883234\n",
      "[nstep: 107] loss: 0.007700541988015175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 108] loss: 0.0027127184439450502\n",
      "[nstep: 108] loss: 0.0076850526966154575\n",
      "[step: 109] loss: 0.002654195064678788\n",
      "[nstep: 109] loss: 0.0076999724842607975\n",
      "[step: 110] loss: 0.0026469624135643244\n",
      "[nstep: 110] loss: 0.007661112118512392\n",
      "[step: 111] loss: 0.0026323869824409485\n",
      "[nstep: 111] loss: 0.007674681954085827\n",
      "[step: 112] loss: 0.0026303455233573914\n",
      "[nstep: 112] loss: 0.007649548351764679\n",
      "[step: 113] loss: 0.002628446090966463\n",
      "[nstep: 113] loss: 0.007656483445316553\n",
      "[step: 114] loss: 0.00262809288688004\n",
      "[nstep: 114] loss: 0.007627964951097965\n",
      "[step: 115] loss: 0.0026157370302826166\n",
      "[nstep: 115] loss: 0.0076329829171299934\n",
      "[step: 116] loss: 0.0025994819588959217\n",
      "[nstep: 116] loss: 0.00762520544230938\n",
      "[step: 117] loss: 0.00257113273255527\n",
      "[nstep: 117] loss: 0.007610914763063192\n",
      "[step: 118] loss: 0.0025726098101586103\n",
      "[nstep: 118] loss: 0.007607107982039452\n",
      "[step: 119] loss: 0.002539076842367649\n",
      "[nstep: 119] loss: 0.007594940718263388\n",
      "[step: 120] loss: 0.0025404999032616615\n",
      "[nstep: 120] loss: 0.0075990743935108185\n",
      "[step: 121] loss: 0.00250334688462317\n",
      "[nstep: 121] loss: 0.00757878553122282\n",
      "[step: 122] loss: 0.0025184310507029295\n",
      "[nstep: 122] loss: 0.00758013129234314\n",
      "[step: 123] loss: 0.0024863893631845713\n",
      "[nstep: 123] loss: 0.007571086753159761\n",
      "[step: 124] loss: 0.0025053059216588736\n",
      "[nstep: 124] loss: 0.0075681679882109165\n",
      "[step: 125] loss: 0.002499910304322839\n",
      "[nstep: 125] loss: 0.0075599756091833115\n",
      "[step: 126] loss: 0.0025828052312135696\n",
      "[nstep: 126] loss: 0.007549946662038565\n",
      "[step: 127] loss: 0.002620894694700837\n",
      "[nstep: 127] loss: 0.00754857761785388\n",
      "[step: 128] loss: 0.0027979337610304356\n",
      "[nstep: 128] loss: 0.007541101425886154\n",
      "[step: 129] loss: 0.0027605842333287\n",
      "[nstep: 129] loss: 0.007537709083408117\n",
      "[step: 130] loss: 0.0026678116992115974\n",
      "[nstep: 130] loss: 0.0075300526805222034\n",
      "[step: 131] loss: 0.0024356963112950325\n",
      "[nstep: 131] loss: 0.007522067055106163\n",
      "[step: 132] loss: 0.002503574825823307\n",
      "[nstep: 132] loss: 0.0075201839208602905\n",
      "[step: 133] loss: 0.002681419951841235\n",
      "[nstep: 133] loss: 0.007512873504310846\n",
      "[step: 134] loss: 0.002511601895093918\n",
      "[nstep: 134] loss: 0.007510414347052574\n",
      "[step: 135] loss: 0.0023765333462506533\n",
      "[nstep: 135] loss: 0.007503542583435774\n",
      "[step: 136] loss: 0.002496663248166442\n",
      "[nstep: 136] loss: 0.007497263140976429\n",
      "[step: 137] loss: 0.0025354961398988962\n",
      "[nstep: 137] loss: 0.007493387442082167\n",
      "[step: 138] loss: 0.002429807325825095\n",
      "[nstep: 138] loss: 0.007487181574106216\n",
      "[step: 139] loss: 0.002414854010567069\n",
      "[nstep: 139] loss: 0.007483772002160549\n",
      "[step: 140] loss: 0.002454740460962057\n",
      "[nstep: 140] loss: 0.007479847874492407\n",
      "[step: 141] loss: 0.0024346199352294207\n",
      "[nstep: 141] loss: 0.007474354933947325\n",
      "[step: 142] loss: 0.002370720263570547\n",
      "[nstep: 142] loss: 0.007471146062016487\n",
      "[step: 143] loss: 0.002350335707888007\n",
      "[nstep: 143] loss: 0.007465176284313202\n",
      "[step: 144] loss: 0.0023792593274265528\n",
      "[nstep: 144] loss: 0.007460612338036299\n",
      "[step: 145] loss: 0.0023790334817022085\n",
      "[nstep: 145] loss: 0.007456552237272263\n",
      "[step: 146] loss: 0.00231012050062418\n",
      "[nstep: 146] loss: 0.0074512236751616\n",
      "[step: 147] loss: 0.0022864146158099174\n",
      "[nstep: 147] loss: 0.007447077892720699\n",
      "[step: 148] loss: 0.002327736234292388\n",
      "[nstep: 148] loss: 0.007443155162036419\n",
      "[step: 149] loss: 0.0023163899313658476\n",
      "[nstep: 149] loss: 0.007438338827341795\n",
      "[step: 150] loss: 0.0022911648266017437\n",
      "[nstep: 150] loss: 0.007434888277202845\n",
      "[step: 151] loss: 0.0022610416635870934\n",
      "[nstep: 151] loss: 0.007431196514517069\n",
      "[step: 152] loss: 0.0022403451148420572\n",
      "[nstep: 152] loss: 0.007428034208714962\n",
      "[step: 153] loss: 0.002260739216580987\n",
      "[nstep: 153] loss: 0.007426898460835218\n",
      "[step: 154] loss: 0.002275089267641306\n",
      "[nstep: 154] loss: 0.007429194170981646\n",
      "[step: 155] loss: 0.0022613871842622757\n",
      "[nstep: 155] loss: 0.007438923232257366\n",
      "[step: 156] loss: 0.0022283352445811033\n",
      "[nstep: 156] loss: 0.0074696349911391735\n",
      "[step: 157] loss: 0.0022009003441780806\n",
      "[nstep: 157] loss: 0.007522763218730688\n",
      "[step: 158] loss: 0.00219751731492579\n",
      "[nstep: 158] loss: 0.007618015632033348\n",
      "[step: 159] loss: 0.0021991212852299213\n",
      "[nstep: 159] loss: 0.007611912675201893\n",
      "[step: 160] loss: 0.002201375551521778\n",
      "[nstep: 160] loss: 0.007510596886277199\n",
      "[step: 161] loss: 0.0022112312726676464\n",
      "[nstep: 161] loss: 0.007395857013761997\n",
      "[step: 162] loss: 0.002200328977778554\n",
      "[nstep: 162] loss: 0.007442113012075424\n",
      "[step: 163] loss: 0.0021934518590569496\n",
      "[nstep: 163] loss: 0.00752399954944849\n",
      "[step: 164] loss: 0.00218097772449255\n",
      "[nstep: 164] loss: 0.007451632991433144\n",
      "[step: 165] loss: 0.002187010832130909\n",
      "[nstep: 165] loss: 0.007383165415376425\n",
      "[step: 166] loss: 0.0021883100271224976\n",
      "[nstep: 166] loss: 0.007422496564686298\n",
      "[step: 167] loss: 0.0022232774645090103\n",
      "[nstep: 167] loss: 0.007445799186825752\n",
      "[step: 168] loss: 0.002271998906508088\n",
      "[nstep: 168] loss: 0.007393410429358482\n",
      "[step: 169] loss: 0.00239438284188509\n",
      "[nstep: 169] loss: 0.007372212130576372\n",
      "[step: 170] loss: 0.002380919875577092\n",
      "[nstep: 170] loss: 0.007413309067487717\n",
      "[step: 171] loss: 0.0023670534137636423\n",
      "[nstep: 171] loss: 0.007407288998365402\n",
      "[step: 172] loss: 0.002201274037361145\n",
      "[nstep: 172] loss: 0.007359698880463839\n",
      "[step: 173] loss: 0.0021186009980738163\n",
      "[nstep: 173] loss: 0.007374668028205633\n",
      "[step: 174] loss: 0.0021387948654592037\n",
      "[nstep: 174] loss: 0.0073958588764071465\n",
      "[step: 175] loss: 0.0022134974133223295\n",
      "[nstep: 175] loss: 0.007363371551036835\n",
      "[step: 176] loss: 0.0022494210861623287\n",
      "[nstep: 176] loss: 0.007348108105361462\n",
      "[step: 177] loss: 0.00219863117672503\n",
      "[nstep: 177] loss: 0.007366334553807974\n",
      "[step: 178] loss: 0.0021057208068668842\n",
      "[nstep: 178] loss: 0.007364408113062382\n",
      "[step: 179] loss: 0.002083640545606613\n",
      "[nstep: 179] loss: 0.007341190706938505\n",
      "[step: 180] loss: 0.0020958862733095884\n",
      "[nstep: 180] loss: 0.007335743401199579\n",
      "[step: 181] loss: 0.002139065647497773\n",
      "[nstep: 181] loss: 0.0073489658534526825\n",
      "[step: 182] loss: 0.002151317661628127\n",
      "[nstep: 182] loss: 0.007343219593167305\n",
      "[step: 183] loss: 0.0021513900719583035\n",
      "[nstep: 183] loss: 0.007324622943997383\n",
      "[step: 184] loss: 0.0021273214370012283\n",
      "[nstep: 184] loss: 0.007324978243559599\n",
      "[step: 185] loss: 0.0020616119727492332\n",
      "[nstep: 185] loss: 0.007332524284720421\n",
      "[step: 186] loss: 0.0020530843175947666\n",
      "[nstep: 186] loss: 0.007325083948671818\n",
      "[step: 187] loss: 0.0020331682171672583\n",
      "[nstep: 187] loss: 0.007313560228794813\n",
      "[step: 188] loss: 0.002033361466601491\n",
      "[nstep: 188] loss: 0.007311047520488501\n",
      "[step: 189] loss: 0.0020436225458979607\n",
      "[nstep: 189] loss: 0.007314911112189293\n",
      "[step: 190] loss: 0.0020432607270777225\n",
      "[nstep: 190] loss: 0.007312084548175335\n",
      "[step: 191] loss: 0.0020790928974747658\n",
      "[nstep: 191] loss: 0.007302194833755493\n",
      "[step: 192] loss: 0.002105451887473464\n",
      "[nstep: 192] loss: 0.007297796662896872\n",
      "[step: 193] loss: 0.0021742733661085367\n",
      "[nstep: 193] loss: 0.007300166878849268\n",
      "[step: 194] loss: 0.002314850687980652\n",
      "[nstep: 194] loss: 0.007298648823052645\n",
      "[step: 195] loss: 0.0023308282252401114\n",
      "[nstep: 195] loss: 0.007292518392205238\n",
      "[step: 196] loss: 0.0022853370755910873\n",
      "[nstep: 196] loss: 0.007286735810339451\n",
      "[step: 197] loss: 0.002071548718959093\n",
      "[nstep: 197] loss: 0.007285150233656168\n",
      "[step: 198] loss: 0.0020042727701365948\n",
      "[nstep: 198] loss: 0.007285052444785833\n",
      "[step: 199] loss: 0.002111948560923338\n",
      "[nstep: 199] loss: 0.007282728794962168\n",
      "[step: 200] loss: 0.002122126752510667\n",
      "[nstep: 200] loss: 0.0072773597203195095\n",
      "[step: 201] loss: 0.0020897001959383488\n",
      "[nstep: 201] loss: 0.0072730062529444695\n",
      "[step: 202] loss: 0.001997231272980571\n",
      "[nstep: 202] loss: 0.007271275389939547\n",
      "[step: 203] loss: 0.0019922535866498947\n",
      "[nstep: 203] loss: 0.007270453963428736\n",
      "[step: 204] loss: 0.002097106073051691\n",
      "[nstep: 204] loss: 0.007267924025654793\n",
      "[step: 205] loss: 0.002064913511276245\n",
      "[nstep: 205] loss: 0.007264246698468924\n",
      "[step: 206] loss: 0.0020057372748851776\n",
      "[nstep: 206] loss: 0.007260309997946024\n",
      "[step: 207] loss: 0.0019860046450048685\n",
      "[nstep: 207] loss: 0.007257568184286356\n",
      "[step: 208] loss: 0.0019768658094108105\n",
      "[nstep: 208] loss: 0.00725586898624897\n",
      "[step: 209] loss: 0.0019934047013521194\n",
      "[nstep: 209] loss: 0.007254272233694792\n",
      "[step: 210] loss: 0.0020224303007125854\n",
      "[nstep: 210] loss: 0.007251783739775419\n",
      "[step: 211] loss: 0.0020288655068725348\n",
      "[nstep: 211] loss: 0.0072486684657633305\n",
      "[step: 212] loss: 0.0019530218560248613\n",
      "[nstep: 212] loss: 0.0072454470209777355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 213] loss: 0.0019836854189634323\n",
      "[nstep: 213] loss: 0.007242561783641577\n",
      "[step: 214] loss: 0.001981691922992468\n",
      "[nstep: 214] loss: 0.007240201812237501\n",
      "[step: 215] loss: 0.001968687167391181\n",
      "[nstep: 215] loss: 0.007238265126943588\n",
      "[step: 216] loss: 0.002051816787570715\n",
      "[nstep: 216] loss: 0.0072363149374723434\n",
      "[step: 217] loss: 0.0019484504591673613\n",
      "[nstep: 217] loss: 0.007234096061438322\n",
      "[step: 218] loss: 0.001963501563295722\n",
      "[nstep: 218] loss: 0.007231646683067083\n",
      "[step: 219] loss: 0.0019207514123991132\n",
      "[nstep: 219] loss: 0.007229014299809933\n",
      "[step: 220] loss: 0.001914613414555788\n",
      "[nstep: 220] loss: 0.007226303219795227\n",
      "[step: 221] loss: 0.0019510386046022177\n",
      "[nstep: 221] loss: 0.007223615422844887\n",
      "[step: 222] loss: 0.0019164454424753785\n",
      "[nstep: 222] loss: 0.007221124600619078\n",
      "[step: 223] loss: 0.0019730604253709316\n",
      "[nstep: 223] loss: 0.0072187138721346855\n",
      "[step: 224] loss: 0.0019189994782209396\n",
      "[nstep: 224] loss: 0.007216412108391523\n",
      "[step: 225] loss: 0.0019402352627366781\n",
      "[nstep: 225] loss: 0.007214188575744629\n",
      "[step: 226] loss: 0.0019382492173463106\n",
      "[nstep: 226] loss: 0.007212054915726185\n",
      "[step: 227] loss: 0.0019411459797993302\n",
      "[nstep: 227] loss: 0.007209960836917162\n",
      "[step: 228] loss: 0.0019946713000535965\n",
      "[nstep: 228] loss: 0.007207934744656086\n",
      "[step: 229] loss: 0.0020756153389811516\n",
      "[nstep: 229] loss: 0.0072060152888298035\n",
      "[step: 230] loss: 0.002121978672221303\n",
      "[nstep: 230] loss: 0.007204311899840832\n",
      "[step: 231] loss: 0.002106757601723075\n",
      "[nstep: 231] loss: 0.007202919106930494\n",
      "[step: 232] loss: 0.002075018361210823\n",
      "[nstep: 232] loss: 0.007202157285064459\n",
      "[step: 233] loss: 0.0020526002626866102\n",
      "[nstep: 233] loss: 0.007202415261417627\n",
      "[step: 234] loss: 0.0019372855313122272\n",
      "[nstep: 234] loss: 0.007204865105450153\n",
      "[step: 235] loss: 0.0019999139476567507\n",
      "[nstep: 235] loss: 0.007210765965282917\n",
      "[step: 236] loss: 0.0019811629317700863\n",
      "[nstep: 236] loss: 0.007224571891129017\n",
      "[step: 237] loss: 0.002075562486425042\n",
      "[nstep: 237] loss: 0.007248144131153822\n",
      "[step: 238] loss: 0.0019134343601763248\n",
      "[nstep: 238] loss: 0.007293162867426872\n",
      "[step: 239] loss: 0.0019090794958174229\n",
      "[nstep: 239] loss: 0.007340315263718367\n",
      "[step: 240] loss: 0.0019391003297641873\n",
      "[nstep: 240] loss: 0.0073897638358175755\n",
      "[step: 241] loss: 0.0019631681498140097\n",
      "[nstep: 241] loss: 0.007349479012191296\n",
      "[step: 242] loss: 0.001983440248295665\n",
      "[nstep: 242] loss: 0.007261462975293398\n",
      "[step: 243] loss: 0.0018560911994427443\n",
      "[nstep: 243] loss: 0.007182604167610407\n",
      "[step: 244] loss: 0.0018646595999598503\n",
      "[nstep: 244] loss: 0.007193021941930056\n",
      "[step: 245] loss: 0.0018750751623883843\n",
      "[nstep: 245] loss: 0.007253542542457581\n",
      "[step: 246] loss: 0.0018694071331992745\n",
      "[nstep: 246] loss: 0.007264705840498209\n",
      "[step: 247] loss: 0.0018993231933563948\n",
      "[nstep: 247] loss: 0.007218089420348406\n",
      "[step: 248] loss: 0.001824578852392733\n",
      "[nstep: 248] loss: 0.007170963566750288\n",
      "[step: 249] loss: 0.0018321162788197398\n",
      "[nstep: 249] loss: 0.007183678448200226\n",
      "[step: 250] loss: 0.0018096553394570947\n",
      "[nstep: 250] loss: 0.007218788377940655\n",
      "[step: 251] loss: 0.0018105304334312677\n",
      "[nstep: 251] loss: 0.007210678420960903\n",
      "[step: 252] loss: 0.0018344601849094033\n",
      "[nstep: 252] loss: 0.007175206672400236\n",
      "[step: 253] loss: 0.0018194790463894606\n",
      "[nstep: 253] loss: 0.0071586729027330875\n",
      "[step: 254] loss: 0.001823236932978034\n",
      "[nstep: 254] loss: 0.007176235783845186\n",
      "[step: 255] loss: 0.001802030485123396\n",
      "[nstep: 255] loss: 0.007191360462456942\n",
      "[step: 256] loss: 0.0017871800810098648\n",
      "[nstep: 256] loss: 0.007175306789577007\n",
      "[step: 257] loss: 0.001784716616384685\n",
      "[nstep: 257] loss: 0.007152004633098841\n",
      "[step: 258] loss: 0.0017637308919802308\n",
      "[nstep: 258] loss: 0.007150951772928238\n",
      "[step: 259] loss: 0.001773826195858419\n",
      "[nstep: 259] loss: 0.007164801936596632\n",
      "[step: 260] loss: 0.0017625982873141766\n",
      "[nstep: 260] loss: 0.007168496493250132\n",
      "[step: 261] loss: 0.0017726178048178554\n",
      "[nstep: 261] loss: 0.007153244689106941\n",
      "[step: 262] loss: 0.0017784740775823593\n",
      "[nstep: 262] loss: 0.007140229921787977\n",
      "[step: 263] loss: 0.0017903521656990051\n",
      "[nstep: 263] loss: 0.007141312584280968\n",
      "[step: 264] loss: 0.0018437494290992618\n",
      "[nstep: 264] loss: 0.007148441858589649\n",
      "[step: 265] loss: 0.0018777379300445318\n",
      "[nstep: 265] loss: 0.0071482304483652115\n",
      "[step: 266] loss: 0.0019980138167738914\n",
      "[nstep: 266] loss: 0.007139779161661863\n",
      "[step: 267] loss: 0.0020929500460624695\n",
      "[nstep: 267] loss: 0.007131807506084442\n",
      "[step: 268] loss: 0.0023791110143065453\n",
      "[nstep: 268] loss: 0.007130003999918699\n",
      "[step: 269] loss: 0.0021362383849918842\n",
      "[nstep: 269] loss: 0.007132108323276043\n",
      "[step: 270] loss: 0.0018430289346724749\n",
      "[nstep: 270] loss: 0.0071333362720906734\n",
      "[step: 271] loss: 0.0019235099898651242\n",
      "[nstep: 271] loss: 0.007130174431949854\n",
      "[step: 272] loss: 0.0019447501981630921\n",
      "[nstep: 272] loss: 0.007124460302293301\n",
      "[step: 273] loss: 0.0018987235380336642\n",
      "[nstep: 273] loss: 0.007119795307517052\n",
      "[step: 274] loss: 0.001795905060134828\n",
      "[nstep: 274] loss: 0.00711874570697546\n",
      "[step: 275] loss: 0.0018896389519795775\n",
      "[nstep: 275] loss: 0.0071199252270162106\n",
      "[step: 276] loss: 0.002019051229581237\n",
      "[nstep: 276] loss: 0.007120023015886545\n",
      "[step: 277] loss: 0.0019299752311781049\n",
      "[nstep: 277] loss: 0.007117442321032286\n",
      "[step: 278] loss: 0.0017617596313357353\n",
      "[nstep: 278] loss: 0.007113114930689335\n",
      "[step: 279] loss: 0.001816843869164586\n",
      "[nstep: 279] loss: 0.007109551224857569\n",
      "[step: 280] loss: 0.0018750776071101427\n",
      "[nstep: 280] loss: 0.007107775658369064\n",
      "[step: 281] loss: 0.0018454822711646557\n",
      "[nstep: 281] loss: 0.0071072629652917385\n",
      "[step: 282] loss: 0.0017465843120589852\n",
      "[nstep: 282] loss: 0.0071068741381168365\n",
      "[step: 283] loss: 0.0017682698089629412\n",
      "[nstep: 283] loss: 0.007105865050107241\n",
      "[step: 284] loss: 0.001838376745581627\n",
      "[nstep: 284] loss: 0.007104018237441778\n",
      "[step: 285] loss: 0.001786129199899733\n",
      "[nstep: 285] loss: 0.007101426366716623\n",
      "[step: 286] loss: 0.0017110076732933521\n",
      "[nstep: 286] loss: 0.007098599337041378\n",
      "[step: 287] loss: 0.001773158204741776\n",
      "[nstep: 287] loss: 0.0070961518213152885\n",
      "[step: 288] loss: 0.0017582490108907223\n",
      "[nstep: 288] loss: 0.00709438556805253\n",
      "[step: 289] loss: 0.0017584317829459906\n",
      "[nstep: 289] loss: 0.007093182299286127\n",
      "[step: 290] loss: 0.001692775171250105\n",
      "[nstep: 290] loss: 0.007092159241437912\n",
      "[step: 291] loss: 0.0017259009182453156\n",
      "[nstep: 291] loss: 0.007091119885444641\n",
      "[step: 292] loss: 0.0017630355432629585\n",
      "[nstep: 292] loss: 0.007089934777468443\n",
      "[step: 293] loss: 0.001749031594954431\n",
      "[nstep: 293] loss: 0.0070886616595089436\n",
      "[step: 294] loss: 0.0016959576169028878\n",
      "[nstep: 294] loss: 0.007087241392582655\n",
      "[step: 295] loss: 0.0016827868530526757\n",
      "[nstep: 295] loss: 0.007085736375302076\n",
      "[step: 296] loss: 0.0016977912746369839\n",
      "[nstep: 296] loss: 0.007084105629473925\n",
      "[step: 297] loss: 0.0017243933398276567\n",
      "[nstep: 297] loss: 0.007082501892000437\n",
      "[step: 298] loss: 0.0017025492852553725\n",
      "[nstep: 298] loss: 0.007080985698848963\n",
      "[step: 299] loss: 0.0016792172100394964\n",
      "[nstep: 299] loss: 0.007079639006406069\n",
      "[step: 300] loss: 0.001659151166677475\n",
      "[nstep: 300] loss: 0.007078446447849274\n",
      "[step: 301] loss: 0.0016501598292961717\n",
      "[nstep: 301] loss: 0.0070775398053228855\n",
      "[step: 302] loss: 0.0016656782245263457\n",
      "[nstep: 302] loss: 0.00707703223451972\n",
      "[step: 303] loss: 0.0016655719373375177\n",
      "[nstep: 303] loss: 0.007077369838953018\n",
      "[step: 304] loss: 0.0016760039143264294\n",
      "[nstep: 304] loss: 0.007078925147652626\n",
      "[step: 305] loss: 0.0016517412150278687\n",
      "[nstep: 305] loss: 0.007083001080900431\n",
      "[step: 306] loss: 0.0016430832911282778\n",
      "[nstep: 306] loss: 0.00709062535315752\n",
      "[step: 307] loss: 0.001626898068934679\n",
      "[nstep: 307] loss: 0.007106159348040819\n",
      "[step: 308] loss: 0.0016277835238724947\n",
      "[nstep: 308] loss: 0.007130757439881563\n",
      "[step: 309] loss: 0.001626132521778345\n",
      "[nstep: 309] loss: 0.007175748236477375\n",
      "[step: 310] loss: 0.0016331881051883101\n",
      "[nstep: 310] loss: 0.0072252182289958\n",
      "[step: 311] loss: 0.00164762232452631\n",
      "[nstep: 311] loss: 0.007285894826054573\n",
      "[step: 312] loss: 0.001661395188421011\n",
      "[nstep: 312] loss: 0.007270061876624823\n",
      "[step: 313] loss: 0.0016972814919427037\n",
      "[nstep: 313] loss: 0.007200471591204405\n",
      "[step: 314] loss: 0.0017350917914882302\n",
      "[nstep: 314] loss: 0.007092488929629326\n",
      "[step: 315] loss: 0.0018564789788797498\n",
      "[nstep: 315] loss: 0.007058108225464821\n",
      "[step: 316] loss: 0.0018518250435590744\n",
      "[nstep: 316] loss: 0.007107023615390062\n",
      "[step: 317] loss: 0.001887344173155725\n",
      "[nstep: 317] loss: 0.00715515436604619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 318] loss: 0.0017631863011047244\n",
      "[nstep: 318] loss: 0.0071438634768128395\n",
      "[step: 319] loss: 0.0016686898889020085\n",
      "[nstep: 319] loss: 0.007079340983182192\n",
      "[step: 320] loss: 0.0016005783108994365\n",
      "[nstep: 320] loss: 0.007052287925034761\n",
      "[step: 321] loss: 0.001659587025642395\n",
      "[nstep: 321] loss: 0.0070822699926793575\n",
      "[step: 322] loss: 0.0017652657115831971\n",
      "[nstep: 322] loss: 0.007108366582542658\n",
      "[step: 323] loss: 0.0017766357632353902\n",
      "[nstep: 323] loss: 0.007093213032931089\n",
      "[step: 324] loss: 0.0017061992548406124\n",
      "[nstep: 324] loss: 0.007056375499814749\n",
      "[step: 325] loss: 0.001616513472981751\n",
      "[nstep: 325] loss: 0.0070488243363797665\n",
      "[step: 326] loss: 0.0015997713198885322\n",
      "[nstep: 326] loss: 0.007068807724863291\n",
      "[step: 327] loss: 0.0016164594562724233\n",
      "[nstep: 327] loss: 0.007078174035996199\n",
      "[step: 328] loss: 0.0016524572856724262\n",
      "[nstep: 328] loss: 0.007064543664455414\n",
      "[step: 329] loss: 0.001649287878535688\n",
      "[nstep: 329] loss: 0.007044881582260132\n",
      "[step: 330] loss: 0.001623644377104938\n",
      "[nstep: 330] loss: 0.007042462006211281\n",
      "[step: 331] loss: 0.0015770166646689177\n",
      "[nstep: 331] loss: 0.007052191067487001\n",
      "[step: 332] loss: 0.0015739394584670663\n",
      "[nstep: 332] loss: 0.007055330090224743\n",
      "[step: 333] loss: 0.001587540376931429\n",
      "[nstep: 333] loss: 0.007047893013805151\n",
      "[step: 334] loss: 0.0016097599873319268\n",
      "[nstep: 334] loss: 0.0070379916578531265\n",
      "[step: 335] loss: 0.001606374396942556\n",
      "[nstep: 335] loss: 0.007035098969936371\n",
      "[step: 336] loss: 0.0015997056616470218\n",
      "[nstep: 336] loss: 0.007037015166133642\n",
      "[step: 337] loss: 0.0015831765485927463\n",
      "[nstep: 337] loss: 0.007037864066660404\n",
      "[step: 338] loss: 0.0015580623876303434\n",
      "[nstep: 338] loss: 0.007036086171865463\n",
      "[step: 339] loss: 0.001548148924484849\n",
      "[nstep: 339] loss: 0.00703262398019433\n",
      "[step: 340] loss: 0.0015407083556056023\n",
      "[nstep: 340] loss: 0.007029475178569555\n",
      "[step: 341] loss: 0.0015419231494888663\n",
      "[nstep: 341] loss: 0.00702601857483387\n",
      "[step: 342] loss: 0.001548538450151682\n",
      "[nstep: 342] loss: 0.0070232511498034\n",
      "[step: 343] loss: 0.0015569062670692801\n",
      "[nstep: 343] loss: 0.00702281016856432\n",
      "[step: 344] loss: 0.001576608745381236\n",
      "[nstep: 344] loss: 0.007024115417152643\n",
      "[step: 345] loss: 0.0016018677270039916\n",
      "[nstep: 345] loss: 0.007024580147117376\n",
      "[step: 346] loss: 0.0016347626224160194\n",
      "[nstep: 346] loss: 0.007021458353847265\n",
      "[step: 347] loss: 0.0017370653804391623\n",
      "[nstep: 347] loss: 0.0070162583142519\n",
      "[step: 348] loss: 0.0017384192906320095\n",
      "[nstep: 348] loss: 0.007012302055954933\n",
      "[step: 349] loss: 0.0017604443710297346\n",
      "[nstep: 349] loss: 0.007011245470494032\n",
      "[step: 350] loss: 0.001692407764494419\n",
      "[nstep: 350] loss: 0.007011866196990013\n",
      "[step: 351] loss: 0.0016689349431544542\n",
      "[nstep: 351] loss: 0.007011755369603634\n",
      "[step: 352] loss: 0.0015507506905123591\n",
      "[nstep: 352] loss: 0.0070104137994349\n",
      "[step: 353] loss: 0.0015684489626437426\n",
      "[nstep: 353] loss: 0.007008871529251337\n",
      "[step: 354] loss: 0.001634855754673481\n",
      "[nstep: 354] loss: 0.007008228451013565\n",
      "[step: 355] loss: 0.001738447928801179\n",
      "[nstep: 355] loss: 0.007008128799498081\n",
      "[step: 356] loss: 0.0017181504517793655\n",
      "[nstep: 356] loss: 0.007007861975580454\n",
      "[step: 357] loss: 0.0016133254393935204\n",
      "[nstep: 357] loss: 0.007006950676441193\n",
      "[step: 358] loss: 0.0015463167801499367\n",
      "[nstep: 358] loss: 0.007006659172475338\n",
      "[step: 359] loss: 0.001521713682450354\n",
      "[nstep: 359] loss: 0.0070078568533062935\n",
      "[step: 360] loss: 0.0015715293120592833\n",
      "[nstep: 360] loss: 0.007012705318629742\n",
      "[step: 361] loss: 0.001597320195287466\n",
      "[nstep: 361] loss: 0.0070218550972640514\n",
      "[step: 362] loss: 0.0015909086214378476\n",
      "[nstep: 362] loss: 0.007042432203888893\n",
      "[step: 363] loss: 0.001533576170913875\n",
      "[nstep: 363] loss: 0.007075931876897812\n",
      "[step: 364] loss: 0.0014970616903156042\n",
      "[nstep: 364] loss: 0.0071492623537778854\n",
      "[step: 365] loss: 0.0015108083607628942\n",
      "[nstep: 365] loss: 0.007220408879220486\n",
      "[step: 366] loss: 0.0015258293133229017\n",
      "[nstep: 366] loss: 0.007311111781746149\n",
      "[step: 367] loss: 0.0015362262492999434\n",
      "[nstep: 367] loss: 0.007235532160848379\n",
      "[step: 368] loss: 0.001511179725639522\n",
      "[nstep: 368] loss: 0.007096989080309868\n",
      "[step: 369] loss: 0.0015009163180366158\n",
      "[nstep: 369] loss: 0.0069965277798473835\n",
      "[step: 370] loss: 0.0014707982772961259\n",
      "[nstep: 370] loss: 0.007039132993668318\n",
      "[step: 371] loss: 0.0014732469571754336\n",
      "[nstep: 371] loss: 0.007131703197956085\n",
      "[step: 372] loss: 0.0014741423074156046\n",
      "[nstep: 372] loss: 0.007112693972885609\n",
      "[step: 373] loss: 0.0014815849717706442\n",
      "[nstep: 373] loss: 0.007030031178146601\n",
      "[step: 374] loss: 0.0014877358917146921\n",
      "[nstep: 374] loss: 0.006995359901338816\n",
      "[step: 375] loss: 0.0014878803631290793\n",
      "[nstep: 375] loss: 0.007039304822683334\n",
      "[step: 376] loss: 0.001486745779402554\n",
      "[nstep: 376] loss: 0.007065995130687952\n",
      "[step: 377] loss: 0.0014789692359045148\n",
      "[nstep: 377] loss: 0.007024839520454407\n",
      "[step: 378] loss: 0.0014780634082853794\n",
      "[nstep: 378] loss: 0.006995855364948511\n",
      "[step: 379] loss: 0.0014686983777210116\n",
      "[nstep: 379] loss: 0.007006553467363119\n",
      "[step: 380] loss: 0.0014661854365840554\n",
      "[nstep: 380] loss: 0.007019378710538149\n",
      "[step: 381] loss: 0.0014640426961705089\n",
      "[nstep: 381] loss: 0.007014261092990637\n",
      "[step: 382] loss: 0.0014625942567363381\n",
      "[nstep: 382] loss: 0.006998585071414709\n",
      "[step: 383] loss: 0.0014691940741613507\n",
      "[nstep: 383] loss: 0.006987226195633411\n",
      "[step: 384] loss: 0.0014725376386195421\n",
      "[nstep: 384] loss: 0.006984887178987265\n",
      "[step: 385] loss: 0.001496660872362554\n",
      "[nstep: 385] loss: 0.006992343347519636\n",
      "[step: 386] loss: 0.00151468173135072\n",
      "[nstep: 386] loss: 0.006999275181442499\n",
      "[step: 387] loss: 0.0015691679436713457\n",
      "[nstep: 387] loss: 0.006984483450651169\n",
      "[step: 388] loss: 0.0016006906516849995\n",
      "[nstep: 388] loss: 0.006965740583837032\n",
      "[step: 389] loss: 0.0017062026308849454\n",
      "[nstep: 389] loss: 0.006968997418880463\n",
      "[step: 390] loss: 0.0016629507299512625\n",
      "[nstep: 390] loss: 0.006982732564210892\n",
      "[step: 391] loss: 0.0016097577754408121\n",
      "[nstep: 391] loss: 0.006980429403483868\n",
      "[step: 392] loss: 0.0015028563793748617\n",
      "[nstep: 392] loss: 0.006967559456825256\n",
      "[step: 393] loss: 0.001452206284739077\n",
      "[nstep: 393] loss: 0.0069629172794520855\n",
      "[step: 394] loss: 0.0014121822314336896\n",
      "[nstep: 394] loss: 0.006962704937905073\n",
      "[step: 395] loss: 0.0014715404249727726\n",
      "[nstep: 395] loss: 0.0069585139863193035\n",
      "[step: 396] loss: 0.0015403369907289743\n",
      "[nstep: 396] loss: 0.0069585940800607204\n",
      "[step: 397] loss: 0.0015863811131566763\n",
      "[nstep: 397] loss: 0.006964149419218302\n",
      "[step: 398] loss: 0.0015494667459279299\n",
      "[nstep: 398] loss: 0.006966177374124527\n",
      "[step: 399] loss: 0.0014776029856875539\n",
      "[nstep: 399] loss: 0.006960217375308275\n",
      "[step: 400] loss: 0.0014601800357922912\n",
      "[nstep: 400] loss: 0.00695635424926877\n",
      "[step: 401] loss: 0.0014097396051511168\n",
      "[nstep: 401] loss: 0.006954621057957411\n",
      "[step: 402] loss: 0.0014794568996876478\n",
      "[nstep: 402] loss: 0.006950213573873043\n",
      "[step: 403] loss: 0.0014988231705501676\n",
      "[nstep: 403] loss: 0.006944676395505667\n",
      "[step: 404] loss: 0.0015635407762601972\n",
      "[nstep: 404] loss: 0.006943197455257177\n",
      "[step: 405] loss: 0.001486526569351554\n",
      "[nstep: 405] loss: 0.006943386048078537\n",
      "[step: 406] loss: 0.0014547683531418443\n",
      "[nstep: 406] loss: 0.006941388361155987\n",
      "[step: 407] loss: 0.0014326561940833926\n",
      "[nstep: 407] loss: 0.0069393920712172985\n",
      "[step: 408] loss: 0.0014078925596550107\n",
      "[nstep: 408] loss: 0.006940103601664305\n",
      "[step: 409] loss: 0.001430060830898583\n",
      "[nstep: 409] loss: 0.006941833533346653\n",
      "[step: 410] loss: 0.0014245891943573952\n",
      "[nstep: 410] loss: 0.006942872889339924\n",
      "[step: 411] loss: 0.0014684414491057396\n",
      "[nstep: 411] loss: 0.006947716698050499\n",
      "[step: 412] loss: 0.0014203920727595687\n",
      "[nstep: 412] loss: 0.0069590043276548386\n",
      "[step: 413] loss: 0.0014194356044754386\n",
      "[nstep: 413] loss: 0.0069849565625190735\n",
      "[step: 414] loss: 0.0013843865599483252\n",
      "[nstep: 414] loss: 0.007023439276963472\n",
      "[step: 415] loss: 0.0013840040192008018\n",
      "[nstep: 415] loss: 0.007102733477950096\n",
      "[step: 416] loss: 0.0013641660334542394\n",
      "[nstep: 416] loss: 0.007149550132453442\n",
      "[step: 417] loss: 0.0013731889193877578\n",
      "[nstep: 417] loss: 0.007190831005573273\n",
      "[step: 418] loss: 0.001376371830701828\n",
      "[nstep: 418] loss: 0.007094360888004303\n",
      "[step: 419] loss: 0.001367930555716157\n",
      "[nstep: 419] loss: 0.00698361499235034\n",
      "[step: 420] loss: 0.0013765495968982577\n",
      "[nstep: 420] loss: 0.006930579897016287\n",
      "[step: 421] loss: 0.0013625535648316145\n",
      "[nstep: 421] loss: 0.006969315465539694\n",
      "[step: 422] loss: 0.0013695224188268185\n",
      "[nstep: 422] loss: 0.007045319769531488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 423] loss: 0.0013550280127674341\n",
      "[nstep: 423] loss: 0.007049852516502142\n",
      "[step: 424] loss: 0.001357631292194128\n",
      "[nstep: 424] loss: 0.0069872550666332245\n",
      "[step: 425] loss: 0.001353312167339027\n",
      "[nstep: 425] loss: 0.006926701404154301\n",
      "[step: 426] loss: 0.0013494818704202771\n",
      "[nstep: 426] loss: 0.006939860992133617\n",
      "[step: 427] loss: 0.0013511490542441607\n",
      "[nstep: 427] loss: 0.006989746820181608\n",
      "[step: 428] loss: 0.001339693320915103\n",
      "[nstep: 428] loss: 0.006996465846896172\n",
      "[step: 429] loss: 0.0013556394260376692\n",
      "[nstep: 429] loss: 0.006957497913390398\n",
      "[step: 430] loss: 0.0013435827568173409\n",
      "[nstep: 430] loss: 0.00691911531612277\n",
      "[step: 431] loss: 0.0013582557439804077\n",
      "[nstep: 431] loss: 0.006925318390130997\n",
      "[step: 432] loss: 0.0013678019167855382\n",
      "[nstep: 432] loss: 0.0069530983455479145\n",
      "[step: 433] loss: 0.001406405819579959\n",
      "[nstep: 433] loss: 0.006958415266126394\n",
      "[step: 434] loss: 0.0014293543063104153\n",
      "[nstep: 434] loss: 0.006937058176845312\n",
      "[step: 435] loss: 0.0015131495893001556\n",
      "[nstep: 435] loss: 0.006913112476468086\n",
      "[step: 436] loss: 0.0015758615918457508\n",
      "[nstep: 436] loss: 0.006913391407579184\n",
      "[step: 437] loss: 0.0017011519521474838\n",
      "[nstep: 437] loss: 0.006930077914148569\n",
      "[step: 438] loss: 0.0015963222831487656\n",
      "[nstep: 438] loss: 0.006938266567885876\n",
      "[step: 439] loss: 0.001479402999393642\n",
      "[nstep: 439] loss: 0.006930076517164707\n",
      "[step: 440] loss: 0.0013343616155907512\n",
      "[nstep: 440] loss: 0.006910986732691526\n",
      "[step: 441] loss: 0.001308770151808858\n",
      "[nstep: 441] loss: 0.006902757100760937\n",
      "[step: 442] loss: 0.0013804992195218801\n",
      "[nstep: 442] loss: 0.006908811163157225\n",
      "[step: 443] loss: 0.0014454659540206194\n",
      "[nstep: 443] loss: 0.006918263155966997\n",
      "[step: 444] loss: 0.0014665701892226934\n",
      "[nstep: 444] loss: 0.006921262945979834\n",
      "[step: 445] loss: 0.0013682799180969596\n",
      "[nstep: 445] loss: 0.006913000252097845\n",
      "[step: 446] loss: 0.0012974810088053346\n",
      "[nstep: 446] loss: 0.00690252473577857\n",
      "[step: 447] loss: 0.0013096689945086837\n",
      "[nstep: 447] loss: 0.0068961638025939465\n",
      "[step: 448] loss: 0.0013647857122123241\n",
      "[nstep: 448] loss: 0.006896456703543663\n",
      "[step: 449] loss: 0.0013757860288023949\n",
      "[nstep: 449] loss: 0.006901092361658812\n",
      "[step: 450] loss: 0.0013214739738032222\n",
      "[nstep: 450] loss: 0.006904750596731901\n",
      "[step: 451] loss: 0.0012794462963938713\n",
      "[nstep: 451] loss: 0.006905151065438986\n",
      "[step: 452] loss: 0.0012802204582840204\n",
      "[nstep: 452] loss: 0.0069011179730296135\n",
      "[step: 453] loss: 0.0013069470878690481\n",
      "[nstep: 453] loss: 0.006895311642438173\n",
      "[step: 454] loss: 0.0013324366882443428\n",
      "[nstep: 454] loss: 0.006889995187520981\n",
      "[step: 455] loss: 0.0013148480793461204\n",
      "[nstep: 455] loss: 0.006886820774525404\n",
      "[step: 456] loss: 0.0012858596164733171\n",
      "[nstep: 456] loss: 0.006886176764965057\n",
      "[step: 457] loss: 0.0012607622193172574\n",
      "[nstep: 457] loss: 0.006886936724185944\n",
      "[step: 458] loss: 0.0012620078632608056\n",
      "[nstep: 458] loss: 0.006888425908982754\n",
      "[step: 459] loss: 0.0012791104381904006\n",
      "[nstep: 459] loss: 0.006889794021844864\n",
      "[step: 460] loss: 0.0012895738473162055\n",
      "[nstep: 460] loss: 0.006890550721436739\n",
      "[step: 461] loss: 0.0012923647882416844\n",
      "[nstep: 461] loss: 0.006889983080327511\n",
      "[step: 462] loss: 0.0012735232012346387\n",
      "[nstep: 462] loss: 0.00688928971067071\n",
      "[step: 463] loss: 0.0012561638141050935\n",
      "[nstep: 463] loss: 0.006887844763696194\n",
      "[step: 464] loss: 0.0012436191318556666\n",
      "[nstep: 464] loss: 0.006886790972203016\n",
      "[step: 465] loss: 0.0012416258687153459\n",
      "[nstep: 465] loss: 0.0068853870034217834\n",
      "[step: 466] loss: 0.00124678248539567\n",
      "[nstep: 466] loss: 0.006884623318910599\n",
      "[step: 467] loss: 0.0012535019777715206\n",
      "[nstep: 467] loss: 0.006883847992867231\n",
      "[step: 468] loss: 0.0012593618594110012\n",
      "[nstep: 468] loss: 0.0068841059692204\n",
      "[step: 469] loss: 0.0012564086355268955\n",
      "[nstep: 469] loss: 0.00688464380800724\n",
      "[step: 470] loss: 0.0012528327060863376\n",
      "[nstep: 470] loss: 0.006887147203087807\n",
      "[step: 471] loss: 0.0012424900196492672\n",
      "[nstep: 471] loss: 0.006890570744872093\n",
      "[step: 472] loss: 0.0012332384940236807\n",
      "[nstep: 472] loss: 0.006898300256580114\n",
      "[step: 473] loss: 0.0012248026905581355\n",
      "[nstep: 473] loss: 0.006907620932906866\n",
      "[step: 474] loss: 0.0012193158036097884\n",
      "[nstep: 474] loss: 0.006925261579453945\n",
      "[step: 475] loss: 0.0012160100741311908\n",
      "[nstep: 475] loss: 0.006942971143871546\n",
      "[step: 476] loss: 0.0012147420784458518\n",
      "[nstep: 476] loss: 0.0069741299375891685\n",
      "[step: 477] loss: 0.00121497071813792\n",
      "[nstep: 477] loss: 0.006991330534219742\n",
      "[step: 478] loss: 0.0012159183388575912\n",
      "[nstep: 478] loss: 0.007011535577476025\n",
      "[step: 479] loss: 0.0012186130043119192\n",
      "[nstep: 479] loss: 0.0069881221279501915\n",
      "[step: 480] loss: 0.0012220845092087984\n",
      "[nstep: 480] loss: 0.0069520906545221806\n",
      "[step: 481] loss: 0.001230386202223599\n",
      "[nstep: 481] loss: 0.006899479776620865\n",
      "[step: 482] loss: 0.0012388991890475154\n",
      "[nstep: 482] loss: 0.006865816190838814\n",
      "[step: 483] loss: 0.0012614855077117682\n",
      "[nstep: 483] loss: 0.0068611279129981995\n",
      "[step: 484] loss: 0.0012783368583768606\n",
      "[nstep: 484] loss: 0.006879628635942936\n",
      "[step: 485] loss: 0.001323046861216426\n",
      "[nstep: 485] loss: 0.006905596703290939\n",
      "[step: 486] loss: 0.0013386145001277328\n",
      "[nstep: 486] loss: 0.00691671809181571\n",
      "[step: 487] loss: 0.001389930723235011\n",
      "[nstep: 487] loss: 0.006910958793014288\n",
      "[step: 488] loss: 0.0013587364228442311\n",
      "[nstep: 488] loss: 0.00688617629930377\n",
      "[step: 489] loss: 0.0013392141554504633\n",
      "[nstep: 489] loss: 0.006862823385745287\n",
      "[step: 490] loss: 0.0012634888989850879\n",
      "[nstep: 490] loss: 0.006851865444332361\n",
      "[step: 491] loss: 0.0012142147170379758\n",
      "[nstep: 491] loss: 0.00685572624206543\n",
      "[step: 492] loss: 0.0011988087790086865\n",
      "[nstep: 492] loss: 0.0068680644035339355\n",
      "[step: 493] loss: 0.0012182362843304873\n",
      "[nstep: 493] loss: 0.006878952030092478\n",
      "[step: 494] loss: 0.001270213513635099\n",
      "[nstep: 494] loss: 0.0068833730183541775\n",
      "[step: 495] loss: 0.001276943483389914\n",
      "[nstep: 495] loss: 0.006876213941723108\n",
      "[step: 496] loss: 0.001281620585359633\n",
      "[nstep: 496] loss: 0.006864201743155718\n",
      "[step: 497] loss: 0.0012222782243043184\n",
      "[nstep: 497] loss: 0.006851729471236467\n",
      "[step: 498] loss: 0.0011808909475803375\n",
      "[nstep: 498] loss: 0.006844408344477415\n",
      "[step: 499] loss: 0.0011729502584785223\n",
      "[nstep: 499] loss: 0.006843173410743475\n",
      "[step: 500] loss: 0.0011948919855058193\n",
      "[nstep: 500] loss: 0.006846516393125057\n",
      "[step: 501] loss: 0.0012285399716347456\n",
      "[nstep: 501] loss: 0.006851877085864544\n",
      "[step: 502] loss: 0.0012229598360136151\n",
      "[nstep: 502] loss: 0.006856266409158707\n",
      "[step: 503] loss: 0.001208416186273098\n",
      "[nstep: 503] loss: 0.006859071087092161\n",
      "[step: 504] loss: 0.0011739034671336412\n",
      "[nstep: 504] loss: 0.006858409382402897\n",
      "[step: 505] loss: 0.0011569351190701127\n",
      "[nstep: 505] loss: 0.00685631949454546\n",
      "[step: 506] loss: 0.0011610655346885324\n",
      "[nstep: 506] loss: 0.0068518309853971004\n",
      "[step: 507] loss: 0.0011742984643206\n",
      "[nstep: 507] loss: 0.006847444921731949\n",
      "[step: 508] loss: 0.0011860233498737216\n",
      "[nstep: 508] loss: 0.006842616945505142\n",
      "[step: 509] loss: 0.0011729669058695436\n",
      "[nstep: 509] loss: 0.006838607136160135\n",
      "[step: 510] loss: 0.0011589543428272009\n",
      "[nstep: 510] loss: 0.006835142150521278\n",
      "[step: 511] loss: 0.0011456180363893509\n",
      "[nstep: 511] loss: 0.006832520943135023\n",
      "[step: 512] loss: 0.0011427865829318762\n",
      "[nstep: 512] loss: 0.006830488331615925\n",
      "[step: 513] loss: 0.0011473414488136768\n",
      "[nstep: 513] loss: 0.006828907411545515\n",
      "[step: 514] loss: 0.0011521251872181892\n",
      "[nstep: 514] loss: 0.00682767853140831\n",
      "[step: 515] loss: 0.0011557748075574636\n",
      "[nstep: 515] loss: 0.006826700177043676\n",
      "[step: 516] loss: 0.0011491645127534866\n",
      "[nstep: 516] loss: 0.006825919263064861\n",
      "[step: 517] loss: 0.00114415492862463\n",
      "[nstep: 517] loss: 0.006825323216617107\n",
      "[step: 518] loss: 0.001140116946771741\n",
      "[nstep: 518] loss: 0.006824986543506384\n",
      "[step: 519] loss: 0.0011452375911176205\n",
      "[nstep: 519] loss: 0.00682503916323185\n",
      "[step: 520] loss: 0.001166030066087842\n",
      "[nstep: 520] loss: 0.006825856398791075\n",
      "[step: 521] loss: 0.0012047612108290195\n",
      "[nstep: 521] loss: 0.006827852688729763\n",
      "[step: 522] loss: 0.0013039779150858521\n",
      "[nstep: 522] loss: 0.006832493934780359\n",
      "[step: 523] loss: 0.0014116386882960796\n",
      "[nstep: 523] loss: 0.006841151975095272\n",
      "[step: 524] loss: 0.0016833614790812135\n",
      "[nstep: 524] loss: 0.006859889719635248\n",
      "[step: 525] loss: 0.0017535027582198381\n",
      "[nstep: 525] loss: 0.006891286000609398\n",
      "[step: 526] loss: 0.0017438736977055669\n",
      "[nstep: 526] loss: 0.006956865079700947\n",
      "[step: 527] loss: 0.0013623663689941168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nstep: 527] loss: 0.007038378156721592\n",
      "[step: 528] loss: 0.0011375516187399626\n",
      "[nstep: 528] loss: 0.007172577083110809\n",
      "[step: 529] loss: 0.0013355358969420195\n",
      "[nstep: 529] loss: 0.007203278597444296\n",
      "[step: 530] loss: 0.0014529043110087514\n",
      "[nstep: 530] loss: 0.007160307373851538\n",
      "[step: 531] loss: 0.001263206941075623\n",
      "[nstep: 531] loss: 0.0069892145693302155\n",
      "[step: 532] loss: 0.0011603140737861395\n",
      "[nstep: 532] loss: 0.006893383339047432\n",
      "[step: 533] loss: 0.0013133693719282746\n",
      "[nstep: 533] loss: 0.006923542357981205\n",
      "[step: 534] loss: 0.0012322135735303164\n",
      "[nstep: 534] loss: 0.006975880824029446\n",
      "[step: 535] loss: 0.0011557573452591896\n",
      "[nstep: 535] loss: 0.006984974257647991\n",
      "[step: 536] loss: 0.0012798183597624302\n",
      "[nstep: 536] loss: 0.006920071318745613\n",
      "[step: 537] loss: 0.0011688111117109656\n",
      "[nstep: 537] loss: 0.006886479444801807\n",
      "[step: 538] loss: 0.001149671501480043\n",
      "[nstep: 538] loss: 0.0068789585493505\n",
      "[step: 539] loss: 0.0012464530300348997\n",
      "[nstep: 539] loss: 0.006893464829772711\n",
      "[step: 540] loss: 0.0011325968662276864\n",
      "[nstep: 540] loss: 0.006919028237462044\n",
      "[step: 541] loss: 0.001152766402810812\n",
      "[nstep: 541] loss: 0.006870857439935207\n",
      "[step: 542] loss: 0.0011776909232139587\n",
      "[nstep: 542] loss: 0.006816060747951269\n",
      "[step: 543] loss: 0.0011365484679117799\n",
      "[nstep: 543] loss: 0.006849716417491436\n",
      "[step: 544] loss: 0.0011580991558730602\n",
      "[nstep: 544] loss: 0.006886098999530077\n",
      "[step: 545] loss: 0.0011095472145825624\n",
      "[nstep: 545] loss: 0.006844796240329742\n",
      "[step: 546] loss: 0.001135100843384862\n",
      "[nstep: 546] loss: 0.006807401310652494\n",
      "[step: 547] loss: 0.001137616578489542\n",
      "[nstep: 547] loss: 0.006830597296357155\n",
      "[step: 548] loss: 0.0011122503783553839\n",
      "[nstep: 548] loss: 0.006846101488918066\n",
      "[step: 549] loss: 0.0011347027029842138\n",
      "[nstep: 549] loss: 0.006832249462604523\n",
      "[step: 550] loss: 0.001093344297260046\n",
      "[nstep: 550] loss: 0.006827168632298708\n",
      "[step: 551] loss: 0.001109489006921649\n",
      "[nstep: 551] loss: 0.0068134646862745285\n",
      "[step: 552] loss: 0.001135027501732111\n",
      "[nstep: 552] loss: 0.00680492352694273\n",
      "[step: 553] loss: 0.0010911254212260246\n",
      "[nstep: 553] loss: 0.0068251085467636585\n",
      "[step: 554] loss: 0.001109834061935544\n",
      "[nstep: 554] loss: 0.006825327407568693\n",
      "[step: 555] loss: 0.0010937630431726575\n",
      "[nstep: 555] loss: 0.006802896503359079\n",
      "[step: 556] loss: 0.0010797686409205198\n",
      "[nstep: 556] loss: 0.006797447334975004\n",
      "[step: 557] loss: 0.0011062417179346085\n",
      "[nstep: 557] loss: 0.006805489771068096\n",
      "[step: 558] loss: 0.0010866508819162846\n",
      "[nstep: 558] loss: 0.0068055116571486\n",
      "[step: 559] loss: 0.0010766993509605527\n",
      "[nstep: 559] loss: 0.00680495472624898\n",
      "[step: 560] loss: 0.001084023155272007\n",
      "[nstep: 560] loss: 0.006802363321185112\n",
      "[step: 561] loss: 0.0010705800959840417\n",
      "[nstep: 561] loss: 0.006791494321078062\n",
      "[step: 562] loss: 0.0010708754416555166\n",
      "[nstep: 562] loss: 0.006788997910916805\n",
      "[step: 563] loss: 0.0010757986456155777\n",
      "[nstep: 563] loss: 0.006796916946768761\n",
      "[step: 564] loss: 0.0010686982423067093\n",
      "[nstep: 564] loss: 0.006796413101255894\n",
      "[step: 565] loss: 0.0010666769230738282\n",
      "[nstep: 565] loss: 0.006791243329644203\n",
      "[step: 566] loss: 0.001065199845470488\n",
      "[nstep: 566] loss: 0.006789069622755051\n",
      "[step: 567] loss: 0.001055838423781097\n",
      "[nstep: 567] loss: 0.006785459350794554\n",
      "[step: 568] loss: 0.0010565825505182147\n",
      "[nstep: 568] loss: 0.00678290193900466\n",
      "[step: 569] loss: 0.0010563016403466463\n",
      "[nstep: 569] loss: 0.00678592873737216\n",
      "[step: 570] loss: 0.0010500447824597359\n",
      "[nstep: 570] loss: 0.006787471007555723\n",
      "[step: 571] loss: 0.0010525747202336788\n",
      "[nstep: 571] loss: 0.006783758290112019\n",
      "[step: 572] loss: 0.0010518707567825913\n",
      "[nstep: 572] loss: 0.006780590396374464\n",
      "[step: 573] loss: 0.0010448165703564882\n",
      "[nstep: 573] loss: 0.006779393181204796\n",
      "[step: 574] loss: 0.001043791649863124\n",
      "[nstep: 574] loss: 0.006777310743927956\n",
      "[step: 575] loss: 0.0010448553366586566\n",
      "[nstep: 575] loss: 0.006776717491447926\n",
      "[step: 576] loss: 0.0010385733330622315\n",
      "[nstep: 576] loss: 0.006778245326131582\n",
      "[step: 577] loss: 0.0010349018266424537\n",
      "[nstep: 577] loss: 0.006777652073651552\n",
      "[step: 578] loss: 0.0010362949687987566\n",
      "[nstep: 578] loss: 0.006775181740522385\n",
      "[step: 579] loss: 0.001033759443089366\n",
      "[nstep: 579] loss: 0.006773652974516153\n",
      "[step: 580] loss: 0.0010292085353285074\n",
      "[nstep: 580] loss: 0.006772192195057869\n",
      "[step: 581] loss: 0.001028936356306076\n",
      "[nstep: 581] loss: 0.006770281121134758\n",
      "[step: 582] loss: 0.0010281880386173725\n",
      "[nstep: 582] loss: 0.006769371684640646\n",
      "[step: 583] loss: 0.0010246902238577604\n",
      "[nstep: 583] loss: 0.006769580766558647\n",
      "[step: 584] loss: 0.0010230547050014138\n",
      "[nstep: 584] loss: 0.006769200321286917\n",
      "[step: 585] loss: 0.0010228051105514169\n",
      "[nstep: 585] loss: 0.006768125109374523\n",
      "[step: 586] loss: 0.0010207919403910637\n",
      "[nstep: 586] loss: 0.006767464801669121\n",
      "[step: 587] loss: 0.0010187935549765825\n",
      "[nstep: 587] loss: 0.0067665246315300465\n",
      "[step: 588] loss: 0.001019580289721489\n",
      "[nstep: 588] loss: 0.006764942314475775\n",
      "[step: 589] loss: 0.0010222081327810884\n",
      "[nstep: 589] loss: 0.006763441488146782\n",
      "[step: 590] loss: 0.0010303235612809658\n",
      "[nstep: 590] loss: 0.006762475706636906\n",
      "[step: 591] loss: 0.0010490204440429807\n",
      "[nstep: 591] loss: 0.006761455908417702\n",
      "[step: 592] loss: 0.0011105708545073867\n",
      "[nstep: 592] loss: 0.006760253570973873\n",
      "[step: 593] loss: 0.0012350163888186216\n",
      "[nstep: 593] loss: 0.006759427487850189\n",
      "[step: 594] loss: 0.0015593038406223059\n",
      "[nstep: 594] loss: 0.006758843548595905\n",
      "[step: 595] loss: 0.0017051007598638535\n",
      "[nstep: 595] loss: 0.006758098490536213\n",
      "[step: 596] loss: 0.0017480639507994056\n",
      "[nstep: 596] loss: 0.006757273804396391\n",
      "[step: 597] loss: 0.0012558698654174805\n",
      "[nstep: 597] loss: 0.006756672635674477\n",
      "[step: 598] loss: 0.0011723048519343138\n",
      "[nstep: 598] loss: 0.006756198592483997\n",
      "[step: 599] loss: 0.0013950718566775322\n",
      "[nstep: 599] loss: 0.006755624897778034\n",
      "[step: 600] loss: 0.0013376984279602766\n",
      "[nstep: 600] loss: 0.006755150854587555\n",
      "[step: 601] loss: 0.001400326145812869\n",
      "[nstep: 601] loss: 0.006755061913281679\n",
      "[step: 602] loss: 0.0011740478221327066\n",
      "[nstep: 602] loss: 0.006755289621651173\n",
      "[step: 603] loss: 0.0011363332159817219\n",
      "[nstep: 603] loss: 0.006756100337952375\n",
      "[step: 604] loss: 0.0013020094484090805\n",
      "[nstep: 604] loss: 0.006757795810699463\n",
      "[step: 605] loss: 0.0011096395319327712\n",
      "[nstep: 605] loss: 0.006761498283594847\n",
      "[step: 606] loss: 0.0011219887528568506\n",
      "[nstep: 606] loss: 0.006767790764570236\n",
      "[step: 607] loss: 0.0011672432301566005\n",
      "[nstep: 607] loss: 0.006779572460800409\n",
      "[step: 608] loss: 0.0010801631724461913\n",
      "[nstep: 608] loss: 0.006798295304179192\n",
      "[step: 609] loss: 0.0011531873606145382\n",
      "[nstep: 609] loss: 0.006833268329501152\n",
      "[step: 610] loss: 0.0010576883796602488\n",
      "[nstep: 610] loss: 0.006880437023937702\n",
      "[step: 611] loss: 0.001112801139242947\n",
      "[nstep: 611] loss: 0.006958308629691601\n",
      "[step: 612] loss: 0.0011185240000486374\n",
      "[nstep: 612] loss: 0.007018251810222864\n",
      "[step: 613] loss: 0.00108038738835603\n",
      "[nstep: 613] loss: 0.00706942193210125\n",
      "[step: 614] loss: 0.0010786779457703233\n",
      "[nstep: 614] loss: 0.007004686165601015\n",
      "[step: 615] loss: 0.0011375207686796784\n",
      "[nstep: 615] loss: 0.006883648224174976\n",
      "[step: 616] loss: 0.0010621162364259362\n",
      "[nstep: 616] loss: 0.006766843609511852\n",
      "[step: 617] loss: 0.0010210734326392412\n",
      "[nstep: 617] loss: 0.006749982945621014\n",
      "[step: 618] loss: 0.0010907860705628991\n",
      "[nstep: 618] loss: 0.006818241439759731\n",
      "[step: 619] loss: 0.0010139868827536702\n",
      "[nstep: 619] loss: 0.006877151783555746\n",
      "[step: 620] loss: 0.0010429822141304612\n",
      "[nstep: 620] loss: 0.006865411065518856\n",
      "[step: 621] loss: 0.0010444771032780409\n",
      "[nstep: 621] loss: 0.006787685211747885\n",
      "[step: 622] loss: 0.0010183644481003284\n",
      "[nstep: 622] loss: 0.006740305572748184\n",
      "[step: 623] loss: 0.0010411289986222982\n",
      "[nstep: 623] loss: 0.00676479609683156\n",
      "[step: 624] loss: 0.0009979800088331103\n",
      "[nstep: 624] loss: 0.006812227424234152\n",
      "[step: 625] loss: 0.0010296488180756569\n",
      "[nstep: 625] loss: 0.006820514798164368\n",
      "[step: 626] loss: 0.0010137176141142845\n",
      "[nstep: 626] loss: 0.006776056252419949\n",
      "[step: 627] loss: 0.0009956653229892254\n",
      "[nstep: 627] loss: 0.006737392395734787\n",
      "[step: 628] loss: 0.001004725811071694\n",
      "[nstep: 628] loss: 0.006743761245161295\n",
      "[step: 629] loss: 0.0010058808838948607\n",
      "[nstep: 629] loss: 0.006773437839001417\n",
      "[step: 630] loss: 0.0009922200115397573\n",
      "[nstep: 630] loss: 0.006783558055758476\n",
      "[step: 631] loss: 0.000986696220934391\n",
      "[nstep: 631] loss: 0.006760324817150831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 632] loss: 0.0009945810306817293\n",
      "[nstep: 632] loss: 0.006734365131705999\n",
      "[step: 633] loss: 0.00098249816801399\n",
      "[nstep: 633] loss: 0.006731532979756594\n",
      "[step: 634] loss: 0.0009769430616870522\n",
      "[nstep: 634] loss: 0.006748104467988014\n",
      "[step: 635] loss: 0.0009835987584665418\n",
      "[nstep: 635] loss: 0.006759243551641703\n",
      "[step: 636] loss: 0.0009709590231068432\n",
      "[nstep: 636] loss: 0.006750766653567553\n",
      "[step: 637] loss: 0.000972562818787992\n",
      "[nstep: 637] loss: 0.006733338814228773\n",
      "[step: 638] loss: 0.0009730241727083921\n",
      "[nstep: 638] loss: 0.006724617909640074\n",
      "[step: 639] loss: 0.0009658714989200234\n",
      "[nstep: 639] loss: 0.006729770917445421\n",
      "[step: 640] loss: 0.0009648678242228925\n",
      "[nstep: 640] loss: 0.006739625707268715\n",
      "[step: 641] loss: 0.0009609140106476843\n",
      "[nstep: 641] loss: 0.006741310469806194\n",
      "[step: 642] loss: 0.000964642851613462\n",
      "[nstep: 642] loss: 0.006733651272952557\n",
      "[step: 643] loss: 0.0009580575278960168\n",
      "[nstep: 643] loss: 0.0067234947346150875\n",
      "[step: 644] loss: 0.0009540297905914485\n",
      "[nstep: 644] loss: 0.00671941926702857\n",
      "[step: 645] loss: 0.0009574320283718407\n",
      "[nstep: 645] loss: 0.006722660735249519\n",
      "[step: 646] loss: 0.0009518077131360769\n",
      "[nstep: 646] loss: 0.006727593019604683\n",
      "[step: 647] loss: 0.0009504780173301697\n",
      "[nstep: 647] loss: 0.006728691514581442\n",
      "[step: 648] loss: 0.0009500044980086386\n",
      "[nstep: 648] loss: 0.00672450615093112\n",
      "[step: 649] loss: 0.0009478282881900668\n",
      "[nstep: 649] loss: 0.006718462333083153\n",
      "[step: 650] loss: 0.0009455504477955401\n",
      "[nstep: 650] loss: 0.006714443676173687\n",
      "[step: 651] loss: 0.0009432231308892369\n",
      "[nstep: 651] loss: 0.006714523304253817\n",
      "[step: 652] loss: 0.0009424806921742857\n",
      "[nstep: 652] loss: 0.006716630421578884\n",
      "[step: 653] loss: 0.0009414833039045334\n",
      "[nstep: 653] loss: 0.006718217860907316\n",
      "[step: 654] loss: 0.0009396056411787868\n",
      "[nstep: 654] loss: 0.006717597134411335\n",
      "[step: 655] loss: 0.0009365853620693088\n",
      "[nstep: 655] loss: 0.006714652758091688\n",
      "[step: 656] loss: 0.0009362851269543171\n",
      "[nstep: 656] loss: 0.006711216177791357\n",
      "[step: 657] loss: 0.0009351318585686386\n",
      "[nstep: 657] loss: 0.006708732806146145\n",
      "[step: 658] loss: 0.000932353432290256\n",
      "[nstep: 658] loss: 0.006707910913974047\n",
      "[step: 659] loss: 0.0009311480680480599\n",
      "[nstep: 659] loss: 0.00670830812305212\n",
      "[step: 660] loss: 0.0009301528334617615\n",
      "[nstep: 660] loss: 0.006708907894790173\n",
      "[step: 661] loss: 0.0009291231399402022\n",
      "[nstep: 661] loss: 0.00670890137553215\n",
      "[step: 662] loss: 0.0009271677117794752\n",
      "[nstep: 662] loss: 0.006707971449941397\n",
      "[step: 663] loss: 0.0009257597266696393\n",
      "[nstep: 663] loss: 0.006706258747726679\n",
      "[step: 664] loss: 0.0009245251421816647\n",
      "[nstep: 664] loss: 0.006704292260110378\n",
      "[step: 665] loss: 0.0009228316484950483\n",
      "[nstep: 665] loss: 0.006702539976686239\n",
      "[step: 666] loss: 0.0009215666796080768\n",
      "[nstep: 666] loss: 0.006701224949210882\n",
      "[step: 667] loss: 0.00092021853197366\n",
      "[nstep: 667] loss: 0.0067004249431192875\n",
      "[step: 668] loss: 0.0009189880802296102\n",
      "[nstep: 668] loss: 0.0066999830305576324\n",
      "[step: 669] loss: 0.0009174293954856694\n",
      "[nstep: 669] loss: 0.006699719931930304\n",
      "[step: 670] loss: 0.0009158215834759176\n",
      "[nstep: 670] loss: 0.006699424237012863\n",
      "[step: 671] loss: 0.0009146131342276931\n",
      "[nstep: 671] loss: 0.006699006538838148\n",
      "[step: 672] loss: 0.0009134559077210724\n",
      "[nstep: 672] loss: 0.006698415149003267\n",
      "[step: 673] loss: 0.0009124427451752126\n",
      "[nstep: 673] loss: 0.00669765705242753\n",
      "[step: 674] loss: 0.0009111627587117255\n",
      "[nstep: 674] loss: 0.00669680442661047\n",
      "[step: 675] loss: 0.0009097695583477616\n",
      "[nstep: 675] loss: 0.0066958582028746605\n",
      "[step: 676] loss: 0.0009083935874514282\n",
      "[nstep: 676] loss: 0.006694913376122713\n",
      "[step: 677] loss: 0.0009069174411706626\n",
      "[nstep: 677] loss: 0.006693941541016102\n",
      "[step: 678] loss: 0.0009056335547938943\n",
      "[nstep: 678] loss: 0.006693028379231691\n",
      "[step: 679] loss: 0.0009043627651408315\n",
      "[nstep: 679] loss: 0.006692160852253437\n",
      "[step: 680] loss: 0.0009031614172272384\n",
      "[nstep: 680] loss: 0.006691372487694025\n",
      "[step: 681] loss: 0.0009021078585647047\n",
      "[nstep: 681] loss: 0.006690664682537317\n",
      "[step: 682] loss: 0.0009011687943711877\n",
      "[nstep: 682] loss: 0.006690082140266895\n",
      "[step: 683] loss: 0.0009006586042232811\n",
      "[nstep: 683] loss: 0.006689636968076229\n",
      "[step: 684] loss: 0.0009011234506033361\n",
      "[nstep: 684] loss: 0.0066894651390612125\n",
      "[step: 685] loss: 0.0009037589770741761\n",
      "[nstep: 685] loss: 0.006689656060189009\n",
      "[step: 686] loss: 0.0009129708632826805\n",
      "[nstep: 686] loss: 0.00669052405282855\n",
      "[step: 687] loss: 0.0009346649749204516\n",
      "[nstep: 687] loss: 0.006692337337881327\n",
      "[step: 688] loss: 0.0010019736364483833\n",
      "[nstep: 688] loss: 0.006696021184325218\n",
      "[step: 689] loss: 0.0011069735046476126\n",
      "[nstep: 689] loss: 0.006702369544655085\n",
      "[step: 690] loss: 0.0014110974734649062\n",
      "[nstep: 690] loss: 0.006714328192174435\n",
      "[step: 691] loss: 0.0013836175203323364\n",
      "[nstep: 691] loss: 0.006733364891260862\n",
      "[step: 692] loss: 0.001482648542150855\n",
      "[nstep: 692] loss: 0.006768072489649057\n",
      "[step: 693] loss: 0.001451454241760075\n",
      "[nstep: 693] loss: 0.006816053297370672\n",
      "[step: 694] loss: 0.0013251279015094042\n",
      "[nstep: 694] loss: 0.006893697194755077\n",
      "[step: 695] loss: 0.0009758370579220355\n",
      "[nstep: 695] loss: 0.006960444618016481\n",
      "[step: 696] loss: 0.001076631830073893\n",
      "[nstep: 696] loss: 0.007018456235527992\n",
      "[step: 697] loss: 0.0012807176681235433\n",
      "[nstep: 697] loss: 0.0069671268574893475\n",
      "[step: 698] loss: 0.0011367370607331395\n",
      "[nstep: 698] loss: 0.00685022072866559\n",
      "[step: 699] loss: 0.0011229263618588448\n",
      "[nstep: 699] loss: 0.0067231967113912106\n",
      "[step: 700] loss: 0.0009900664445012808\n",
      "[nstep: 700] loss: 0.006686486769467592\n",
      "[step: 701] loss: 0.0011850356822833419\n",
      "[nstep: 701] loss: 0.006741900462657213\n",
      "[step: 702] loss: 0.001325346645899117\n",
      "[nstep: 702] loss: 0.006804146803915501\n",
      "[step: 703] loss: 0.0010101586813107133\n",
      "[nstep: 703] loss: 0.00680746603757143\n",
      "[step: 704] loss: 0.001254654605872929\n",
      "[nstep: 704] loss: 0.00674018356949091\n",
      "[step: 705] loss: 0.0012694201432168484\n",
      "[nstep: 705] loss: 0.006689681205898523\n",
      "[step: 706] loss: 0.001189461792819202\n",
      "[nstep: 706] loss: 0.00670353788882494\n",
      "[step: 707] loss: 0.0011153644882142544\n",
      "[nstep: 707] loss: 0.006743402685970068\n",
      "[step: 708] loss: 0.0011429886799305677\n",
      "[nstep: 708] loss: 0.006753277499228716\n",
      "[step: 709] loss: 0.0012024755124002695\n",
      "[nstep: 709] loss: 0.006714396644383669\n",
      "[step: 710] loss: 0.001053615938872099\n",
      "[nstep: 710] loss: 0.006683987565338612\n",
      "[step: 711] loss: 0.0010264132870361209\n",
      "[nstep: 711] loss: 0.006693138275295496\n",
      "[step: 712] loss: 0.001097572036087513\n",
      "[nstep: 712] loss: 0.006714127492159605\n",
      "[step: 713] loss: 0.0010708224726840854\n",
      "[nstep: 713] loss: 0.006715191528201103\n",
      "[step: 714] loss: 0.0009487330098636448\n",
      "[nstep: 714] loss: 0.006690851412713528\n",
      "[step: 715] loss: 0.0010493925074115396\n",
      "[nstep: 715] loss: 0.0066764396615326405\n",
      "[step: 716] loss: 0.0010529733262956142\n",
      "[nstep: 716] loss: 0.006685342639684677\n",
      "[step: 717] loss: 0.0009407748002558947\n",
      "[nstep: 717] loss: 0.006695479620248079\n",
      "[step: 718] loss: 0.0010318183340132236\n",
      "[nstep: 718] loss: 0.006692181341350079\n",
      "[step: 719] loss: 0.0010194506030529737\n",
      "[nstep: 719] loss: 0.006677515804767609\n",
      "[step: 720] loss: 0.000996802351437509\n",
      "[nstep: 720] loss: 0.00667122658342123\n",
      "[step: 721] loss: 0.0009299999801442027\n",
      "[nstep: 721] loss: 0.006677103694528341\n",
      "[step: 722] loss: 0.0010214378125965595\n",
      "[nstep: 722] loss: 0.006680021993815899\n",
      "[step: 723] loss: 0.0009585267980583012\n",
      "[nstep: 723] loss: 0.006674626842141151\n",
      "[step: 724] loss: 0.0009156906744465232\n",
      "[nstep: 724] loss: 0.006667252629995346\n",
      "[step: 725] loss: 0.0009741549147292972\n",
      "[nstep: 725] loss: 0.006666593719273806\n",
      "[step: 726] loss: 0.000956891686655581\n",
      "[nstep: 726] loss: 0.006670768838375807\n",
      "[step: 727] loss: 0.0009094173437915742\n",
      "[nstep: 727] loss: 0.006669144611805677\n",
      "[step: 728] loss: 0.0009187456453219056\n",
      "[nstep: 728] loss: 0.006661843508481979\n",
      "[step: 729] loss: 0.0009390756022185087\n",
      "[nstep: 729] loss: 0.006658084224909544\n",
      "[step: 730] loss: 0.0009150522528216243\n",
      "[nstep: 730] loss: 0.006660642568022013\n",
      "[step: 731] loss: 0.0008831392624415457\n",
      "[nstep: 731] loss: 0.006664510350674391\n",
      "[step: 732] loss: 0.0009296003263443708\n",
      "[nstep: 732] loss: 0.006661850959062576\n",
      "[step: 733] loss: 0.000902474217582494\n",
      "[nstep: 733] loss: 0.006654772441834211\n",
      "[step: 734] loss: 0.0008916733204387128\n",
      "[nstep: 734] loss: 0.006651441566646099\n",
      "[step: 735] loss: 0.0008847813005559146\n",
      "[nstep: 735] loss: 0.006653445772826672\n",
      "[step: 736] loss: 0.0008976344834081829\n",
      "[nstep: 736] loss: 0.00665642786771059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 737] loss: 0.0008891624747775495\n",
      "[nstep: 737] loss: 0.006655190140008926\n",
      "[step: 738] loss: 0.0008668277296237648\n",
      "[nstep: 738] loss: 0.006650969386100769\n",
      "[step: 739] loss: 0.0008822676027193666\n",
      "[nstep: 739] loss: 0.006648290902376175\n",
      "[step: 740] loss: 0.0008811270818114281\n",
      "[nstep: 740] loss: 0.006648400332778692\n",
      "[step: 741] loss: 0.0008717502350918949\n",
      "[nstep: 741] loss: 0.00664916168898344\n",
      "[step: 742] loss: 0.0008651360985822976\n",
      "[nstep: 742] loss: 0.006647935137152672\n",
      "[step: 743] loss: 0.0008667154470458627\n",
      "[nstep: 743] loss: 0.006645668298006058\n",
      "[step: 744] loss: 0.0008757178438827395\n",
      "[nstep: 744] loss: 0.006644308101385832\n",
      "[step: 745] loss: 0.0008606563787907362\n",
      "[nstep: 745] loss: 0.006644456181675196\n",
      "[step: 746] loss: 0.0008550375932827592\n",
      "[nstep: 746] loss: 0.006644811015576124\n",
      "[step: 747] loss: 0.000863206631038338\n",
      "[nstep: 747] loss: 0.0066437250934541225\n",
      "[step: 748] loss: 0.0008585715549997985\n",
      "[nstep: 748] loss: 0.006641803774982691\n",
      "[step: 749] loss: 0.0008567097247578204\n",
      "[nstep: 749] loss: 0.006640017032623291\n",
      "[step: 750] loss: 0.0008492054766975343\n",
      "[nstep: 750] loss: 0.006639314349740744\n",
      "[step: 751] loss: 0.0008510876214131713\n",
      "[nstep: 751] loss: 0.0066392673179507256\n",
      "[step: 752] loss: 0.0008545486489310861\n",
      "[nstep: 752] loss: 0.006638812832534313\n",
      "[step: 753] loss: 0.0008487894083373249\n",
      "[nstep: 753] loss: 0.0066378493793308735\n",
      "[step: 754] loss: 0.000845748174469918\n",
      "[nstep: 754] loss: 0.006636572070419788\n",
      "[step: 755] loss: 0.0008443006663583219\n",
      "[nstep: 755] loss: 0.0066356961615383625\n",
      "[step: 756] loss: 0.0008444724371656775\n",
      "[nstep: 756] loss: 0.006635306403040886\n",
      "[step: 757] loss: 0.0008452270412817597\n",
      "[nstep: 757] loss: 0.006634967867285013\n",
      "[step: 758] loss: 0.000842409033793956\n",
      "[nstep: 758] loss: 0.006634395103901625\n",
      "[step: 759] loss: 0.0008378821075893939\n",
      "[nstep: 759] loss: 0.006633436307311058\n",
      "[step: 760] loss: 0.0008376220939680934\n",
      "[nstep: 760] loss: 0.006632407661527395\n",
      "[step: 761] loss: 0.0008380661020055413\n",
      "[nstep: 761] loss: 0.006631564348936081\n",
      "[step: 762] loss: 0.0008369660354219377\n",
      "[nstep: 762] loss: 0.006630984600633383\n",
      "[step: 763] loss: 0.0008360511856153607\n",
      "[nstep: 763] loss: 0.006630561780184507\n",
      "[step: 764] loss: 0.0008331064018420875\n",
      "[nstep: 764] loss: 0.006630072370171547\n",
      "[step: 765] loss: 0.0008311236742883921\n",
      "[nstep: 765] loss: 0.006629529409110546\n",
      "[step: 766] loss: 0.0008310122648254037\n",
      "[nstep: 766] loss: 0.006628980860114098\n",
      "[step: 767] loss: 0.0008300974150188267\n",
      "[nstep: 767] loss: 0.006628678645938635\n",
      "[step: 768] loss: 0.0008293158025480807\n",
      "[nstep: 768] loss: 0.006628791801631451\n",
      "[step: 769] loss: 0.0008286568336188793\n",
      "[nstep: 769] loss: 0.00662958761677146\n",
      "[step: 770] loss: 0.0008271331316791475\n",
      "[nstep: 770] loss: 0.006631300784647465\n",
      "[step: 771] loss: 0.0008251894032582641\n",
      "[nstep: 771] loss: 0.006634635850787163\n",
      "[step: 772] loss: 0.0008237712900154293\n",
      "[nstep: 772] loss: 0.006640424020588398\n",
      "[step: 773] loss: 0.0008228530641645193\n",
      "[nstep: 773] loss: 0.006651188712567091\n",
      "[step: 774] loss: 0.0008217840804718435\n",
      "[nstep: 774] loss: 0.006669060792773962\n",
      "[step: 775] loss: 0.0008212083484977484\n",
      "[nstep: 775] loss: 0.006701708771288395\n",
      "[step: 776] loss: 0.0008208181243389845\n",
      "[nstep: 776] loss: 0.006749959196895361\n",
      "[step: 777] loss: 0.0008197185816243291\n",
      "[nstep: 777] loss: 0.006828658282756805\n",
      "[step: 778] loss: 0.0008188249776139855\n",
      "[nstep: 778] loss: 0.006909139454364777\n",
      "[step: 779] loss: 0.0008179975557141006\n",
      "[nstep: 779] loss: 0.006989439949393272\n",
      "[step: 780] loss: 0.0008169931825250387\n",
      "[nstep: 780] loss: 0.006968553643673658\n",
      "[step: 781] loss: 0.0008159017306752503\n",
      "[nstep: 781] loss: 0.006858666893094778\n",
      "[step: 782] loss: 0.0008149700588546693\n",
      "[nstep: 782] loss: 0.006699875928461552\n",
      "[step: 783] loss: 0.0008141239522956312\n",
      "[nstep: 783] loss: 0.00662237498909235\n",
      "[step: 784] loss: 0.0008134723757393658\n",
      "[nstep: 784] loss: 0.0066631268709897995\n",
      "[step: 785] loss: 0.0008129232446663082\n",
      "[nstep: 785] loss: 0.0067461817525327206\n",
      "[step: 786] loss: 0.0008131296490319073\n",
      "[nstep: 786] loss: 0.006776005495339632\n",
      "[step: 787] loss: 0.0008137848344631493\n",
      "[nstep: 787] loss: 0.0067091090604662895\n",
      "[step: 788] loss: 0.0008161821169778705\n",
      "[nstep: 788] loss: 0.006634660996496677\n",
      "[step: 789] loss: 0.0008202837198041379\n",
      "[nstep: 789] loss: 0.006635697092860937\n",
      "[step: 790] loss: 0.0008309370023198426\n",
      "[nstep: 790] loss: 0.00668858177959919\n",
      "[step: 791] loss: 0.0008449498564004898\n",
      "[nstep: 791] loss: 0.006712986156344414\n",
      "[step: 792] loss: 0.0008788593113422394\n",
      "[nstep: 792] loss: 0.00667168851941824\n",
      "[step: 793] loss: 0.0009027172345668077\n",
      "[nstep: 793] loss: 0.006626907270401716\n",
      "[step: 794] loss: 0.000956500880420208\n",
      "[nstep: 794] loss: 0.00663100415840745\n",
      "[step: 795] loss: 0.0009461223962716758\n",
      "[nstep: 795] loss: 0.006662363652139902\n",
      "[step: 796] loss: 0.0009499826119281352\n",
      "[nstep: 796] loss: 0.006670725531876087\n",
      "[step: 797] loss: 0.0009278742945753038\n",
      "[nstep: 797] loss: 0.0066400812938809395\n",
      "[step: 798] loss: 0.0008999399142339826\n",
      "[nstep: 798] loss: 0.0066145299933850765\n",
      "[step: 799] loss: 0.000846425595227629\n",
      "[nstep: 799] loss: 0.0066227298229932785\n",
      "[step: 800] loss: 0.0008074793149717152\n",
      "[nstep: 800] loss: 0.006642931140959263\n",
      "[step: 801] loss: 0.0008099197293631732\n",
      "[nstep: 801] loss: 0.006647211033850908\n",
      "[step: 802] loss: 0.0008423912804573774\n",
      "[nstep: 802] loss: 0.006625490728765726\n",
      "[step: 803] loss: 0.0008684111526235938\n",
      "[nstep: 803] loss: 0.006607492920011282\n",
      "[step: 804] loss: 0.0008569394703954458\n",
      "[nstep: 804] loss: 0.006612509489059448\n",
      "[step: 805] loss: 0.0008272597915492952\n",
      "[nstep: 805] loss: 0.006626476533710957\n",
      "[step: 806] loss: 0.0008069775649346411\n",
      "[nstep: 806] loss: 0.006631695199757814\n",
      "[step: 807] loss: 0.0008097293903119862\n",
      "[nstep: 807] loss: 0.006618222687393427\n",
      "[step: 808] loss: 0.0008168139029294252\n",
      "[nstep: 808] loss: 0.006603465415537357\n",
      "[step: 809] loss: 0.000815429026260972\n",
      "[nstep: 809] loss: 0.006603858433663845\n",
      "[step: 810] loss: 0.0008088426548056304\n",
      "[nstep: 810] loss: 0.006612459663301706\n",
      "[step: 811] loss: 0.0008074294310063124\n",
      "[nstep: 811] loss: 0.006618480198085308\n",
      "[step: 812] loss: 0.0008195373811759055\n",
      "[nstep: 812] loss: 0.006612387485802174\n",
      "[step: 813] loss: 0.0008290959522128105\n",
      "[nstep: 813] loss: 0.006601451430469751\n",
      "[step: 814] loss: 0.0008396824123337865\n",
      "[nstep: 814] loss: 0.006597650703042746\n",
      "[step: 815] loss: 0.0008191501256078482\n",
      "[nstep: 815] loss: 0.006600948516279459\n",
      "[step: 816] loss: 0.0008003353723324835\n",
      "[nstep: 816] loss: 0.0066062374971807\n",
      "[step: 817] loss: 0.0007940061041153967\n",
      "[nstep: 817] loss: 0.006605429109185934\n",
      "[step: 818] loss: 0.0008014614577405155\n",
      "[nstep: 818] loss: 0.0065993634052574635\n",
      "[step: 819] loss: 0.000805791059974581\n",
      "[nstep: 819] loss: 0.0065943594090640545\n",
      "[step: 820] loss: 0.000800205918494612\n",
      "[nstep: 820] loss: 0.006593306083232164\n",
      "[step: 821] loss: 0.0007947537233121693\n",
      "[nstep: 821] loss: 0.006595993880182505\n",
      "[step: 822] loss: 0.0007946165278553963\n",
      "[nstep: 822] loss: 0.006597588304430246\n",
      "[step: 823] loss: 0.0008047599694691598\n",
      "[nstep: 823] loss: 0.006596102379262447\n",
      "[step: 824] loss: 0.0008178116986528039\n",
      "[nstep: 824] loss: 0.006592655088752508\n",
      "[step: 825] loss: 0.0008290082914754748\n",
      "[nstep: 825] loss: 0.006589546333998442\n",
      "[step: 826] loss: 0.0008323834044858813\n",
      "[nstep: 826] loss: 0.006588865537196398\n",
      "[step: 827] loss: 0.0008407834684476256\n",
      "[nstep: 827] loss: 0.006589581724256277\n",
      "[step: 828] loss: 0.0008522819844074547\n",
      "[nstep: 828] loss: 0.00659038545563817\n",
      "[step: 829] loss: 0.0008915969519875944\n",
      "[nstep: 829] loss: 0.006589808966964483\n",
      "[step: 830] loss: 0.0008854209445416927\n",
      "[nstep: 830] loss: 0.0065879980102181435\n",
      "[step: 831] loss: 0.0008813655585981905\n",
      "[nstep: 831] loss: 0.006585957482457161\n",
      "[step: 832] loss: 0.0008303197100758553\n",
      "[nstep: 832] loss: 0.006584402173757553\n",
      "[step: 833] loss: 0.0008127167820930481\n",
      "[nstep: 833] loss: 0.006584043614566326\n",
      "[step: 834] loss: 0.0008131949580274522\n",
      "[nstep: 834] loss: 0.006584122311323881\n",
      "[step: 835] loss: 0.0008054470526985824\n",
      "[nstep: 835] loss: 0.00658417958766222\n",
      "[step: 836] loss: 0.0007912550354376435\n",
      "[nstep: 836] loss: 0.006583641283214092\n",
      "[step: 837] loss: 0.0007822868647053838\n",
      "[nstep: 837] loss: 0.006582495290786028\n",
      "[step: 838] loss: 0.0007951229927130044\n",
      "[nstep: 838] loss: 0.006581150460988283\n",
      "[step: 839] loss: 0.000816882005892694\n",
      "[nstep: 839] loss: 0.006579864304512739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 840] loss: 0.0008277601446025074\n",
      "[nstep: 840] loss: 0.006579011678695679\n",
      "[step: 841] loss: 0.0008159168064594269\n",
      "[nstep: 841] loss: 0.006578449625521898\n",
      "[step: 842] loss: 0.0007982272654771805\n",
      "[nstep: 842] loss: 0.006578155793249607\n",
      "[step: 843] loss: 0.0007882168283686042\n",
      "[nstep: 843] loss: 0.006577820982784033\n",
      "[step: 844] loss: 0.0007916198228485882\n",
      "[nstep: 844] loss: 0.006577370688319206\n",
      "[step: 845] loss: 0.0007898719632066786\n",
      "[nstep: 845] loss: 0.006576718296855688\n",
      "[step: 846] loss: 0.0007841627229936421\n",
      "[nstep: 846] loss: 0.006575870793312788\n",
      "[step: 847] loss: 0.0007690333877690136\n",
      "[nstep: 847] loss: 0.006574958562850952\n",
      "[step: 848] loss: 0.0007608581800013781\n",
      "[nstep: 848] loss: 0.006574012339115143\n",
      "[step: 849] loss: 0.0007613966008648276\n",
      "[nstep: 849] loss: 0.006573163904249668\n",
      "[step: 850] loss: 0.0007663427968509495\n",
      "[nstep: 850] loss: 0.00657236110419035\n",
      "[step: 851] loss: 0.0007730491342954338\n",
      "[nstep: 851] loss: 0.00657167611643672\n",
      "[step: 852] loss: 0.0007765564369037747\n",
      "[nstep: 852] loss: 0.0065710400231182575\n",
      "[step: 853] loss: 0.000782209390308708\n",
      "[nstep: 853] loss: 0.006570468191057444\n",
      "[step: 854] loss: 0.0007867800886742771\n",
      "[nstep: 854] loss: 0.0065699066035449505\n",
      "[step: 855] loss: 0.0008075084188021719\n",
      "[nstep: 855] loss: 0.006569373421370983\n",
      "[step: 856] loss: 0.0008407792774960399\n",
      "[nstep: 856] loss: 0.006568842101842165\n",
      "[step: 857] loss: 0.0009140591137111187\n",
      "[nstep: 857] loss: 0.006568331737071276\n",
      "[step: 858] loss: 0.0010055223247036338\n",
      "[nstep: 858] loss: 0.006567835342139006\n",
      "[step: 859] loss: 0.001167156733572483\n",
      "[nstep: 859] loss: 0.0065673766657710075\n",
      "[step: 860] loss: 0.0012175161391496658\n",
      "[nstep: 860] loss: 0.006566978525370359\n",
      "[step: 861] loss: 0.0011994147207587957\n",
      "[nstep: 861] loss: 0.0065666972659528255\n",
      "[step: 862] loss: 0.0009551902185194194\n",
      "[nstep: 862] loss: 0.006566558964550495\n",
      "[step: 863] loss: 0.0007822337211109698\n",
      "[nstep: 863] loss: 0.006566701456904411\n",
      "[step: 864] loss: 0.0008219046867452562\n",
      "[nstep: 864] loss: 0.00656725000590086\n",
      "[step: 865] loss: 0.0009487555362284184\n",
      "[nstep: 865] loss: 0.006568570621311665\n",
      "[step: 866] loss: 0.000976939219981432\n",
      "[nstep: 866] loss: 0.006570947822183371\n",
      "[step: 867] loss: 0.0008543829317204654\n",
      "[nstep: 867] loss: 0.006575471721589565\n",
      "[step: 868] loss: 0.0007743836613371968\n",
      "[nstep: 868] loss: 0.006582761183381081\n",
      "[step: 869] loss: 0.0008315924205817282\n",
      "[nstep: 869] loss: 0.006596377119421959\n",
      "[step: 870] loss: 0.0008846824639476836\n",
      "[nstep: 870] loss: 0.006616022437810898\n",
      "[step: 871] loss: 0.0008438693475909531\n",
      "[nstep: 871] loss: 0.006652286741882563\n",
      "[step: 872] loss: 0.0007750806398689747\n",
      "[nstep: 872] loss: 0.006695564370602369\n",
      "[step: 873] loss: 0.0007750448421575129\n",
      "[nstep: 873] loss: 0.006775176152586937\n",
      "[step: 874] loss: 0.0008222709875553846\n",
      "[nstep: 874] loss: 0.0068460144102573395\n",
      "[step: 875] loss: 0.0008231198298744857\n",
      "[nstep: 875] loss: 0.0069562955759465694\n",
      "[step: 876] loss: 0.0007748609641566873\n",
      "[nstep: 876] loss: 0.007015829905867577\n",
      "[step: 877] loss: 0.0007546587148681283\n",
      "[nstep: 877] loss: 0.007033712230622768\n",
      "[step: 878] loss: 0.0007833238341845572\n",
      "[nstep: 878] loss: 0.006908687297254801\n",
      "[step: 879] loss: 0.0007972729508765042\n",
      "[nstep: 879] loss: 0.006699463352560997\n",
      "[step: 880] loss: 0.0007691203500144184\n",
      "[nstep: 880] loss: 0.0065826657228171825\n",
      "[step: 881] loss: 0.0007510082214139402\n",
      "[nstep: 881] loss: 0.006704494822770357\n",
      "[step: 882] loss: 0.0007680336129851639\n",
      "[nstep: 882] loss: 0.006847701966762543\n",
      "[step: 883] loss: 0.0007793729309923947\n",
      "[nstep: 883] loss: 0.0067307245917618275\n",
      "[step: 884] loss: 0.0007607193547300994\n",
      "[nstep: 884] loss: 0.006580420304089785\n",
      "[step: 885] loss: 0.000742397562135011\n",
      "[nstep: 885] loss: 0.006634157616645098\n",
      "[step: 886] loss: 0.0007500004139728844\n",
      "[nstep: 886] loss: 0.006695040501654148\n",
      "[step: 887] loss: 0.0007615463109686971\n",
      "[nstep: 887] loss: 0.006652483716607094\n",
      "[step: 888] loss: 0.0007539159851148725\n",
      "[nstep: 888] loss: 0.0066201030276715755\n",
      "[step: 889] loss: 0.0007418295135721564\n",
      "[nstep: 889] loss: 0.006611840333789587\n",
      "[step: 890] loss: 0.0007456023013219237\n",
      "[nstep: 890] loss: 0.006587837357074022\n",
      "[step: 891] loss: 0.0007581022218801081\n",
      "[nstep: 891] loss: 0.006615163758397102\n",
      "[step: 892] loss: 0.0007652419153600931\n",
      "[nstep: 892] loss: 0.006637899205088615\n",
      "[step: 893] loss: 0.0007801965693943202\n",
      "[nstep: 893] loss: 0.006576183717697859\n",
      "[step: 894] loss: 0.0008084714645519853\n",
      "[nstep: 894] loss: 0.00655682198703289\n",
      "[step: 895] loss: 0.0008719489560462534\n",
      "[nstep: 895] loss: 0.00659798551350832\n",
      "[step: 896] loss: 0.0008544813608750701\n",
      "[nstep: 896] loss: 0.006592367310076952\n",
      "[step: 897] loss: 0.0008167229825630784\n",
      "[nstep: 897] loss: 0.0065712896175682545\n",
      "[step: 898] loss: 0.0007544977706857026\n",
      "[nstep: 898] loss: 0.006569201592355967\n",
      "[step: 899] loss: 0.0007771895616315305\n",
      "[nstep: 899] loss: 0.006560210138559341\n",
      "[step: 900] loss: 0.0008220456656999886\n",
      "[nstep: 900] loss: 0.00656307116150856\n",
      "[step: 901] loss: 0.0007810578099451959\n",
      "[nstep: 901] loss: 0.00657795462757349\n",
      "[step: 902] loss: 0.0007466413080692291\n",
      "[nstep: 902] loss: 0.006564800161868334\n",
      "[step: 903] loss: 0.0007567150751128793\n",
      "[nstep: 903] loss: 0.0065466794185340405\n",
      "[step: 904] loss: 0.0007850441033951938\n",
      "[nstep: 904] loss: 0.006556023843586445\n",
      "[step: 905] loss: 0.000772764440625906\n",
      "[nstep: 905] loss: 0.006560627371072769\n",
      "[step: 906] loss: 0.0007362159667536616\n",
      "[nstep: 906] loss: 0.006555088330060244\n",
      "[step: 907] loss: 0.0007362141623161733\n",
      "[nstep: 907] loss: 0.0065564666874706745\n",
      "[step: 908] loss: 0.0007581519312225282\n",
      "[nstep: 908] loss: 0.006548803765326738\n",
      "[step: 909] loss: 0.0007554112817160785\n",
      "[nstep: 909] loss: 0.00654226541519165\n",
      "[step: 910] loss: 0.0007381221512332559\n",
      "[nstep: 910] loss: 0.006549778860062361\n",
      "[step: 911] loss: 0.0007311858353205025\n",
      "[nstep: 911] loss: 0.0065515656024217606\n",
      "[step: 912] loss: 0.0007397898589260876\n",
      "[nstep: 912] loss: 0.006544407457113266\n",
      "[step: 913] loss: 0.0007490802090615034\n",
      "[nstep: 913] loss: 0.006542551796883345\n",
      "[step: 914] loss: 0.00073844357393682\n",
      "[nstep: 914] loss: 0.006542136427015066\n",
      "[step: 915] loss: 0.0007244082516990602\n",
      "[nstep: 915] loss: 0.0065394798293709755\n",
      "[step: 916] loss: 0.0007176918443292379\n",
      "[nstep: 916] loss: 0.006542018614709377\n",
      "[step: 917] loss: 0.0007233624928630888\n",
      "[nstep: 917] loss: 0.0065430873073637486\n",
      "[step: 918] loss: 0.0007319948053918779\n",
      "[nstep: 918] loss: 0.006537862122058868\n",
      "[step: 919] loss: 0.0007332820096053183\n",
      "[nstep: 919] loss: 0.006535852327942848\n",
      "[step: 920] loss: 0.0007340621668845415\n",
      "[nstep: 920] loss: 0.0065369070507586\n",
      "[step: 921] loss: 0.0007333210669457912\n",
      "[nstep: 921] loss: 0.006535830441862345\n",
      "[step: 922] loss: 0.0007376556168310344\n",
      "[nstep: 922] loss: 0.006535552907735109\n",
      "[step: 923] loss: 0.0007413707207888365\n",
      "[nstep: 923] loss: 0.0065361373126506805\n",
      "[step: 924] loss: 0.000752953696064651\n",
      "[nstep: 924] loss: 0.006534049287438393\n",
      "[step: 925] loss: 0.0007669487968087196\n",
      "[nstep: 925] loss: 0.006531498860567808\n",
      "[step: 926] loss: 0.00079166708746925\n",
      "[nstep: 926] loss: 0.006531620863825083\n",
      "[step: 927] loss: 0.0008180909208022058\n",
      "[nstep: 927] loss: 0.00653166975826025\n",
      "[step: 928] loss: 0.000867219059728086\n",
      "[nstep: 928] loss: 0.006530707236379385\n",
      "[step: 929] loss: 0.0008999072597362101\n",
      "[nstep: 929] loss: 0.006530649960041046\n",
      "[step: 930] loss: 0.000963643891736865\n",
      "[nstep: 930] loss: 0.0065302359871566296\n",
      "[step: 931] loss: 0.0008969433256424963\n",
      "[nstep: 931] loss: 0.006528506055474281\n",
      "[step: 932] loss: 0.0008194656111299992\n",
      "[nstep: 932] loss: 0.006527303252369165\n",
      "[step: 933] loss: 0.0007485050009563565\n",
      "[nstep: 933] loss: 0.006527088116854429\n",
      "[step: 934] loss: 0.0007455031154677272\n",
      "[nstep: 934] loss: 0.006526544690132141\n",
      "[step: 935] loss: 0.0007722121081314981\n",
      "[nstep: 935] loss: 0.006525842472910881\n",
      "[step: 936] loss: 0.0007684857118874788\n",
      "[nstep: 936] loss: 0.006525708362460136\n",
      "[step: 937] loss: 0.0007741554290987551\n",
      "[nstep: 937] loss: 0.006525361444801092\n",
      "[step: 938] loss: 0.0007865321822464466\n",
      "[nstep: 938] loss: 0.006524331402033567\n",
      "[step: 939] loss: 0.0007728229975327849\n",
      "[nstep: 939] loss: 0.0065234629437327385\n",
      "[step: 940] loss: 0.000733383756596595\n",
      "[nstep: 940] loss: 0.00652291439473629\n",
      "[step: 941] loss: 0.0007166434661485255\n",
      "[nstep: 941] loss: 0.006522123236209154\n",
      "[step: 942] loss: 0.0007317294948734343\n",
      "[nstep: 942] loss: 0.006521292962133884\n",
      "[step: 943] loss: 0.0007462540525011718\n",
      "[nstep: 943] loss: 0.006520863156765699\n",
      "[step: 944] loss: 0.0007450089906342328\n",
      "[nstep: 944] loss: 0.006520486436784267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 945] loss: 0.0007338337600231171\n",
      "[nstep: 945] loss: 0.006519890855997801\n",
      "[step: 946] loss: 0.0007214095094241202\n",
      "[nstep: 946] loss: 0.006519325543195009\n",
      "[step: 947] loss: 0.0007095055771060288\n",
      "[nstep: 947] loss: 0.006518935319036245\n",
      "[step: 948] loss: 0.0007117530331015587\n",
      "[nstep: 948] loss: 0.006518423557281494\n",
      "[step: 949] loss: 0.0007225774461403489\n",
      "[nstep: 949] loss: 0.0065177553333342075\n",
      "[step: 950] loss: 0.0007257873076014221\n",
      "[nstep: 950] loss: 0.006517181172966957\n",
      "[step: 951] loss: 0.0007159580709412694\n",
      "[nstep: 951] loss: 0.006516674067825079\n",
      "[step: 952] loss: 0.0007041627541184425\n",
      "[nstep: 952] loss: 0.006516084540635347\n",
      "[step: 953] loss: 0.0007037617033347487\n",
      "[nstep: 953] loss: 0.006515459157526493\n",
      "[step: 954] loss: 0.0007108788704499602\n",
      "[nstep: 954] loss: 0.006514930631965399\n",
      "[step: 955] loss: 0.000719198549631983\n",
      "[nstep: 955] loss: 0.006514470558613539\n",
      "[step: 956] loss: 0.0007233831565827131\n",
      "[nstep: 956] loss: 0.006513969972729683\n",
      "[step: 957] loss: 0.0007367093930952251\n",
      "[nstep: 957] loss: 0.006513535976409912\n",
      "[step: 958] loss: 0.0007474511512555182\n",
      "[nstep: 958] loss: 0.006513249594718218\n",
      "[step: 959] loss: 0.0007675179513171315\n",
      "[nstep: 959] loss: 0.006513157859444618\n",
      "[step: 960] loss: 0.0007613070192746818\n",
      "[nstep: 960] loss: 0.006513284519314766\n",
      "[step: 961] loss: 0.0007715711835771799\n",
      "[nstep: 961] loss: 0.0065139345824718475\n",
      "[step: 962] loss: 0.0007817865698598325\n",
      "[nstep: 962] loss: 0.006515422370284796\n",
      "[step: 963] loss: 0.0007951879524625838\n",
      "[nstep: 963] loss: 0.0065186526626348495\n",
      "[step: 964] loss: 0.0007962376112118363\n",
      "[nstep: 964] loss: 0.006524293217808008\n",
      "[step: 965] loss: 0.0007813645643182099\n",
      "[nstep: 965] loss: 0.006535698194056749\n",
      "[step: 966] loss: 0.000750578532461077\n",
      "[nstep: 966] loss: 0.006554269697517157\n",
      "[step: 967] loss: 0.000723745150025934\n",
      "[nstep: 967] loss: 0.006590615026652813\n",
      "[step: 968] loss: 0.0007029013941064477\n",
      "[nstep: 968] loss: 0.006639920640736818\n",
      "[step: 969] loss: 0.0006979405879974365\n",
      "[nstep: 969] loss: 0.006724068894982338\n",
      "[step: 970] loss: 0.0007037892937660217\n",
      "[nstep: 970] loss: 0.0067896293476223946\n",
      "[step: 971] loss: 0.0007111019222065806\n",
      "[nstep: 971] loss: 0.006845655385404825\n",
      "[step: 972] loss: 0.0007100760121829808\n",
      "[nstep: 972] loss: 0.006781931035220623\n",
      "[step: 973] loss: 0.0007029215921647847\n",
      "[nstep: 973] loss: 0.0066593666560947895\n",
      "[step: 974] loss: 0.0007003949722275138\n",
      "[nstep: 974] loss: 0.006537327542901039\n",
      "[step: 975] loss: 0.0007062085205689073\n",
      "[nstep: 975] loss: 0.006512449588626623\n",
      "[step: 976] loss: 0.0007234683725982904\n",
      "[nstep: 976] loss: 0.006576142739504576\n",
      "[step: 977] loss: 0.0007420862093567848\n",
      "[nstep: 977] loss: 0.006637020502239466\n",
      "[step: 978] loss: 0.0007868126849643886\n",
      "[nstep: 978] loss: 0.006625977344810963\n",
      "[step: 979] loss: 0.0007965631084516644\n",
      "[nstep: 979] loss: 0.006550343241542578\n",
      "[step: 980] loss: 0.000807564880233258\n",
      "[nstep: 980] loss: 0.006506407633423805\n",
      "[step: 981] loss: 0.0007607648149132729\n",
      "[nstep: 981] loss: 0.0065346090123057365\n",
      "[step: 982] loss: 0.000765330798458308\n",
      "[nstep: 982] loss: 0.006580108776688576\n",
      "[step: 983] loss: 0.0007942912634462118\n",
      "[nstep: 983] loss: 0.006580224260687828\n",
      "[step: 984] loss: 0.0007948157726787031\n",
      "[nstep: 984] loss: 0.006535280961543322\n",
      "[step: 985] loss: 0.0007451712153851986\n",
      "[nstep: 985] loss: 0.006503596901893616\n",
      "[step: 986] loss: 0.0007030398119240999\n",
      "[nstep: 986] loss: 0.006519131828099489\n",
      "[step: 987] loss: 0.0006967678782530129\n",
      "[nstep: 987] loss: 0.006549650337547064\n",
      "[step: 988] loss: 0.0007101258961483836\n",
      "[nstep: 988] loss: 0.006551522761583328\n",
      "[step: 989] loss: 0.0007093623280525208\n",
      "[nstep: 989] loss: 0.006524026859551668\n",
      "[step: 990] loss: 0.0007024889928288758\n",
      "[nstep: 990] loss: 0.006500565446913242\n",
      "[step: 991] loss: 0.000701016397215426\n",
      "[nstep: 991] loss: 0.006505997385829687\n",
      "[step: 992] loss: 0.0007054652669467032\n",
      "[nstep: 992] loss: 0.006526260171085596\n",
      "[step: 993] loss: 0.0007190851611085236\n",
      "[nstep: 993] loss: 0.006531822960823774\n",
      "[step: 994] loss: 0.0007234018412418664\n",
      "[nstep: 994] loss: 0.006517324130982161\n",
      "[step: 995] loss: 0.0007284624152816832\n",
      "[nstep: 995] loss: 0.006499603856354952\n",
      "[step: 996] loss: 0.0007193249766714871\n",
      "[nstep: 996] loss: 0.006496401969343424\n",
      "[step: 997] loss: 0.0007126840646378696\n",
      "[nstep: 997] loss: 0.006507562007755041\n",
      "[step: 998] loss: 0.0007148520089685917\n",
      "[nstep: 998] loss: 0.006515751592814922\n",
      "[step: 999] loss: 0.0007246028399094939\n",
      "[nstep: 999] loss: 0.006511729210615158\n",
      "[step: 1000] loss: 0.0007260386482812464\n",
      "[nstep: 1000] loss: 0.006500717718154192\n",
      "RMSE: 0.03320777416229248 \n",
      "RMSE_with_noise0.08086922764778137 \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    mpl.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = np.float32)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1001\n",
    "epsilon = 1\n",
    "e = math.exp(epsilon)\n",
    "q = (1/(e+1))\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL만들기\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_mean(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    #multi_cell2 =  tf.contrib.rnn.MultiRNNCell([cell2]*2)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])*2\n",
    "x2 = x1+0.33*2\n",
    "x3 = x2+0.33*2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[nstep: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    rmse_val_with_noise = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict_with_noise})\n",
    "    print(\"RMSE: {} \\nRMSE_with_noise{} \".format(rmse_val,rmse_val_with_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation(real&predict) :0.9838788016409673\n",
      "correlation(real&noise) :0.9408963627942616\n"
     ]
    }
   ],
   "source": [
    "def correlation(data1,data2):\n",
    "    \n",
    "\n",
    "    data1_mean = np.mean(data1)\n",
    "    data2_mean = np.mean(data2)\n",
    "\n",
    "    X_1 = data1-data1_mean\n",
    "    Y_1 = data2-data2_mean\n",
    "    #분자\n",
    "    a_1 = np.sum(np.multiply(X_1,Y_1))\n",
    "\n",
    "    #분모\n",
    "    b_1 = np.sqrt(np.sum(np.square(X_1)))\n",
    "    c_1 = np.sqrt(np.sum(np.square(Y_1)))\n",
    "\n",
    "    #상관계수\n",
    "    r_1 = (a_1/(b_1*c_1))\n",
    "\n",
    "    print(\"correlation(real&predict) :{}\".format(r_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.9878891  0.96181054 ... 0.57012402 0.55135712 0.70255796]\n",
      " [0.9878891  1.         0.98099501 ... 0.5611616  0.53883772 0.68989686]\n",
      " [0.96181054 0.98099501 1.         ... 0.60776359 0.58202756 0.72302576]\n",
      " ...\n",
      " [0.57012402 0.5611616  0.60776359 ... 1.         0.98879793 0.97421917]\n",
      " [0.55135712 0.53883772 0.58202756 ... 0.98879793 1.         0.95518998]\n",
      " [0.70255796 0.68989686 0.72302576 ... 0.97421917 0.95518998 1.        ]] [[1.         0.9878891  0.96181054 ... 0.44971721 0.59663495 0.60433858]\n",
      " [0.9878891  1.         0.98099501 ... 0.45095317 0.57352385 0.58368285]\n",
      " [0.96181054 0.98099501 1.         ... 0.49497364 0.59088409 0.60590495]\n",
      " ...\n",
      " [0.44971721 0.45095317 0.49497364 ... 1.         0.93935503 0.94930187]\n",
      " [0.59663495 0.57352385 0.59088409 ... 0.93935503 1.         0.97327264]\n",
      " [0.60433858 0.58368285 0.60590495 ... 0.94930187 0.97327264 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tnp = np.corrcoef(testY,test_predict)\n",
    "tnn = np.corrcoef(testY,test_predict_with_noise)\n",
    "print(tnp,tnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y = np.array(range(0,len(testY)))\n",
    "index_x = np.array(range(0,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y = np.array(range(0,len(testY)))\n",
    "index_x = np.array(range(0,25))\n",
    "index_x_grid, index_y_grid = np.meshgrid(index_x,index_y)\n",
    "index_x_3d,index_y_3d = index_x_grid.ravel(),index_y_grid.ravel()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111,projection='3d')\n",
    "wire = ax1.plot_wireframe(index_x_grid*2,index_y_grid*2,testY,color='r',linewidth=0.2)\n",
    "wire = ax1.plot_wireframe(index_x_grid*2,index_y_grid*2,test_predict,color='g',linewidth=0.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c2dd6ad0f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_wireframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_x_grid\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_y_grid\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mwire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_wireframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_x_grid\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_y_grid\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_predict_with_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3266\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mmanagers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index_y = np.array(range(0,len(testY)))\n",
    "index_x = np.array(range(0,25))\n",
    "index_x_grid, index_y_grid = np.meshgrid(index_x,index_y)\n",
    "index_x_3d,index_y_3d = index_x_grid.ravel(),index_y_grid.ravel()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(111,projection='3d')\n",
    "wire = ax2.plot_wireframe(index_x_grid*2,index_y_grid*2,testY,color='r',linewidth=0.2)\n",
    "wire = ax2.plot_wireframe(index_x_grid*2,index_y_grid*2,test_predict_with_noise,color='b',linewidth=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance & Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02466743 0.03248346 0.08497622 ... 0.01174273 0.03616501 0.05264915] \n",
      " 0.03160617722748938 \n",
      " 38.05383738189723\n"
     ]
    }
   ],
   "source": [
    "#Euclidean Distance\n",
    "ED = abs(testY[:,0]-test_predict[:,0])\n",
    "print(ED,\"\\n\",np.mean(ED),\"\\n\",sum(ED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5783497323946785\n"
     ]
    }
   ],
   "source": [
    "rmp = testY[:,0] - test_predict[:,0]\n",
    "rmp_2 = np.square(rmp)\n",
    "ED_2 = np.sqrt(sum(rmp_2))\n",
    "print(ED_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01949119639398486\n"
     ]
    }
   ],
   "source": [
    "x = np.array(testY[:,0]).reshape(-1,1)\n",
    "y = np.array(test_predict[:,0]).reshape(-1,1)\n",
    "\n",
    "EN = lambda x, y: np.abs(x - y)\n",
    "\n",
    "d, cost_matrix, acc_cost_matrix, path = dtw(x, y, dist=EN)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06399202, 0.05678671, 0.16861615, ..., 0.04548428, 0.11664701,\n",
       "        0.03301746],\n",
       "       [0.05242814, 0.04522283, 0.15705228, ..., 0.05704816, 0.12821089,\n",
       "        0.04458134],\n",
       "       [0.04503882, 0.05224413, 0.05958532, ..., 0.15451512, 0.22567785,\n",
       "        0.1420483 ],\n",
       "       ...,\n",
       "       [0.15255663, 0.14535132, 0.25718077, ..., 0.04308033, 0.0280824 ,\n",
       "        0.05554715],\n",
       "       [0.20762271, 0.2004174 , 0.31224684, ..., 0.09814641, 0.02698368,\n",
       "        0.11061323],\n",
       "       [0.20248321, 0.1952779 , 0.30710734, ..., 0.09300691, 0.02184418,\n",
       "        0.10547373]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7033273148571233"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_cost_matrix[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01949119639398486"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 18, 19, 19, 20, 21, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 33, 34, 35, 36, 37, 38, 38, 38, 38, 38, 38, 38, 39, 40, 41,\n",
       "        42, 43, 44, 45, 46, 47, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 56,\n",
       "        56, 56, 56, 56, 57, 58, 59, 60, 61, 62, 63, 63, 64, 64, 65, 65, 65,\n",
       "        66, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
       "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 92, 92, 92, 92, 93, 94,\n",
       "        94, 94]),\n",
       " array([ 0,  1,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 32, 32, 32, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "        43, 43, 44, 45, 45, 46, 47, 48, 48, 49, 49, 50, 50, 51, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 58, 59, 59, 60, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 78, 78, 78, 79, 79, 79,\n",
       "        80, 81, 82, 83, 84, 85, 85, 85, 85, 85, 86, 87, 88, 89, 90, 91, 92,\n",
       "        93, 94]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW(A, B, window=sys.maxint, d=lambda x, y: abs(x - y)):\n",
    "# 비용 행렬 초기화 \n",
    "    A, B = np.array(A), np.array(B)\n",
    "    M, N = len(A), len(B)\n",
    "    cost = sys.maxint * np.ones((M, N))\n",
    "\n",
    "# 첫번째 로우,컬럼 채우기\n",
    "    cost[0, 0] = d(A[0], B[0])\n",
    "    for i in range(1, M):\n",
    "        cost[i, 0] = cost[i - 1, 0] + d(A[i], B[0])\n",
    "\n",
    "    for j in range(1, N):\n",
    "        cost[0, j] = cost[0, j - 1] + d(A[0], B[j])\n",
    "# 나머지 행렬 채우기 \n",
    "    for i in range(1, M):\n",
    "        for j in range(max(1, i - window), min(N, i + window)):\n",
    "            choices = cost[i - 1, j - 1], cost[i, j - 1], cost[i - 1, j]\n",
    "            cost[i, j] = min(choices) + d(A[i], B[j])\n",
    "\n",
    "# 최적 경로 구하기 \n",
    "    n, m = N - 1, M - 1\n",
    "    path = []\n",
    "\n",
    "    while (m, n) != (0, 0):\n",
    "        path.append((m, n))\n",
    "        m, n = min((m - 1, n), (m, n - 1), (m - 1, n - 1), key=lambda x: cost[x[0], x[1]])\n",
    "\n",
    "    path.append((0, 0))\n",
    "    return cost[-1, -1], path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Seoul_Jongno-gu,\n",
    "2. Seoul_Jung-gu,\n",
    "3. Seoul_Yongsan-gu,\n",
    "4. Seoul_Seongdong-gu,\n",
    "5. Seoul_Gwangjin-gu,\n",
    "6. Seoul_Dongdaemun-gu, \n",
    "7. Seoul_Jungnang-gu,\n",
    "8. Seoul_Seongbuk-gu,\n",
    "9. Seoul_Gangbuk-gu,\n",
    "10. Seoul_Dobong-gu,\n",
    "11. Seoul_Nowon-gu,\n",
    "12. Seoul_Eunpyeong-gu,\n",
    "13. Seoul_Seodaemun-gu,\n",
    "14. Seoul_Mapo-gu,\n",
    "15. Seoul_Yangcheon-gu,\n",
    "16. Seoul_Gangseo-gu,\n",
    "17. Seoul_Guro-gu,\n",
    "18. Seoul_Geumcheon-gu,\n",
    "19. Seoul_Yeongdeungpo-gu,\n",
    "20. Seoul_Dongjak-gu,\n",
    "21. Seoul_Gwanak-gu,\n",
    "22. Seoul_Seocho-gu,\n",
    "23. Seoul_Gangnam-gu,\n",
    "24. Seoul_Songpa-gu,\n",
    "25. Seoul_Gangdong-gu,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-cbd7dfb7e3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eunpyeong-gu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3266\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mmanagers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.plot(index_y,testY[:,11],label = 'real',color ='r')\n",
    "plt.plot(index_y,test_predict[:,11],label = 'predict',color ='b')\n",
    "plt.plot(index_y,test_predict_with_noise[:,11],label = 'noise',color ='g')\n",
    "plt.title('Eunpyeong-gu')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
