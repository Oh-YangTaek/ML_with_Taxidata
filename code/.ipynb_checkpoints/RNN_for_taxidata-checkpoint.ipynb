{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.24513110518455505\n",
      "[step: 1] loss: 0.17276914417743683\n",
      "[step: 2] loss: 0.12625710666179657\n",
      "[step: 3] loss: 0.09312521666288376\n",
      "[step: 4] loss: 0.06968091428279877\n",
      "[step: 5] loss: 0.053251445293426514\n",
      "[step: 6] loss: 0.04264592379331589\n",
      "[step: 7] loss: 0.03496498242020607\n",
      "[step: 8] loss: 0.030584165826439857\n",
      "[step: 9] loss: 0.02799321338534355\n",
      "[step: 10] loss: 0.024628156796097755\n",
      "[step: 11] loss: 0.022028572857379913\n",
      "[step: 12] loss: 0.0208213422447443\n",
      "[step: 13] loss: 0.019313616678118706\n",
      "[step: 14] loss: 0.019049759954214096\n",
      "[step: 15] loss: 0.018254227936267853\n",
      "[step: 16] loss: 0.017184969037771225\n",
      "[step: 17] loss: 0.01622062362730503\n",
      "[step: 18] loss: 0.015558469109237194\n",
      "[step: 19] loss: 0.014935644343495369\n",
      "[step: 20] loss: 0.01468423567712307\n",
      "[step: 21] loss: 0.013777244836091995\n",
      "[step: 22] loss: 0.013815595768392086\n",
      "[step: 23] loss: 0.012686308473348618\n",
      "[step: 24] loss: 0.012535630725324154\n",
      "[step: 25] loss: 0.011629439890384674\n",
      "[step: 26] loss: 0.011381182819604874\n",
      "[step: 27] loss: 0.01083948090672493\n",
      "[step: 28] loss: 0.010427426546812057\n",
      "[step: 29] loss: 0.010169552639126778\n",
      "[step: 30] loss: 0.009747565723955631\n",
      "[step: 31] loss: 0.009579821489751339\n",
      "[step: 32] loss: 0.00932229496538639\n",
      "[step: 33] loss: 0.009131323546171188\n",
      "[step: 34] loss: 0.008859327994287014\n",
      "[step: 35] loss: 0.008595947176218033\n",
      "[step: 36] loss: 0.00825483724474907\n",
      "[step: 37] loss: 0.008077556267380714\n",
      "[step: 38] loss: 0.007685958873480558\n",
      "[step: 39] loss: 0.007599688600748777\n",
      "[step: 40] loss: 0.007203962653875351\n",
      "[step: 41] loss: 0.007092166692018509\n",
      "[step: 42] loss: 0.006854902487248182\n",
      "[step: 43] loss: 0.006653579417616129\n",
      "[step: 44] loss: 0.006482682190835476\n",
      "[step: 45] loss: 0.006247575394809246\n",
      "[step: 46] loss: 0.006123124621808529\n",
      "[step: 47] loss: 0.0058628348633646965\n",
      "[step: 48] loss: 0.005745511967688799\n",
      "[step: 49] loss: 0.005505919456481934\n",
      "[step: 50] loss: 0.0053916629403829575\n",
      "[step: 51] loss: 0.005200500600039959\n",
      "[step: 52] loss: 0.005110315978527069\n",
      "[step: 53] loss: 0.00492619164288044\n",
      "[step: 54] loss: 0.004836276639252901\n",
      "[step: 55] loss: 0.0046972231939435005\n",
      "[step: 56] loss: 0.0046163881197571754\n",
      "[step: 57] loss: 0.0044958321377635\n",
      "[step: 58] loss: 0.004414649680256844\n",
      "[step: 59] loss: 0.00431381119415164\n",
      "[step: 60] loss: 0.004234559368342161\n",
      "[step: 61] loss: 0.004167997278273106\n",
      "[step: 62] loss: 0.004096907563507557\n",
      "[step: 63] loss: 0.004069090820848942\n",
      "[step: 64] loss: 0.004064206499606371\n",
      "[step: 65] loss: 0.004198696929961443\n",
      "[step: 66] loss: 0.0038928890135139227\n",
      "[step: 67] loss: 0.0039626420475542545\n",
      "[step: 68] loss: 0.0040344311855733395\n",
      "[step: 69] loss: 0.003903399920091033\n",
      "[step: 70] loss: 0.0037520139012485743\n",
      "[step: 71] loss: 0.003897571936249733\n",
      "[step: 72] loss: 0.003661154303699732\n",
      "[step: 73] loss: 0.003635046072304249\n",
      "[step: 74] loss: 0.0037060219328850508\n",
      "[step: 75] loss: 0.0035224517341703176\n",
      "[step: 76] loss: 0.0035403461661189795\n",
      "[step: 77] loss: 0.0035231777001172304\n",
      "[step: 78] loss: 0.0033984303008764982\n",
      "[step: 79] loss: 0.0034434727858752012\n",
      "[step: 80] loss: 0.003405753057450056\n",
      "[step: 81] loss: 0.003319153795018792\n",
      "[step: 82] loss: 0.0033548790961503983\n",
      "[step: 83] loss: 0.003288405714556575\n",
      "[step: 84] loss: 0.0032312823459506035\n",
      "[step: 85] loss: 0.0032586839515715837\n",
      "[step: 86] loss: 0.0031965242233127356\n",
      "[step: 87] loss: 0.0031601113732904196\n",
      "[step: 88] loss: 0.0031733321957290173\n",
      "[step: 89] loss: 0.0031191809102892876\n",
      "[step: 90] loss: 0.003091831924393773\n",
      "[step: 91] loss: 0.0030988582875579596\n",
      "[step: 92] loss: 0.0030641823541373014\n",
      "[step: 93] loss: 0.0030263832304626703\n",
      "[step: 94] loss: 0.0030412347987294197\n",
      "[step: 95] loss: 0.0030044331215322018\n",
      "[step: 96] loss: 0.0029772690031677485\n",
      "[step: 97] loss: 0.002966815372928977\n",
      "[step: 98] loss: 0.002963489620015025\n",
      "[step: 99] loss: 0.002920662984251976\n",
      "[step: 100] loss: 0.00291713560000062\n",
      "[step: 101] loss: 0.0029076021164655685\n",
      "[step: 102] loss: 0.0028899682220071554\n",
      "[step: 103] loss: 0.002865175949409604\n",
      "[step: 104] loss: 0.002856515347957611\n",
      "[step: 105] loss: 0.0028529546689242125\n",
      "[step: 106] loss: 0.0028317063115537167\n",
      "[step: 107] loss: 0.002816380700096488\n",
      "[step: 108] loss: 0.002802835311740637\n",
      "[step: 109] loss: 0.002799554727971554\n",
      "[step: 110] loss: 0.002788199344649911\n",
      "[step: 111] loss: 0.002771585015580058\n",
      "[step: 112] loss: 0.002759165596216917\n",
      "[step: 113] loss: 0.002746733371168375\n",
      "[step: 114] loss: 0.0027411626651883125\n",
      "[step: 115] loss: 0.00273378798738122\n",
      "[step: 116] loss: 0.0027244535740464926\n",
      "[step: 117] loss: 0.0027151908725500107\n",
      "[step: 118] loss: 0.002703038975596428\n",
      "[step: 119] loss: 0.002692253328859806\n",
      "[step: 120] loss: 0.002682899124920368\n",
      "[step: 121] loss: 0.0026721959002316\n",
      "[step: 122] loss: 0.0026637993287295103\n",
      "[step: 123] loss: 0.002655643969774246\n",
      "[step: 124] loss: 0.0026477829087525606\n",
      "[step: 125] loss: 0.0026434261817485094\n",
      "[step: 126] loss: 0.002645455999299884\n",
      "[step: 127] loss: 0.0026621678844094276\n",
      "[step: 128] loss: 0.0027338669169694185\n",
      "[step: 129] loss: 0.0028972935397177935\n",
      "[step: 130] loss: 0.003307302249595523\n",
      "[step: 131] loss: 0.003311291104182601\n",
      "[step: 132] loss: 0.0030764611437916756\n",
      "[step: 133] loss: 0.0026215449906885624\n",
      "[step: 134] loss: 0.0027672869618982077\n",
      "[step: 135] loss: 0.0030495701357722282\n",
      "[step: 136] loss: 0.002690396271646023\n",
      "[step: 137] loss: 0.0026200474239885807\n",
      "[step: 138] loss: 0.0028597177006304264\n",
      "[step: 139] loss: 0.002639255952090025\n",
      "[step: 140] loss: 0.0025860301684588194\n",
      "[step: 141] loss: 0.002744503552094102\n",
      "[step: 142] loss: 0.0025717082899063826\n",
      "[step: 143] loss: 0.002575570484623313\n",
      "[step: 144] loss: 0.0026679341681301594\n",
      "[step: 145] loss: 0.002522821305319667\n",
      "[step: 146] loss: 0.002566396025940776\n",
      "[step: 147] loss: 0.00260250479914248\n",
      "[step: 148] loss: 0.002492440165951848\n",
      "[step: 149] loss: 0.0025487318634986877\n",
      "[step: 150] loss: 0.0025492229033261538\n",
      "[step: 151] loss: 0.002471152925863862\n",
      "[step: 152] loss: 0.0025238459929823875\n",
      "[step: 153] loss: 0.0025080442428588867\n",
      "[step: 154] loss: 0.0024526333436369896\n",
      "[step: 155] loss: 0.0024974243715405464\n",
      "[step: 156] loss: 0.002474184613674879\n",
      "[step: 157] loss: 0.0024364525452256203\n",
      "[step: 158] loss: 0.0024687210097908974\n",
      "[step: 159] loss: 0.0024478386621922255\n",
      "[step: 160] loss: 0.002418259624391794\n",
      "[step: 161] loss: 0.0024410111363977194\n",
      "[step: 162] loss: 0.002426228253170848\n",
      "[step: 163] loss: 0.002399650402367115\n",
      "[step: 164] loss: 0.0024151073303073645\n",
      "[step: 165] loss: 0.0024072802625596523\n",
      "[step: 166] loss: 0.0023817725013941526\n",
      "[step: 167] loss: 0.002389180473983288\n",
      "[step: 168] loss: 0.002389682224020362\n",
      "[step: 169] loss: 0.0023667621426284313\n",
      "[step: 170] loss: 0.0023642973974347115\n",
      "[step: 171] loss: 0.0023682177998125553\n",
      "[step: 172] loss: 0.0023523729760199785\n",
      "[step: 173] loss: 0.00234199152328074\n",
      "[step: 174] loss: 0.002345368266105652\n",
      "[step: 175] loss: 0.0023392706643790007\n",
      "[step: 176] loss: 0.0023274098057299852\n",
      "[step: 177] loss: 0.0023304233327507973\n",
      "[step: 178] loss: 0.0023577201645821333\n",
      "[step: 179] loss: 0.0024478265549987555\n",
      "[step: 180] loss: 0.00275922822766006\n",
      "[step: 181] loss: 0.0025519405025988817\n",
      "[step: 182] loss: 0.0023255283012986183\n",
      "[step: 183] loss: 0.0025180145166814327\n",
      "[step: 184] loss: 0.002418596763163805\n",
      "[step: 185] loss: 0.0023801708593964577\n",
      "[step: 186] loss: 0.0024568287190049887\n",
      "[step: 187] loss: 0.0023077551741153\n",
      "[step: 188] loss: 0.002421760931611061\n",
      "[step: 189] loss: 0.002377915196120739\n",
      "[step: 190] loss: 0.002292054705321789\n",
      "[step: 191] loss: 0.0024243176449090242\n",
      "[step: 192] loss: 0.002372307935729623\n",
      "[step: 193] loss: 0.0023147533647716045\n",
      "[step: 194] loss: 0.002368387533351779\n",
      "[step: 195] loss: 0.0022936100140213966\n",
      "[step: 196] loss: 0.002352957148104906\n",
      "[step: 197] loss: 0.0023193350061774254\n",
      "[step: 198] loss: 0.0022574025206267834\n",
      "[step: 199] loss: 0.0023326962254941463\n",
      "[step: 200] loss: 0.0022713886573910713\n",
      "[step: 201] loss: 0.002294034231454134\n",
      "[step: 202] loss: 0.0022511384449899197\n",
      "[step: 203] loss: 0.002245409181341529\n",
      "[step: 204] loss: 0.0022945005912333727\n",
      "[step: 205] loss: 0.0022471274714916945\n",
      "[step: 206] loss: 0.0022416263818740845\n",
      "[step: 207] loss: 0.0022275601513683796\n",
      "[step: 208] loss: 0.0022230350878089666\n",
      "[step: 209] loss: 0.002239059656858444\n",
      "[step: 210] loss: 0.002202217001467943\n",
      "[step: 211] loss: 0.002203201176598668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 212] loss: 0.002201726194471121\n",
      "[step: 213] loss: 0.002192618791013956\n",
      "[step: 214] loss: 0.002204346703365445\n",
      "[step: 215] loss: 0.002189913298934698\n",
      "[step: 216] loss: 0.002174450783059001\n",
      "[step: 217] loss: 0.002181145129725337\n",
      "[step: 218] loss: 0.0021677811164408922\n",
      "[step: 219] loss: 0.0021801076363772154\n",
      "[step: 220] loss: 0.0021812915802001953\n",
      "[step: 221] loss: 0.0021609929390251637\n",
      "[step: 222] loss: 0.002158929593861103\n",
      "[step: 223] loss: 0.0021498887799680233\n",
      "[step: 224] loss: 0.002137118950486183\n",
      "[step: 225] loss: 0.0021442922297865152\n",
      "[step: 226] loss: 0.0021445145830512047\n",
      "[step: 227] loss: 0.002149427542462945\n",
      "[step: 228] loss: 0.0021615675650537014\n",
      "[step: 229] loss: 0.0021782589610666037\n",
      "[step: 230] loss: 0.002191550098359585\n",
      "[step: 231] loss: 0.0022321788128465414\n",
      "[step: 232] loss: 0.0022186958231031895\n",
      "[step: 233] loss: 0.00218926090747118\n",
      "[step: 234] loss: 0.002158903982490301\n",
      "[step: 235] loss: 0.0021198547910898924\n",
      "[step: 236] loss: 0.0021009808406233788\n",
      "[step: 237] loss: 0.0021075434051454067\n",
      "[step: 238] loss: 0.0021252762526273727\n",
      "[step: 239] loss: 0.002138372277840972\n",
      "[step: 240] loss: 0.0022181274835020304\n",
      "[step: 241] loss: 0.0022968349512666464\n",
      "[step: 242] loss: 0.002451268257573247\n",
      "[step: 243] loss: 0.0023875536862760782\n",
      "[step: 244] loss: 0.002393941627815366\n",
      "[step: 245] loss: 0.0021592359989881516\n",
      "[step: 246] loss: 0.002144220285117626\n",
      "[step: 247] loss: 0.0022992724552750587\n",
      "[step: 248] loss: 0.002222988987341523\n",
      "[step: 249] loss: 0.0022053811699151993\n",
      "[step: 250] loss: 0.0021017026156187057\n",
      "[step: 251] loss: 0.00208096276037395\n",
      "[step: 252] loss: 0.0021527388598769903\n",
      "[step: 253] loss: 0.0021492510568350554\n",
      "[step: 254] loss: 0.0021350376773625612\n",
      "[step: 255] loss: 0.002089168643578887\n",
      "[step: 256] loss: 0.002048025606200099\n",
      "[step: 257] loss: 0.002059550955891609\n",
      "[step: 258] loss: 0.002089429646730423\n",
      "[step: 259] loss: 0.0020834181923419237\n",
      "[step: 260] loss: 0.0020527432207018137\n",
      "[step: 261] loss: 0.0020293998531997204\n",
      "[step: 262] loss: 0.0020332401618361473\n",
      "[step: 263] loss: 0.002054019132629037\n",
      "[step: 264] loss: 0.002074048388749361\n",
      "[step: 265] loss: 0.002059386344626546\n",
      "[step: 266] loss: 0.002035171492025256\n",
      "[step: 267] loss: 0.002013307996094227\n",
      "[step: 268] loss: 0.0020056823268532753\n",
      "[step: 269] loss: 0.002016106154769659\n",
      "[step: 270] loss: 0.002024991437792778\n",
      "[step: 271] loss: 0.002031587064266205\n",
      "[step: 272] loss: 0.00202974583953619\n",
      "[step: 273] loss: 0.002028410555794835\n",
      "[step: 274] loss: 0.002009693533182144\n",
      "[step: 275] loss: 0.0019947371911257505\n",
      "[step: 276] loss: 0.0019842786714434624\n",
      "[step: 277] loss: 0.0019768723286688328\n",
      "[step: 278] loss: 0.0019823876209557056\n",
      "[step: 279] loss: 0.0019875906873494387\n",
      "[step: 280] loss: 0.0019948165863752365\n",
      "[step: 281] loss: 0.002010759199038148\n",
      "[step: 282] loss: 0.002062574727460742\n",
      "[step: 283] loss: 0.0020706881769001484\n",
      "[step: 284] loss: 0.0020797420293092728\n",
      "[step: 285] loss: 0.002072863280773163\n",
      "[step: 286] loss: 0.0020462945103645325\n",
      "[step: 287] loss: 0.001983337104320526\n",
      "[step: 288] loss: 0.0019485077355057001\n",
      "[step: 289] loss: 0.0019603462424129248\n",
      "[step: 290] loss: 0.001970549114048481\n",
      "[step: 291] loss: 0.0019806819036602974\n",
      "[step: 292] loss: 0.0020092420745640993\n",
      "[step: 293] loss: 0.002098053926602006\n",
      "[step: 294] loss: 0.0020917339716106653\n",
      "[step: 295] loss: 0.0020840237848460674\n",
      "[step: 296] loss: 0.0020588559564203024\n",
      "[step: 297] loss: 0.002038198057562113\n",
      "[step: 298] loss: 0.0019418903393670917\n",
      "[step: 299] loss: 0.001936225569806993\n",
      "[step: 300] loss: 0.001985993003472686\n",
      "[step: 301] loss: 0.0019814223051071167\n",
      "[step: 302] loss: 0.001998340478166938\n",
      "[step: 303] loss: 0.002000292530283332\n",
      "[step: 304] loss: 0.001982858870178461\n",
      "[step: 305] loss: 0.0019242875277996063\n",
      "[step: 306] loss: 0.0019270279444754124\n",
      "[step: 307] loss: 0.0019209864549338818\n",
      "[step: 308] loss: 0.0019136901246383786\n",
      "[step: 309] loss: 0.0019538418855518103\n",
      "[step: 310] loss: 0.0019275386584922671\n",
      "[step: 311] loss: 0.001906236750073731\n",
      "[step: 312] loss: 0.001905261306092143\n",
      "[step: 313] loss: 0.0018967820797115564\n",
      "[step: 314] loss: 0.0018673489103093743\n",
      "[step: 315] loss: 0.0018854454392567277\n",
      "[step: 316] loss: 0.001896548317745328\n",
      "[step: 317] loss: 0.001879555289633572\n",
      "[step: 318] loss: 0.0018952410900965333\n",
      "[step: 319] loss: 0.0018858980620279908\n",
      "[step: 320] loss: 0.0018638507463037968\n",
      "[step: 321] loss: 0.0018628634279593825\n",
      "[step: 322] loss: 0.0018696683691814542\n",
      "[step: 323] loss: 0.0018416916718706489\n",
      "[step: 324] loss: 0.001840539276599884\n",
      "[step: 325] loss: 0.0018456713296473026\n",
      "[step: 326] loss: 0.001830410212278366\n",
      "[step: 327] loss: 0.0018257155315950513\n",
      "[step: 328] loss: 0.001832411508075893\n",
      "[step: 329] loss: 0.0018280141521245241\n",
      "[step: 330] loss: 0.0018195203738287091\n",
      "[step: 331] loss: 0.001829363638535142\n",
      "[step: 332] loss: 0.0018364643910899758\n",
      "[step: 333] loss: 0.0018553385743871331\n",
      "[step: 334] loss: 0.0018962007015943527\n",
      "[step: 335] loss: 0.0019920356571674347\n",
      "[step: 336] loss: 0.0021334465127438307\n",
      "[step: 337] loss: 0.00232653203420341\n",
      "[step: 338] loss: 0.0024269530549645424\n",
      "[step: 339] loss: 0.002324591390788555\n",
      "[step: 340] loss: 0.001981874695047736\n",
      "[step: 341] loss: 0.0017994145164266229\n",
      "[step: 342] loss: 0.0019420707831159234\n",
      "[step: 343] loss: 0.0020856631454080343\n",
      "[step: 344] loss: 0.0019657693337649107\n",
      "[step: 345] loss: 0.001780009944923222\n",
      "[step: 346] loss: 0.0018436736427247524\n",
      "[step: 347] loss: 0.0019580379594117403\n",
      "[step: 348] loss: 0.001892688567750156\n",
      "[step: 349] loss: 0.0017823242815211415\n",
      "[step: 350] loss: 0.0017889299197122455\n",
      "[step: 351] loss: 0.0018764715641736984\n",
      "[step: 352] loss: 0.001853400724940002\n",
      "[step: 353] loss: 0.0017497101798653603\n",
      "[step: 354] loss: 0.001780810416676104\n",
      "[step: 355] loss: 0.001826528343372047\n",
      "[step: 356] loss: 0.0017846317496150732\n",
      "[step: 357] loss: 0.0017492318293079734\n",
      "[step: 358] loss: 0.0017490244936197996\n",
      "[step: 359] loss: 0.0017853116150945425\n",
      "[step: 360] loss: 0.0017547161551192403\n",
      "[step: 361] loss: 0.0017086955485865474\n",
      "[step: 362] loss: 0.0017411552835255861\n",
      "[step: 363] loss: 0.0017477967776358128\n",
      "[step: 364] loss: 0.0017128820763900876\n",
      "[step: 365] loss: 0.0017069804016500711\n",
      "[step: 366] loss: 0.0017065713182091713\n",
      "[step: 367] loss: 0.0017160855932161212\n",
      "[step: 368] loss: 0.001706391223706305\n",
      "[step: 369] loss: 0.0016755845863372087\n",
      "[step: 370] loss: 0.0016885950462892652\n",
      "[step: 371] loss: 0.0016977970954030752\n",
      "[step: 372] loss: 0.0016730745555832982\n",
      "[step: 373] loss: 0.001667920034378767\n",
      "[step: 374] loss: 0.0016657272353768349\n",
      "[step: 375] loss: 0.0016641098773106933\n",
      "[step: 376] loss: 0.0016644521383568645\n",
      "[step: 377] loss: 0.0016466503730043769\n",
      "[step: 378] loss: 0.0016384822083637118\n",
      "[step: 379] loss: 0.0016464423388242722\n",
      "[step: 380] loss: 0.0016405421774834394\n",
      "[step: 381] loss: 0.0016296071698889136\n",
      "[step: 382] loss: 0.0016252663917839527\n",
      "[step: 383] loss: 0.0016194124473258853\n",
      "[step: 384] loss: 0.0016156298806890845\n",
      "[step: 385] loss: 0.0016163064865395427\n",
      "[step: 386] loss: 0.0016105553368106484\n",
      "[step: 387] loss: 0.0016001624753698707\n",
      "[step: 388] loss: 0.0015967754879966378\n",
      "[step: 389] loss: 0.0015950811794027686\n",
      "[step: 390] loss: 0.0015902210725471377\n",
      "[step: 391] loss: 0.0015869020717218518\n",
      "[step: 392] loss: 0.0015828678151592612\n",
      "[step: 393] loss: 0.0015755920903757215\n",
      "[step: 394] loss: 0.001569721382111311\n",
      "[step: 395] loss: 0.0015671310247853398\n",
      "[step: 396] loss: 0.0015640307683497667\n",
      "[step: 397] loss: 0.0015593827702105045\n",
      "[step: 398] loss: 0.0015557095175608993\n",
      "[step: 399] loss: 0.0015525706112384796\n",
      "[step: 400] loss: 0.001548177213408053\n",
      "[step: 401] loss: 0.001542591373436153\n",
      "[step: 402] loss: 0.0015379944816231728\n",
      "[step: 403] loss: 0.0015340355457738042\n",
      "[step: 404] loss: 0.001529467524960637\n",
      "[step: 405] loss: 0.0015244897222146392\n",
      "[step: 406] loss: 0.001520227757282555\n",
      "[step: 407] loss: 0.0015165888471528888\n",
      "[step: 408] loss: 0.001512881019152701\n",
      "[step: 409] loss: 0.001508799265138805\n",
      "[step: 410] loss: 0.0015051680384203792\n",
      "[step: 411] loss: 0.0015026930486783385\n",
      "[step: 412] loss: 0.001502252765931189\n",
      "[step: 413] loss: 0.0015057689743116498\n",
      "[step: 414] loss: 0.001520975143648684\n",
      "[step: 415] loss: 0.0015622996725142002\n",
      "[step: 416] loss: 0.0016863539349287748\n",
      "[step: 417] loss: 0.0018800562247633934\n",
      "[step: 418] loss: 0.002258267719298601\n",
      "[step: 419] loss: 0.0024739012587815523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 420] loss: 0.00239758612588048\n",
      "[step: 421] loss: 0.001728215953335166\n",
      "[step: 422] loss: 0.0015412670327350497\n",
      "[step: 423] loss: 0.0018960883608087897\n",
      "[step: 424] loss: 0.0018089775694534183\n",
      "[step: 425] loss: 0.0016432056436315179\n",
      "[step: 426] loss: 0.0015898306155577302\n",
      "[step: 427] loss: 0.0017219826113432646\n",
      "[step: 428] loss: 0.0017342704813927412\n",
      "[step: 429] loss: 0.0014805028913542628\n",
      "[step: 430] loss: 0.001697845058515668\n",
      "[step: 431] loss: 0.0016107000410556793\n",
      "[step: 432] loss: 0.0015905429609119892\n",
      "[step: 433] loss: 0.0014897078508511186\n",
      "[step: 434] loss: 0.0016139349900186062\n",
      "[step: 435] loss: 0.0015017091063782573\n",
      "[step: 436] loss: 0.0015035521937534213\n",
      "[step: 437] loss: 0.0015191204147413373\n",
      "[step: 438] loss: 0.0015270574949681759\n",
      "[step: 439] loss: 0.0014648025389760733\n",
      "[step: 440] loss: 0.001484710373915732\n",
      "[step: 441] loss: 0.0014861234230920672\n",
      "[step: 442] loss: 0.0014567140024155378\n",
      "[step: 443] loss: 0.0014474596828222275\n",
      "[step: 444] loss: 0.0014702308690175414\n",
      "[step: 445] loss: 0.0014463970437645912\n",
      "[step: 446] loss: 0.0014346587704494596\n",
      "[step: 447] loss: 0.0014366088435053825\n",
      "[step: 448] loss: 0.001440242282114923\n",
      "[step: 449] loss: 0.0014178024139255285\n",
      "[step: 450] loss: 0.0014196323463693261\n",
      "[step: 451] loss: 0.001422832952812314\n",
      "[step: 452] loss: 0.0014162699226289988\n",
      "[step: 453] loss: 0.0014031255850568414\n",
      "[step: 454] loss: 0.001406745403073728\n",
      "[step: 455] loss: 0.0014057331718504429\n",
      "[step: 456] loss: 0.0013954180758446455\n",
      "[step: 457] loss: 0.0013908464461565018\n",
      "[step: 458] loss: 0.001391268684528768\n",
      "[step: 459] loss: 0.0013912826543673873\n",
      "[step: 460] loss: 0.0013796607963740826\n",
      "[step: 461] loss: 0.0013801861787214875\n",
      "[step: 462] loss: 0.0013768179342150688\n",
      "[step: 463] loss: 0.001378190005198121\n",
      "[step: 464] loss: 0.0013680342817679048\n",
      "[step: 465] loss: 0.001366787008009851\n",
      "[step: 466] loss: 0.001365744392387569\n",
      "[step: 467] loss: 0.001362752402201295\n",
      "[step: 468] loss: 0.001359448884613812\n",
      "[step: 469] loss: 0.0013532961020246148\n",
      "[step: 470] loss: 0.0013543523382395506\n",
      "[step: 471] loss: 0.0013508618576452136\n",
      "[step: 472] loss: 0.0013480953639373183\n",
      "[step: 473] loss: 0.001344567397609353\n",
      "[step: 474] loss: 0.001340968650765717\n",
      "[step: 475] loss: 0.0013405653880909085\n",
      "[step: 476] loss: 0.0013375035487115383\n",
      "[step: 477] loss: 0.0013344488106667995\n",
      "[step: 478] loss: 0.0013318632263690233\n",
      "[step: 479] loss: 0.0013284384040161967\n",
      "[step: 480] loss: 0.001326958299614489\n",
      "[step: 481] loss: 0.001324955839663744\n",
      "[step: 482] loss: 0.0013219838729128242\n",
      "[step: 483] loss: 0.0013196326326578856\n",
      "[step: 484] loss: 0.001316781505011022\n",
      "[step: 485] loss: 0.0013138620415702462\n",
      "[step: 486] loss: 0.001312100444920361\n",
      "[step: 487] loss: 0.0013098936760798097\n",
      "[step: 488] loss: 0.0013074004091322422\n",
      "[step: 489] loss: 0.0013054957380518317\n",
      "[step: 490] loss: 0.0013030774425715208\n",
      "[step: 491] loss: 0.0013003284111618996\n",
      "[step: 492] loss: 0.0012980640167370439\n",
      "[step: 493] loss: 0.0012957083526998758\n",
      "[step: 494] loss: 0.001293074805289507\n",
      "[step: 495] loss: 0.0012907268246635795\n",
      "[step: 496] loss: 0.0012885434553027153\n",
      "[step: 497] loss: 0.0012862219009548426\n",
      "[step: 498] loss: 0.001283839694224298\n",
      "[step: 499] loss: 0.0012816281523555517\n",
      "[step: 500] loss: 0.0012795230140909553\n",
      "[step: 501] loss: 0.0012773028574883938\n",
      "[step: 502] loss: 0.001275092945434153\n",
      "[step: 503] loss: 0.001273203524760902\n",
      "[step: 504] loss: 0.0012717387871816754\n",
      "[step: 505] loss: 0.0012712342431768775\n",
      "[step: 506] loss: 0.0012733946787193418\n",
      "[step: 507] loss: 0.0012837419053539634\n",
      "[step: 508] loss: 0.0013160351663827896\n",
      "[step: 509] loss: 0.0014176121912896633\n",
      "[step: 510] loss: 0.0016582654789090157\n",
      "[step: 511] loss: 0.002197917550802231\n",
      "[step: 512] loss: 0.002565661445260048\n",
      "[step: 513] loss: 0.0021818566601723433\n",
      "[step: 514] loss: 0.001398457563482225\n",
      "[step: 515] loss: 0.0015836625825613737\n",
      "[step: 516] loss: 0.001997143030166626\n",
      "[step: 517] loss: 0.0015550079988315701\n",
      "[step: 518] loss: 0.0014441596576943994\n",
      "[step: 519] loss: 0.0016629707533866167\n",
      "[step: 520] loss: 0.0015745391137897968\n",
      "[step: 521] loss: 0.0013475565938279033\n",
      "[step: 522] loss: 0.0015573677374050021\n",
      "[step: 523] loss: 0.0015234991442412138\n",
      "[step: 524] loss: 0.0013165029231458902\n",
      "[step: 525] loss: 0.0014716122532263398\n",
      "[step: 526] loss: 0.001421355758793652\n",
      "[step: 527] loss: 0.0013141777599230409\n",
      "[step: 528] loss: 0.0013915628660470247\n",
      "[step: 529] loss: 0.0013778136344626546\n",
      "[step: 530] loss: 0.0012750413734465837\n",
      "[step: 531] loss: 0.001375194638967514\n",
      "[step: 532] loss: 0.0013162309769541025\n",
      "[step: 533] loss: 0.0012815037043765187\n",
      "[step: 534] loss: 0.0013228274183347821\n",
      "[step: 535] loss: 0.0013157465727999806\n",
      "[step: 536] loss: 0.0012499454896897078\n",
      "[step: 537] loss: 0.0013089984422549605\n",
      "[step: 538] loss: 0.0012825580779463053\n",
      "[step: 539] loss: 0.0012517928844317794\n",
      "[step: 540] loss: 0.0012682636734098196\n",
      "[step: 541] loss: 0.0012751341564580798\n",
      "[step: 542] loss: 0.0012314538471400738\n",
      "[step: 543] loss: 0.0012604579096660018\n",
      "[step: 544] loss: 0.0012468731729313731\n",
      "[step: 545] loss: 0.0012368595926091075\n",
      "[step: 546] loss: 0.0012378022074699402\n",
      "[step: 547] loss: 0.001242136931978166\n",
      "[step: 548] loss: 0.0012225467944517732\n",
      "[step: 549] loss: 0.0012342603877186775\n",
      "[step: 550] loss: 0.0012274800101295114\n",
      "[step: 551] loss: 0.0012167600216343999\n",
      "[step: 552] loss: 0.0012229867279529572\n",
      "[step: 553] loss: 0.001219711615704\n",
      "[step: 554] loss: 0.001209714449942112\n",
      "[step: 555] loss: 0.0012096447171643376\n",
      "[step: 556] loss: 0.0012169049587100744\n",
      "[step: 557] loss: 0.001202393090352416\n",
      "[step: 558] loss: 0.0012020844733342528\n",
      "[step: 559] loss: 0.001204828848131001\n",
      "[step: 560] loss: 0.0012014694511890411\n",
      "[step: 561] loss: 0.0011951642809435725\n",
      "[step: 562] loss: 0.0011956983944401145\n",
      "[step: 563] loss: 0.0011957340175285935\n",
      "[step: 564] loss: 0.00118966493755579\n",
      "[step: 565] loss: 0.0011907401494681835\n",
      "[step: 566] loss: 0.0011892295442521572\n",
      "[step: 567] loss: 0.0011857206700369716\n",
      "[step: 568] loss: 0.0011830624425783753\n",
      "[step: 569] loss: 0.001183794578537345\n",
      "[step: 570] loss: 0.001183141372166574\n",
      "[step: 571] loss: 0.001178857870399952\n",
      "[step: 572] loss: 0.0011781530920416117\n",
      "[step: 573] loss: 0.0011768197873607278\n",
      "[step: 574] loss: 0.0011744409566745162\n",
      "[step: 575] loss: 0.001172372023575008\n",
      "[step: 576] loss: 0.0011715691071003675\n",
      "[step: 577] loss: 0.0011712590930983424\n",
      "[step: 578] loss: 0.001168911694549024\n",
      "[step: 579] loss: 0.0011675467249006033\n",
      "[step: 580] loss: 0.001167204580269754\n",
      "[step: 581] loss: 0.0011664483463391662\n",
      "[step: 582] loss: 0.001165946712717414\n",
      "[step: 583] loss: 0.0011662866454571486\n",
      "[step: 584] loss: 0.0011706058867275715\n",
      "[step: 585] loss: 0.0011794324964284897\n",
      "[step: 586] loss: 0.0012033346574753523\n",
      "[step: 587] loss: 0.0012449785135686398\n",
      "[step: 588] loss: 0.0013488091062754393\n",
      "[step: 589] loss: 0.0013818625593557954\n",
      "[step: 590] loss: 0.0013932480942457914\n",
      "[step: 591] loss: 0.001229027402587235\n",
      "[step: 592] loss: 0.0012209325795993209\n",
      "[step: 593] loss: 0.00126344570890069\n",
      "[step: 594] loss: 0.0012128413654863834\n",
      "[step: 595] loss: 0.0012414207449182868\n",
      "[step: 596] loss: 0.0012434391537681222\n",
      "[step: 597] loss: 0.001163115375675261\n",
      "[step: 598] loss: 0.0012014525709673762\n",
      "[step: 599] loss: 0.00123903201892972\n",
      "[step: 600] loss: 0.0011651786044239998\n",
      "[step: 601] loss: 0.0011823441600427032\n",
      "[step: 602] loss: 0.0011937118833884597\n",
      "[step: 603] loss: 0.0011628124630078673\n",
      "[step: 604] loss: 0.0011940841795876622\n",
      "[step: 605] loss: 0.001169233932159841\n",
      "[step: 606] loss: 0.0011481022229418159\n",
      "[step: 607] loss: 0.001177911995910108\n",
      "[step: 608] loss: 0.0011590778594836593\n",
      "[step: 609] loss: 0.0011585247702896595\n",
      "[step: 610] loss: 0.0011606329353526235\n",
      "[step: 611] loss: 0.0011361486976966262\n",
      "[step: 612] loss: 0.0011518989922478795\n",
      "[step: 613] loss: 0.0011516001541167498\n",
      "[step: 614] loss: 0.0011374662863090634\n",
      "[step: 615] loss: 0.001146458089351654\n",
      "[step: 616] loss: 0.0011345025850459933\n",
      "[step: 617] loss: 0.0011286817025393248\n",
      "[step: 618] loss: 0.0011374900350347161\n",
      "[step: 619] loss: 0.0011301942868158221\n",
      "[step: 620] loss: 0.001129953539930284\n",
      "[step: 621] loss: 0.0011331049026921391\n",
      "[step: 622] loss: 0.0011239142622798681\n",
      "[step: 623] loss: 0.0011222584871575236\n",
      "[step: 624] loss: 0.001123958034440875\n",
      "[step: 625] loss: 0.0011174759129062295\n",
      "[step: 626] loss: 0.001116913161240518\n",
      "[step: 627] loss: 0.001119839260354638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 628] loss: 0.0011163519229739904\n",
      "[step: 629] loss: 0.0011158664710819721\n",
      "[step: 630] loss: 0.0011193695245310664\n",
      "[step: 631] loss: 0.0011196290142834187\n",
      "[step: 632] loss: 0.0011220424203202128\n",
      "[step: 633] loss: 0.0011345150414854288\n",
      "[step: 634] loss: 0.001147749018855393\n",
      "[step: 635] loss: 0.0011800656793639064\n",
      "[step: 636] loss: 0.0012180296471342444\n",
      "[step: 637] loss: 0.0012895247200503945\n",
      "[step: 638] loss: 0.001318042748607695\n",
      "[step: 639] loss: 0.0013122381642460823\n",
      "[step: 640] loss: 0.001217440003529191\n",
      "[step: 641] loss: 0.0011282660998404026\n",
      "[step: 642] loss: 0.001106834039092064\n",
      "[step: 643] loss: 0.0011547461617738008\n",
      "[step: 644] loss: 0.0012077923165634274\n",
      "[step: 645] loss: 0.0011985027231276035\n",
      "[step: 646] loss: 0.0011500578839331865\n",
      "[step: 647] loss: 0.001108297030441463\n",
      "[step: 648] loss: 0.001105881412513554\n",
      "[step: 649] loss: 0.0011329303961247206\n",
      "[step: 650] loss: 0.0011560850543901324\n",
      "[step: 651] loss: 0.001146774971857667\n",
      "[step: 652] loss: 0.0011128948535770178\n",
      "[step: 653] loss: 0.0010920765344053507\n",
      "[step: 654] loss: 0.001101569039747119\n",
      "[step: 655] loss: 0.0011207786155864596\n",
      "[step: 656] loss: 0.0011241671163588762\n",
      "[step: 657] loss: 0.00111068831756711\n",
      "[step: 658] loss: 0.0010952036827802658\n",
      "[step: 659] loss: 0.0010869241086766124\n",
      "[step: 660] loss: 0.001088136457838118\n",
      "[step: 661] loss: 0.0010957680642604828\n",
      "[step: 662] loss: 0.0011040462413802743\n",
      "[step: 663] loss: 0.0011082852724939585\n",
      "[step: 664] loss: 0.0011013491312041879\n",
      "[step: 665] loss: 0.0010918916668742895\n",
      "[step: 666] loss: 0.0010830950923264027\n",
      "[step: 667] loss: 0.0010774587281048298\n",
      "[step: 668] loss: 0.0010749533539637923\n",
      "[step: 669] loss: 0.001076126703992486\n",
      "[step: 670] loss: 0.0010798340663313866\n",
      "[step: 671] loss: 0.0010845053475350142\n",
      "[step: 672] loss: 0.0010898476466536522\n",
      "[step: 673] loss: 0.001093807048164308\n",
      "[step: 674] loss: 0.00109921267721802\n",
      "[step: 675] loss: 0.0011039951350539923\n",
      "[step: 676] loss: 0.0011137569090351462\n",
      "[step: 677] loss: 0.001118201995268464\n",
      "[step: 678] loss: 0.001129046780988574\n",
      "[step: 679] loss: 0.0011286314111202955\n",
      "[step: 680] loss: 0.0011297676246613264\n",
      "[step: 681] loss: 0.001120676868595183\n",
      "[step: 682] loss: 0.0011090049520134926\n",
      "[step: 683] loss: 0.0010927986586466432\n",
      "[step: 684] loss: 0.0010765392798930407\n",
      "[step: 685] loss: 0.0010638902895152569\n",
      "[step: 686] loss: 0.0010580355301499367\n",
      "[step: 687] loss: 0.0010585786076262593\n",
      "[step: 688] loss: 0.0010631066979840398\n",
      "[step: 689] loss: 0.001069117453880608\n",
      "[step: 690] loss: 0.0010757202981039882\n",
      "[step: 691] loss: 0.0010844733333215117\n",
      "[step: 692] loss: 0.0010936317266896367\n",
      "[step: 693] loss: 0.0011068619787693024\n",
      "[step: 694] loss: 0.0011148260673508048\n",
      "[step: 695] loss: 0.0011264811037108302\n",
      "[step: 696] loss: 0.0011261407053098083\n",
      "[step: 697] loss: 0.001127334195189178\n",
      "[step: 698] loss: 0.0011147847399115562\n",
      "[step: 699] loss: 0.0010998189682140946\n",
      "[step: 700] loss: 0.0010813194094225764\n",
      "[step: 701] loss: 0.0010648889001458883\n",
      "[step: 702] loss: 0.0010528098791837692\n",
      "[step: 703] loss: 0.0010458832839503884\n",
      "[step: 704] loss: 0.001044226111844182\n",
      "[step: 705] loss: 0.0010472253197804093\n",
      "[step: 706] loss: 0.001053806277923286\n",
      "[step: 707] loss: 0.0010621225228533149\n",
      "[step: 708] loss: 0.0010713583324104548\n",
      "[step: 709] loss: 0.0010800143936648965\n",
      "[step: 710] loss: 0.0010880309855565429\n",
      "[step: 711] loss: 0.0010937777115032077\n",
      "[step: 712] loss: 0.0011002396931871772\n",
      "[step: 713] loss: 0.001102482550777495\n",
      "[step: 714] loss: 0.0011111574713140726\n",
      "[step: 715] loss: 0.001109936973080039\n",
      "[step: 716] loss: 0.0011170307407155633\n",
      "[step: 717] loss: 0.0010949125280603766\n",
      "[step: 718] loss: 0.0010729293571785092\n",
      "[step: 719] loss: 0.0010472191497683525\n",
      "[step: 720] loss: 0.0010425116633996367\n",
      "[step: 721] loss: 0.0010536430636420846\n",
      "[step: 722] loss: 0.0010592818725854158\n",
      "[step: 723] loss: 0.0010523124365136027\n",
      "[step: 724] loss: 0.001037511508911848\n",
      "[step: 725] loss: 0.0010318291606381536\n",
      "[step: 726] loss: 0.0010377950966358185\n",
      "[step: 727] loss: 0.0010445916559547186\n",
      "[step: 728] loss: 0.0010465682717040181\n",
      "[step: 729] loss: 0.0010393636766821146\n",
      "[step: 730] loss: 0.0010361213935539126\n",
      "[step: 731] loss: 0.0010413043200969696\n",
      "[step: 732] loss: 0.001052471692673862\n",
      "[step: 733] loss: 0.0010639207903295755\n",
      "[step: 734] loss: 0.001076058717444539\n",
      "[step: 735] loss: 0.001096121734008193\n",
      "[step: 736] loss: 0.0011335971066728234\n",
      "[step: 737] loss: 0.0011852027382701635\n",
      "[step: 738] loss: 0.0012637660838663578\n",
      "[step: 739] loss: 0.001290621585212648\n",
      "[step: 740] loss: 0.0012885787291452289\n",
      "[step: 741] loss: 0.001162925735116005\n",
      "[step: 742] loss: 0.0010518565541133285\n",
      "[step: 743] loss: 0.0010353955440223217\n",
      "[step: 744] loss: 0.0010744546307250857\n",
      "[step: 745] loss: 0.001087794080376625\n",
      "[step: 746] loss: 0.0010735070100054145\n",
      "[step: 747] loss: 0.0011114667868241668\n",
      "[step: 748] loss: 0.0011504474096000195\n",
      "[step: 749] loss: 0.001096363179385662\n",
      "[step: 750] loss: 0.001053786021657288\n",
      "[step: 751] loss: 0.0010554696200415492\n",
      "[step: 752] loss: 0.0010376375867053866\n",
      "[step: 753] loss: 0.001015526824630797\n",
      "[step: 754] loss: 0.0010305511532351375\n",
      "[step: 755] loss: 0.0010568758007138968\n",
      "[step: 756] loss: 0.0010505144018679857\n",
      "[step: 757] loss: 0.0010273270308971405\n",
      "[step: 758] loss: 0.001022234559059143\n",
      "[step: 759] loss: 0.0010173805058002472\n",
      "[step: 760] loss: 0.001005341880954802\n",
      "[step: 761] loss: 0.0010120953666046262\n",
      "[step: 762] loss: 0.001027237973175943\n",
      "[step: 763] loss: 0.0010249040788039565\n",
      "[step: 764] loss: 0.0010179231176152825\n",
      "[step: 765] loss: 0.0010191970504820347\n",
      "[step: 766] loss: 0.001015927642583847\n",
      "[step: 767] loss: 0.0010052206926047802\n",
      "[step: 768] loss: 0.0009964386699721217\n",
      "[step: 769] loss: 0.0009964029304683208\n",
      "[step: 770] loss: 0.0009979238966479897\n",
      "[step: 771] loss: 0.0009959028102457523\n",
      "[step: 772] loss: 0.0009958789451047778\n",
      "[step: 773] loss: 0.0010006377706304193\n",
      "[step: 774] loss: 0.0010061708744615316\n",
      "[step: 775] loss: 0.0010087249102070928\n",
      "[step: 776] loss: 0.0010115306358784437\n",
      "[step: 777] loss: 0.001018801354803145\n",
      "[step: 778] loss: 0.0010325763141736388\n",
      "[step: 779] loss: 0.0010448152897879481\n",
      "[step: 780] loss: 0.0010612339247018099\n",
      "[step: 781] loss: 0.0010723136365413666\n",
      "[step: 782] loss: 0.0010854351567104459\n",
      "[step: 783] loss: 0.0010900218039751053\n",
      "[step: 784] loss: 0.0010828588856384158\n",
      "[step: 785] loss: 0.001059546833857894\n",
      "[step: 786] loss: 0.0010269973427057266\n",
      "[step: 787] loss: 0.0009973407723009586\n",
      "[step: 788] loss: 0.0009814451914280653\n",
      "[step: 789] loss: 0.0009813116630539298\n",
      "[step: 790] loss: 0.0009915923001244664\n",
      "[step: 791] loss: 0.0010051642311736941\n",
      "[step: 792] loss: 0.0010173290502279997\n",
      "[step: 793] loss: 0.0010260329581797123\n",
      "[step: 794] loss: 0.0010288909543305635\n",
      "[step: 795] loss: 0.0010273485677316785\n",
      "[step: 796] loss: 0.0010173212504014373\n",
      "[step: 797] loss: 0.0010048977565020323\n",
      "[step: 798] loss: 0.0009898100979626179\n",
      "[step: 799] loss: 0.00097824283875525\n",
      "[step: 800] loss: 0.00097086833557114\n",
      "[step: 801] loss: 0.0009685332188382745\n",
      "[step: 802] loss: 0.0009702222887426615\n",
      "[step: 803] loss: 0.0009745617280714214\n",
      "[step: 804] loss: 0.0009811391355469823\n",
      "[step: 805] loss: 0.0009892440866678953\n",
      "[step: 806] loss: 0.001001221127808094\n",
      "[step: 807] loss: 0.001016203430481255\n",
      "[step: 808] loss: 0.001038751332089305\n",
      "[step: 809] loss: 0.0010640248656272888\n",
      "[step: 810] loss: 0.0010943954112008214\n",
      "[step: 811] loss: 0.0011142469011247158\n",
      "[step: 812] loss: 0.001117051113396883\n",
      "[step: 813] loss: 0.0010876876767724752\n",
      "[step: 814] loss: 0.001036738627590239\n",
      "[step: 815] loss: 0.0009859190322458744\n",
      "[step: 816] loss: 0.0009615428280085325\n",
      "[step: 817] loss: 0.000969691900536418\n",
      "[step: 818] loss: 0.0009970240062102675\n",
      "[step: 819] loss: 0.001023577176965773\n",
      "[step: 820] loss: 0.00103548145852983\n",
      "[step: 821] loss: 0.0010265999007970095\n",
      "[step: 822] loss: 0.0010042754001915455\n",
      "[step: 823] loss: 0.0009791060583665967\n",
      "[step: 824] loss: 0.0009642832446843386\n",
      "[step: 825] loss: 0.0009654221939854324\n",
      "[step: 826] loss: 0.0009817559039220214\n",
      "[step: 827] loss: 0.0010009735124185681\n",
      "[step: 828] loss: 0.0010250690393149853\n",
      "[step: 829] loss: 0.0010299389250576496\n",
      "[step: 830] loss: 0.0010340804001316428\n",
      "[step: 831] loss: 0.0010065281530842185\n",
      "[step: 832] loss: 0.000978899304755032\n",
      "[step: 833] loss: 0.0009554203134030104\n",
      "[step: 834] loss: 0.0009514536941424012\n",
      "[step: 835] loss: 0.0009630187414586544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 836] loss: 0.0009719937806949019\n",
      "[step: 837] loss: 0.0009729458251968026\n",
      "[step: 838] loss: 0.0009588616085238755\n",
      "[step: 839] loss: 0.0009473114623688161\n",
      "[step: 840] loss: 0.0009449687204323709\n",
      "[step: 841] loss: 0.0009513254626654088\n",
      "[step: 842] loss: 0.0009592935093678534\n",
      "[step: 843] loss: 0.0009588542161509395\n",
      "[step: 844] loss: 0.0009540501050651073\n",
      "[step: 845] loss: 0.0009487351635470986\n",
      "[step: 846] loss: 0.0009509147494100034\n",
      "[step: 847] loss: 0.0009624296799302101\n",
      "[step: 848] loss: 0.000982364872470498\n",
      "[step: 849] loss: 0.001013708533719182\n",
      "[step: 850] loss: 0.0010614466154947877\n",
      "[step: 851] loss: 0.0011351630091667175\n",
      "[step: 852] loss: 0.0012285261182114482\n",
      "[step: 853] loss: 0.0012865023454651237\n",
      "[step: 854] loss: 0.0012458261335268617\n",
      "[step: 855] loss: 0.0010767366038635373\n",
      "[step: 856] loss: 0.0009533072006888688\n",
      "[step: 857] loss: 0.0009971346007660031\n",
      "[step: 858] loss: 0.0011093082139268517\n",
      "[step: 859] loss: 0.0011372251901775599\n",
      "[step: 860] loss: 0.0011418091598898172\n",
      "[step: 861] loss: 0.0011814229656010866\n",
      "[step: 862] loss: 0.0013008746318519115\n",
      "[step: 863] loss: 0.0010273947846144438\n",
      "[step: 864] loss: 0.0009987978264689445\n",
      "[step: 865] loss: 0.0011155657703056931\n",
      "[step: 866] loss: 0.0009883149759843946\n",
      "[step: 867] loss: 0.0010140828089788556\n",
      "[step: 868] loss: 0.0010326295159757137\n",
      "[step: 869] loss: 0.000976955983787775\n",
      "[step: 870] loss: 0.00100703292991966\n",
      "[step: 871] loss: 0.0009907438652589917\n",
      "[step: 872] loss: 0.0009815033990889788\n",
      "[step: 873] loss: 0.00098189408890903\n",
      "[step: 874] loss: 0.0009826399618759751\n",
      "[step: 875] loss: 0.0009723515249788761\n",
      "[step: 876] loss: 0.0009563453495502472\n",
      "[step: 877] loss: 0.0009829486953094602\n",
      "[step: 878] loss: 0.000952470232732594\n",
      "[step: 879] loss: 0.0009529676754027605\n",
      "[step: 880] loss: 0.0009721036767587066\n",
      "[step: 881] loss: 0.0009310260647907853\n",
      "[step: 882] loss: 0.0009550128597766161\n",
      "[step: 883] loss: 0.0009483480826020241\n",
      "[step: 884] loss: 0.0009279310470446944\n",
      "[step: 885] loss: 0.0009495134581811726\n",
      "[step: 886] loss: 0.0009292279719375074\n",
      "[step: 887] loss: 0.0009289974113926291\n",
      "[step: 888] loss: 0.0009381909621879458\n",
      "[step: 889] loss: 0.0009213718003593385\n",
      "[step: 890] loss: 0.0009298556833527982\n",
      "[step: 891] loss: 0.0009290893794968724\n",
      "[step: 892] loss: 0.0009187785908579826\n",
      "[step: 893] loss: 0.0009271070593968034\n",
      "[step: 894] loss: 0.0009215954341925681\n",
      "[step: 895] loss: 0.000916407210752368\n",
      "[step: 896] loss: 0.0009229034767486155\n",
      "[step: 897] loss: 0.0009161101188510656\n",
      "[step: 898] loss: 0.0009130826219916344\n",
      "[step: 899] loss: 0.0009179123444482684\n",
      "[step: 900] loss: 0.0009124052594415843\n",
      "[step: 901] loss: 0.0009099175804294646\n",
      "[step: 902] loss: 0.0009135924046859145\n",
      "[step: 903] loss: 0.0009097261936403811\n",
      "[step: 904] loss: 0.0009069479419849813\n",
      "[step: 905] loss: 0.0009094575070776045\n",
      "[step: 906] loss: 0.000907350389752537\n",
      "[step: 907] loss: 0.0009049669606611133\n",
      "[step: 908] loss: 0.0009060174343176186\n",
      "[step: 909] loss: 0.0009046501363627613\n",
      "[step: 910] loss: 0.00090290472144261\n",
      "[step: 911] loss: 0.0009033798705786467\n",
      "[step: 912] loss: 0.0009023234015330672\n",
      "[step: 913] loss: 0.0009009708883240819\n",
      "[step: 914] loss: 0.0009020378929562867\n",
      "[step: 915] loss: 0.0009030041401274502\n",
      "[step: 916] loss: 0.0009048217325471342\n",
      "[step: 917] loss: 0.0009134509600698948\n",
      "[step: 918] loss: 0.0009360602707602084\n",
      "[step: 919] loss: 0.0009877936681732535\n",
      "[step: 920] loss: 0.001115370076149702\n",
      "[step: 921] loss: 0.0013665261212736368\n",
      "[step: 922] loss: 0.0017885658890008926\n",
      "[step: 923] loss: 0.002030835021287203\n",
      "[step: 924] loss: 0.0017083915881812572\n",
      "[step: 925] loss: 0.0010593089973554015\n",
      "[step: 926] loss: 0.0010628856252878904\n",
      "[step: 927] loss: 0.00145461515057832\n",
      "[step: 928] loss: 0.0012501163873821497\n",
      "[step: 929] loss: 0.000979559263214469\n",
      "[step: 930] loss: 0.0011990605853497982\n",
      "[step: 931] loss: 0.0011829163413494825\n",
      "[step: 932] loss: 0.001030948362313211\n",
      "[step: 933] loss: 0.001101260189898312\n",
      "[step: 934] loss: 0.0010576385539025068\n",
      "[step: 935] loss: 0.001072026090696454\n",
      "[step: 936] loss: 0.0011283187195658684\n",
      "[step: 937] loss: 0.0009487283532507718\n",
      "[step: 938] loss: 0.0010754474205896258\n",
      "[step: 939] loss: 0.0011577910045161843\n",
      "[step: 940] loss: 0.0009427866898477077\n",
      "[step: 941] loss: 0.001102688955143094\n",
      "[step: 942] loss: 0.0010485975071787834\n",
      "[step: 943] loss: 0.0010247209575027227\n",
      "[step: 944] loss: 0.0010661231353878975\n",
      "[step: 945] loss: 0.0009190284763462842\n",
      "[step: 946] loss: 0.0010660761035978794\n",
      "[step: 947] loss: 0.0009573985589668155\n",
      "[step: 948] loss: 0.0009633761364966631\n",
      "[step: 949] loss: 0.0009683086536824703\n",
      "[step: 950] loss: 0.0009569971589371562\n",
      "[step: 951] loss: 0.0009782405104488134\n",
      "[step: 952] loss: 0.0009007619228214025\n",
      "[step: 953] loss: 0.0009618953918106854\n",
      "[step: 954] loss: 0.0009194009471684694\n",
      "[step: 955] loss: 0.0009375222725793719\n",
      "[step: 956] loss: 0.0009058535797521472\n",
      "[step: 957] loss: 0.0009257218334823847\n",
      "[step: 958] loss: 0.0009227495174854994\n",
      "[step: 959] loss: 0.0009004747262224555\n",
      "[step: 960] loss: 0.0009139468311332166\n",
      "[step: 961] loss: 0.0008970876806415617\n",
      "[step: 962] loss: 0.0009177225292660296\n",
      "[step: 963] loss: 0.0008890604949556291\n",
      "[step: 964] loss: 0.0009005138999782503\n",
      "[step: 965] loss: 0.0008961798739619553\n",
      "[step: 966] loss: 0.0008955087978392839\n",
      "[step: 967] loss: 0.0008939981926232576\n",
      "[step: 968] loss: 0.0008832576568238437\n",
      "[step: 969] loss: 0.0008948388276621699\n",
      "[step: 970] loss: 0.0008844815893098712\n",
      "[step: 971] loss: 0.0008876659558154643\n",
      "[step: 972] loss: 0.0008813068270683289\n",
      "[step: 973] loss: 0.0008821619558148086\n",
      "[step: 974] loss: 0.0008855598862282932\n",
      "[step: 975] loss: 0.0008776010945439339\n",
      "[step: 976] loss: 0.0008808945422060788\n",
      "[step: 977] loss: 0.0008767695399001241\n",
      "[step: 978] loss: 0.0008784413221292198\n",
      "[step: 979] loss: 0.0008774568559601903\n",
      "[step: 980] loss: 0.0008736138697713614\n",
      "[step: 981] loss: 0.0008755095768719912\n",
      "[step: 982] loss: 0.0008723054779693484\n",
      "[step: 983] loss: 0.0008736902964301407\n",
      "[step: 984] loss: 0.0008724125218577683\n",
      "[step: 985] loss: 0.0008696314180269837\n",
      "[step: 986] loss: 0.0008706578519195318\n",
      "[step: 987] loss: 0.00086887989891693\n",
      "[step: 988] loss: 0.0008692151750437915\n",
      "[step: 989] loss: 0.0008682218613103032\n",
      "[step: 990] loss: 0.000866258458700031\n",
      "[step: 991] loss: 0.0008667459478601813\n",
      "[step: 992] loss: 0.0008654372068122029\n",
      "[step: 993] loss: 0.0008651536190882325\n",
      "[step: 994] loss: 0.0008649167721159756\n",
      "[step: 995] loss: 0.000863339810166508\n",
      "[step: 996] loss: 0.0008631207165308297\n",
      "[step: 997] loss: 0.0008622741443105042\n",
      "[step: 998] loss: 0.0008615945698693395\n",
      "[step: 999] loss: 0.0008616052218712866\n",
      "[step: 1000] loss: 0.000860548228956759\n",
      "[step: 1001] loss: 0.0008599293068982661\n",
      "[step: 1002] loss: 0.0008594659739173949\n",
      "[step: 1003] loss: 0.000858499959576875\n",
      "[step: 1004] loss: 0.0008581390138715506\n",
      "[step: 1005] loss: 0.000857626146171242\n",
      "[step: 1006] loss: 0.0008569035562686622\n",
      "[step: 1007] loss: 0.0008566005853936076\n",
      "[step: 1008] loss: 0.0008559705456718802\n",
      "[step: 1009] loss: 0.0008552441140636802\n",
      "[step: 1010] loss: 0.0008547957986593246\n",
      "[step: 1011] loss: 0.0008541232673451304\n",
      "[step: 1012] loss: 0.0008534865919500589\n",
      "[step: 1013] loss: 0.000853041943628341\n",
      "[step: 1014] loss: 0.0008524185395799577\n",
      "[step: 1015] loss: 0.0008518442627973855\n",
      "[step: 1016] loss: 0.0008514161454513669\n",
      "[step: 1017] loss: 0.00085085240425542\n",
      "[step: 1018] loss: 0.0008503078715875745\n",
      "[step: 1019] loss: 0.0008498664246872067\n",
      "[step: 1020] loss: 0.0008493488421663642\n",
      "[step: 1021] loss: 0.0008488513994961977\n",
      "[step: 1022] loss: 0.0008484692079946399\n",
      "[step: 1023] loss: 0.000848087074700743\n",
      "[step: 1024] loss: 0.0008477868395857513\n",
      "[step: 1025] loss: 0.000847741321194917\n",
      "[step: 1026] loss: 0.0008480535470880568\n",
      "[step: 1027] loss: 0.0008489856263622642\n",
      "[step: 1028] loss: 0.0008513221982866526\n",
      "[step: 1029] loss: 0.0008560921996831894\n",
      "[step: 1030] loss: 0.000866235583089292\n",
      "[step: 1031] loss: 0.0008849571458995342\n",
      "[step: 1032] loss: 0.0009229666320607066\n",
      "[step: 1033] loss: 0.0009778359672054648\n",
      "[step: 1034] loss: 0.0010643304558470845\n",
      "[step: 1035] loss: 0.0011001116363331676\n",
      "[step: 1036] loss: 0.0010772653622552752\n",
      "[step: 1037] loss: 0.0009581607300788164\n",
      "[step: 1038] loss: 0.0008787874248810112\n",
      "[step: 1039] loss: 0.0009027154883369803\n",
      "[step: 1040] loss: 0.0009287266875617206\n",
      "[step: 1041] loss: 0.0009054120164364576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1042] loss: 0.0008734650327824056\n",
      "[step: 1043] loss: 0.000909249996766448\n",
      "[step: 1044] loss: 0.0009301027748733759\n",
      "[step: 1045] loss: 0.0008669340168125927\n",
      "[step: 1046] loss: 0.0008586195763200521\n",
      "[step: 1047] loss: 0.0008893857593648136\n",
      "[step: 1048] loss: 0.0008591096848249435\n",
      "[step: 1049] loss: 0.0008470127359032631\n",
      "[step: 1050] loss: 0.0008739376207813621\n",
      "[step: 1051] loss: 0.0008648440125398338\n",
      "[step: 1052] loss: 0.0008496735244989395\n",
      "[step: 1053] loss: 0.0008604557951912284\n",
      "[step: 1054] loss: 0.0008561401045881212\n",
      "[step: 1055] loss: 0.0008393952739425004\n",
      "[step: 1056] loss: 0.000842866487801075\n",
      "[step: 1057] loss: 0.0008483512792736292\n",
      "[step: 1058] loss: 0.000840095744933933\n",
      "[step: 1059] loss: 0.00083882175385952\n",
      "[step: 1060] loss: 0.0008463471313007176\n",
      "[step: 1061] loss: 0.0008444191189482808\n",
      "[step: 1062] loss: 0.0008378605707548559\n",
      "[step: 1063] loss: 0.0008404036052525043\n",
      "[step: 1064] loss: 0.0008421578677371144\n",
      "[step: 1065] loss: 0.000836246064864099\n",
      "[step: 1066] loss: 0.0008330535492859781\n",
      "[step: 1067] loss: 0.0008356104954145849\n",
      "[step: 1068] loss: 0.0008348709670826793\n",
      "[step: 1069] loss: 0.0008307163370773196\n",
      "[step: 1070] loss: 0.00082927115727216\n",
      "[step: 1071] loss: 0.0008309222175739706\n",
      "[step: 1072] loss: 0.0008308539399877191\n",
      "[step: 1073] loss: 0.0008280593319796026\n",
      "[step: 1074] loss: 0.0008264643256552517\n",
      "[step: 1075] loss: 0.000827409909106791\n",
      "[step: 1076] loss: 0.0008280350011773407\n",
      "[step: 1077] loss: 0.0008268135716207325\n",
      "[step: 1078] loss: 0.0008252920233644545\n",
      "[step: 1079] loss: 0.0008256116998381913\n",
      "[step: 1080] loss: 0.0008272978593595326\n",
      "[step: 1081] loss: 0.0008290000259876251\n",
      "[step: 1082] loss: 0.0008309464319609106\n",
      "[step: 1083] loss: 0.0008356215548701584\n",
      "[step: 1084] loss: 0.0008462598198093474\n",
      "[step: 1085] loss: 0.0008664709166623652\n",
      "[step: 1086] loss: 0.0009029071079567075\n",
      "[step: 1087] loss: 0.0009618055773898959\n",
      "[step: 1088] loss: 0.0010517393238842487\n",
      "[step: 1089] loss: 0.0011487083975225687\n",
      "[step: 1090] loss: 0.001212513423524797\n",
      "[step: 1091] loss: 0.0011436997447162867\n",
      "[step: 1092] loss: 0.0009726998396217823\n",
      "[step: 1093] loss: 0.0008388488204218447\n",
      "[step: 1094] loss: 0.0008719481411390007\n",
      "[step: 1095] loss: 0.0009678666247054935\n",
      "[step: 1096] loss: 0.0009572303970344365\n",
      "[step: 1097] loss: 0.0008901192923076451\n",
      "[step: 1098] loss: 0.0008971362258307636\n",
      "[step: 1099] loss: 0.0009101663599722087\n",
      "[step: 1100] loss: 0.0008695965516380966\n",
      "[step: 1101] loss: 0.0008336391183547676\n",
      "[step: 1102] loss: 0.0008678272715769708\n",
      "[step: 1103] loss: 0.0008934533689171076\n",
      "[step: 1104] loss: 0.0008411144954152405\n",
      "[step: 1105] loss: 0.0008291351259686053\n",
      "[step: 1106] loss: 0.0008556004613637924\n",
      "[step: 1107] loss: 0.0008431473979726434\n",
      "[step: 1108] loss: 0.0008257597801275551\n",
      "[step: 1109] loss: 0.0008359680068679154\n",
      "[step: 1110] loss: 0.0008397835190407932\n",
      "[step: 1111] loss: 0.0008258537272922695\n",
      "[step: 1112] loss: 0.0008221083553507924\n",
      "[step: 1113] loss: 0.00082792789908126\n",
      "[step: 1114] loss: 0.0008238399750553071\n",
      "[step: 1115] loss: 0.0008182624005712569\n",
      "[step: 1116] loss: 0.0008227543439716101\n",
      "[step: 1117] loss: 0.0008234326960518956\n",
      "[step: 1118] loss: 0.0008165321778506041\n",
      "[step: 1119] loss: 0.0008166708284988999\n",
      "[step: 1120] loss: 0.0008185066399164498\n",
      "[step: 1121] loss: 0.0008139581186696887\n",
      "[step: 1122] loss: 0.0008104187436401844\n",
      "[step: 1123] loss: 0.0008126856409944594\n",
      "[step: 1124] loss: 0.0008132330258376896\n",
      "[step: 1125] loss: 0.0008091127965599298\n",
      "[step: 1126] loss: 0.000807145785074681\n",
      "[step: 1127] loss: 0.0008095551165752113\n",
      "[step: 1128] loss: 0.0008103203144855797\n",
      "[step: 1129] loss: 0.0008071396150626242\n",
      "[step: 1130] loss: 0.0008053016499616206\n",
      "[step: 1131] loss: 0.000807182164862752\n",
      "[step: 1132] loss: 0.0008090217597782612\n",
      "[step: 1133] loss: 0.0008082867134362459\n",
      "[step: 1134] loss: 0.0008080059778876603\n",
      "[step: 1135] loss: 0.0008119000121951103\n",
      "[step: 1136] loss: 0.0008214459521695971\n",
      "[step: 1137] loss: 0.0008343649096786976\n",
      "[step: 1138] loss: 0.0008605700568296015\n",
      "[step: 1139] loss: 0.0008967785397544503\n",
      "[step: 1140] loss: 0.0009628839907236397\n",
      "[step: 1141] loss: 0.0010213929926976562\n",
      "[step: 1142] loss: 0.0010665818117558956\n",
      "[step: 1143] loss: 0.001019794843159616\n",
      "[step: 1144] loss: 0.0009217311162501574\n",
      "[step: 1145] loss: 0.0008388756541535258\n",
      "[step: 1146] loss: 0.0008276863372884691\n",
      "[step: 1147] loss: 0.0008639598963782191\n",
      "[step: 1148] loss: 0.0008815773762762547\n",
      "[step: 1149] loss: 0.0008818734786473215\n",
      "[step: 1150] loss: 0.0008735597366467118\n",
      "[step: 1151] loss: 0.0008464039419777691\n",
      "[step: 1152] loss: 0.0008138615521602333\n",
      "[step: 1153] loss: 0.0008096002275124192\n",
      "[step: 1154] loss: 0.0008357351180166006\n",
      "[step: 1155] loss: 0.0008467536536045372\n",
      "[step: 1156] loss: 0.0008278570603579283\n",
      "[step: 1157] loss: 0.0008134724921546876\n",
      "[step: 1158] loss: 0.0008136135875247419\n",
      "[step: 1159] loss: 0.0008096621604636312\n",
      "[step: 1160] loss: 0.0008055652724578977\n",
      "[step: 1161] loss: 0.000809013145044446\n",
      "[step: 1162] loss: 0.0008144174353219569\n",
      "[step: 1163] loss: 0.0008138561388477683\n",
      "[step: 1164] loss: 0.0008064102730713785\n",
      "[step: 1165] loss: 0.000801634625531733\n",
      "[step: 1166] loss: 0.0007979449583217502\n",
      "[step: 1167] loss: 0.0007952184532769024\n",
      "[step: 1168] loss: 0.0007968196296133101\n",
      "[step: 1169] loss: 0.0008002941031008959\n",
      "[step: 1170] loss: 0.0008011356694623828\n",
      "[step: 1171] loss: 0.0007992452592588961\n",
      "[step: 1172] loss: 0.0007978776702657342\n",
      "[step: 1173] loss: 0.0007977172499522567\n",
      "[step: 1174] loss: 0.0007961800438351929\n",
      "[step: 1175] loss: 0.0007924127276055515\n",
      "[step: 1176] loss: 0.0007895103190094233\n",
      "[step: 1177] loss: 0.0007886234670877457\n",
      "[step: 1178] loss: 0.0007888377294875681\n",
      "[step: 1179] loss: 0.000788529054261744\n",
      "[step: 1180] loss: 0.0007876534364186227\n",
      "[step: 1181] loss: 0.0007873752037994564\n",
      "[step: 1182] loss: 0.0007879309123381972\n",
      "[step: 1183] loss: 0.0007890416891314089\n",
      "[step: 1184] loss: 0.0007901450735516846\n",
      "[step: 1185] loss: 0.0007911979337222874\n",
      "[step: 1186] loss: 0.0007928799604997039\n",
      "[step: 1187] loss: 0.0007961702649481595\n",
      "[step: 1188] loss: 0.0008022753754630685\n",
      "[step: 1189] loss: 0.0008131408831104636\n",
      "[step: 1190] loss: 0.000828801654279232\n",
      "[step: 1191] loss: 0.0008560807327739894\n",
      "[step: 1192] loss: 0.0008878079825080931\n",
      "[step: 1193] loss: 0.0009363030549138784\n",
      "[step: 1194] loss: 0.0009699456859380007\n",
      "[step: 1195] loss: 0.0009905243059620261\n",
      "[step: 1196] loss: 0.0009714178158901632\n",
      "[step: 1197] loss: 0.0009241109364666045\n",
      "[step: 1198] loss: 0.0008780125062912703\n",
      "[step: 1199] loss: 0.0008433088078163564\n",
      "[step: 1200] loss: 0.0008220383897423744\n",
      "[step: 1201] loss: 0.00081066251732409\n",
      "[step: 1202] loss: 0.0008264895295724273\n",
      "[step: 1203] loss: 0.0008486656588502228\n",
      "[step: 1204] loss: 0.0008400031365454197\n",
      "[step: 1205] loss: 0.0008025197312235832\n",
      "[step: 1206] loss: 0.000781437789555639\n",
      "[step: 1207] loss: 0.0007950842846184969\n",
      "[step: 1208] loss: 0.0008092951611615717\n",
      "[step: 1209] loss: 0.000802662514615804\n",
      "[step: 1210] loss: 0.0007974729523994029\n",
      "[step: 1211] loss: 0.0008055376820266247\n",
      "[step: 1212] loss: 0.0008031319011934102\n",
      "[step: 1213] loss: 0.0007894327864050865\n",
      "[step: 1214] loss: 0.0007804708438925445\n",
      "[step: 1215] loss: 0.0007846595253795385\n",
      "[step: 1216] loss: 0.0007856544107198715\n",
      "[step: 1217] loss: 0.0007792658288963139\n",
      "[step: 1218] loss: 0.0007787517388351262\n",
      "[step: 1219] loss: 0.0007861446938477457\n",
      "[step: 1220] loss: 0.000790130696259439\n",
      "[step: 1221] loss: 0.0007867683889344335\n",
      "[step: 1222] loss: 0.000783960276748985\n",
      "[step: 1223] loss: 0.0007859471952542663\n",
      "[step: 1224] loss: 0.0007893056608736515\n",
      "[step: 1225] loss: 0.0007870681001804769\n",
      "[step: 1226] loss: 0.0007838433375582099\n",
      "[step: 1227] loss: 0.0007827174267731607\n",
      "[step: 1228] loss: 0.000784240139182657\n",
      "[step: 1229] loss: 0.0007851405534893274\n",
      "[step: 1230] loss: 0.0007839176687411964\n",
      "[step: 1231] loss: 0.0007828986272215843\n",
      "[step: 1232] loss: 0.000784665287937969\n",
      "[step: 1233] loss: 0.0007883005309849977\n",
      "[step: 1234] loss: 0.0007943630334921181\n",
      "[step: 1235] loss: 0.0007985947886481881\n",
      "[step: 1236] loss: 0.0008063640561886132\n",
      "[step: 1237] loss: 0.0008150780340656638\n",
      "[step: 1238] loss: 0.0008292249403893948\n",
      "[step: 1239] loss: 0.0008447454310953617\n",
      "[step: 1240] loss: 0.0008603115566074848\n",
      "[step: 1241] loss: 0.000870061747264117\n",
      "[step: 1242] loss: 0.0008696148288436234\n",
      "[step: 1243] loss: 0.0008543867152184248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1244] loss: 0.0008274300489574671\n",
      "[step: 1245] loss: 0.0007962771924212575\n",
      "[step: 1246] loss: 0.0007728859782218933\n",
      "[step: 1247] loss: 0.0007645292789675295\n",
      "[step: 1248] loss: 0.0007704406161792576\n",
      "[step: 1249] loss: 0.0007840858306735754\n",
      "[step: 1250] loss: 0.0007975559565238655\n",
      "[step: 1251] loss: 0.0008061972912400961\n",
      "[step: 1252] loss: 0.0008081580745056272\n",
      "[step: 1253] loss: 0.000803562521468848\n",
      "[step: 1254] loss: 0.0007951305597089231\n",
      "[step: 1255] loss: 0.0007841161568649113\n",
      "[step: 1256] loss: 0.0007736390689387918\n",
      "[step: 1257] loss: 0.0007653656648471951\n",
      "[step: 1258] loss: 0.0007607928127981722\n",
      "[step: 1259] loss: 0.0007601469988003373\n",
      "[step: 1260] loss: 0.0007626343867741525\n",
      "[step: 1261] loss: 0.0007669601473025978\n",
      "[step: 1262] loss: 0.0007718801498413086\n",
      "[step: 1263] loss: 0.0007769966614432633\n",
      "[step: 1264] loss: 0.0007825560169294477\n",
      "[step: 1265] loss: 0.0007899673655629158\n",
      "[step: 1266] loss: 0.0008021536632440984\n",
      "[step: 1267] loss: 0.0008191622328013182\n",
      "[step: 1268] loss: 0.0008519926923327148\n",
      "[step: 1269] loss: 0.0008784890524111688\n",
      "[step: 1270] loss: 0.000926840933971107\n",
      "[step: 1271] loss: 0.0009053699905052781\n",
      "[step: 1272] loss: 0.0008712265407666564\n",
      "[step: 1273] loss: 0.0008017870131880045\n",
      "[step: 1274] loss: 0.000785354757681489\n",
      "[step: 1275] loss: 0.0008261146722361445\n",
      "[step: 1276] loss: 0.000840366876218468\n",
      "[step: 1277] loss: 0.0008193826652131975\n",
      "[step: 1278] loss: 0.0007964749820530415\n",
      "[step: 1279] loss: 0.0008289468823932111\n",
      "[step: 1280] loss: 0.0008681812323629856\n",
      "[step: 1281] loss: 0.0008610517834313214\n",
      "[step: 1282] loss: 0.0008510365150868893\n",
      "[step: 1283] loss: 0.0008547424804419279\n",
      "[step: 1284] loss: 0.000827112584374845\n",
      "[step: 1285] loss: 0.0007799952873028815\n",
      "[step: 1286] loss: 0.0007669001934118569\n",
      "[step: 1287] loss: 0.0007813285337761045\n",
      "[step: 1288] loss: 0.000778333458583802\n",
      "[step: 1289] loss: 0.000764480559155345\n",
      "[step: 1290] loss: 0.0007745223119854927\n",
      "[step: 1291] loss: 0.0007888226537033916\n",
      "[step: 1292] loss: 0.0007771503296680748\n",
      "[step: 1293] loss: 0.0007729133940301836\n",
      "[step: 1294] loss: 0.0007815140415914357\n",
      "[step: 1295] loss: 0.0007770416559651494\n",
      "[step: 1296] loss: 0.0007624676800332963\n",
      "[step: 1297] loss: 0.0007571600726805627\n",
      "[step: 1298] loss: 0.0007588723092339933\n",
      "[step: 1299] loss: 0.0007556639611721039\n",
      "[step: 1300] loss: 0.0007510548457503319\n",
      "[step: 1301] loss: 0.0007545722764916718\n",
      "[step: 1302] loss: 0.0007594379130750895\n",
      "[step: 1303] loss: 0.0007574751507490873\n",
      "[step: 1304] loss: 0.0007544007385149598\n",
      "[step: 1305] loss: 0.0007566514541395009\n",
      "[step: 1306] loss: 0.0007615978829562664\n",
      "[step: 1307] loss: 0.0007636053487658501\n",
      "[step: 1308] loss: 0.0007634988287463784\n",
      "[step: 1309] loss: 0.000765830569434911\n",
      "[step: 1310] loss: 0.0007716811378486454\n",
      "[step: 1311] loss: 0.0007769945077598095\n",
      "[step: 1312] loss: 0.0007837770390324295\n",
      "[step: 1313] loss: 0.0007901891367509961\n",
      "[step: 1314] loss: 0.0008018172811716795\n",
      "[step: 1315] loss: 0.0008172178058885038\n",
      "[step: 1316] loss: 0.0008321203058585525\n",
      "[step: 1317] loss: 0.0008438382064923644\n",
      "[step: 1318] loss: 0.000846111390274018\n",
      "[step: 1319] loss: 0.0008384112152270973\n",
      "[step: 1320] loss: 0.0008208423387259245\n",
      "[step: 1321] loss: 0.0007967682904563844\n",
      "[step: 1322] loss: 0.0007774746627546847\n",
      "[step: 1323] loss: 0.0007637469680048525\n",
      "[step: 1324] loss: 0.0007616169168613851\n",
      "[step: 1325] loss: 0.0007612166227772832\n",
      "[step: 1326] loss: 0.0007626834558323026\n",
      "[step: 1327] loss: 0.0007605250575579703\n",
      "[step: 1328] loss: 0.0007595941424369812\n",
      "[step: 1329] loss: 0.0007633247296325862\n",
      "[step: 1330] loss: 0.000771205173805356\n",
      "[step: 1331] loss: 0.0007799993036314845\n",
      "[step: 1332] loss: 0.0007814365089870989\n",
      "[step: 1333] loss: 0.0007810240495018661\n",
      "[step: 1334] loss: 0.00077443802729249\n",
      "[step: 1335] loss: 0.0007713497616350651\n",
      "[step: 1336] loss: 0.0007677041576243937\n",
      "[step: 1337] loss: 0.0007654607179574668\n",
      "[step: 1338] loss: 0.000761780422180891\n",
      "[step: 1339] loss: 0.0007579452940262854\n",
      "[step: 1340] loss: 0.000754670356400311\n",
      "[step: 1341] loss: 0.0007518159691244364\n",
      "[step: 1342] loss: 0.0007487676339223981\n",
      "[step: 1343] loss: 0.0007450506091117859\n",
      "[step: 1344] loss: 0.0007412618724629283\n",
      "[step: 1345] loss: 0.0007384457858279347\n",
      "[step: 1346] loss: 0.0007374804699793458\n",
      "[step: 1347] loss: 0.0007385891512967646\n",
      "[step: 1348] loss: 0.0007405659416690469\n",
      "[step: 1349] loss: 0.0007437921594828367\n",
      "[step: 1350] loss: 0.0007465795497409999\n",
      "[step: 1351] loss: 0.0007527309353463352\n",
      "[step: 1352] loss: 0.0007616494549438357\n",
      "[step: 1353] loss: 0.000782125978730619\n",
      "[step: 1354] loss: 0.0008149545756168664\n",
      "[step: 1355] loss: 0.0008753555594012141\n",
      "[step: 1356] loss: 0.0009577241144143045\n",
      "[step: 1357] loss: 0.0010644765570759773\n",
      "[step: 1358] loss: 0.0011437541106715798\n",
      "[step: 1359] loss: 0.001132674515247345\n",
      "[step: 1360] loss: 0.0010288771009072661\n",
      "[step: 1361] loss: 0.0008721203776076436\n",
      "[step: 1362] loss: 0.000805893971119076\n",
      "[step: 1363] loss: 0.0008507171296514571\n",
      "[step: 1364] loss: 0.0009370096959173679\n",
      "[step: 1365] loss: 0.0008996996912173927\n",
      "[step: 1366] loss: 0.00078536756336689\n",
      "[step: 1367] loss: 0.0007437328458763659\n",
      "[step: 1368] loss: 0.0008068812312558293\n",
      "[step: 1369] loss: 0.0008567076292820275\n",
      "[step: 1370] loss: 0.0008113653748296201\n",
      "[step: 1371] loss: 0.0007535922341048717\n",
      "[step: 1372] loss: 0.0007574018090963364\n",
      "[step: 1373] loss: 0.000783856026828289\n",
      "[step: 1374] loss: 0.0007842745399102569\n",
      "[step: 1375] loss: 0.0007637874223291874\n",
      "[step: 1376] loss: 0.0007596269133500755\n",
      "[step: 1377] loss: 0.000758672715164721\n",
      "[step: 1378] loss: 0.0007506500696763396\n",
      "[step: 1379] loss: 0.0007523764506913722\n",
      "[step: 1380] loss: 0.0007499674102291465\n",
      "[step: 1381] loss: 0.000743538374081254\n",
      "[step: 1382] loss: 0.0007418508757837117\n",
      "[step: 1383] loss: 0.0007483322988264263\n",
      "[step: 1384] loss: 0.0007489071576856077\n",
      "[step: 1385] loss: 0.0007354444824159145\n",
      "[step: 1386] loss: 0.0007268456392921507\n",
      "[step: 1387] loss: 0.0007324634352698922\n",
      "[step: 1388] loss: 0.0007404049974866211\n",
      "[step: 1389] loss: 0.0007386275101453066\n",
      "[step: 1390] loss: 0.0007301017176359892\n",
      "[step: 1391] loss: 0.0007272408693097532\n",
      "[step: 1392] loss: 0.0007299960125237703\n",
      "[step: 1393] loss: 0.0007312385132536292\n",
      "[step: 1394] loss: 0.0007295464747585356\n",
      "[step: 1395] loss: 0.0007265213062055409\n",
      "[step: 1396] loss: 0.0007249824120663106\n",
      "[step: 1397] loss: 0.0007237967220135033\n",
      "[step: 1398] loss: 0.0007229396142065525\n",
      "[step: 1399] loss: 0.0007236138335429132\n",
      "[step: 1400] loss: 0.0007247829926200211\n",
      "[step: 1401] loss: 0.000724497833289206\n",
      "[step: 1402] loss: 0.0007222767453640699\n",
      "[step: 1403] loss: 0.0007208303431980312\n",
      "[step: 1404] loss: 0.0007216050289571285\n",
      "[step: 1405] loss: 0.0007234028889797628\n",
      "[step: 1406] loss: 0.0007243411964736879\n",
      "[step: 1407] loss: 0.0007241318235173821\n",
      "[step: 1408] loss: 0.0007243682630360126\n",
      "[step: 1409] loss: 0.0007262020953930914\n",
      "[step: 1410] loss: 0.0007295547402463853\n",
      "[step: 1411] loss: 0.0007350384257733822\n",
      "[step: 1412] loss: 0.0007433946593664587\n",
      "[step: 1413] loss: 0.0007602580590173602\n",
      "[step: 1414] loss: 0.0007813790580257773\n",
      "[step: 1415] loss: 0.0008232358377426863\n",
      "[step: 1416] loss: 0.0008485378348268569\n",
      "[step: 1417] loss: 0.0008884165436029434\n",
      "[step: 1418] loss: 0.0008552533690817654\n",
      "[step: 1419] loss: 0.0008068815805017948\n",
      "[step: 1420] loss: 0.000761466333642602\n",
      "[step: 1421] loss: 0.0007606265135109425\n",
      "[step: 1422] loss: 0.000783902476541698\n",
      "[step: 1423] loss: 0.0007668406469747424\n",
      "[step: 1424] loss: 0.0007403623312711716\n",
      "[step: 1425] loss: 0.0007277376716956496\n",
      "[step: 1426] loss: 0.0007371206884272397\n",
      "[step: 1427] loss: 0.0007413665298372507\n",
      "[step: 1428] loss: 0.0007321877055801451\n",
      "[step: 1429] loss: 0.000735764333512634\n",
      "[step: 1430] loss: 0.000739172101020813\n",
      "[step: 1431] loss: 0.000728620623704046\n",
      "[step: 1432] loss: 0.0007225260487757623\n",
      "[step: 1433] loss: 0.0007304502069018781\n",
      "[step: 1434] loss: 0.000733213615603745\n",
      "[step: 1435] loss: 0.0007215808145701885\n",
      "[step: 1436] loss: 0.000712999957613647\n",
      "[step: 1437] loss: 0.0007170617463998497\n",
      "[step: 1438] loss: 0.0007212337804958224\n",
      "[step: 1439] loss: 0.0007169090677052736\n",
      "[step: 1440] loss: 0.0007128811557777226\n",
      "[step: 1441] loss: 0.000715286994818598\n",
      "[step: 1442] loss: 0.0007173327612690628\n",
      "[step: 1443] loss: 0.0007153396145440638\n",
      "[step: 1444] loss: 0.0007131682941690087\n",
      "[step: 1445] loss: 0.0007157991640269756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1446] loss: 0.0007195600774139166\n",
      "[step: 1447] loss: 0.0007199971005320549\n",
      "[step: 1448] loss: 0.0007189259631559253\n",
      "[step: 1449] loss: 0.0007204316207207739\n",
      "[step: 1450] loss: 0.0007258732803165913\n",
      "[step: 1451] loss: 0.0007346459315158427\n",
      "[step: 1452] loss: 0.0007444541552104056\n",
      "[step: 1453] loss: 0.000759837799705565\n",
      "[step: 1454] loss: 0.000781150592956692\n",
      "[step: 1455] loss: 0.0008100224658846855\n",
      "[step: 1456] loss: 0.0008414068724960089\n",
      "[step: 1457] loss: 0.0008669389644637704\n",
      "[step: 1458] loss: 0.0008787040133029222\n",
      "[step: 1459] loss: 0.0008606166811659932\n",
      "[step: 1460] loss: 0.0008169642533175647\n",
      "[step: 1461] loss: 0.0007602974073961377\n",
      "[step: 1462] loss: 0.0007215323275886476\n",
      "[step: 1463] loss: 0.0007170190801844001\n",
      "[step: 1464] loss: 0.0007415549480356276\n",
      "[step: 1465] loss: 0.0007687901379540563\n",
      "[step: 1466] loss: 0.0007821407052688301\n",
      "[step: 1467] loss: 0.0007702505681663752\n",
      "[step: 1468] loss: 0.0007435738807544112\n",
      "[step: 1469] loss: 0.0007166758878156543\n",
      "[step: 1470] loss: 0.000703623634763062\n",
      "[step: 1471] loss: 0.000707351544406265\n",
      "[step: 1472] loss: 0.0007210565963760018\n",
      "[step: 1473] loss: 0.000735973531845957\n",
      "[step: 1474] loss: 0.0007435385487042367\n",
      "[step: 1475] loss: 0.0007424812647514045\n",
      "[step: 1476] loss: 0.0007321924204006791\n",
      "[step: 1477] loss: 0.0007199106621555984\n",
      "[step: 1478] loss: 0.0007081442745402455\n",
      "[step: 1479] loss: 0.0007025107624940574\n",
      "[step: 1480] loss: 0.0007030159467831254\n",
      "[step: 1481] loss: 0.0007078169146552682\n",
      "[step: 1482] loss: 0.0007136433850973845\n",
      "[step: 1483] loss: 0.0007181059336289763\n",
      "[step: 1484] loss: 0.0007198172388598323\n",
      "[step: 1485] loss: 0.000718533294275403\n",
      "[step: 1486] loss: 0.0007152725593186915\n",
      "[step: 1487] loss: 0.0007106055854819715\n",
      "[step: 1488] loss: 0.000706236984115094\n",
      "[step: 1489] loss: 0.0007024878286756575\n",
      "[step: 1490] loss: 0.0007005282095633447\n",
      "[step: 1491] loss: 0.0007001343765296042\n",
      "[step: 1492] loss: 0.0007018575561232865\n",
      "[step: 1493] loss: 0.0007050599670037627\n",
      "[step: 1494] loss: 0.0007114522159099579\n",
      "[step: 1495] loss: 0.0007193302735686302\n",
      "[step: 1496] loss: 0.000734589179046452\n",
      "[step: 1497] loss: 0.0007487370749004185\n",
      "[step: 1498] loss: 0.0007785210036672652\n",
      "[step: 1499] loss: 0.0007862597121857107\n",
      "RMSE: 0.03631867095828056\n",
      "pred: [0.49422693 0.48271537 0.8466097  0.78555036 0.7664817  0.74598944\n",
      " 0.7626411  0.7525322  0.7896195  0.6631365  0.6810761  0.7014675\n",
      " 0.76025    0.8035438  0.6995979  0.62991214 0.69527835 0.6742923\n",
      " 0.5540687  0.7349634  0.7370029  0.75025314 0.5864083  0.7235457\n",
      " 0.7269252 ]\n",
      "real: [0.55240455 0.42313885 1.         0.83916638 0.82361251 0.78596038\n",
      " 0.78386201 0.76055792 0.79908991 0.61919099 0.64821382 0.70596378\n",
      " 0.7866441  0.92623249 0.68265413 0.64876231 0.70188333 0.71897882\n",
      " 0.59543155 0.82084879 0.87041319 0.87212843 0.66318553 0.70669962\n",
      " 0.68007622]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501_201611.txt',delimiter = ',',dtype = None)\n",
    "xy2 = np.genfromtxt('/Users/yeseo/desktop/City_Counted_TaxiMach_Link_Dataset_Full_201704 -201708.txt',delimiter = ',',dtype = None)\n",
    "test_xy = np.genfromtxt('/Users/yeseo/desktop/City_Counted_TaxiMach_Link_Dataset_Full_201709.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "xy=np.vstack((xy,xy2))\n",
    "\n",
    "xy= xy[:,:27]\n",
    "a=xy[:,:2]\n",
    "b=xy[:,2:]\n",
    "b = MinMaxScaler(b)\n",
    "xy=np.hstack((a,b))\n",
    "\n",
    "test_xy= test_xy[:,:27]\n",
    "test_a=test_xy[:,:2]\n",
    "test_b=test_xy[:,2:]\n",
    "test_b = MinMaxScaler(test_b)\n",
    "test_xy=np.hstack((test_a,test_b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1500\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "train_set = xy\n",
    "test_set = test_xy\n",
    "\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "outputs,_states = tf.nn.dynamic_rnn(cell,X,dtype = tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "loss =tf.reduce_mean(tf.square(Y_pred-Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train,loss],feed_dict={X:trainX, Y:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X:testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    print(\"real: {}\".format(testY[-1,:]))\n",
    "    \n",
    "    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.3)\n",
    "    plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
