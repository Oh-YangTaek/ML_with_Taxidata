{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-84d4544980df>:53: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 11.284062385559082\n",
      "[step: 1] loss: 9.70776653289795\n",
      "[step: 2] loss: 8.50062084197998\n",
      "[step: 3] loss: 7.465725898742676\n",
      "[step: 4] loss: 6.520405292510986\n",
      "[step: 5] loss: 5.659430980682373\n",
      "[step: 6] loss: 4.896528720855713\n",
      "[step: 7] loss: 4.222663879394531\n",
      "[step: 8] loss: 3.627551555633545\n",
      "[step: 9] loss: 3.1056289672851562\n",
      "[step: 10] loss: 2.652656316757202\n",
      "[step: 11] loss: 2.262049674987793\n",
      "[step: 12] loss: 1.9242448806762695\n",
      "[step: 13] loss: 1.6318542957305908\n",
      "[step: 14] loss: 1.3789689540863037\n",
      "[step: 15] loss: 1.1606454849243164\n",
      "[step: 16] loss: 0.9726731777191162\n",
      "[step: 17] loss: 0.8113548755645752\n",
      "[step: 18] loss: 0.6734438538551331\n",
      "[step: 19] loss: 0.5561686158180237\n",
      "[step: 20] loss: 0.45723360776901245\n",
      "[step: 21] loss: 0.3747275173664093\n",
      "[step: 22] loss: 0.306970477104187\n",
      "[step: 23] loss: 0.2523781359195709\n",
      "[step: 24] loss: 0.20939263701438904\n",
      "[step: 25] loss: 0.17647497355937958\n",
      "[step: 26] loss: 0.15212322771549225\n",
      "[step: 27] loss: 0.13488154113292694\n",
      "[step: 28] loss: 0.12334578484296799\n",
      "[step: 29] loss: 0.11617259681224823\n",
      "[step: 30] loss: 0.11211193352937698\n",
      "[step: 31] loss: 0.11004644632339478\n",
      "[step: 32] loss: 0.10902633517980576\n",
      "[step: 33] loss: 0.10828710347414017\n",
      "[step: 34] loss: 0.10725057870149612\n",
      "[step: 35] loss: 0.10551543533802032\n",
      "[step: 36] loss: 0.10284393280744553\n",
      "[step: 37] loss: 0.09914438426494598\n",
      "[step: 38] loss: 0.09444811195135117\n",
      "[step: 39] loss: 0.08888135850429535\n",
      "[step: 40] loss: 0.08263318240642548\n",
      "[step: 41] loss: 0.07592959702014923\n",
      "[step: 42] loss: 0.06900635361671448\n",
      "[step: 43] loss: 0.06208985298871994\n",
      "[step: 44] loss: 0.05537869781255722\n",
      "[step: 45] loss: 0.04903125390410423\n",
      "[step: 46] loss: 0.04315970465540886\n",
      "[step: 47] loss: 0.037828654050827026\n",
      "[step: 48] loss: 0.033061012625694275\n",
      "[step: 49] loss: 0.028846409171819687\n",
      "[step: 50] loss: 0.0251526590436697\n",
      "[step: 51] loss: 0.021936802193522453\n",
      "[step: 52] loss: 0.0191543810069561\n",
      "[step: 53] loss: 0.016765529289841652\n",
      "[step: 54] loss: 0.014736759476363659\n",
      "[step: 55] loss: 0.013039447367191315\n",
      "[step: 56] loss: 0.011645814403891563\n",
      "[step: 57] loss: 0.010524630546569824\n",
      "[step: 58] loss: 0.009638223797082901\n",
      "[step: 59] loss: 0.008942291140556335\n",
      "[step: 60] loss: 0.008388135582208633\n",
      "[step: 61] loss: 0.007927029393613338\n",
      "[step: 62] loss: 0.007515633013099432\n",
      "[step: 63] loss: 0.007120115682482719\n",
      "[step: 64] loss: 0.006718799006193876\n",
      "[step: 65] loss: 0.00630167406052351\n",
      "[step: 66] loss: 0.005868139676749706\n",
      "[step: 67] loss: 0.005423066206276417\n",
      "[step: 68] loss: 0.004973147995769978\n",
      "[step: 69] loss: 0.004524064250290394\n",
      "[step: 70] loss: 0.004079480655491352\n",
      "[step: 71] loss: 0.0036415965296328068\n",
      "[step: 72] loss: 0.0032126056030392647\n",
      "[step: 73] loss: 0.0027963926550000906\n",
      "[step: 74] loss: 0.002399185672402382\n",
      "[step: 75] loss: 0.0020293323323130608\n",
      "[step: 76] loss: 0.0016957512125372887\n",
      "[step: 77] loss: 0.001405999413691461\n",
      "[step: 78] loss: 0.001164423068985343\n",
      "[step: 79] loss: 0.0009712046594358981\n",
      "[step: 80] loss: 0.000822453002911061\n",
      "[step: 81] loss: 0.0007113204337656498\n",
      "[step: 82] loss: 0.0006297013023868203\n",
      "[step: 83] loss: 0.0005698360037058592\n",
      "[step: 84] loss: 0.0005254563293419778\n",
      "[step: 85] loss: 0.0004921656218357384\n",
      "[step: 86] loss: 0.0004671009664889425\n",
      "[step: 87] loss: 0.00044833114952780306\n",
      "[step: 88] loss: 0.0004341595631558448\n",
      "[step: 89] loss: 0.0004227900062687695\n",
      "[step: 90] loss: 0.000412343826610595\n",
      "[step: 91] loss: 0.0004010871343780309\n",
      "[step: 92] loss: 0.0003878398274537176\n",
      "[step: 93] loss: 0.0003720854874700308\n",
      "[step: 94] loss: 0.0003539587196428329\n",
      "[step: 95] loss: 0.000333967967890203\n",
      "[step: 96] loss: 0.00031258867238648236\n",
      "[step: 97] loss: 0.0002900777035392821\n",
      "[step: 98] loss: 0.00026637892005965114\n",
      "[step: 99] loss: 0.00024129256780724972\n",
      "[step: 100] loss: 0.00021474569803103805\n",
      "[step: 101] loss: 0.00018704688409343362\n",
      "[step: 102] loss: 0.00015898844867479056\n",
      "[step: 103] loss: 0.0001317348942393437\n",
      "[step: 104] loss: 0.00010656222002580762\n",
      "[step: 105] loss: 8.456400246359408e-05\n",
      "[step: 106] loss: 6.640484934905544e-05\n",
      "[step: 107] loss: 5.223124389885925e-05\n",
      "[step: 108] loss: 4.1719926230143756e-05\n",
      "[step: 109] loss: 3.4274089557584375e-05\n",
      "[step: 110] loss: 2.9209308195277117e-05\n",
      "[step: 111] loss: 2.5912380806403235e-05\n",
      "[step: 112] loss: 2.3910544769023545e-05\n",
      "[step: 113] loss: 2.286595008627046e-05\n",
      "[step: 114] loss: 2.250648867629934e-05\n",
      "[step: 115] loss: 2.258443782920949e-05\n",
      "[step: 116] loss: 2.2853344489703886e-05\n",
      "[step: 117] loss: 2.308785042259842e-05\n",
      "[step: 118] loss: 2.3106103981263004e-05\n",
      "[step: 119] loss: 2.2812688257545233e-05\n",
      "[step: 120] loss: 2.2191381503944285e-05\n",
      "[step: 121] loss: 2.128420783265028e-05\n",
      "[step: 122] loss: 2.015777317865286e-05\n",
      "[step: 123] loss: 1.8866157915908843e-05\n",
      "[step: 124] loss: 1.74431006598752e-05\n",
      "[step: 125] loss: 1.590146894159261e-05\n",
      "[step: 126] loss: 1.4264845958678052e-05\n",
      "[step: 127] loss: 1.2563050404423848e-05\n",
      "[step: 128] loss: 1.0856248081836384e-05\n",
      "[step: 129] loss: 9.216605576511938e-06\n",
      "[step: 130] loss: 7.714821549598128e-06\n",
      "[step: 131] loss: 6.403011411748594e-06\n",
      "[step: 132] loss: 5.300430530041922e-06\n",
      "[step: 133] loss: 4.405765594128752e-06\n",
      "[step: 134] loss: 3.6935637126589427e-06\n",
      "[step: 135] loss: 3.1373172078019707e-06\n",
      "[step: 136] loss: 2.7102794319944223e-06\n",
      "[step: 137] loss: 2.3908912680781214e-06\n",
      "[step: 138] loss: 2.1600972104351968e-06\n",
      "[step: 139] loss: 1.9988960957562085e-06\n",
      "[step: 140] loss: 1.8838879896065919e-06\n",
      "[step: 141] loss: 1.7933148228621576e-06\n",
      "[step: 142] loss: 1.7073829212677083e-06\n",
      "[step: 143] loss: 1.6144797427841695e-06\n",
      "[step: 144] loss: 1.5111576203707955e-06\n",
      "[step: 145] loss: 1.4010574886924587e-06\n",
      "[step: 146] loss: 1.2908753888041247e-06\n",
      "[step: 147] loss: 1.1862425708386581e-06\n",
      "[step: 148] loss: 1.0918598718490102e-06\n",
      "[step: 149] loss: 1.0071219094243133e-06\n",
      "[step: 150] loss: 9.301439263253997e-07\n",
      "[step: 151] loss: 8.582108534938016e-07\n",
      "[step: 152] loss: 7.890299684731872e-07\n",
      "[step: 153] loss: 7.210756507447513e-07\n",
      "[step: 154] loss: 6.543544941450818e-07\n",
      "[step: 155] loss: 5.890360057492217e-07\n",
      "[step: 156] loss: 5.246620844445715e-07\n",
      "[step: 157] loss: 4.6228046812757384e-07\n",
      "[step: 158] loss: 4.0275858737004455e-07\n",
      "[step: 159] loss: 3.4831182915695535e-07\n",
      "[step: 160] loss: 3.001904360644403e-07\n",
      "[step: 161] loss: 2.5990678409471e-07\n",
      "[step: 162] loss: 2.275481563174253e-07\n",
      "[step: 163] loss: 2.024579828230344e-07\n",
      "[step: 164] loss: 1.8307146376628225e-07\n",
      "[step: 165] loss: 1.6763880239523132e-07\n",
      "[step: 166] loss: 1.541565097795683e-07\n",
      "[step: 167] loss: 1.4190142394454597e-07\n",
      "[step: 168] loss: 1.2984921227143786e-07\n",
      "[step: 169] loss: 1.180254614041587e-07\n",
      "[step: 170] loss: 1.0668772887356681e-07\n",
      "[step: 171] loss: 9.585910731857439e-08\n",
      "[step: 172] loss: 8.582460253592217e-08\n",
      "[step: 173] loss: 7.651259892327289e-08\n",
      "[step: 174] loss: 6.791713502707353e-08\n",
      "[step: 175] loss: 6.004395203262902e-08\n",
      "[step: 176] loss: 5.2783196480277184e-08\n",
      "[step: 177] loss: 4.638940609424935e-08\n",
      "[step: 178] loss: 4.079010906821168e-08\n",
      "[step: 179] loss: 3.6093950939175556e-08\n",
      "[step: 180] loss: 3.23403455126936e-08\n",
      "[step: 181] loss: 2.9505006438057535e-08\n",
      "[step: 182] loss: 2.7530434820732808e-08\n",
      "[step: 183] loss: 2.6200904557072136e-08\n",
      "[step: 184] loss: 2.5322037799924146e-08\n",
      "[step: 185] loss: 2.4624982941645612e-08\n",
      "[step: 186] loss: 2.399308485223628e-08\n",
      "[step: 187] loss: 2.313323044234039e-08\n",
      "[step: 188] loss: 2.199133142255505e-08\n",
      "[step: 189] loss: 2.0504183240177554e-08\n",
      "[step: 190] loss: 1.8613880214957135e-08\n",
      "[step: 191] loss: 1.637536861665012e-08\n",
      "[step: 192] loss: 1.3933872722304841e-08\n",
      "[step: 193] loss: 1.1417321310602802e-08\n",
      "[step: 194] loss: 9.085589169899322e-09\n",
      "[step: 195] loss: 7.017054493019259e-09\n",
      "[step: 196] loss: 5.319164220196626e-09\n",
      "[step: 197] loss: 4.039888423790217e-09\n",
      "[step: 198] loss: 3.160804062929401e-09\n",
      "[step: 199] loss: 2.6367212857536515e-09\n",
      "[step: 200] loss: 2.369098028864869e-09\n",
      "[step: 201] loss: 2.2988628778364273e-09\n",
      "[step: 202] loss: 2.401010945618509e-09\n",
      "[step: 203] loss: 2.569575663358137e-09\n",
      "[step: 204] loss: 2.7625486342941485e-09\n",
      "[step: 205] loss: 2.9278695024004264e-09\n",
      "[step: 206] loss: 3.003433057813254e-09\n",
      "[step: 207] loss: 2.9742925899967076e-09\n",
      "[step: 208] loss: 2.8326341272588706e-09\n",
      "[step: 209] loss: 2.626102224567717e-09\n",
      "[step: 210] loss: 2.3800845738719545e-09\n",
      "[step: 211] loss: 2.1156201235328354e-09\n",
      "[step: 212] loss: 1.8528636402947996e-09\n",
      "[step: 213] loss: 1.5739000103209833e-09\n",
      "[step: 214] loss: 1.3181993274713477e-09\n",
      "[step: 215] loss: 1.086674972228252e-09\n",
      "[step: 216] loss: 8.781388949330449e-10\n",
      "[step: 217] loss: 7.162463400156582e-10\n",
      "[step: 218] loss: 5.865860575049453e-10\n",
      "[step: 219] loss: 4.913847106990943e-10\n",
      "[step: 220] loss: 4.2339493022680585e-10\n",
      "[step: 221] loss: 3.6818467719079706e-10\n",
      "[step: 222] loss: 3.281546423483661e-10\n",
      "[step: 223] loss: 2.9961771952358163e-10\n",
      "[step: 224] loss: 2.8317423406143405e-10\n",
      "[step: 225] loss: 2.778964836025466e-10\n",
      "[step: 226] loss: 2.7399013613482737e-10\n",
      "[step: 227] loss: 2.754572681062939e-10\n",
      "[step: 228] loss: 2.753812455846827e-10\n",
      "[step: 229] loss: 2.6316840373574735e-10\n",
      "[step: 230] loss: 2.496953199759844e-10\n",
      "[step: 231] loss: 2.3134330562335492e-10\n",
      "[step: 232] loss: 2.0565105174341625e-10\n",
      "[step: 233] loss: 1.807128890973786e-10\n",
      "[step: 234] loss: 1.5689352594883132e-10\n",
      "[step: 235] loss: 1.3735584891705344e-10\n",
      "[step: 236] loss: 1.1514433156945003e-10\n",
      "[step: 237] loss: 1.0079184015188147e-10\n",
      "[step: 238] loss: 8.625617581303757e-11\n",
      "[step: 239] loss: 7.545526703456318e-11\n",
      "[step: 240] loss: 7.09801939446919e-11\n",
      "[step: 241] loss: 6.809936642371284e-11\n",
      "[step: 242] loss: 6.316430017916375e-11\n",
      "[step: 243] loss: 5.5996665637714216e-11\n",
      "[step: 244] loss: 5.231272359740302e-11\n",
      "[step: 245] loss: 4.73276175194659e-11\n",
      "[step: 246] loss: 4.087827992771409e-11\n",
      "[step: 247] loss: 3.596170583541891e-11\n",
      "[step: 248] loss: 3.114224850220282e-11\n",
      "[step: 249] loss: 2.666667754669394e-11\n",
      "[step: 250] loss: 2.2039526453854208e-11\n",
      "[step: 251] loss: 1.89208336498492e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 252] loss: 1.534939363112553e-11\n",
      "[step: 253] loss: 1.444841555425791e-11\n",
      "[step: 254] loss: 1.2601190924055317e-11\n",
      "[step: 255] loss: 1.2819123366980545e-11\n",
      "[step: 256] loss: 1.2563623752459563e-11\n",
      "[step: 257] loss: 1.2980818676899819e-11\n",
      "[step: 258] loss: 1.1865433115210156e-11\n",
      "[step: 259] loss: 1.1798287173625521e-11\n",
      "[step: 260] loss: 1.1488206219656405e-11\n",
      "[step: 261] loss: 9.839919677678832e-12\n",
      "[step: 262] loss: 9.740248538281371e-12\n",
      "[step: 263] loss: 8.64693386337656e-12\n",
      "[step: 264] loss: 7.613714346488276e-12\n",
      "[step: 265] loss: 6.490133517417229e-12\n",
      "[step: 266] loss: 5.458887248482869e-12\n",
      "[step: 267] loss: 4.7227651477077526e-12\n",
      "[step: 268] loss: 3.941016783748363e-12\n",
      "[step: 269] loss: 3.135271989945787e-12\n",
      "[step: 270] loss: 2.773801154043465e-12\n",
      "[step: 271] loss: 2.1402751532867414e-12\n",
      "[step: 272] loss: 1.7241103293305637e-12\n",
      "[step: 273] loss: 1.4914785986114287e-12\n",
      "[step: 274] loss: 1.3725576664819217e-12\n",
      "[step: 275] loss: 1.3035178405423897e-12\n",
      "[step: 276] loss: 1.2935075103040483e-12\n",
      "[step: 277] loss: 1.398845991297526e-12\n",
      "[step: 278] loss: 1.4518836436924754e-12\n",
      "[step: 279] loss: 1.3416468457039255e-12\n",
      "[step: 280] loss: 1.3514522617316671e-12\n",
      "[step: 281] loss: 1.1378725652683164e-12\n",
      "[step: 282] loss: 1.039195964523676e-12\n",
      "[step: 283] loss: 9.52147215238508e-13\n",
      "[step: 284] loss: 9.10669326406599e-13\n",
      "[step: 285] loss: 7.832690117164087e-13\n",
      "[step: 286] loss: 6.940453528732349e-13\n",
      "[step: 287] loss: 6.963190874592629e-13\n",
      "[step: 288] loss: 6.1043612656006e-13\n",
      "[step: 289] loss: 5.75619575875902e-13\n",
      "[step: 290] loss: 5.054179188883345e-13\n",
      "[step: 291] loss: 4.109868099056585e-13\n",
      "[step: 292] loss: 3.363937004386558e-13\n",
      "[step: 293] loss: 3.5918823796353416e-13\n",
      "[step: 294] loss: 3.4895643232640827e-13\n",
      "[step: 295] loss: 3.448352714485736e-13\n",
      "[step: 296] loss: 2.5746570674056724e-13\n",
      "[step: 297] loss: 2.6852296970171496e-13\n",
      "[step: 298] loss: 1.842176449362229e-13\n",
      "[step: 299] loss: 1.8092416399686367e-13\n",
      "[step: 300] loss: 1.832717327508379e-13\n",
      "[step: 301] loss: 1.7362333630272647e-13\n",
      "[step: 302] loss: 1.4408085998156989e-13\n",
      "[step: 303] loss: 1.3498590808493083e-13\n",
      "[step: 304] loss: 1.2589096974081893e-13\n",
      "[step: 305] loss: 1.0713263907729739e-13\n",
      "[step: 306] loss: 3.9631075791956086e-14\n",
      "[step: 307] loss: 5.554723483481086e-14\n",
      "[step: 308] loss: 6.222633356939983e-14\n",
      "[step: 309] loss: 6.277867358990899e-14\n",
      "[step: 310] loss: 5.821620854655485e-14\n",
      "[step: 311] loss: 6.047606534477501e-14\n",
      "[step: 312] loss: 6.216860657997855e-14\n",
      "[step: 313] loss: 3.259836790694276e-14\n",
      "[step: 314] loss: 2.7329804340296172e-14\n",
      "[step: 315] loss: 6.139977171441477e-14\n",
      "[step: 316] loss: 5.4583558674456065e-14\n",
      "[step: 317] loss: 2.0483614804334138e-14\n",
      "[step: 318] loss: 1.1388667989892763e-14\n",
      "[step: 319] loss: 9.257039274392694e-15\n",
      "[step: 320] loss: 1.3804512681613382e-14\n",
      "[step: 321] loss: 1.6078250655773146e-14\n",
      "[step: 322] loss: 1.3804512681613382e-14\n",
      "[step: 323] loss: 1.3804512681613382e-14\n",
      "[step: 324] loss: 1.3804512681613382e-14\n",
      "[step: 325] loss: 1.1530776401519512e-14\n",
      "[step: 326] loss: 1.1530776401519512e-14\n",
      "[step: 327] loss: 1.1388667989892763e-14\n",
      "[step: 328] loss: 1.1388667989892763e-14\n",
      "[step: 329] loss: 1.3662404269986633e-14\n",
      "[step: 330] loss: 1.3662404269986633e-14\n",
      "[step: 331] loss: 9.114930862765946e-15\n",
      "[step: 332] loss: 1.620925658304376e-16\n",
      "[step: 333] loss: 1.620925658304376e-16\n",
      "[step: 334] loss: 1.620925658304376e-16\n",
      "[step: 335] loss: 1.620925658304376e-16\n",
      "[step: 336] loss: 1.620925658304376e-16\n",
      "[step: 337] loss: 1.5598633178346094e-16\n",
      "[step: 338] loss: 1.5099033293720807e-16\n",
      "[step: 339] loss: 9.099942614764281e-15\n",
      "[step: 340] loss: 9.099942614764281e-15\n",
      "[step: 341] loss: 2.2787327156914864e-15\n",
      "[step: 342] loss: 4.9960038093365514e-18\n",
      "[step: 343] loss: 4.9960038093365514e-18\n",
      "[step: 344] loss: 1.4710455605678916e-16\n",
      "[step: 345] loss: 1.4710455605678916e-16\n",
      "[step: 346] loss: 1.4710455605678916e-16\n",
      "[step: 347] loss: 4.9960038093365514e-18\n",
      "[step: 348] loss: 4.9960038093365514e-18\n",
      "[step: 349] loss: 2.2787327156914864e-15\n",
      "[step: 350] loss: 4.9960038093365514e-18\n",
      "[step: 351] loss: 4.9960038093365514e-18\n",
      "[step: 352] loss: 1.4710455605678916e-16\n",
      "[step: 353] loss: 1.4710455605678916e-16\n",
      "[step: 354] loss: 1.4710455605678916e-16\n",
      "[step: 355] loss: 1.4710455605678916e-16\n",
      "[step: 356] loss: 4.9960038093365514e-18\n",
      "[step: 357] loss: 4.9960038093365514e-18\n",
      "[step: 358] loss: 4.9960038093365514e-18\n",
      "[step: 359] loss: 4.9960038093365514e-18\n",
      "[step: 360] loss: 4.9960038093365514e-18\n",
      "[step: 361] loss: 4.9960038093365514e-18\n",
      "[step: 362] loss: 4.9960038093365514e-18\n",
      "[step: 363] loss: 4.9960038093365514e-18\n",
      "[step: 364] loss: 4.9960038093365514e-18\n",
      "[step: 365] loss: 4.9960038093365514e-18\n",
      "[step: 366] loss: 4.9960038093365514e-18\n",
      "[step: 367] loss: 4.9960038093365514e-18\n",
      "[step: 368] loss: 4.9960038093365514e-18\n",
      "[step: 369] loss: 4.9960038093365514e-18\n",
      "[step: 370] loss: 4.9960038093365514e-18\n",
      "[step: 371] loss: 4.9960038093365514e-18\n",
      "[step: 372] loss: 4.9960038093365514e-18\n",
      "[step: 373] loss: 4.9960038093365514e-18\n",
      "[step: 374] loss: 4.9960038093365514e-18\n",
      "[step: 375] loss: 4.9960038093365514e-18\n",
      "[step: 376] loss: 4.9960038093365514e-18\n",
      "[step: 377] loss: 4.9960038093365514e-18\n",
      "[step: 378] loss: 4.9960038093365514e-18\n",
      "[step: 379] loss: 4.9960038093365514e-18\n",
      "[step: 380] loss: 4.9960038093365514e-18\n",
      "[step: 381] loss: 4.9960038093365514e-18\n",
      "[step: 382] loss: 4.9960038093365514e-18\n",
      "[step: 383] loss: 4.9960038093365514e-18\n",
      "[step: 384] loss: 4.9960038093365514e-18\n",
      "[step: 385] loss: 4.9960038093365514e-18\n",
      "[step: 386] loss: 4.9960038093365514e-18\n",
      "[step: 387] loss: 4.9960038093365514e-18\n",
      "[step: 388] loss: 4.9960038093365514e-18\n",
      "[step: 389] loss: 4.9960038093365514e-18\n",
      "[step: 390] loss: 4.9960038093365514e-18\n",
      "[step: 391] loss: 4.9960038093365514e-18\n",
      "[step: 392] loss: 4.9960038093365514e-18\n",
      "[step: 393] loss: 4.9960038093365514e-18\n",
      "[step: 394] loss: 4.9960038093365514e-18\n",
      "[step: 395] loss: 4.9960038093365514e-18\n",
      "[step: 396] loss: 4.9960038093365514e-18\n",
      "[step: 397] loss: 4.9960038093365514e-18\n",
      "[step: 398] loss: 4.9960038093365514e-18\n",
      "[step: 399] loss: 4.9960038093365514e-18\n",
      "[step: 400] loss: 4.9960038093365514e-18\n",
      "[step: 401] loss: 4.9960038093365514e-18\n",
      "[step: 402] loss: 4.9960038093365514e-18\n",
      "[step: 403] loss: 4.9960038093365514e-18\n",
      "[step: 404] loss: 4.9960038093365514e-18\n",
      "[step: 405] loss: 4.9960038093365514e-18\n",
      "[step: 406] loss: 4.9960038093365514e-18\n",
      "[step: 407] loss: 4.9960038093365514e-18\n",
      "[step: 408] loss: 4.9960038093365514e-18\n",
      "[step: 409] loss: 4.9960038093365514e-18\n",
      "[step: 410] loss: 4.9960038093365514e-18\n",
      "[step: 411] loss: 4.9960038093365514e-18\n",
      "[step: 412] loss: 4.9960038093365514e-18\n",
      "[step: 413] loss: 4.9960038093365514e-18\n",
      "[step: 414] loss: 4.9960038093365514e-18\n",
      "[step: 415] loss: 4.9960038093365514e-18\n",
      "[step: 416] loss: 4.9960038093365514e-18\n",
      "[step: 417] loss: 4.9960038093365514e-18\n",
      "[step: 418] loss: 4.9960038093365514e-18\n",
      "[step: 419] loss: 4.9960038093365514e-18\n",
      "[step: 420] loss: 4.9960038093365514e-18\n",
      "[step: 421] loss: 4.9960038093365514e-18\n",
      "[step: 422] loss: 4.9960038093365514e-18\n",
      "[step: 423] loss: 4.9960038093365514e-18\n",
      "[step: 424] loss: 4.9960038093365514e-18\n",
      "[step: 425] loss: 4.9960038093365514e-18\n",
      "[step: 426] loss: 4.9960038093365514e-18\n",
      "[step: 427] loss: 4.9960038093365514e-18\n",
      "[step: 428] loss: 4.9960038093365514e-18\n",
      "[step: 429] loss: 4.9960038093365514e-18\n",
      "[step: 430] loss: 4.9960038093365514e-18\n",
      "[step: 431] loss: 4.9960038093365514e-18\n",
      "[step: 432] loss: 4.9960038093365514e-18\n",
      "[step: 433] loss: 4.9960038093365514e-18\n",
      "[step: 434] loss: 4.9960038093365514e-18\n",
      "[step: 435] loss: 4.9960038093365514e-18\n",
      "[step: 436] loss: 4.9960038093365514e-18\n",
      "[step: 437] loss: 4.9960038093365514e-18\n",
      "[step: 438] loss: 4.9960038093365514e-18\n",
      "[step: 439] loss: 4.9960038093365514e-18\n",
      "[step: 440] loss: 4.9960038093365514e-18\n",
      "[step: 441] loss: 4.9960038093365514e-18\n",
      "[step: 442] loss: 4.9960038093365514e-18\n",
      "[step: 443] loss: 4.9960038093365514e-18\n",
      "[step: 444] loss: 4.9960038093365514e-18\n",
      "[step: 445] loss: 4.9960038093365514e-18\n",
      "[step: 446] loss: 4.9960038093365514e-18\n",
      "[step: 447] loss: 4.9960038093365514e-18\n",
      "[step: 448] loss: 4.9960038093365514e-18\n",
      "[step: 449] loss: 4.9960038093365514e-18\n",
      "[step: 450] loss: 4.9960038093365514e-18\n",
      "[step: 451] loss: 4.9960038093365514e-18\n",
      "[step: 452] loss: 4.9960038093365514e-18\n",
      "[step: 453] loss: 4.9960038093365514e-18\n",
      "[step: 454] loss: 4.9960038093365514e-18\n",
      "[step: 455] loss: 4.9960038093365514e-18\n",
      "[step: 456] loss: 4.9960038093365514e-18\n",
      "[step: 457] loss: 4.9960038093365514e-18\n",
      "[step: 458] loss: 4.9960038093365514e-18\n",
      "[step: 459] loss: 4.9960038093365514e-18\n",
      "[step: 460] loss: 4.9960038093365514e-18\n",
      "[step: 461] loss: 4.9960038093365514e-18\n",
      "[step: 462] loss: 4.9960038093365514e-18\n",
      "[step: 463] loss: 4.9960038093365514e-18\n",
      "[step: 464] loss: 4.9960038093365514e-18\n",
      "[step: 465] loss: 4.9960038093365514e-18\n",
      "[step: 466] loss: 4.9960038093365514e-18\n",
      "[step: 467] loss: 4.9960038093365514e-18\n",
      "[step: 468] loss: 4.9960038093365514e-18\n",
      "[step: 469] loss: 4.9960038093365514e-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 470] loss: 4.9960038093365514e-18\n",
      "[step: 471] loss: 4.9960038093365514e-18\n",
      "[step: 472] loss: 4.9960038093365514e-18\n",
      "[step: 473] loss: 4.9960038093365514e-18\n",
      "[step: 474] loss: 4.9960038093365514e-18\n",
      "[step: 475] loss: 4.9960038093365514e-18\n",
      "[step: 476] loss: 4.9960038093365514e-18\n",
      "[step: 477] loss: 4.9960038093365514e-18\n",
      "[step: 478] loss: 4.9960038093365514e-18\n",
      "[step: 479] loss: 4.9960038093365514e-18\n",
      "[step: 480] loss: 4.9960038093365514e-18\n",
      "[step: 481] loss: 4.9960038093365514e-18\n",
      "[step: 482] loss: 4.9960038093365514e-18\n",
      "[step: 483] loss: 4.9960038093365514e-18\n",
      "[step: 484] loss: 4.9960038093365514e-18\n",
      "[step: 485] loss: 4.9960038093365514e-18\n",
      "[step: 486] loss: 4.9960038093365514e-18\n",
      "[step: 487] loss: 4.9960038093365514e-18\n",
      "[step: 488] loss: 4.9960038093365514e-18\n",
      "[step: 489] loss: 4.9960038093365514e-18\n",
      "[step: 490] loss: 4.9960038093365514e-18\n",
      "[step: 491] loss: 4.9960038093365514e-18\n",
      "[step: 492] loss: 4.9960038093365514e-18\n",
      "[step: 493] loss: 4.9960038093365514e-18\n",
      "[step: 494] loss: 4.9960038093365514e-18\n",
      "[step: 495] loss: 4.9960038093365514e-18\n",
      "[step: 496] loss: 4.9960038093365514e-18\n",
      "[step: 497] loss: 4.9960038093365514e-18\n",
      "[step: 498] loss: 4.9960038093365514e-18\n",
      "[step: 499] loss: 4.9960038093365514e-18\n",
      "[step: 500] loss: 4.9960038093365514e-18\n",
      "[step: 501] loss: 4.9960038093365514e-18\n",
      "[step: 502] loss: 4.9960038093365514e-18\n",
      "[step: 503] loss: 4.9960038093365514e-18\n",
      "[step: 504] loss: 4.9960038093365514e-18\n",
      "[step: 505] loss: 4.9960038093365514e-18\n",
      "[step: 506] loss: 4.9960038093365514e-18\n",
      "[step: 507] loss: 4.9960038093365514e-18\n",
      "[step: 508] loss: 4.9960038093365514e-18\n",
      "[step: 509] loss: 4.9960038093365514e-18\n",
      "[step: 510] loss: 4.9960038093365514e-18\n",
      "[step: 511] loss: 4.9960038093365514e-18\n",
      "[step: 512] loss: 4.9960038093365514e-18\n",
      "[step: 513] loss: 4.9960038093365514e-18\n",
      "[step: 514] loss: 4.9960038093365514e-18\n",
      "[step: 515] loss: 4.9960038093365514e-18\n",
      "[step: 516] loss: 4.9960038093365514e-18\n",
      "[step: 517] loss: 4.9960038093365514e-18\n",
      "[step: 518] loss: 4.9960038093365514e-18\n",
      "[step: 519] loss: 4.9960038093365514e-18\n",
      "[step: 520] loss: 4.9960038093365514e-18\n",
      "[step: 521] loss: 4.9960038093365514e-18\n",
      "[step: 522] loss: 4.9960038093365514e-18\n",
      "[step: 523] loss: 4.9960038093365514e-18\n",
      "[step: 524] loss: 4.9960038093365514e-18\n",
      "[step: 525] loss: 4.9960038093365514e-18\n",
      "[step: 526] loss: 4.9960038093365514e-18\n",
      "[step: 527] loss: 4.9960038093365514e-18\n",
      "[step: 528] loss: 4.9960038093365514e-18\n",
      "[step: 529] loss: 4.9960038093365514e-18\n",
      "[step: 530] loss: 4.9960038093365514e-18\n",
      "[step: 531] loss: 4.9960038093365514e-18\n",
      "[step: 532] loss: 4.9960038093365514e-18\n",
      "[step: 533] loss: 4.9960038093365514e-18\n",
      "[step: 534] loss: 4.9960038093365514e-18\n",
      "[step: 535] loss: 4.9960038093365514e-18\n",
      "[step: 536] loss: 4.9960038093365514e-18\n",
      "[step: 537] loss: 4.9960038093365514e-18\n",
      "[step: 538] loss: 4.9960038093365514e-18\n",
      "[step: 539] loss: 4.9960038093365514e-18\n",
      "[step: 540] loss: 4.9960038093365514e-18\n",
      "[step: 541] loss: 4.9960038093365514e-18\n",
      "[step: 542] loss: 4.9960038093365514e-18\n",
      "[step: 543] loss: 4.9960038093365514e-18\n",
      "[step: 544] loss: 4.9960038093365514e-18\n",
      "[step: 545] loss: 4.9960038093365514e-18\n",
      "[step: 546] loss: 4.9960038093365514e-18\n",
      "[step: 547] loss: 4.9960038093365514e-18\n",
      "[step: 548] loss: 4.9960038093365514e-18\n",
      "[step: 549] loss: 4.9960038093365514e-18\n",
      "[step: 550] loss: 4.9960038093365514e-18\n",
      "[step: 551] loss: 4.9960038093365514e-18\n",
      "[step: 552] loss: 4.9960038093365514e-18\n",
      "[step: 553] loss: 4.9960038093365514e-18\n",
      "[step: 554] loss: 4.9960038093365514e-18\n",
      "[step: 555] loss: 4.9960038093365514e-18\n",
      "[step: 556] loss: 4.9960038093365514e-18\n",
      "[step: 557] loss: 4.9960038093365514e-18\n",
      "[step: 558] loss: 4.9960038093365514e-18\n",
      "[step: 559] loss: 4.9960038093365514e-18\n",
      "[step: 560] loss: 4.9960038093365514e-18\n",
      "[step: 561] loss: 4.9960038093365514e-18\n",
      "[step: 562] loss: 4.9960038093365514e-18\n",
      "[step: 563] loss: 4.9960038093365514e-18\n",
      "[step: 564] loss: 4.9960038093365514e-18\n",
      "[step: 565] loss: 4.9960038093365514e-18\n",
      "[step: 566] loss: 4.9960038093365514e-18\n",
      "[step: 567] loss: 4.9960038093365514e-18\n",
      "[step: 568] loss: 4.9960038093365514e-18\n",
      "[step: 569] loss: 4.9960038093365514e-18\n",
      "[step: 570] loss: 4.9960038093365514e-18\n",
      "[step: 571] loss: 4.9960038093365514e-18\n",
      "[step: 572] loss: 4.9960038093365514e-18\n",
      "[step: 573] loss: 4.9960038093365514e-18\n",
      "[step: 574] loss: 4.9960038093365514e-18\n",
      "[step: 575] loss: 4.9960038093365514e-18\n",
      "[step: 576] loss: 4.9960038093365514e-18\n",
      "[step: 577] loss: 4.9960038093365514e-18\n",
      "[step: 578] loss: 4.9960038093365514e-18\n",
      "[step: 579] loss: 4.9960038093365514e-18\n",
      "[step: 580] loss: 4.9960038093365514e-18\n",
      "[step: 581] loss: 4.9960038093365514e-18\n",
      "[step: 582] loss: 4.9960038093365514e-18\n",
      "[step: 583] loss: 4.9960038093365514e-18\n",
      "[step: 584] loss: 4.9960038093365514e-18\n",
      "[step: 585] loss: 4.9960038093365514e-18\n",
      "[step: 586] loss: 4.9960038093365514e-18\n",
      "[step: 587] loss: 4.9960038093365514e-18\n",
      "[step: 588] loss: 4.9960038093365514e-18\n",
      "[step: 589] loss: 4.9960038093365514e-18\n",
      "[step: 590] loss: 4.9960038093365514e-18\n",
      "[step: 591] loss: 4.9960038093365514e-18\n",
      "[step: 592] loss: 4.9960038093365514e-18\n",
      "[step: 593] loss: 4.9960038093365514e-18\n",
      "[step: 594] loss: 4.9960038093365514e-18\n",
      "[step: 595] loss: 4.9960038093365514e-18\n",
      "[step: 596] loss: 4.9960038093365514e-18\n",
      "[step: 597] loss: 4.9960038093365514e-18\n",
      "[step: 598] loss: 4.9960038093365514e-18\n",
      "[step: 599] loss: 4.9960038093365514e-18\n",
      "[step: 600] loss: 4.9960038093365514e-18\n",
      "[step: 601] loss: 4.9960038093365514e-18\n",
      "[step: 602] loss: 4.9960038093365514e-18\n",
      "[step: 603] loss: 4.9960038093365514e-18\n",
      "[step: 604] loss: 4.9960038093365514e-18\n",
      "[step: 605] loss: 4.9960038093365514e-18\n",
      "[step: 606] loss: 4.9960038093365514e-18\n",
      "[step: 607] loss: 4.9960038093365514e-18\n",
      "[step: 608] loss: 4.9960038093365514e-18\n",
      "[step: 609] loss: 4.9960038093365514e-18\n",
      "[step: 610] loss: 4.9960038093365514e-18\n",
      "[step: 611] loss: 4.9960038093365514e-18\n",
      "[step: 612] loss: 4.9960038093365514e-18\n",
      "[step: 613] loss: 4.9960038093365514e-18\n",
      "[step: 614] loss: 4.9960038093365514e-18\n",
      "[step: 615] loss: 4.9960038093365514e-18\n",
      "[step: 616] loss: 4.9960038093365514e-18\n",
      "[step: 617] loss: 4.9960038093365514e-18\n",
      "[step: 618] loss: 4.9960038093365514e-18\n",
      "[step: 619] loss: 4.9960038093365514e-18\n",
      "[step: 620] loss: 4.9960038093365514e-18\n",
      "[step: 621] loss: 4.9960038093365514e-18\n",
      "[step: 622] loss: 4.9960038093365514e-18\n",
      "[step: 623] loss: 4.9960038093365514e-18\n",
      "[step: 624] loss: 4.9960038093365514e-18\n",
      "[step: 625] loss: 4.9960038093365514e-18\n",
      "[step: 626] loss: 4.9960038093365514e-18\n",
      "[step: 627] loss: 4.9960038093365514e-18\n",
      "[step: 628] loss: 9.381384875719928e-17\n",
      "[step: 629] loss: 9.381384875719928e-17\n",
      "[step: 630] loss: 9.381384875719928e-17\n",
      "[step: 631] loss: 9.381384875719928e-17\n",
      "[step: 632] loss: 9.381384875719928e-17\n",
      "[step: 633] loss: 8.881783998477905e-18\n",
      "[step: 634] loss: 8.881783998477905e-18\n",
      "[step: 635] loss: 8.881783998477905e-18\n",
      "[step: 636] loss: 8.881783998477905e-18\n",
      "[step: 637] loss: 8.881783998477905e-18\n",
      "[step: 638] loss: 8.881783998477905e-18\n",
      "[step: 639] loss: 8.881783998477905e-18\n",
      "[step: 640] loss: 8.881783998477905e-18\n",
      "[step: 641] loss: 8.881783998477905e-18\n",
      "[step: 642] loss: 8.881783998477905e-18\n",
      "[step: 643] loss: 8.881783998477905e-18\n",
      "[step: 644] loss: 8.881783998477905e-18\n",
      "[step: 645] loss: 8.881783998477905e-18\n",
      "[step: 646] loss: 8.881783998477905e-18\n",
      "[step: 647] loss: 8.881783998477905e-18\n",
      "[step: 648] loss: 8.881783998477905e-18\n",
      "[step: 649] loss: 8.881783998477905e-18\n",
      "[step: 650] loss: 8.881783998477905e-18\n",
      "[step: 651] loss: 8.881783998477905e-18\n",
      "[step: 652] loss: 7.993606094938482e-17\n",
      "[step: 653] loss: 7.993606094938482e-17\n",
      "[step: 654] loss: 7.993606094938482e-17\n",
      "[step: 655] loss: 7.993606094938482e-17\n",
      "[step: 656] loss: 7.993606094938482e-17\n",
      "[step: 657] loss: 1.3877787807814457e-17\n",
      "[step: 658] loss: 1.3877787807814457e-17\n",
      "[step: 659] loss: 1.3877787807814457e-17\n",
      "[step: 660] loss: 1.3877787807814457e-17\n",
      "[step: 661] loss: 1.3877787807814457e-17\n",
      "[step: 662] loss: 1.3877787807814457e-17\n",
      "[step: 663] loss: 1.3877787807814457e-17\n",
      "[step: 664] loss: 1.3877787807814457e-17\n",
      "[step: 665] loss: 1.3877787807814457e-17\n",
      "[step: 666] loss: 1.3877787807814457e-17\n",
      "[step: 667] loss: 1.3877787807814457e-17\n",
      "[step: 668] loss: 1.3877787807814457e-17\n",
      "[step: 669] loss: 1.3877787807814457e-17\n",
      "[step: 670] loss: 6.716849510740434e-17\n",
      "[step: 671] loss: 6.716849510740434e-17\n",
      "[step: 672] loss: 6.716849510740434e-17\n",
      "[step: 673] loss: 6.716849510740434e-17\n",
      "[step: 674] loss: 6.716849510740434e-17\n",
      "[step: 675] loss: 6.716849510740434e-17\n",
      "[step: 676] loss: 1.9984015237346206e-17\n",
      "[step: 677] loss: 1.9984015237346206e-17\n",
      "[step: 678] loss: 1.9984015237346206e-17\n",
      "[step: 679] loss: 1.9984015237346206e-17\n",
      "[step: 680] loss: 1.9984015237346206e-17\n",
      "[step: 681] loss: 1.9984015237346206e-17\n",
      "[step: 682] loss: 1.9984015237346206e-17\n",
      "[step: 683] loss: 1.9984015237346206e-17\n",
      "[step: 684] loss: 1.9984015237346206e-17\n",
      "[step: 685] loss: 1.9984015237346206e-17\n",
      "[step: 686] loss: 1.9984015237346206e-17\n",
      "[step: 687] loss: 6.716849510740434e-17\n",
      "[step: 688] loss: 6.716849510740434e-17\n",
      "[step: 689] loss: 6.716849510740434e-17\n",
      "[step: 690] loss: 6.716849510740434e-17\n",
      "[step: 691] loss: 6.716849510740434e-17\n",
      "[step: 692] loss: 6.716849510740434e-17\n",
      "[step: 693] loss: 1.9984015237346206e-17\n",
      "[step: 694] loss: 1.9984015237346206e-17\n",
      "[step: 695] loss: 1.9984015237346206e-17\n",
      "[step: 696] loss: 1.9984015237346206e-17\n",
      "[step: 697] loss: 1.9984015237346206e-17\n",
      "[step: 698] loss: 1.9984015237346206e-17\n",
      "[step: 699] loss: 1.9984015237346206e-17\n",
      "[step: 700] loss: 1.9984015237346206e-17\n",
      "[step: 701] loss: 1.9984015237346206e-17\n",
      "[step: 702] loss: 1.9984015237346206e-17\n",
      "[step: 703] loss: 1.9984015237346206e-17\n",
      "[step: 704] loss: 6.716849510740434e-17\n",
      "[step: 705] loss: 6.716849510740434e-17\n",
      "[step: 706] loss: 6.716849510740434e-17\n",
      "[step: 707] loss: 6.716849510740434e-17\n",
      "[step: 708] loss: 6.716849510740434e-17\n",
      "[step: 709] loss: 6.716849510740434e-17\n",
      "[step: 710] loss: 1.9984015237346206e-17\n",
      "[step: 711] loss: 1.9984015237346206e-17\n",
      "[step: 712] loss: 1.9984015237346206e-17\n",
      "[step: 713] loss: 1.9984015237346206e-17\n",
      "[step: 714] loss: 1.9984015237346206e-17\n",
      "[step: 715] loss: 1.9984015237346206e-17\n",
      "[step: 716] loss: 1.9984015237346206e-17\n",
      "[step: 717] loss: 1.9984015237346206e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 718] loss: 1.9984015237346206e-17\n",
      "[step: 719] loss: 1.9984015237346206e-17\n",
      "[step: 720] loss: 1.9984015237346206e-17\n",
      "[step: 721] loss: 6.716849510740434e-17\n",
      "[step: 722] loss: 6.716849510740434e-17\n",
      "[step: 723] loss: 6.716849510740434e-17\n",
      "[step: 724] loss: 6.716849510740434e-17\n",
      "[step: 725] loss: 6.716849510740434e-17\n",
      "[step: 726] loss: 1.3877787807814457e-17\n",
      "[step: 727] loss: 1.3877787807814457e-17\n",
      "[step: 728] loss: 1.3877787807814457e-17\n",
      "[step: 729] loss: 1.3877787807814457e-17\n",
      "[step: 730] loss: 1.3877787807814457e-17\n",
      "[step: 731] loss: 1.3877787807814457e-17\n",
      "[step: 732] loss: 1.3877787807814457e-17\n",
      "[step: 733] loss: 1.3877787807814457e-17\n",
      "[step: 734] loss: 1.3877787807814457e-17\n",
      "[step: 735] loss: 1.3877787807814457e-17\n",
      "[step: 736] loss: 1.3877787807814457e-17\n",
      "[step: 737] loss: 1.3877787807814457e-17\n",
      "[step: 738] loss: 7.993606094938482e-17\n",
      "[step: 739] loss: 7.993606094938482e-17\n",
      "[step: 740] loss: 7.993606094938482e-17\n",
      "[step: 741] loss: 7.993606094938482e-17\n",
      "[step: 742] loss: 7.993606094938482e-17\n",
      "[step: 743] loss: 1.3877787807814457e-17\n",
      "[step: 744] loss: 1.3877787807814457e-17\n",
      "[step: 745] loss: 1.3877787807814457e-17\n",
      "[step: 746] loss: 1.3877787807814457e-17\n",
      "[step: 747] loss: 1.3877787807814457e-17\n",
      "[step: 748] loss: 1.3877787807814457e-17\n",
      "[step: 749] loss: 1.3877787807814457e-17\n",
      "[step: 750] loss: 1.3877787807814457e-17\n",
      "[step: 751] loss: 1.3877787807814457e-17\n",
      "[step: 752] loss: 1.3877787807814457e-17\n",
      "[step: 753] loss: 1.3877787807814457e-17\n",
      "[step: 754] loss: 1.3877787807814457e-17\n",
      "[step: 755] loss: 6.716849510740434e-17\n",
      "[step: 756] loss: 6.716849510740434e-17\n",
      "[step: 757] loss: 6.716849510740434e-17\n",
      "[step: 758] loss: 6.716849510740434e-17\n",
      "[step: 759] loss: 6.716849510740434e-17\n",
      "[step: 760] loss: 1.9984015237346206e-17\n",
      "[step: 761] loss: 1.9984015237346206e-17\n",
      "[step: 762] loss: 1.9984015237346206e-17\n",
      "[step: 763] loss: 1.9984015237346206e-17\n",
      "[step: 764] loss: 1.9984015237346206e-17\n",
      "[step: 765] loss: 1.9984015237346206e-17\n",
      "[step: 766] loss: 1.9984015237346206e-17\n",
      "[step: 767] loss: 1.9984015237346206e-17\n",
      "[step: 768] loss: 1.9984015237346206e-17\n",
      "[step: 769] loss: 1.9984015237346206e-17\n",
      "[step: 770] loss: 6.716849510740434e-17\n",
      "[step: 771] loss: 6.716849510740434e-17\n",
      "[step: 772] loss: 6.716849510740434e-17\n",
      "[step: 773] loss: 6.716849510740434e-17\n",
      "[step: 774] loss: 6.716849510740434e-17\n",
      "[step: 775] loss: 1.9984015237346206e-17\n",
      "[step: 776] loss: 1.9984015237346206e-17\n",
      "[step: 777] loss: 1.9984015237346206e-17\n",
      "[step: 778] loss: 1.9984015237346206e-17\n",
      "[step: 779] loss: 1.9984015237346206e-17\n",
      "[step: 780] loss: 1.9984015237346206e-17\n",
      "[step: 781] loss: 1.9984015237346206e-17\n",
      "[step: 782] loss: 1.9984015237346206e-17\n",
      "[step: 783] loss: 1.9984015237346206e-17\n",
      "[step: 784] loss: 5.551115123125783e-17\n",
      "[step: 785] loss: 5.551115123125783e-17\n",
      "[step: 786] loss: 5.551115123125783e-17\n",
      "[step: 787] loss: 5.551115123125783e-17\n",
      "[step: 788] loss: 5.551115123125783e-17\n",
      "[step: 789] loss: 1.9984015237346206e-17\n",
      "[step: 790] loss: 1.9984015237346206e-17\n",
      "[step: 791] loss: 1.9984015237346206e-17\n",
      "[step: 792] loss: 1.9984015237346206e-17\n",
      "[step: 793] loss: 1.9984015237346206e-17\n",
      "[step: 794] loss: 1.9984015237346206e-17\n",
      "[step: 795] loss: 1.9984015237346206e-17\n",
      "[step: 796] loss: 1.9984015237346206e-17\n",
      "[step: 797] loss: 1.9984015237346206e-17\n",
      "[step: 798] loss: 6.716849510740434e-17\n",
      "[step: 799] loss: 6.716849510740434e-17\n",
      "[step: 800] loss: 6.716849510740434e-17\n",
      "[step: 801] loss: 6.716849510740434e-17\n",
      "[step: 802] loss: 6.716849510740434e-17\n",
      "[step: 803] loss: 1.9984015237346206e-17\n",
      "[step: 804] loss: 1.9984015237346206e-17\n",
      "[step: 805] loss: 1.9984015237346206e-17\n",
      "[step: 806] loss: 1.9984015237346206e-17\n",
      "[step: 807] loss: 1.9984015237346206e-17\n",
      "[step: 808] loss: 1.9984015237346206e-17\n",
      "[step: 809] loss: 1.9984015237346206e-17\n",
      "[step: 810] loss: 1.9984015237346206e-17\n",
      "[step: 811] loss: 1.9984015237346206e-17\n",
      "[step: 812] loss: 5.551115123125783e-17\n",
      "[step: 813] loss: 5.551115123125783e-17\n",
      "[step: 814] loss: 5.551115123125783e-17\n",
      "[step: 815] loss: 5.551115123125783e-17\n",
      "[step: 816] loss: 5.551115123125783e-17\n",
      "[step: 817] loss: 2.7200464632711927e-17\n",
      "[step: 818] loss: 2.7200464632711927e-17\n",
      "[step: 819] loss: 2.7200464632711927e-17\n",
      "[step: 820] loss: 2.7200464632711927e-17\n",
      "[step: 821] loss: 2.7200464632711927e-17\n",
      "[step: 822] loss: 2.7200464632711927e-17\n",
      "[step: 823] loss: 2.7200464632711927e-17\n",
      "[step: 824] loss: 2.7200464632711927e-17\n",
      "[step: 825] loss: 5.551115123125783e-17\n",
      "[step: 826] loss: 5.551115123125783e-17\n",
      "[step: 827] loss: 5.551115123125783e-17\n",
      "[step: 828] loss: 5.551115123125783e-17\n",
      "[step: 829] loss: 5.551115123125783e-17\n",
      "[step: 830] loss: 5.551115123125783e-17\n",
      "[step: 831] loss: 2.7200464632711927e-17\n",
      "[step: 832] loss: 2.7200464632711927e-17\n",
      "[step: 833] loss: 2.7200464632711927e-17\n",
      "[step: 834] loss: 2.7200464632711927e-17\n",
      "[step: 835] loss: 2.7200464632711927e-17\n",
      "[step: 836] loss: 2.7200464632711927e-17\n",
      "[step: 837] loss: 2.7200464632711927e-17\n",
      "[step: 838] loss: 2.7200464632711927e-17\n",
      "[step: 839] loss: 5.551115123125783e-17\n",
      "[step: 840] loss: 5.551115123125783e-17\n",
      "[step: 841] loss: 5.551115123125783e-17\n",
      "[step: 842] loss: 5.551115123125783e-17\n",
      "[step: 843] loss: 5.551115123125783e-17\n",
      "[step: 844] loss: 2.7200464632711927e-17\n",
      "[step: 845] loss: 2.7200464632711927e-17\n",
      "[step: 846] loss: 2.7200464632711927e-17\n",
      "[step: 847] loss: 2.7200464632711927e-17\n",
      "[step: 848] loss: 2.7200464632711927e-17\n",
      "[step: 849] loss: 2.7200464632711927e-17\n",
      "[step: 850] loss: 2.7200464632711927e-17\n",
      "[step: 851] loss: 4.496403262966774e-17\n",
      "[step: 852] loss: 4.496403262966774e-17\n",
      "[step: 853] loss: 4.496403262966774e-17\n",
      "[step: 854] loss: 4.496403262966774e-17\n",
      "[step: 855] loss: 4.496403262966774e-17\n",
      "[step: 856] loss: 4.496403262966774e-17\n",
      "[step: 857] loss: 3.552713599391162e-17\n",
      "[step: 858] loss: 3.552713599391162e-17\n",
      "[step: 859] loss: 3.552713599391162e-17\n",
      "[step: 860] loss: 3.552713599391162e-17\n",
      "[step: 861] loss: 3.552713599391162e-17\n",
      "[step: 862] loss: 3.552713599391162e-17\n",
      "[step: 863] loss: 3.552713599391162e-17\n",
      "[step: 864] loss: 4.496403262966774e-17\n",
      "[step: 865] loss: 4.496403262966774e-17\n",
      "[step: 866] loss: 4.496403262966774e-17\n",
      "[step: 867] loss: 4.496403262966774e-17\n",
      "[step: 868] loss: 4.496403262966774e-17\n",
      "[step: 869] loss: 4.496403262966774e-17\n",
      "[step: 870] loss: 3.552713599391162e-17\n",
      "[step: 871] loss: 3.552713599391162e-17\n",
      "[step: 872] loss: 3.552713599391162e-17\n",
      "[step: 873] loss: 3.552713599391162e-17\n",
      "[step: 874] loss: 3.552713599391162e-17\n",
      "[step: 875] loss: 3.552713599391162e-17\n",
      "[step: 876] loss: 3.552713599391162e-17\n",
      "[step: 877] loss: 4.496403262966774e-17\n",
      "[step: 878] loss: 4.496403262966774e-17\n",
      "[step: 879] loss: 4.496403262966774e-17\n",
      "[step: 880] loss: 4.496403262966774e-17\n",
      "[step: 881] loss: 4.496403262966774e-17\n",
      "[step: 882] loss: 4.496403262966774e-17\n",
      "[step: 883] loss: 3.552713599391162e-17\n",
      "[step: 884] loss: 3.552713599391162e-17\n",
      "[step: 885] loss: 3.552713599391162e-17\n",
      "[step: 886] loss: 3.552713599391162e-17\n",
      "[step: 887] loss: 3.552713599391162e-17\n",
      "[step: 888] loss: 3.552713599391162e-17\n",
      "[step: 889] loss: 3.552713599391162e-17\n",
      "[step: 890] loss: 4.496403262966774e-17\n",
      "[step: 891] loss: 4.496403262966774e-17\n",
      "[step: 892] loss: 4.496403262966774e-17\n",
      "[step: 893] loss: 4.496403262966774e-17\n",
      "[step: 894] loss: 4.496403262966774e-17\n",
      "[step: 895] loss: 4.496403262966774e-17\n",
      "[step: 896] loss: 3.552713599391162e-17\n",
      "[step: 897] loss: 3.552713599391162e-17\n",
      "[step: 898] loss: 3.552713599391162e-17\n",
      "[step: 899] loss: 3.552713599391162e-17\n",
      "[step: 900] loss: 3.552713599391162e-17\n",
      "[step: 901] loss: 3.552713599391162e-17\n",
      "[step: 902] loss: 3.552713599391162e-17\n",
      "[step: 903] loss: 3.552713599391162e-17\n",
      "[step: 904] loss: 3.552713599391162e-17\n",
      "[step: 905] loss: 3.552713599391162e-17\n",
      "[step: 906] loss: 3.552713599391162e-17\n",
      "[step: 907] loss: 3.552713599391162e-17\n",
      "[step: 908] loss: 4.496403262966774e-17\n",
      "[step: 909] loss: 4.496403262966774e-17\n",
      "[step: 910] loss: 4.496403262966774e-17\n",
      "[step: 911] loss: 4.496403262966774e-17\n",
      "[step: 912] loss: 4.496403262966774e-17\n",
      "[step: 913] loss: 4.496403262966774e-17\n",
      "[step: 914] loss: 3.552713599391162e-17\n",
      "[step: 915] loss: 3.552713599391162e-17\n",
      "[step: 916] loss: 3.552713599391162e-17\n",
      "[step: 917] loss: 3.552713599391162e-17\n",
      "[step: 918] loss: 3.552713599391162e-17\n",
      "[step: 919] loss: 3.552713599391162e-17\n",
      "[step: 920] loss: 3.552713599391162e-17\n",
      "[step: 921] loss: 4.496403262966774e-17\n",
      "[step: 922] loss: 4.496403262966774e-17\n",
      "[step: 923] loss: 4.496403262966774e-17\n",
      "[step: 924] loss: 4.496403262966774e-17\n",
      "[step: 925] loss: 4.496403262966774e-17\n",
      "[step: 926] loss: 4.496403262966774e-17\n",
      "[step: 927] loss: 3.552713599391162e-17\n",
      "[step: 928] loss: 3.552713599391162e-17\n",
      "[step: 929] loss: 3.552713599391162e-17\n",
      "[step: 930] loss: 3.552713599391162e-17\n",
      "[step: 931] loss: 3.552713599391162e-17\n",
      "[step: 932] loss: 3.552713599391162e-17\n",
      "[step: 933] loss: 3.552713599391162e-17\n",
      "[step: 934] loss: 3.552713599391162e-17\n",
      "[step: 935] loss: 3.552713599391162e-17\n",
      "[step: 936] loss: 3.552713599391162e-17\n",
      "[step: 937] loss: 3.552713599391162e-17\n",
      "[step: 938] loss: 3.552713599391162e-17\n",
      "[step: 939] loss: 4.496403262966774e-17\n",
      "[step: 940] loss: 4.496403262966774e-17\n",
      "[step: 941] loss: 4.496403262966774e-17\n",
      "[step: 942] loss: 4.496403262966774e-17\n",
      "[step: 943] loss: 4.496403262966774e-17\n",
      "[step: 944] loss: 4.496403262966774e-17\n",
      "[step: 945] loss: 3.552713599391162e-17\n",
      "[step: 946] loss: 3.552713599391162e-17\n",
      "[step: 947] loss: 3.552713599391162e-17\n",
      "[step: 948] loss: 3.552713599391162e-17\n",
      "[step: 949] loss: 3.552713599391162e-17\n",
      "[step: 950] loss: 3.552713599391162e-17\n",
      "[step: 951] loss: 3.552713599391162e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 952] loss: 3.552713599391162e-17\n",
      "[step: 953] loss: 3.552713599391162e-17\n",
      "[step: 954] loss: 3.552713599391162e-17\n",
      "[step: 955] loss: 3.552713599391162e-17\n",
      "[step: 956] loss: 3.552713599391162e-17\n",
      "[step: 957] loss: 4.496403262966774e-17\n",
      "[step: 958] loss: 4.496403262966774e-17\n",
      "[step: 959] loss: 4.496403262966774e-17\n",
      "[step: 960] loss: 4.496403262966774e-17\n",
      "[step: 961] loss: 4.496403262966774e-17\n",
      "[step: 962] loss: 4.496403262966774e-17\n",
      "[step: 963] loss: 3.552713599391162e-17\n",
      "[step: 964] loss: 3.552713599391162e-17\n",
      "[step: 965] loss: 3.552713599391162e-17\n",
      "[step: 966] loss: 3.552713599391162e-17\n",
      "[step: 967] loss: 3.552713599391162e-17\n",
      "[step: 968] loss: 3.552713599391162e-17\n",
      "[step: 969] loss: 3.552713599391162e-17\n",
      "[step: 970] loss: 3.552713599391162e-17\n",
      "[step: 971] loss: 3.552713599391162e-17\n",
      "[step: 972] loss: 3.552713599391162e-17\n",
      "[step: 973] loss: 3.552713599391162e-17\n",
      "[step: 974] loss: 3.552713599391162e-17\n",
      "[step: 975] loss: 4.496403262966774e-17\n",
      "[step: 976] loss: 4.496403262966774e-17\n",
      "[step: 977] loss: 4.496403262966774e-17\n",
      "[step: 978] loss: 4.496403262966774e-17\n",
      "[step: 979] loss: 4.496403262966774e-17\n",
      "[step: 980] loss: 2.7200464632711927e-17\n",
      "[step: 981] loss: 2.7200464632711927e-17\n",
      "[step: 982] loss: 2.7200464632711927e-17\n",
      "[step: 983] loss: 2.7200464632711927e-17\n",
      "[step: 984] loss: 2.7200464632711927e-17\n",
      "[step: 985] loss: 2.7200464632711927e-17\n",
      "[step: 986] loss: 2.7200464632711927e-17\n",
      "[step: 987] loss: 5.551115123125783e-17\n",
      "[step: 988] loss: 5.551115123125783e-17\n",
      "[step: 989] loss: 5.551115123125783e-17\n",
      "[step: 990] loss: 5.551115123125783e-17\n",
      "[step: 991] loss: 5.551115123125783e-17\n",
      "[step: 992] loss: 2.7200464632711927e-17\n",
      "[step: 993] loss: 2.7200464632711927e-17\n",
      "[step: 994] loss: 2.7200464632711927e-17\n",
      "[step: 995] loss: 2.7200464632711927e-17\n",
      "[step: 996] loss: 2.7200464632711927e-17\n",
      "[step: 997] loss: 2.7200464632711927e-17\n",
      "[step: 998] loss: 2.7200464632711927e-17\n",
      "[step: 999] loss: 5.551115123125783e-17\n",
      "[step: 1000] loss: 5.551115123125783e-17\n",
      "[step: 1001] loss: 5.551115123125783e-17\n",
      "[step: 1002] loss: 5.551115123125783e-17\n",
      "[step: 1003] loss: 5.551115123125783e-17\n",
      "[step: 1004] loss: 2.7200464632711927e-17\n",
      "[step: 1005] loss: 2.7200464632711927e-17\n",
      "[step: 1006] loss: 2.7200464632711927e-17\n",
      "[step: 1007] loss: 2.7200464632711927e-17\n",
      "[step: 1008] loss: 2.7200464632711927e-17\n",
      "[step: 1009] loss: 2.7200464632711927e-17\n",
      "[step: 1010] loss: 2.7200464632711927e-17\n",
      "[step: 1011] loss: 5.551115123125783e-17\n",
      "[step: 1012] loss: 5.551115123125783e-17\n",
      "[step: 1013] loss: 5.551115123125783e-17\n",
      "[step: 1014] loss: 5.551115123125783e-17\n",
      "[step: 1015] loss: 5.551115123125783e-17\n",
      "[step: 1016] loss: 2.7200464632711927e-17\n",
      "[step: 1017] loss: 2.7200464632711927e-17\n",
      "[step: 1018] loss: 2.7200464632711927e-17\n",
      "[step: 1019] loss: 2.7200464632711927e-17\n",
      "[step: 1020] loss: 2.7200464632711927e-17\n",
      "[step: 1021] loss: 2.7200464632711927e-17\n",
      "[step: 1022] loss: 2.7200464632711927e-17\n",
      "[step: 1023] loss: 5.551115123125783e-17\n",
      "[step: 1024] loss: 5.551115123125783e-17\n",
      "[step: 1025] loss: 5.551115123125783e-17\n",
      "[step: 1026] loss: 5.551115123125783e-17\n",
      "[step: 1027] loss: 5.551115123125783e-17\n",
      "[step: 1028] loss: 2.7200464632711927e-17\n",
      "[step: 1029] loss: 2.7200464632711927e-17\n",
      "[step: 1030] loss: 2.7200464632711927e-17\n",
      "[step: 1031] loss: 2.7200464632711927e-17\n",
      "[step: 1032] loss: 2.7200464632711927e-17\n",
      "[step: 1033] loss: 2.7200464632711927e-17\n",
      "[step: 1034] loss: 2.7200464632711927e-17\n",
      "[step: 1035] loss: 5.551115123125783e-17\n",
      "[step: 1036] loss: 5.551115123125783e-17\n",
      "[step: 1037] loss: 5.551115123125783e-17\n",
      "[step: 1038] loss: 5.551115123125783e-17\n",
      "[step: 1039] loss: 5.551115123125783e-17\n",
      "[step: 1040] loss: 2.7200464632711927e-17\n",
      "[step: 1041] loss: 2.7200464632711927e-17\n",
      "[step: 1042] loss: 2.7200464632711927e-17\n",
      "[step: 1043] loss: 2.7200464632711927e-17\n",
      "[step: 1044] loss: 2.7200464632711927e-17\n",
      "[step: 1045] loss: 2.7200464632711927e-17\n",
      "[step: 1046] loss: 2.7200464632711927e-17\n",
      "[step: 1047] loss: 5.551115123125783e-17\n",
      "[step: 1048] loss: 5.551115123125783e-17\n",
      "[step: 1049] loss: 5.551115123125783e-17\n",
      "[step: 1050] loss: 5.551115123125783e-17\n",
      "[step: 1051] loss: 5.551115123125783e-17\n",
      "[step: 1052] loss: 2.7200464632711927e-17\n",
      "[step: 1053] loss: 2.7200464632711927e-17\n",
      "[step: 1054] loss: 2.7200464632711927e-17\n",
      "[step: 1055] loss: 2.7200464632711927e-17\n",
      "[step: 1056] loss: 2.7200464632711927e-17\n",
      "[step: 1057] loss: 2.7200464632711927e-17\n",
      "[step: 1058] loss: 2.7200464632711927e-17\n",
      "[step: 1059] loss: 5.551115123125783e-17\n",
      "[step: 1060] loss: 5.551115123125783e-17\n",
      "[step: 1061] loss: 5.551115123125783e-17\n",
      "[step: 1062] loss: 5.551115123125783e-17\n",
      "[step: 1063] loss: 5.551115123125783e-17\n",
      "[step: 1064] loss: 2.7200464632711927e-17\n",
      "[step: 1065] loss: 2.7200464632711927e-17\n",
      "[step: 1066] loss: 2.7200464632711927e-17\n",
      "[step: 1067] loss: 2.7200464632711927e-17\n",
      "[step: 1068] loss: 2.7200464632711927e-17\n",
      "[step: 1069] loss: 2.7200464632711927e-17\n",
      "[step: 1070] loss: 2.7200464632711927e-17\n",
      "[step: 1071] loss: 5.551115123125783e-17\n",
      "[step: 1072] loss: 5.551115123125783e-17\n",
      "[step: 1073] loss: 5.551115123125783e-17\n",
      "[step: 1074] loss: 5.551115123125783e-17\n",
      "[step: 1075] loss: 5.551115123125783e-17\n",
      "[step: 1076] loss: 2.7200464632711927e-17\n",
      "[step: 1077] loss: 2.7200464632711927e-17\n",
      "[step: 1078] loss: 2.7200464632711927e-17\n",
      "[step: 1079] loss: 2.7200464632711927e-17\n",
      "[step: 1080] loss: 2.7200464632711927e-17\n",
      "[step: 1081] loss: 2.7200464632711927e-17\n",
      "[step: 1082] loss: 2.7200464632711927e-17\n",
      "[step: 1083] loss: 5.551115123125783e-17\n",
      "[step: 1084] loss: 5.551115123125783e-17\n",
      "[step: 1085] loss: 5.551115123125783e-17\n",
      "[step: 1086] loss: 5.551115123125783e-17\n",
      "[step: 1087] loss: 5.551115123125783e-17\n",
      "[step: 1088] loss: 2.7200464632711927e-17\n",
      "[step: 1089] loss: 2.7200464632711927e-17\n",
      "[step: 1090] loss: 2.7200464632711927e-17\n",
      "[step: 1091] loss: 2.7200464632711927e-17\n",
      "[step: 1092] loss: 2.7200464632711927e-17\n",
      "[step: 1093] loss: 2.7200464632711927e-17\n",
      "[step: 1094] loss: 2.7200464632711927e-17\n",
      "[step: 1095] loss: 5.551115123125783e-17\n",
      "[step: 1096] loss: 5.551115123125783e-17\n",
      "[step: 1097] loss: 5.551115123125783e-17\n",
      "[step: 1098] loss: 5.551115123125783e-17\n",
      "[step: 1099] loss: 5.551115123125783e-17\n",
      "[step: 1100] loss: 2.7200464632711927e-17\n",
      "[step: 1101] loss: 2.7200464632711927e-17\n",
      "[step: 1102] loss: 2.7200464632711927e-17\n",
      "[step: 1103] loss: 2.7200464632711927e-17\n",
      "[step: 1104] loss: 2.7200464632711927e-17\n",
      "[step: 1105] loss: 2.7200464632711927e-17\n",
      "[step: 1106] loss: 2.7200464632711927e-17\n",
      "[step: 1107] loss: 5.551115123125783e-17\n",
      "[step: 1108] loss: 5.551115123125783e-17\n",
      "[step: 1109] loss: 5.551115123125783e-17\n",
      "[step: 1110] loss: 5.551115123125783e-17\n",
      "[step: 1111] loss: 5.551115123125783e-17\n",
      "[step: 1112] loss: 2.7200464632711927e-17\n",
      "[step: 1113] loss: 2.7200464632711927e-17\n",
      "[step: 1114] loss: 2.7200464632711927e-17\n",
      "[step: 1115] loss: 2.7200464632711927e-17\n",
      "[step: 1116] loss: 2.7200464632711927e-17\n",
      "[step: 1117] loss: 2.7200464632711927e-17\n",
      "[step: 1118] loss: 2.7200464632711927e-17\n",
      "[step: 1119] loss: 5.551115123125783e-17\n",
      "[step: 1120] loss: 5.551115123125783e-17\n",
      "[step: 1121] loss: 5.551115123125783e-17\n",
      "[step: 1122] loss: 5.551115123125783e-17\n",
      "[step: 1123] loss: 5.551115123125783e-17\n",
      "[step: 1124] loss: 2.7200464632711927e-17\n",
      "[step: 1125] loss: 2.7200464632711927e-17\n",
      "[step: 1126] loss: 2.7200464632711927e-17\n",
      "[step: 1127] loss: 2.7200464632711927e-17\n",
      "[step: 1128] loss: 2.7200464632711927e-17\n",
      "[step: 1129] loss: 2.7200464632711927e-17\n",
      "[step: 1130] loss: 2.7200464632711927e-17\n",
      "[step: 1131] loss: 5.551115123125783e-17\n",
      "[step: 1132] loss: 5.551115123125783e-17\n",
      "[step: 1133] loss: 5.551115123125783e-17\n",
      "[step: 1134] loss: 5.551115123125783e-17\n",
      "[step: 1135] loss: 5.551115123125783e-17\n",
      "[step: 1136] loss: 2.7200464632711927e-17\n",
      "[step: 1137] loss: 2.7200464632711927e-17\n",
      "[step: 1138] loss: 2.7200464632711927e-17\n",
      "[step: 1139] loss: 2.7200464632711927e-17\n",
      "[step: 1140] loss: 2.7200464632711927e-17\n",
      "[step: 1141] loss: 2.7200464632711927e-17\n",
      "[step: 1142] loss: 2.7200464632711927e-17\n",
      "[step: 1143] loss: 5.551115123125783e-17\n",
      "[step: 1144] loss: 5.551115123125783e-17\n",
      "[step: 1145] loss: 5.551115123125783e-17\n",
      "[step: 1146] loss: 5.551115123125783e-17\n",
      "[step: 1147] loss: 5.551115123125783e-17\n",
      "[step: 1148] loss: 2.7200464632711927e-17\n",
      "[step: 1149] loss: 2.7200464632711927e-17\n",
      "[step: 1150] loss: 2.7200464632711927e-17\n",
      "[step: 1151] loss: 2.7200464632711927e-17\n",
      "[step: 1152] loss: 2.7200464632711927e-17\n",
      "[step: 1153] loss: 2.7200464632711927e-17\n",
      "[step: 1154] loss: 2.7200464632711927e-17\n",
      "[step: 1155] loss: 5.551115123125783e-17\n",
      "[step: 1156] loss: 5.551115123125783e-17\n",
      "[step: 1157] loss: 5.551115123125783e-17\n",
      "[step: 1158] loss: 5.551115123125783e-17\n",
      "[step: 1159] loss: 1.9984015237346206e-17\n",
      "[step: 1160] loss: 1.9984015237346206e-17\n",
      "[step: 1161] loss: 1.9984015237346206e-17\n",
      "[step: 1162] loss: 1.9984015237346206e-17\n",
      "[step: 1163] loss: 1.9984015237346206e-17\n",
      "[step: 1164] loss: 1.9984015237346206e-17\n",
      "[step: 1165] loss: 1.9984015237346206e-17\n",
      "[step: 1166] loss: 6.716849510740434e-17\n",
      "[step: 1167] loss: 6.716849510740434e-17\n",
      "[step: 1168] loss: 6.716849510740434e-17\n",
      "[step: 1169] loss: 6.716849510740434e-17\n",
      "[step: 1170] loss: 1.9984015237346206e-17\n",
      "[step: 1171] loss: 1.9984015237346206e-17\n",
      "[step: 1172] loss: 1.9984015237346206e-17\n",
      "[step: 1173] loss: 1.9984015237346206e-17\n",
      "[step: 1174] loss: 1.9984015237346206e-17\n",
      "[step: 1175] loss: 1.9984015237346206e-17\n",
      "[step: 1176] loss: 1.9984015237346206e-17\n",
      "[step: 1177] loss: 6.716849510740434e-17\n",
      "[step: 1178] loss: 6.716849510740434e-17\n",
      "[step: 1179] loss: 6.716849510740434e-17\n",
      "[step: 1180] loss: 6.716849510740434e-17\n",
      "[step: 1181] loss: 1.9984015237346206e-17\n",
      "[step: 1182] loss: 1.9984015237346206e-17\n",
      "[step: 1183] loss: 1.9984015237346206e-17\n",
      "[step: 1184] loss: 1.9984015237346206e-17\n",
      "[step: 1185] loss: 1.9984015237346206e-17\n",
      "[step: 1186] loss: 1.9984015237346206e-17\n",
      "[step: 1187] loss: 1.9984015237346206e-17\n",
      "[step: 1188] loss: 6.716849510740434e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1189] loss: 6.716849510740434e-17\n",
      "[step: 1190] loss: 6.716849510740434e-17\n",
      "[step: 1191] loss: 6.716849510740434e-17\n",
      "[step: 1192] loss: 1.9984015237346206e-17\n",
      "[step: 1193] loss: 1.9984015237346206e-17\n",
      "[step: 1194] loss: 1.9984015237346206e-17\n",
      "[step: 1195] loss: 1.9984015237346206e-17\n",
      "[step: 1196] loss: 1.9984015237346206e-17\n",
      "[step: 1197] loss: 1.9984015237346206e-17\n",
      "[step: 1198] loss: 1.9984015237346206e-17\n",
      "[step: 1199] loss: 6.716849510740434e-17\n",
      "[step: 1200] loss: 6.716849510740434e-17\n",
      "[step: 1201] loss: 6.716849510740434e-17\n",
      "[step: 1202] loss: 6.716849510740434e-17\n",
      "[step: 1203] loss: 1.9984015237346206e-17\n",
      "[step: 1204] loss: 1.9984015237346206e-17\n",
      "[step: 1205] loss: 1.9984015237346206e-17\n",
      "[step: 1206] loss: 1.9984015237346206e-17\n",
      "[step: 1207] loss: 1.9984015237346206e-17\n",
      "[step: 1208] loss: 1.9984015237346206e-17\n",
      "[step: 1209] loss: 1.9984015237346206e-17\n",
      "[step: 1210] loss: 6.716849510740434e-17\n",
      "[step: 1211] loss: 6.716849510740434e-17\n",
      "[step: 1212] loss: 6.716849510740434e-17\n",
      "[step: 1213] loss: 6.716849510740434e-17\n",
      "[step: 1214] loss: 1.9984015237346206e-17\n",
      "[step: 1215] loss: 1.9984015237346206e-17\n",
      "[step: 1216] loss: 1.9984015237346206e-17\n",
      "[step: 1217] loss: 1.9984015237346206e-17\n",
      "[step: 1218] loss: 1.9984015237346206e-17\n",
      "[step: 1219] loss: 1.9984015237346206e-17\n",
      "[step: 1220] loss: 1.9984015237346206e-17\n",
      "[step: 1221] loss: 6.716849510740434e-17\n",
      "[step: 1222] loss: 6.716849510740434e-17\n",
      "[step: 1223] loss: 6.716849510740434e-17\n",
      "[step: 1224] loss: 6.716849510740434e-17\n",
      "[step: 1225] loss: 1.9984015237346206e-17\n",
      "[step: 1226] loss: 1.9984015237346206e-17\n",
      "[step: 1227] loss: 1.9984015237346206e-17\n",
      "[step: 1228] loss: 1.9984015237346206e-17\n",
      "[step: 1229] loss: 1.9984015237346206e-17\n",
      "[step: 1230] loss: 1.9984015237346206e-17\n",
      "[step: 1231] loss: 1.9984015237346206e-17\n",
      "[step: 1232] loss: 6.716849510740434e-17\n",
      "[step: 1233] loss: 6.716849510740434e-17\n",
      "[step: 1234] loss: 6.716849510740434e-17\n",
      "[step: 1235] loss: 6.716849510740434e-17\n",
      "[step: 1236] loss: 1.9984015237346206e-17\n",
      "[step: 1237] loss: 1.9984015237346206e-17\n",
      "[step: 1238] loss: 1.9984015237346206e-17\n",
      "[step: 1239] loss: 1.9984015237346206e-17\n",
      "[step: 1240] loss: 1.9984015237346206e-17\n",
      "[step: 1241] loss: 1.9984015237346206e-17\n",
      "[step: 1242] loss: 1.9984015237346206e-17\n",
      "[step: 1243] loss: 6.716849510740434e-17\n",
      "[step: 1244] loss: 6.716849510740434e-17\n",
      "[step: 1245] loss: 6.716849510740434e-17\n",
      "[step: 1246] loss: 6.716849510740434e-17\n",
      "[step: 1247] loss: 1.9984015237346206e-17\n",
      "[step: 1248] loss: 1.9984015237346206e-17\n",
      "[step: 1249] loss: 1.9984015237346206e-17\n",
      "[step: 1250] loss: 1.9984015237346206e-17\n",
      "[step: 1251] loss: 1.9984015237346206e-17\n",
      "[step: 1252] loss: 1.9984015237346206e-17\n",
      "[step: 1253] loss: 1.9984015237346206e-17\n",
      "[step: 1254] loss: 6.716849510740434e-17\n",
      "[step: 1255] loss: 6.716849510740434e-17\n",
      "[step: 1256] loss: 6.716849510740434e-17\n",
      "[step: 1257] loss: 6.716849510740434e-17\n",
      "[step: 1258] loss: 1.9984015237346206e-17\n",
      "[step: 1259] loss: 1.9984015237346206e-17\n",
      "[step: 1260] loss: 1.9984015237346206e-17\n",
      "[step: 1261] loss: 1.9984015237346206e-17\n",
      "[step: 1262] loss: 1.9984015237346206e-17\n",
      "[step: 1263] loss: 1.9984015237346206e-17\n",
      "[step: 1264] loss: 1.9984015237346206e-17\n",
      "[step: 1265] loss: 6.716849510740434e-17\n",
      "[step: 1266] loss: 6.716849510740434e-17\n",
      "[step: 1267] loss: 6.716849510740434e-17\n",
      "[step: 1268] loss: 6.716849510740434e-17\n",
      "[step: 1269] loss: 1.9984015237346206e-17\n",
      "[step: 1270] loss: 1.9984015237346206e-17\n",
      "[step: 1271] loss: 1.9984015237346206e-17\n",
      "[step: 1272] loss: 1.9984015237346206e-17\n",
      "[step: 1273] loss: 1.9984015237346206e-17\n",
      "[step: 1274] loss: 1.9984015237346206e-17\n",
      "[step: 1275] loss: 1.9984015237346206e-17\n",
      "[step: 1276] loss: 6.716849510740434e-17\n",
      "[step: 1277] loss: 6.716849510740434e-17\n",
      "[step: 1278] loss: 6.716849510740434e-17\n",
      "[step: 1279] loss: 6.716849510740434e-17\n",
      "[step: 1280] loss: 1.9984015237346206e-17\n",
      "[step: 1281] loss: 1.9984015237346206e-17\n",
      "[step: 1282] loss: 1.9984015237346206e-17\n",
      "[step: 1283] loss: 1.9984015237346206e-17\n",
      "[step: 1284] loss: 1.9984015237346206e-17\n",
      "[step: 1285] loss: 1.9984015237346206e-17\n",
      "[step: 1286] loss: 1.9984015237346206e-17\n",
      "[step: 1287] loss: 6.716849510740434e-17\n",
      "[step: 1288] loss: 6.716849510740434e-17\n",
      "[step: 1289] loss: 6.716849510740434e-17\n",
      "[step: 1290] loss: 6.716849510740434e-17\n",
      "[step: 1291] loss: 1.9984015237346206e-17\n",
      "[step: 1292] loss: 1.9984015237346206e-17\n",
      "[step: 1293] loss: 1.9984015237346206e-17\n",
      "[step: 1294] loss: 1.9984015237346206e-17\n",
      "[step: 1295] loss: 1.9984015237346206e-17\n",
      "[step: 1296] loss: 1.9984015237346206e-17\n",
      "[step: 1297] loss: 1.9984015237346206e-17\n",
      "[step: 1298] loss: 6.716849510740434e-17\n",
      "[step: 1299] loss: 6.716849510740434e-17\n",
      "[step: 1300] loss: 6.716849510740434e-17\n",
      "[step: 1301] loss: 6.716849510740434e-17\n",
      "[step: 1302] loss: 1.9984015237346206e-17\n",
      "[step: 1303] loss: 1.9984015237346206e-17\n",
      "[step: 1304] loss: 1.9984015237346206e-17\n",
      "[step: 1305] loss: 1.9984015237346206e-17\n",
      "[step: 1306] loss: 1.9984015237346206e-17\n",
      "[step: 1307] loss: 1.9984015237346206e-17\n",
      "[step: 1308] loss: 1.9984015237346206e-17\n",
      "[step: 1309] loss: 6.716849510740434e-17\n",
      "[step: 1310] loss: 6.716849510740434e-17\n",
      "[step: 1311] loss: 6.716849510740434e-17\n",
      "[step: 1312] loss: 6.716849510740434e-17\n",
      "[step: 1313] loss: 1.9984015237346206e-17\n",
      "[step: 1314] loss: 1.9984015237346206e-17\n",
      "[step: 1315] loss: 1.9984015237346206e-17\n",
      "[step: 1316] loss: 1.9984015237346206e-17\n",
      "[step: 1317] loss: 1.9984015237346206e-17\n",
      "[step: 1318] loss: 1.9984015237346206e-17\n",
      "[step: 1319] loss: 1.9984015237346206e-17\n",
      "[step: 1320] loss: 6.716849510740434e-17\n",
      "[step: 1321] loss: 6.716849510740434e-17\n",
      "[step: 1322] loss: 6.716849510740434e-17\n",
      "[step: 1323] loss: 6.716849510740434e-17\n",
      "[step: 1324] loss: 1.9984015237346206e-17\n",
      "[step: 1325] loss: 1.9984015237346206e-17\n",
      "[step: 1326] loss: 1.9984015237346206e-17\n",
      "[step: 1327] loss: 1.9984015237346206e-17\n",
      "[step: 1328] loss: 1.9984015237346206e-17\n",
      "[step: 1329] loss: 1.9984015237346206e-17\n",
      "[step: 1330] loss: 1.9984015237346206e-17\n",
      "[step: 1331] loss: 6.716849510740434e-17\n",
      "[step: 1332] loss: 6.716849510740434e-17\n",
      "[step: 1333] loss: 6.716849510740434e-17\n",
      "[step: 1334] loss: 6.716849510740434e-17\n",
      "[step: 1335] loss: 1.9984015237346206e-17\n",
      "[step: 1336] loss: 1.9984015237346206e-17\n",
      "[step: 1337] loss: 1.9984015237346206e-17\n",
      "[step: 1338] loss: 1.9984015237346206e-17\n",
      "[step: 1339] loss: 1.9984015237346206e-17\n",
      "[step: 1340] loss: 1.9984015237346206e-17\n",
      "[step: 1341] loss: 1.9984015237346206e-17\n",
      "[step: 1342] loss: 1.3877787807814457e-17\n",
      "[step: 1343] loss: 7.993606094938482e-17\n",
      "[step: 1344] loss: 7.993606094938482e-17\n",
      "[step: 1345] loss: 7.993606094938482e-17\n",
      "[step: 1346] loss: 7.993606094938482e-17\n",
      "[step: 1347] loss: 1.3877787807814457e-17\n",
      "[step: 1348] loss: 1.9984015237346206e-17\n",
      "[step: 1349] loss: 1.9984015237346206e-17\n",
      "[step: 1350] loss: 1.9984015237346206e-17\n",
      "[step: 1351] loss: 1.9984015237346206e-17\n",
      "[step: 1352] loss: 1.9984015237346206e-17\n",
      "[step: 1353] loss: 1.9984015237346206e-17\n",
      "[step: 1354] loss: 1.3877787807814457e-17\n",
      "[step: 1355] loss: 7.993606094938482e-17\n",
      "[step: 1356] loss: 7.993606094938482e-17\n",
      "[step: 1357] loss: 7.993606094938482e-17\n",
      "[step: 1358] loss: 8.881783998477905e-18\n",
      "[step: 1359] loss: 8.881783998477905e-18\n",
      "[step: 1360] loss: 8.881783998477905e-18\n",
      "[step: 1361] loss: 8.881783998477905e-18\n",
      "[step: 1362] loss: 8.881783998477905e-18\n",
      "[step: 1363] loss: 8.881783998477905e-18\n",
      "[step: 1364] loss: 8.881783998477905e-18\n",
      "[step: 1365] loss: 8.881783998477905e-18\n",
      "[step: 1366] loss: 8.881783998477905e-18\n",
      "[step: 1367] loss: 9.381384875719928e-17\n",
      "[step: 1368] loss: 9.381384875719928e-17\n",
      "[step: 1369] loss: 9.381384875719928e-17\n",
      "[step: 1370] loss: 8.881783998477905e-18\n",
      "[step: 1371] loss: 8.881783998477905e-18\n",
      "[step: 1372] loss: 8.881783998477905e-18\n",
      "[step: 1373] loss: 8.881783998477905e-18\n",
      "[step: 1374] loss: 8.881783998477905e-18\n",
      "[step: 1375] loss: 8.881783998477905e-18\n",
      "[step: 1376] loss: 8.881783998477905e-18\n",
      "[step: 1377] loss: 8.881783998477905e-18\n",
      "[step: 1378] loss: 8.881783998477905e-18\n",
      "[step: 1379] loss: 8.881783998477905e-18\n",
      "[step: 1380] loss: 9.381384875719928e-17\n",
      "[step: 1381] loss: 9.381384875719928e-17\n",
      "[step: 1382] loss: 9.381384875719928e-17\n",
      "[step: 1383] loss: 8.881783998477905e-18\n",
      "[step: 1384] loss: 8.881783998477905e-18\n",
      "[step: 1385] loss: 8.881783998477905e-18\n",
      "[step: 1386] loss: 8.881783998477905e-18\n",
      "[step: 1387] loss: 8.881783998477905e-18\n",
      "[step: 1388] loss: 8.881783998477905e-18\n",
      "[step: 1389] loss: 8.881783998477905e-18\n",
      "[step: 1390] loss: 8.881783998477905e-18\n",
      "[step: 1391] loss: 8.881783998477905e-18\n",
      "[step: 1392] loss: 8.881783998477905e-18\n",
      "[step: 1393] loss: 9.381384875719928e-17\n",
      "[step: 1394] loss: 9.381384875719928e-17\n",
      "[step: 1395] loss: 9.381384875719928e-17\n",
      "[step: 1396] loss: 8.881783998477905e-18\n",
      "[step: 1397] loss: 8.881783998477905e-18\n",
      "[step: 1398] loss: 8.881783998477905e-18\n",
      "[step: 1399] loss: 8.881783998477905e-18\n",
      "[step: 1400] loss: 8.881783998477905e-18\n",
      "[step: 1401] loss: 8.881783998477905e-18\n",
      "[step: 1402] loss: 8.881783998477905e-18\n",
      "[step: 1403] loss: 8.881783998477905e-18\n",
      "[step: 1404] loss: 8.881783998477905e-18\n",
      "[step: 1405] loss: 7.993606094938482e-17\n",
      "[step: 1406] loss: 7.993606094938482e-17\n",
      "[step: 1407] loss: 7.993606094938482e-17\n",
      "[step: 1408] loss: 1.3877787807814457e-17\n",
      "[step: 1409] loss: 1.3877787807814457e-17\n",
      "[step: 1410] loss: 1.3877787807814457e-17\n",
      "[step: 1411] loss: 1.3877787807814457e-17\n",
      "[step: 1412] loss: 1.3877787807814457e-17\n",
      "[step: 1413] loss: 1.3877787807814457e-17\n",
      "[step: 1414] loss: 1.3877787807814457e-17\n",
      "[step: 1415] loss: 6.716849510740434e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1416] loss: 6.716849510740434e-17\n",
      "[step: 1417] loss: 6.716849510740434e-17\n",
      "[step: 1418] loss: 1.9984015237346206e-17\n",
      "[step: 1419] loss: 1.9984015237346206e-17\n",
      "[step: 1420] loss: 1.9984015237346206e-17\n",
      "[step: 1421] loss: 1.9984015237346206e-17\n",
      "[step: 1422] loss: 1.9984015237346206e-17\n",
      "[step: 1423] loss: 1.9984015237346206e-17\n",
      "[step: 1424] loss: 6.716849510740434e-17\n",
      "[step: 1425] loss: 6.716849510740434e-17\n",
      "[step: 1426] loss: 6.716849510740434e-17\n",
      "[step: 1427] loss: 1.9984015237346206e-17\n",
      "[step: 1428] loss: 1.9984015237346206e-17\n",
      "[step: 1429] loss: 1.9984015237346206e-17\n",
      "[step: 1430] loss: 1.9984015237346206e-17\n",
      "[step: 1431] loss: 1.9984015237346206e-17\n",
      "[step: 1432] loss: 1.9984015237346206e-17\n",
      "[step: 1433] loss: 6.716849510740434e-17\n",
      "[step: 1434] loss: 6.716849510740434e-17\n",
      "[step: 1435] loss: 6.716849510740434e-17\n",
      "[step: 1436] loss: 1.3877787807814457e-17\n",
      "[step: 1437] loss: 1.3877787807814457e-17\n",
      "[step: 1438] loss: 1.3877787807814457e-17\n",
      "[step: 1439] loss: 1.3877787807814457e-17\n",
      "[step: 1440] loss: 1.3877787807814457e-17\n",
      "[step: 1441] loss: 1.3877787807814457e-17\n",
      "[step: 1442] loss: 1.3877787807814457e-17\n",
      "[step: 1443] loss: 7.993606094938482e-17\n",
      "[step: 1444] loss: 7.993606094938482e-17\n",
      "[step: 1445] loss: 7.993606094938482e-17\n",
      "[step: 1446] loss: 1.3877787807814457e-17\n",
      "[step: 1447] loss: 1.3877787807814457e-17\n",
      "[step: 1448] loss: 1.3877787807814457e-17\n",
      "[step: 1449] loss: 1.3877787807814457e-17\n",
      "[step: 1450] loss: 1.3877787807814457e-17\n",
      "[step: 1451] loss: 1.3877787807814457e-17\n",
      "[step: 1452] loss: 1.3877787807814457e-17\n",
      "[step: 1453] loss: 7.993606094938482e-17\n",
      "[step: 1454] loss: 7.993606094938482e-17\n",
      "[step: 1455] loss: 7.993606094938482e-17\n",
      "[step: 1456] loss: 1.3877787807814457e-17\n",
      "[step: 1457] loss: 1.3877787807814457e-17\n",
      "[step: 1458] loss: 1.3877787807814457e-17\n",
      "[step: 1459] loss: 1.3877787807814457e-17\n",
      "[step: 1460] loss: 1.3877787807814457e-17\n",
      "[step: 1461] loss: 1.3877787807814457e-17\n",
      "[step: 1462] loss: 1.3877787807814457e-17\n",
      "[step: 1463] loss: 7.993606094938482e-17\n",
      "[step: 1464] loss: 7.993606094938482e-17\n",
      "[step: 1465] loss: 7.993606094938482e-17\n",
      "[step: 1466] loss: 1.3877787807814457e-17\n",
      "[step: 1467] loss: 1.3877787807814457e-17\n",
      "[step: 1468] loss: 1.3877787807814457e-17\n",
      "[step: 1469] loss: 1.3877787807814457e-17\n",
      "[step: 1470] loss: 1.3877787807814457e-17\n",
      "[step: 1471] loss: 1.3877787807814457e-17\n",
      "[step: 1472] loss: 1.3877787807814457e-17\n",
      "[step: 1473] loss: 7.993606094938482e-17\n",
      "[step: 1474] loss: 7.993606094938482e-17\n",
      "[step: 1475] loss: 7.993606094938482e-17\n",
      "[step: 1476] loss: 1.3877787807814457e-17\n",
      "[step: 1477] loss: 1.3877787807814457e-17\n",
      "[step: 1478] loss: 1.3877787807814457e-17\n",
      "[step: 1479] loss: 1.3877787807814457e-17\n",
      "[step: 1480] loss: 1.3877787807814457e-17\n",
      "[step: 1481] loss: 1.3877787807814457e-17\n",
      "[step: 1482] loss: 1.3877787807814457e-17\n",
      "[step: 1483] loss: 7.993606094938482e-17\n",
      "[step: 1484] loss: 7.993606094938482e-17\n",
      "[step: 1485] loss: 7.993606094938482e-17\n",
      "[step: 1486] loss: 1.3877787807814457e-17\n",
      "[step: 1487] loss: 1.3877787807814457e-17\n",
      "[step: 1488] loss: 1.3877787807814457e-17\n",
      "[step: 1489] loss: 1.3877787807814457e-17\n",
      "[step: 1490] loss: 1.3877787807814457e-17\n",
      "[step: 1491] loss: 1.3877787807814457e-17\n",
      "[step: 1492] loss: 1.3877787807814457e-17\n",
      "[step: 1493] loss: 7.993606094938482e-17\n",
      "[step: 1494] loss: 7.993606094938482e-17\n",
      "[step: 1495] loss: 7.993606094938482e-17\n",
      "[step: 1496] loss: 1.3877787807814457e-17\n",
      "[step: 1497] loss: 1.3877787807814457e-17\n",
      "[step: 1498] loss: 1.3877787807814457e-17\n",
      "[step: 1499] loss: 1.3877787807814457e-17\n",
      "RMSE: 2.654869556427002\n",
      "pred: [2.9996731e+00 4.9995294e+00 1.9998494e+00 1.9997151e+00 3.4230947e-04\n",
      " 1.0001466e+00 4.0000010e+00 3.9999757e+00 3.9996483e+00 3.9999237e+00\n",
      " 1.9999098e+00 3.9996300e+00 1.9997166e+00 5.9995937e+00 4.9995203e+00\n",
      " 2.9999387e+00 3.9997497e+00 3.9995954e+00 1.0000451e+00 1.9998494e+00\n",
      " 2.9997344e+00 3.0000062e+00 9.9975568e-01 5.9994988e+00 3.9995303e+00]\n",
      "real: [2 6 2 2 5 2 5 0 4 2 3 1 2 4 5 1 3 1 4 3 5 2 4 1 6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/desktop/test_write.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "\n",
    "xy= xy[:,:27]\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1500\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "train_set = xy[:7]\n",
    "test_set = xy[7:]\n",
    "\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "outputs,_states = tf.nn.dynamic_rnn(cell,X,dtype = tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "loss =tf.reduce_mean(tf.square(Y_pred-Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train,loss],feed_dict={X:trainX, Y:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X:testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    print(\"real: {}\".format(testY[-1,:]))\n",
    "    \n",
    "    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.3)\n",
    "    plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
