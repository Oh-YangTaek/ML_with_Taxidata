{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/yeseo/ML_with_Taxidata/code/save_model_code/model/my_model.ckpt\n",
      "[step: 0] loss: 0.0020109154284000397\n",
      "[step: 1] loss: 0.002210955135524273\n",
      "[step: 2] loss: 0.002764706267043948\n",
      "[step: 3] loss: 0.003098532324656844\n",
      "[step: 4] loss: 0.002830271143466234\n",
      "[step: 5] loss: 0.00267417193390429\n",
      "[step: 6] loss: 0.0023939621169120073\n",
      "[step: 7] loss: 0.0025931596755981445\n",
      "[step: 8] loss: 0.002389867091551423\n",
      "[step: 9] loss: 0.00218906719237566\n",
      "[step: 10] loss: 0.0020117945969104767\n",
      "[step: 11] loss: 0.0018564540660008788\n",
      "[step: 12] loss: 0.0017348772380501032\n",
      "[step: 13] loss: 0.0019648170564323664\n",
      "[step: 14] loss: 0.0014228301588445902\n",
      "[step: 15] loss: 0.001591710839420557\n",
      "[step: 16] loss: 0.0016863829223439097\n",
      "[step: 17] loss: 0.0014241727767512202\n",
      "[step: 18] loss: 0.0013349064392969012\n",
      "[step: 19] loss: 0.001432431978173554\n",
      "[step: 20] loss: 0.0014056521467864513\n",
      "[step: 21] loss: 0.0012767126318067312\n",
      "[step: 22] loss: 0.0013490237761288881\n",
      "[step: 23] loss: 0.001236642012372613\n",
      "[step: 24] loss: 0.0012087885988876224\n",
      "[step: 25] loss: 0.001248514512553811\n",
      "[step: 26] loss: 0.001215573400259018\n",
      "[step: 27] loss: 0.001164447981864214\n",
      "[step: 28] loss: 0.0011363718658685684\n",
      "[step: 29] loss: 0.0011658747680485249\n",
      "[step: 30] loss: 0.001107407733798027\n",
      "[step: 31] loss: 0.0010995793854817748\n",
      "[step: 32] loss: 0.001109599368646741\n",
      "[step: 33] loss: 0.0010782582685351372\n",
      "[step: 34] loss: 0.0010439017787575722\n",
      "[step: 35] loss: 0.001053145038895309\n",
      "[step: 36] loss: 0.0010407669469714165\n",
      "[step: 37] loss: 0.0010374248959124088\n",
      "[step: 38] loss: 0.0010190659668296576\n",
      "[step: 39] loss: 0.0010057390900328755\n",
      "[step: 40] loss: 0.0010033330181613564\n",
      "[step: 41] loss: 0.0009936658898368478\n",
      "[step: 42] loss: 0.0009878966957330704\n",
      "[step: 43] loss: 0.0009706527926027775\n",
      "[step: 44] loss: 0.0009724603150971234\n",
      "[step: 45] loss: 0.0009629816049709916\n",
      "[step: 46] loss: 0.0009548015659675002\n",
      "[step: 47] loss: 0.0009500597370788455\n",
      "[step: 48] loss: 0.0009437743574380875\n",
      "[step: 49] loss: 0.0009377269307151437\n",
      "[step: 50] loss: 0.0009311985340900719\n",
      "[step: 51] loss: 0.0009292599279433489\n",
      "[step: 52] loss: 0.0009184923837892711\n",
      "[step: 53] loss: 0.0009181429049931467\n",
      "[step: 54] loss: 0.0009122342453338206\n",
      "[step: 55] loss: 0.0009068078361451626\n",
      "[step: 56] loss: 0.0009035359835252166\n",
      "[step: 57] loss: 0.0008989457855932415\n",
      "[step: 58] loss: 0.0008956767851486802\n",
      "[step: 59] loss: 0.0008900367538444698\n",
      "[step: 60] loss: 0.0008886840660125017\n",
      "[step: 61] loss: 0.0008842502720654011\n",
      "[step: 62] loss: 0.0008798237540759146\n",
      "[step: 63] loss: 0.0008780888165347278\n",
      "[step: 64] loss: 0.0008742847130633891\n",
      "[step: 65] loss: 0.0008706248481757939\n",
      "[step: 66] loss: 0.0008687401423230767\n",
      "[step: 67] loss: 0.0008648830698803067\n",
      "[step: 68] loss: 0.0008623558096587658\n",
      "[step: 69] loss: 0.000859405379742384\n",
      "[step: 70] loss: 0.000856796046718955\n",
      "[step: 71] loss: 0.0008543182630091906\n",
      "[step: 72] loss: 0.00085145776392892\n",
      "[step: 73] loss: 0.0008489213068969548\n",
      "[step: 74] loss: 0.0008464321726933122\n",
      "[step: 75] loss: 0.0008443883853033185\n",
      "[step: 76] loss: 0.0008416844648309052\n",
      "[step: 77] loss: 0.0008396141929551959\n",
      "[step: 78] loss: 0.0008374890894629061\n",
      "[step: 79] loss: 0.0008350868592970073\n",
      "[step: 80] loss: 0.0008332249708473682\n",
      "[step: 81] loss: 0.0008311446290463209\n",
      "[step: 82] loss: 0.0008291528793051839\n",
      "[step: 83] loss: 0.0008273624116554856\n",
      "[step: 84] loss: 0.0008252857369370759\n",
      "[step: 85] loss: 0.0008234510896727443\n",
      "[step: 86] loss: 0.0008216414717026055\n",
      "[step: 87] loss: 0.0008198951836675406\n",
      "[step: 88] loss: 0.0008180286386050284\n",
      "[step: 89] loss: 0.0008164525497704744\n",
      "[step: 90] loss: 0.0008147530606947839\n",
      "[step: 91] loss: 0.000813271151855588\n",
      "[step: 92] loss: 0.0008120772545225918\n",
      "[step: 93] loss: 0.0008115783566609025\n",
      "[step: 94] loss: 0.0008130937349051237\n",
      "[step: 95] loss: 0.0008202333119697869\n",
      "[step: 96] loss: 0.0008444232516922057\n",
      "[step: 97] loss: 0.0008994570234790444\n",
      "[step: 98] loss: 0.0009891075314953923\n",
      "[step: 99] loss: 0.0010357595747336745\n",
      "[step: 100] loss: 0.0009557274170219898\n",
      "[step: 101] loss: 0.000828070507850498\n",
      "[step: 102] loss: 0.0008330261334776878\n",
      "[step: 103] loss: 0.000917509023565799\n",
      "[step: 104] loss: 0.0009120439644902945\n",
      "[step: 105] loss: 0.000830678443890065\n",
      "[step: 106] loss: 0.0008075234363786876\n",
      "[step: 107] loss: 0.0008562090806663036\n",
      "[step: 108] loss: 0.0008735093288123608\n",
      "[step: 109] loss: 0.0008300876361317933\n",
      "[step: 110] loss: 0.0007989060250110924\n",
      "[step: 111] loss: 0.0008193572866730392\n",
      "[step: 112] loss: 0.0008416846394538879\n",
      "[step: 113] loss: 0.0008184147300198674\n",
      "[step: 114] loss: 0.0007896070019342005\n",
      "[step: 115] loss: 0.0008004038245417178\n",
      "[step: 116] loss: 0.0008166153565980494\n",
      "[step: 117] loss: 0.0008062397828325629\n",
      "[step: 118] loss: 0.0007892578141763806\n",
      "[step: 119] loss: 0.000786028744187206\n",
      "[step: 120] loss: 0.0007964893011376262\n",
      "[step: 121] loss: 0.0008010949240997434\n",
      "[step: 122] loss: 0.0007884574588388205\n",
      "[step: 123] loss: 0.0007773791439831257\n",
      "[step: 124] loss: 0.0007819370948709548\n",
      "[step: 125] loss: 0.0007884385995566845\n",
      "[step: 126] loss: 0.0007876773015595973\n",
      "[step: 127] loss: 0.0007804448832757771\n",
      "[step: 128] loss: 0.0007728105993010104\n",
      "[step: 129] loss: 0.0007719194400124252\n",
      "[step: 130] loss: 0.0007770293159410357\n",
      "[step: 131] loss: 0.000777952081989497\n",
      "[step: 132] loss: 0.0007738836575299501\n",
      "[step: 133] loss: 0.0007693379884585738\n",
      "[step: 134] loss: 0.0007659693947061896\n",
      "[step: 135] loss: 0.0007658577524125576\n",
      "[step: 136] loss: 0.0007684101583436131\n",
      "[step: 137] loss: 0.0007692640647292137\n",
      "[step: 138] loss: 0.0007679713307879865\n",
      "[step: 139] loss: 0.000765258155297488\n",
      "[step: 140] loss: 0.0007618785602971911\n",
      "[step: 141] loss: 0.0007592222536914051\n",
      "[step: 142] loss: 0.0007584053091704845\n",
      "[step: 143] loss: 0.0007585093262605369\n",
      "[step: 144] loss: 0.0007589187589474022\n",
      "[step: 145] loss: 0.0007594399503432214\n",
      "[step: 146] loss: 0.0007597420481033623\n",
      "[step: 147] loss: 0.0007594939088448882\n",
      "[step: 148] loss: 0.0007591150351800025\n",
      "[step: 149] loss: 0.0007588756852783263\n",
      "[step: 150] loss: 0.0007581565296277404\n",
      "[step: 151] loss: 0.0007574267219752073\n",
      "[step: 152] loss: 0.0007571332389488816\n",
      "[step: 153] loss: 0.000757319328840822\n",
      "[step: 154] loss: 0.0007580423844046891\n",
      "[step: 155] loss: 0.00075992860365659\n",
      "[step: 156] loss: 0.0007626547594554722\n",
      "[step: 157] loss: 0.0007672292413190007\n",
      "[step: 158] loss: 0.0007733809179626405\n",
      "[step: 159] loss: 0.000782154907938093\n",
      "[step: 160] loss: 0.0007916207541711628\n",
      "[step: 161] loss: 0.0008013101178221405\n",
      "[step: 162] loss: 0.0008057936211116612\n",
      "[step: 163] loss: 0.0008036763756535947\n",
      "[step: 164] loss: 0.0007908430998213589\n",
      "[step: 165] loss: 0.0007719136192463338\n",
      "[step: 166] loss: 0.0007526999106630683\n",
      "[step: 167] loss: 0.0007414732244797051\n",
      "[step: 168] loss: 0.0007409849786199629\n",
      "[step: 169] loss: 0.0007482960936613381\n",
      "[step: 170] loss: 0.0007584484410472214\n",
      "[step: 171] loss: 0.0007666405290365219\n",
      "[step: 172] loss: 0.0007707660552114248\n",
      "[step: 173] loss: 0.0007682572468183935\n",
      "[step: 174] loss: 0.0007605941500514746\n",
      "[step: 175] loss: 0.0007498297491110861\n",
      "[step: 176] loss: 0.0007402448682114482\n",
      "[step: 177] loss: 0.0007345492485910654\n",
      "[step: 178] loss: 0.0007336248527280986\n",
      "[step: 179] loss: 0.0007363614276982844\n",
      "[step: 180] loss: 0.0007408182136714458\n",
      "[step: 181] loss: 0.0007454858860000968\n",
      "[step: 182] loss: 0.0007490849820896983\n",
      "[step: 183] loss: 0.0007513295277021825\n",
      "[step: 184] loss: 0.0007514615426771343\n",
      "[step: 185] loss: 0.0007500145584344864\n",
      "[step: 186] loss: 0.0007468642434105277\n",
      "[step: 187] loss: 0.0007429652032442391\n",
      "[step: 188] loss: 0.0007385034696199\n",
      "[step: 189] loss: 0.0007342892931774259\n",
      "[step: 190] loss: 0.000730588217265904\n",
      "[step: 191] loss: 0.0007277040858753026\n",
      "[step: 192] loss: 0.0007256129756569862\n",
      "[step: 193] loss: 0.0007242459105327725\n",
      "[step: 194] loss: 0.0007234337390400469\n",
      "[step: 195] loss: 0.0007230064366012812\n",
      "[step: 196] loss: 0.0007228622562251985\n",
      "[step: 197] loss: 0.0007230284973047674\n",
      "[step: 198] loss: 0.0007236896781250834\n",
      "[step: 199] loss: 0.0007251846836879849\n",
      "[step: 200] loss: 0.0007282242295332253\n",
      "[step: 201] loss: 0.0007339687435887754\n",
      "[step: 202] loss: 0.0007448699907399714\n",
      "[step: 203] loss: 0.0007643374847248197\n",
      "[step: 204] loss: 0.0007993226172402501\n",
      "[step: 205] loss: 0.0008543607546016574\n",
      "[step: 206] loss: 0.0009340611868537962\n",
      "[step: 207] loss: 0.0010063301306217909\n",
      "[step: 208] loss: 0.001025783596560359\n",
      "[step: 209] loss: 0.0009261998347938061\n",
      "[step: 210] loss: 0.0007837996236048639\n",
      "[step: 211] loss: 0.0007270051864907146\n",
      "[step: 212] loss: 0.0007889210828579962\n",
      "[step: 213] loss: 0.0008533294312655926\n",
      "[step: 214] loss: 0.0008281088666990399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 215] loss: 0.0007639569812454283\n",
      "[step: 216] loss: 0.0007337469724006951\n",
      "[step: 217] loss: 0.0007517421618103981\n",
      "[step: 218] loss: 0.0007791860261932015\n",
      "[step: 219] loss: 0.0007798703154549003\n",
      "[step: 220] loss: 0.0007503972738049924\n",
      "[step: 221] loss: 0.000719983596354723\n",
      "[step: 222] loss: 0.000730662199202925\n",
      "[step: 223] loss: 0.0007582549005746841\n",
      "[step: 224] loss: 0.0007511427975259721\n",
      "[step: 225] loss: 0.0007220580009743571\n",
      "[step: 226] loss: 0.0007141330861486495\n",
      "[step: 227] loss: 0.0007292968221008778\n",
      "[step: 228] loss: 0.00073517847340554\n",
      "[step: 229] loss: 0.0007244838052429259\n",
      "[step: 230] loss: 0.0007183390553109348\n",
      "[step: 231] loss: 0.0007192207267507911\n",
      "[step: 232] loss: 0.0007151151075959206\n",
      "[step: 233] loss: 0.0007127199205569923\n",
      "[step: 234] loss: 0.0007175779319368303\n",
      "[step: 235] loss: 0.0007197009981609881\n",
      "[step: 236] loss: 0.0007128513534553349\n",
      "[step: 237] loss: 0.0007052155560813844\n",
      "[step: 238] loss: 0.000706781109329313\n",
      "[step: 239] loss: 0.000711375440005213\n",
      "[step: 240] loss: 0.0007105005206540227\n",
      "[step: 241] loss: 0.000707666389644146\n",
      "[step: 242] loss: 0.0007070233696140349\n",
      "[step: 243] loss: 0.0007060336065478623\n",
      "[step: 244] loss: 0.0007028464460745454\n",
      "[step: 245] loss: 0.000700593926012516\n",
      "[step: 246] loss: 0.0007017974858172238\n",
      "[step: 247] loss: 0.0007036990718916059\n",
      "[step: 248] loss: 0.0007030701963230968\n",
      "[step: 249] loss: 0.0007013207068666816\n",
      "[step: 250] loss: 0.0007005371735431254\n",
      "[step: 251] loss: 0.0007001118501648307\n",
      "[step: 252] loss: 0.0006987007218413055\n",
      "[step: 253] loss: 0.000696782604791224\n",
      "[step: 254] loss: 0.0006959803286008537\n",
      "[step: 255] loss: 0.0006961996550671756\n",
      "[step: 256] loss: 0.0006961480830796063\n",
      "[step: 257] loss: 0.000695522059686482\n",
      "[step: 258] loss: 0.0006951087270863354\n",
      "[step: 259] loss: 0.0006952970870770514\n",
      "[step: 260] loss: 0.0006955672870390117\n",
      "[step: 261] loss: 0.0006953675765544176\n",
      "[step: 262] loss: 0.0006950220558792353\n",
      "[step: 263] loss: 0.0006950841634534299\n",
      "[step: 264] loss: 0.0006956671713851392\n",
      "[step: 265] loss: 0.0006965062930248678\n",
      "[step: 266] loss: 0.0006976802833378315\n",
      "[step: 267] loss: 0.0006997729651629925\n",
      "[step: 268] loss: 0.000703715137206018\n",
      "[step: 269] loss: 0.0007102670497260988\n",
      "[step: 270] loss: 0.0007209968753159046\n",
      "[step: 271] loss: 0.0007377195870503783\n",
      "[step: 272] loss: 0.0007644179277122021\n",
      "[step: 273] loss: 0.0008023533155210316\n",
      "[step: 274] loss: 0.0008534100488759577\n",
      "[step: 275] loss: 0.000900820130482316\n",
      "[step: 276] loss: 0.0009250034927390516\n",
      "[step: 277] loss: 0.0008868854492902756\n",
      "[step: 278] loss: 0.0008006819989532232\n",
      "[step: 279] loss: 0.0007180393440648913\n",
      "[step: 280] loss: 0.0006989527028053999\n",
      "[step: 281] loss: 0.0007350226514972746\n",
      "[step: 282] loss: 0.0007713157683610916\n",
      "[step: 283] loss: 0.0007727653137408197\n",
      "[step: 284] loss: 0.0007441008347086608\n",
      "[step: 285] loss: 0.0007151221507228911\n",
      "[step: 286] loss: 0.0006994094583205879\n",
      "[step: 287] loss: 0.0007045880192890763\n",
      "[step: 288] loss: 0.0007205393048934639\n",
      "[step: 289] loss: 0.0007284683524630964\n",
      "[step: 290] loss: 0.0007174327620305121\n",
      "[step: 291] loss: 0.0006934794364497066\n",
      "[step: 292] loss: 0.0006840821006335318\n",
      "[step: 293] loss: 0.0006954308482818305\n",
      "[step: 294] loss: 0.00070838542887941\n",
      "[step: 295] loss: 0.0007059177733026445\n",
      "[step: 296] loss: 0.0006909502553753555\n",
      "[step: 297] loss: 0.0006823186995461583\n",
      "[step: 298] loss: 0.0006855214596726\n",
      "[step: 299] loss: 0.0006904429174028337\n",
      "[step: 300] loss: 0.0006900179432705045\n",
      "[step: 301] loss: 0.0006861280999146402\n",
      "[step: 302] loss: 0.0006850869976915419\n",
      "[step: 303] loss: 0.0006861647707410157\n",
      "[step: 304] loss: 0.0006840571877546608\n",
      "[step: 305] loss: 0.0006794995861127973\n",
      "[step: 306] loss: 0.0006764997960999608\n",
      "[step: 307] loss: 0.0006780116818845272\n",
      "[step: 308] loss: 0.0006813942454755306\n",
      "[step: 309] loss: 0.0006820766720920801\n",
      "[step: 310] loss: 0.000679994176607579\n",
      "[step: 311] loss: 0.0006774470675736666\n",
      "[step: 312] loss: 0.0006765282014384866\n",
      "[step: 313] loss: 0.0006765216821804643\n",
      "[step: 314] loss: 0.0006755873328074813\n",
      "[step: 315] loss: 0.0006735959905199707\n",
      "[step: 316] loss: 0.0006718263612128794\n",
      "[step: 317] loss: 0.0006714411429129541\n",
      "[step: 318] loss: 0.0006720786914229393\n",
      "[step: 319] loss: 0.0006725366692990065\n",
      "[step: 320] loss: 0.0006722647231072187\n",
      "[step: 321] loss: 0.0006716394564136863\n",
      "[step: 322] loss: 0.0006714730989187956\n",
      "[step: 323] loss: 0.0006720548844896257\n",
      "[step: 324] loss: 0.0006730618770234287\n",
      "[step: 325] loss: 0.0006741224206052721\n",
      "[step: 326] loss: 0.0006752815097570419\n",
      "[step: 327] loss: 0.0006771500338800251\n",
      "[step: 328] loss: 0.0006806528544984758\n",
      "[step: 329] loss: 0.0006866373587399721\n",
      "[step: 330] loss: 0.0006964713102206588\n",
      "[step: 331] loss: 0.0007117049535736442\n",
      "[step: 332] loss: 0.0007354337139986455\n",
      "[step: 333] loss: 0.0007697955588810146\n",
      "[step: 334] loss: 0.0008173977839760482\n",
      "[step: 335] loss: 0.0008687963709235191\n",
      "[step: 336] loss: 0.000909020600374788\n",
      "[step: 337] loss: 0.0008981988648883998\n",
      "[step: 338] loss: 0.0008284496725536883\n",
      "[step: 339] loss: 0.0007284439634531736\n",
      "[step: 340] loss: 0.0006739532691426575\n",
      "[step: 341] loss: 0.0006933743716217577\n",
      "[step: 342] loss: 0.0007418082095682621\n",
      "[step: 343] loss: 0.000758233480155468\n",
      "[step: 344] loss: 0.0007300188881345093\n",
      "[step: 345] loss: 0.0006994580035097897\n",
      "[step: 346] loss: 0.0006931991665624082\n",
      "[step: 347] loss: 0.0007023706566542387\n",
      "[step: 348] loss: 0.0006937311263754964\n",
      "[step: 349] loss: 0.000679996213875711\n",
      "[step: 350] loss: 0.0006815697997808456\n",
      "[step: 351] loss: 0.0006942992913536727\n",
      "[step: 352] loss: 0.0006995703442953527\n",
      "[step: 353] loss: 0.0006779984687454998\n",
      "[step: 354] loss: 0.0006615007878281176\n",
      "[step: 355] loss: 0.0006650274735875428\n",
      "[step: 356] loss: 0.0006770874606445432\n",
      "[step: 357] loss: 0.0006803902797400951\n",
      "[step: 358] loss: 0.0006695704068988562\n",
      "[step: 359] loss: 0.0006641232175752521\n",
      "[step: 360] loss: 0.0006679196376353502\n",
      "[step: 361] loss: 0.0006701013771817088\n",
      "[step: 362] loss: 0.0006643593078479171\n",
      "[step: 363] loss: 0.0006573618738912046\n",
      "[step: 364] loss: 0.0006577258463948965\n",
      "[step: 365] loss: 0.0006626826943829656\n",
      "[step: 366] loss: 0.0006640870124101639\n",
      "[step: 367] loss: 0.00066085607977584\n",
      "[step: 368] loss: 0.0006581982597708702\n",
      "[step: 369] loss: 0.0006590460543520749\n",
      "[step: 370] loss: 0.0006608328549191356\n",
      "[step: 371] loss: 0.0006593714933842421\n",
      "[step: 372] loss: 0.0006560742622241378\n",
      "[step: 373] loss: 0.0006536222063004971\n",
      "[step: 374] loss: 0.0006536524742841721\n",
      "[step: 375] loss: 0.0006545079522766173\n",
      "[step: 376] loss: 0.0006540757603943348\n",
      "[step: 377] loss: 0.000652362359687686\n",
      "[step: 378] loss: 0.000650700181722641\n",
      "[step: 379] loss: 0.0006502541364170611\n",
      "[step: 380] loss: 0.0006507343496195972\n",
      "[step: 381] loss: 0.0006510252715088427\n",
      "[step: 382] loss: 0.0006505572819150984\n",
      "[step: 383] loss: 0.0006495184497907758\n",
      "[step: 384] loss: 0.0006486784550361335\n",
      "[step: 385] loss: 0.0006485244957730174\n",
      "[step: 386] loss: 0.0006489756633527577\n",
      "[step: 387] loss: 0.0006497502909041941\n",
      "[step: 388] loss: 0.0006506505305878818\n",
      "[step: 389] loss: 0.0006523157353512943\n",
      "[step: 390] loss: 0.0006557887536473572\n",
      "[step: 391] loss: 0.0006633963203057647\n",
      "[step: 392] loss: 0.0006789684994146228\n",
      "[step: 393] loss: 0.0007095318869687617\n",
      "[step: 394] loss: 0.0007678337278775871\n",
      "[step: 395] loss: 0.0008701966144144535\n",
      "[step: 396] loss: 0.0010275142267346382\n",
      "[step: 397] loss: 0.0011919686803594232\n",
      "[step: 398] loss: 0.00122497184202075\n",
      "[step: 399] loss: 0.0010374672710895538\n",
      "[step: 400] loss: 0.0008132001967169344\n",
      "[step: 401] loss: 0.0008291691774502397\n",
      "[step: 402] loss: 0.0009545335778966546\n",
      "[step: 403] loss: 0.0007632842171005905\n",
      "[step: 404] loss: 0.0006990920519456267\n",
      "[step: 405] loss: 0.0008635686244815588\n",
      "[step: 406] loss: 0.0007764225592836738\n",
      "[step: 407] loss: 0.0006620204076170921\n",
      "[step: 408] loss: 0.0007252266514115036\n",
      "[step: 409] loss: 0.0007327846833504736\n",
      "[step: 410] loss: 0.0006615170859731734\n",
      "[step: 411] loss: 0.0006901502492837608\n",
      "[step: 412] loss: 0.0007038862677291036\n",
      "[step: 413] loss: 0.0006555005675181746\n",
      "[step: 414] loss: 0.0006737173534929752\n",
      "[step: 415] loss: 0.0006816788227297366\n",
      "[step: 416] loss: 0.000653505849186331\n",
      "[step: 417] loss: 0.0006682342500425875\n",
      "[step: 418] loss: 0.0006654853350482881\n",
      "[step: 419] loss: 0.0006469791987910867\n",
      "[step: 420] loss: 0.000659691693726927\n",
      "[step: 421] loss: 0.000656132644508034\n",
      "[step: 422] loss: 0.0006460703443735838\n",
      "[step: 423] loss: 0.0006544794305227697\n",
      "[step: 424] loss: 0.0006495939451269805\n",
      "[step: 425] loss: 0.0006435608374886215\n",
      "[step: 426] loss: 0.0006492901011370122\n",
      "[step: 427] loss: 0.0006462031160481274\n",
      "[step: 428] loss: 0.0006425648462027311\n",
      "[step: 429] loss: 0.0006458148709498346\n",
      "[step: 430] loss: 0.0006427186308428645\n",
      "[step: 431] loss: 0.0006402468425221741\n",
      "[step: 432] loss: 0.0006423923186957836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 433] loss: 0.0006404604646377265\n",
      "[step: 434] loss: 0.0006387790199369192\n",
      "[step: 435] loss: 0.0006403254228644073\n",
      "[step: 436] loss: 0.0006384609150700271\n",
      "[step: 437] loss: 0.0006367881433106959\n",
      "[step: 438] loss: 0.0006379251717589796\n",
      "[step: 439] loss: 0.000636666954960674\n",
      "[step: 440] loss: 0.0006351922056637704\n",
      "[step: 441] loss: 0.0006360241095535457\n",
      "[step: 442] loss: 0.0006355135119520128\n",
      "[step: 443] loss: 0.0006340826512314379\n",
      "[step: 444] loss: 0.0006342199631035328\n",
      "[step: 445] loss: 0.0006341025582514703\n",
      "[step: 446] loss: 0.0006329857860691845\n",
      "[step: 447] loss: 0.0006325670401565731\n",
      "[step: 448] loss: 0.0006325396243482828\n",
      "[step: 449] loss: 0.0006317109218798578\n",
      "[step: 450] loss: 0.0006311556790024042\n",
      "[step: 451] loss: 0.0006311885663308203\n",
      "[step: 452] loss: 0.0006306986324489117\n",
      "[step: 453] loss: 0.0006299460073933005\n",
      "[step: 454] loss: 0.0006298028747551143\n",
      "[step: 455] loss: 0.0006296189967542887\n",
      "[step: 456] loss: 0.0006290454766713083\n",
      "[step: 457] loss: 0.0006286457646638155\n",
      "[step: 458] loss: 0.0006285099661909044\n",
      "[step: 459] loss: 0.0006281795795075595\n",
      "[step: 460] loss: 0.0006278208456933498\n",
      "[step: 461] loss: 0.0006277902284637094\n",
      "[step: 462] loss: 0.0006280087982304394\n",
      "[step: 463] loss: 0.0006284330156631768\n",
      "[step: 464] loss: 0.0006296700448729098\n",
      "[step: 465] loss: 0.0006326834554784\n",
      "[step: 466] loss: 0.0006388886249624193\n",
      "[step: 467] loss: 0.0006514576962217689\n",
      "[step: 468] loss: 0.0006759640527889132\n",
      "[step: 469] loss: 0.0007211485644802451\n",
      "[step: 470] loss: 0.0007881163037382066\n",
      "[step: 471] loss: 0.0008710573893040419\n",
      "[step: 472] loss: 0.0009351091575808823\n",
      "[step: 473] loss: 0.0009601698257029057\n",
      "[step: 474] loss: 0.0009241785737685859\n",
      "[step: 475] loss: 0.0008338004699908197\n",
      "[step: 476] loss: 0.0006912984536029398\n",
      "[step: 477] loss: 0.0006474360707215965\n",
      "[step: 478] loss: 0.0007243515574373305\n",
      "[step: 479] loss: 0.0007682527648285031\n",
      "[step: 480] loss: 0.0007236883975565434\n",
      "[step: 481] loss: 0.0006615749443881214\n",
      "[step: 482] loss: 0.0006534814019687474\n",
      "[step: 483] loss: 0.0006784054567106068\n",
      "[step: 484] loss: 0.0006774090579710901\n",
      "[step: 485] loss: 0.0006756868096999824\n",
      "[step: 486] loss: 0.000666524691041559\n",
      "[step: 487] loss: 0.0006354208453558385\n",
      "[step: 488] loss: 0.0006437855190597475\n",
      "[step: 489] loss: 0.0006697395001538098\n",
      "[step: 490] loss: 0.0006576690939255059\n",
      "[step: 491] loss: 0.0006371431518346071\n",
      "[step: 492] loss: 0.0006347805610857904\n",
      "[step: 493] loss: 0.0006371720228344202\n",
      "[step: 494] loss: 0.0006354874349199235\n",
      "[step: 495] loss: 0.0006375803495757282\n",
      "[step: 496] loss: 0.0006401837454177439\n",
      "[step: 497] loss: 0.000629155954811722\n",
      "[step: 498] loss: 0.000621135754045099\n",
      "[step: 499] loss: 0.0006305963615886867\n",
      "[step: 500] loss: 0.0006330801988951862\n",
      "[step: 501] loss: 0.0006273113540373743\n",
      "[step: 502] loss: 0.0006275582709349692\n",
      "[step: 503] loss: 0.0006250388105399907\n",
      "[step: 504] loss: 0.0006194891757331789\n",
      "[step: 505] loss: 0.0006215060129761696\n",
      "[step: 506] loss: 0.0006248617428354919\n",
      "[step: 507] loss: 0.0006234179600141943\n",
      "[step: 508] loss: 0.0006210082792676985\n",
      "[step: 509] loss: 0.0006196472095325589\n",
      "[step: 510] loss: 0.0006178547046147287\n",
      "[step: 511] loss: 0.0006165719241835177\n",
      "[step: 512] loss: 0.0006176354363560677\n",
      "[step: 513] loss: 0.0006184654776006937\n",
      "[step: 514] loss: 0.0006174165173433721\n",
      "[step: 515] loss: 0.0006168300751596689\n",
      "[step: 516] loss: 0.000616527337115258\n",
      "[step: 517] loss: 0.000614409742411226\n",
      "[step: 518] loss: 0.0006129827816039324\n",
      "[step: 519] loss: 0.0006135505391284823\n",
      "[step: 520] loss: 0.0006137292948551476\n",
      "[step: 521] loss: 0.0006130365072749555\n",
      "[step: 522] loss: 0.000613007228821516\n",
      "[step: 523] loss: 0.0006132382550276816\n",
      "[step: 524] loss: 0.0006129389512352645\n",
      "[step: 525] loss: 0.0006122204358689487\n",
      "[step: 526] loss: 0.000611503841355443\n",
      "[step: 527] loss: 0.0006109158275648952\n",
      "[step: 528] loss: 0.0006103423656895757\n",
      "[step: 529] loss: 0.0006097491714172065\n",
      "[step: 530] loss: 0.0006093864794820547\n",
      "[step: 531] loss: 0.000609041890129447\n",
      "[step: 532] loss: 0.0006084595224820077\n",
      "[step: 533] loss: 0.0006079887389205396\n",
      "[step: 534] loss: 0.0006078789010643959\n",
      "[step: 535] loss: 0.0006077116122469306\n",
      "[step: 536] loss: 0.0006073660333640873\n",
      "[step: 537] loss: 0.0006070862873457372\n",
      "[step: 538] loss: 0.0006069451337680221\n",
      "[step: 539] loss: 0.0006068029906600714\n",
      "[step: 540] loss: 0.0006067306385375559\n",
      "[step: 541] loss: 0.0006068988004699349\n",
      "[step: 542] loss: 0.0006075133569538593\n",
      "[step: 543] loss: 0.0006088126683607697\n",
      "[step: 544] loss: 0.0006112817209213972\n",
      "[step: 545] loss: 0.0006159816402941942\n",
      "[step: 546] loss: 0.0006249865400604904\n",
      "[step: 547] loss: 0.0006419950514100492\n",
      "[step: 548] loss: 0.0006736882496625185\n",
      "[step: 549] loss: 0.000731922744307667\n",
      "[step: 550] loss: 0.0008304251823574305\n",
      "[step: 551] loss: 0.0009787795133888721\n",
      "[step: 552] loss: 0.0011311041889712214\n",
      "[step: 553] loss: 0.0011698153102770448\n",
      "[step: 554] loss: 0.0009887375636026263\n",
      "[step: 555] loss: 0.0007108499412424862\n",
      "[step: 556] loss: 0.000649890920612961\n",
      "[step: 557] loss: 0.0007885061204433441\n",
      "[step: 558] loss: 0.0008448249427601695\n",
      "[step: 559] loss: 0.0007607018342241645\n",
      "[step: 560] loss: 0.0006780452094972134\n",
      "[step: 561] loss: 0.0006943037151359022\n",
      "[step: 562] loss: 0.0007497987244278193\n",
      "[step: 563] loss: 0.00070740602677688\n",
      "[step: 564] loss: 0.0006481864838860929\n",
      "[step: 565] loss: 0.0006528070662170649\n",
      "[step: 566] loss: 0.0006900124135427177\n",
      "[step: 567] loss: 0.0006893714307807386\n",
      "[step: 568] loss: 0.0006302997935563326\n",
      "[step: 569] loss: 0.0006201199139468372\n",
      "[step: 570] loss: 0.0006690225563943386\n",
      "[step: 571] loss: 0.0006572003476321697\n",
      "[step: 572] loss: 0.0006131053087301552\n",
      "[step: 573] loss: 0.0006224953685887158\n",
      "[step: 574] loss: 0.000641846505459398\n",
      "[step: 575] loss: 0.0006322840927168727\n",
      "[step: 576] loss: 0.0006153906579129398\n",
      "[step: 577] loss: 0.0006136383162811399\n",
      "[step: 578] loss: 0.0006217596819624305\n",
      "[step: 579] loss: 0.0006211040890775621\n",
      "[step: 580] loss: 0.0006127998349256814\n",
      "[step: 581] loss: 0.0006093443371355534\n",
      "[step: 582] loss: 0.0006088201189413667\n",
      "[step: 583] loss: 0.0006101967301219702\n",
      "[step: 584] loss: 0.0006127480301074684\n",
      "[step: 585] loss: 0.0006060602026991546\n",
      "[step: 586] loss: 0.000599488674197346\n",
      "[step: 587] loss: 0.0006058707367628813\n",
      "[step: 588] loss: 0.0006090127280913293\n",
      "[step: 589] loss: 0.0006020092405378819\n",
      "[step: 590] loss: 0.0005990186473354697\n",
      "[step: 591] loss: 0.0006008111522533\n",
      "[step: 592] loss: 0.0006014297832734883\n",
      "[step: 593] loss: 0.0006007058545947075\n",
      "[step: 594] loss: 0.0005987517652101815\n",
      "[step: 595] loss: 0.0005974233499728143\n",
      "[step: 596] loss: 0.0005970142083242536\n",
      "[step: 597] loss: 0.0005970234051346779\n",
      "[step: 598] loss: 0.0005978546105325222\n",
      "[step: 599] loss: 0.0005974126397632062\n",
      "[step: 600] loss: 0.0005947494646534324\n",
      "[step: 601] loss: 0.0005937749519944191\n",
      "[step: 602] loss: 0.000594954180996865\n",
      "[step: 603] loss: 0.0005950370687060058\n",
      "[step: 604] loss: 0.0005940707633271813\n",
      "[step: 605] loss: 0.0005936151719652116\n",
      "[step: 606] loss: 0.0005930339684709907\n",
      "[step: 607] loss: 0.000592442462220788\n",
      "[step: 608] loss: 0.0005924495053477585\n",
      "[step: 609] loss: 0.0005930103943683207\n",
      "[step: 610] loss: 0.0005942468415014446\n",
      "[step: 611] loss: 0.0005961311981081963\n",
      "[step: 612] loss: 0.000599810853600502\n",
      "[step: 613] loss: 0.000606417772360146\n",
      "[step: 614] loss: 0.0006209709099493921\n",
      "[step: 615] loss: 0.0006392229115590453\n",
      "[step: 616] loss: 0.0006616260507144034\n",
      "[step: 617] loss: 0.0006532268016599119\n",
      "[step: 618] loss: 0.0006345724686980247\n",
      "[step: 619] loss: 0.0006069582886993885\n",
      "[step: 620] loss: 0.0005958361434750259\n",
      "[step: 621] loss: 0.0006019414286129177\n",
      "[step: 622] loss: 0.0006152057321742177\n",
      "[step: 623] loss: 0.0006159076001495123\n",
      "[step: 624] loss: 0.0005963827716186643\n",
      "[step: 625] loss: 0.0005887452862225473\n",
      "[step: 626] loss: 0.000601033098064363\n",
      "[step: 627] loss: 0.0006050188094377518\n",
      "[step: 628] loss: 0.0005945790326222777\n",
      "[step: 629] loss: 0.0005887573934160173\n",
      "[step: 630] loss: 0.000593382865190506\n",
      "[step: 631] loss: 0.0005974890082143247\n",
      "[step: 632] loss: 0.0005941904382780194\n",
      "[step: 633] loss: 0.0005908799939788878\n",
      "[step: 634] loss: 0.0005933945067226887\n",
      "[step: 635] loss: 0.0006015911349095404\n",
      "[step: 636] loss: 0.0006100813625380397\n",
      "[step: 637] loss: 0.0006216258043423295\n",
      "[step: 638] loss: 0.0006501378957182169\n",
      "[step: 639] loss: 0.0006978806923143566\n",
      "[step: 640] loss: 0.000751657469663769\n",
      "[step: 641] loss: 0.0007857683813199401\n",
      "[step: 642] loss: 0.0007911925786174834\n",
      "[step: 643] loss: 0.0007793714758008718\n",
      "[step: 644] loss: 0.0007333233952522278\n",
      "[step: 645] loss: 0.0006620274507440627\n",
      "[step: 646] loss: 0.0006112902774475515\n",
      "[step: 647] loss: 0.0006130893598310649\n",
      "[step: 648] loss: 0.0006634318269789219\n",
      "[step: 649] loss: 0.0006883940077386796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 650] loss: 0.0006466303602792323\n",
      "[step: 651] loss: 0.0006062648608349264\n",
      "[step: 652] loss: 0.0006144858198240399\n",
      "[step: 653] loss: 0.0006143605569377542\n",
      "[step: 654] loss: 0.0005990443169139326\n",
      "[step: 655] loss: 0.0006068235961720347\n",
      "[step: 656] loss: 0.0006235038163140416\n",
      "[step: 657] loss: 0.0006139206816442311\n",
      "[step: 658] loss: 0.0005895250360481441\n",
      "[step: 659] loss: 0.0005911628250032663\n",
      "[step: 660] loss: 0.0005983305745758116\n",
      "[step: 661] loss: 0.0005916472291573882\n",
      "[step: 662] loss: 0.0005940214032307267\n",
      "[step: 663] loss: 0.0006035558180883527\n",
      "[step: 664] loss: 0.0006010393262840807\n",
      "[step: 665] loss: 0.0005911397165618837\n",
      "[step: 666] loss: 0.0005876595969311893\n",
      "[step: 667] loss: 0.0005861197132617235\n",
      "[step: 668] loss: 0.0005821493687108159\n",
      "[step: 669] loss: 0.0005802747327834368\n",
      "[step: 670] loss: 0.0005835810443386436\n",
      "[step: 671] loss: 0.0005868095904588699\n",
      "[step: 672] loss: 0.000585220055654645\n",
      "[step: 673] loss: 0.000582676031626761\n",
      "[step: 674] loss: 0.000582357868552208\n",
      "[step: 675] loss: 0.0005819913349114358\n",
      "[step: 676] loss: 0.0005790044669993222\n",
      "[step: 677] loss: 0.0005761238862760365\n",
      "[step: 678] loss: 0.0005760898347944021\n",
      "[step: 679] loss: 0.0005770519492216408\n",
      "[step: 680] loss: 0.0005762293003499508\n",
      "[step: 681] loss: 0.0005747964605689049\n",
      "[step: 682] loss: 0.0005750887794420123\n",
      "[step: 683] loss: 0.0005763992667198181\n",
      "[step: 684] loss: 0.0005771194701083004\n",
      "[step: 685] loss: 0.0005777639453299344\n",
      "[step: 686] loss: 0.0005797967314720154\n",
      "[step: 687] loss: 0.0005836915806867182\n",
      "[step: 688] loss: 0.0005901483236812055\n",
      "[step: 689] loss: 0.0006020981236360967\n",
      "[step: 690] loss: 0.0006225890247151256\n",
      "[step: 691] loss: 0.0006598960608243942\n",
      "[step: 692] loss: 0.000710857508238405\n",
      "[step: 693] loss: 0.0007820670725777745\n",
      "[step: 694] loss: 0.0008368159760721028\n",
      "[step: 695] loss: 0.0008565622847527266\n",
      "[step: 696] loss: 0.0008327202522195876\n",
      "[step: 697] loss: 0.0007322412566281855\n",
      "[step: 698] loss: 0.0006424624589271843\n",
      "[step: 699] loss: 0.0006249929429031909\n",
      "[step: 700] loss: 0.0006511486717499793\n",
      "[step: 701] loss: 0.0007000249461270869\n",
      "[step: 702] loss: 0.0006978093879297376\n",
      "[step: 703] loss: 0.0006310571334324777\n",
      "[step: 704] loss: 0.00058655597968027\n",
      "[step: 705] loss: 0.000616608012933284\n",
      "[step: 706] loss: 0.0006305155693553388\n",
      "[step: 707] loss: 0.0006091324612498283\n",
      "[step: 708] loss: 0.0006149305845610797\n",
      "[step: 709] loss: 0.0006071804091334343\n",
      "[step: 710] loss: 0.0005815097247250378\n",
      "[step: 711] loss: 0.0005818806239403784\n",
      "[step: 712] loss: 0.0006001466535963118\n",
      "[step: 713] loss: 0.0006013720412738621\n",
      "[step: 714] loss: 0.000595852907281369\n",
      "[step: 715] loss: 0.00059446383966133\n",
      "[step: 716] loss: 0.0005822498351335526\n",
      "[step: 717] loss: 0.000575675570871681\n",
      "[step: 718] loss: 0.0005779237253591418\n",
      "[step: 719] loss: 0.0005767378606833518\n",
      "[step: 720] loss: 0.0005793339223600924\n",
      "[step: 721] loss: 0.0005827790591865778\n",
      "[step: 722] loss: 0.0005769465351477265\n",
      "[step: 723] loss: 0.0005702954367734492\n",
      "[step: 724] loss: 0.0005722158239223063\n",
      "[step: 725] loss: 0.0005711253616027534\n",
      "[step: 726] loss: 0.0005681029870174825\n",
      "[step: 727] loss: 0.0005719907931052148\n",
      "[step: 728] loss: 0.0005748389521613717\n",
      "[step: 729] loss: 0.000571416923776269\n",
      "[step: 730] loss: 0.0005699312896467745\n",
      "[step: 731] loss: 0.0005703154602088034\n",
      "[step: 732] loss: 0.000567435345146805\n",
      "[step: 733] loss: 0.0005649992963299155\n",
      "[step: 734] loss: 0.000565021182410419\n",
      "[step: 735] loss: 0.0005642096512019634\n",
      "[step: 736] loss: 0.0005632624379359186\n",
      "[step: 737] loss: 0.0005638714064843953\n",
      "[step: 738] loss: 0.0005641081952489913\n",
      "[step: 739] loss: 0.0005637745489366353\n",
      "[step: 740] loss: 0.0005648485966958106\n",
      "[step: 741] loss: 0.000566880451515317\n",
      "[step: 742] loss: 0.0005688611418008804\n",
      "[step: 743] loss: 0.0005727266543544829\n",
      "[step: 744] loss: 0.0005804187967441976\n",
      "[step: 745] loss: 0.0005933791981078684\n",
      "[step: 746] loss: 0.0006112224655225873\n",
      "[step: 747] loss: 0.0006410273490473628\n",
      "[step: 748] loss: 0.0006754706846550107\n",
      "[step: 749] loss: 0.000714918423909694\n",
      "[step: 750] loss: 0.0007329974323511124\n",
      "[step: 751] loss: 0.0007258477271534503\n",
      "[step: 752] loss: 0.0006905990303494036\n",
      "[step: 753] loss: 0.0006374731892719865\n",
      "[step: 754] loss: 0.000595662510022521\n",
      "[step: 755] loss: 0.0005810724105685949\n",
      "[step: 756] loss: 0.0005895070498809218\n",
      "[step: 757] loss: 0.0006109240930527449\n",
      "[step: 758] loss: 0.0006249319412745535\n",
      "[step: 759] loss: 0.0006198178743943572\n",
      "[step: 760] loss: 0.0005947067984379828\n",
      "[step: 761] loss: 0.0005709400284104049\n",
      "[step: 762] loss: 0.0005653721163980663\n",
      "[step: 763] loss: 0.0005769632989540696\n",
      "[step: 764] loss: 0.0005890050670132041\n",
      "[step: 765] loss: 0.000589044822845608\n",
      "[step: 766] loss: 0.0005825405241921544\n",
      "[step: 767] loss: 0.0005786176770925522\n",
      "[step: 768] loss: 0.0005806779954582453\n",
      "[step: 769] loss: 0.0005770710995420814\n",
      "[step: 770] loss: 0.00056776701239869\n",
      "[step: 771] loss: 0.0005593451205641031\n",
      "[step: 772] loss: 0.0005596310365945101\n",
      "[step: 773] loss: 0.0005630013765767217\n",
      "[step: 774] loss: 0.0005610829102806747\n",
      "[step: 775] loss: 0.0005570690846070647\n",
      "[step: 776] loss: 0.0005581784644164145\n",
      "[step: 777] loss: 0.000563186826184392\n",
      "[step: 778] loss: 0.0005651502870023251\n",
      "[step: 779] loss: 0.000563508365303278\n",
      "[step: 780] loss: 0.0005626625497825444\n",
      "[step: 781] loss: 0.0005647113430313766\n",
      "[step: 782] loss: 0.000567674869671464\n",
      "[step: 783] loss: 0.0005689439713023603\n",
      "[step: 784] loss: 0.0005689067766070366\n",
      "[step: 785] loss: 0.0005696634179912508\n",
      "[step: 786] loss: 0.0005726617528125644\n",
      "[step: 787] loss: 0.0005770418210886419\n",
      "[step: 788] loss: 0.0005825310945510864\n",
      "[step: 789] loss: 0.0005877749645151198\n",
      "[step: 790] loss: 0.0005949713522568345\n",
      "[step: 791] loss: 0.0006026637856848538\n",
      "[step: 792] loss: 0.0006139299948699772\n",
      "[step: 793] loss: 0.0006198353949002922\n",
      "[step: 794] loss: 0.000621472077909857\n",
      "[step: 795] loss: 0.0006134773720987141\n",
      "[step: 796] loss: 0.0006025054026395082\n",
      "[step: 797] loss: 0.0005932750063948333\n",
      "[step: 798] loss: 0.000583494664169848\n",
      "[step: 799] loss: 0.000573633355088532\n",
      "[step: 800] loss: 0.0005632851389236748\n",
      "[step: 801] loss: 0.0005544355371966958\n",
      "[step: 802] loss: 0.0005515394732356071\n",
      "[step: 803] loss: 0.0005548109766095877\n",
      "[step: 804] loss: 0.0005615464760921896\n",
      "[step: 805] loss: 0.000567391631193459\n",
      "[step: 806] loss: 0.0005702134221792221\n",
      "[step: 807] loss: 0.0005693601560778916\n",
      "[step: 808] loss: 0.0005692613194696605\n",
      "[step: 809] loss: 0.0005745883099734783\n",
      "[step: 810] loss: 0.0005863579572178423\n",
      "[step: 811] loss: 0.0006033175159245729\n",
      "[step: 812] loss: 0.0006171245477162302\n",
      "[step: 813] loss: 0.0006253886385820806\n",
      "[step: 814] loss: 0.0006271484890021384\n",
      "[step: 815] loss: 0.0006323502748273313\n",
      "[step: 816] loss: 0.000644247978925705\n",
      "[step: 817] loss: 0.0006399178528226912\n",
      "[step: 818] loss: 0.0006202431977726519\n",
      "[step: 819] loss: 0.0005869505112059414\n",
      "[step: 820] loss: 0.0005603347672149539\n",
      "[step: 821] loss: 0.0005568531923927367\n",
      "[step: 822] loss: 0.000570132804568857\n",
      "[step: 823] loss: 0.0005751171847805381\n",
      "[step: 824] loss: 0.000566986680496484\n",
      "[step: 825] loss: 0.0005581714794971049\n",
      "[step: 826] loss: 0.000567565904930234\n",
      "[step: 827] loss: 0.0005867195432074368\n",
      "[step: 828] loss: 0.000588825496379286\n",
      "[step: 829] loss: 0.0005833696923218668\n",
      "[step: 830] loss: 0.0005860062665306032\n",
      "[step: 831] loss: 0.0005974824307486415\n",
      "[step: 832] loss: 0.0006033740355633199\n",
      "[step: 833] loss: 0.0005977390101179481\n",
      "[step: 834] loss: 0.0005855574854649603\n",
      "[step: 835] loss: 0.0005768613773398101\n",
      "[step: 836] loss: 0.00057121372083202\n",
      "[step: 837] loss: 0.000563606561627239\n",
      "[step: 838] loss: 0.0005531223141588271\n",
      "[step: 839] loss: 0.0005448880838230252\n",
      "[step: 840] loss: 0.0005445785936899483\n",
      "[step: 841] loss: 0.0005484759458340704\n",
      "[step: 842] loss: 0.0005491076735779643\n",
      "[step: 843] loss: 0.0005462729604914784\n",
      "[step: 844] loss: 0.0005461227265186608\n",
      "[step: 845] loss: 0.0005501021514646709\n",
      "[step: 846] loss: 0.0005538178374990821\n",
      "[step: 847] loss: 0.0005528301699087024\n",
      "[step: 848] loss: 0.0005516872042790055\n",
      "[step: 849] loss: 0.0005546085303649306\n",
      "[step: 850] loss: 0.0005629497463814914\n",
      "[step: 851] loss: 0.0005761437932960689\n",
      "[step: 852] loss: 0.0005902625853195786\n",
      "[step: 853] loss: 0.000612900301348418\n",
      "[step: 854] loss: 0.0006442615413106978\n",
      "[step: 855] loss: 0.0006929764640517533\n",
      "[step: 856] loss: 0.0007564994739368558\n",
      "[step: 857] loss: 0.000782773073296994\n",
      "[step: 858] loss: 0.0007656536181457341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 859] loss: 0.0007096666959114373\n",
      "[step: 860] loss: 0.0006452472880482674\n",
      "[step: 861] loss: 0.0005871141911484301\n",
      "[step: 862] loss: 0.0005680017056874931\n",
      "[step: 863] loss: 0.0006181075004860759\n",
      "[step: 864] loss: 0.0006360624684020877\n",
      "[step: 865] loss: 0.0006030644290149212\n",
      "[step: 866] loss: 0.0005886249127797782\n",
      "[step: 867] loss: 0.0005724986549466848\n",
      "[step: 868] loss: 0.0005517310928553343\n",
      "[step: 869] loss: 0.0005811998271383345\n",
      "[step: 870] loss: 0.0006073315162211657\n",
      "[step: 871] loss: 0.0005739969783462584\n",
      "[step: 872] loss: 0.0005573468515649438\n",
      "[step: 873] loss: 0.0005633686669170856\n",
      "[step: 874] loss: 0.0005503225256688893\n",
      "[step: 875] loss: 0.0005498379468917847\n",
      "[step: 876] loss: 0.0005685650394298136\n",
      "[step: 877] loss: 0.0005673662526533008\n",
      "[step: 878] loss: 0.0005495466175489128\n",
      "[step: 879] loss: 0.000547354226000607\n",
      "[step: 880] loss: 0.0005452950135804713\n",
      "[step: 881] loss: 0.0005385341355577111\n",
      "[step: 882] loss: 0.0005434567574411631\n",
      "[step: 883] loss: 0.0005502441199496388\n",
      "[step: 884] loss: 0.0005455130594782531\n",
      "[step: 885] loss: 0.0005397510249167681\n",
      "[step: 886] loss: 0.0005417350330390036\n",
      "[step: 887] loss: 0.0005387008422985673\n",
      "[step: 888] loss: 0.0005323521327227354\n",
      "[step: 889] loss: 0.0005341063952073455\n",
      "[step: 890] loss: 0.0005375912878662348\n",
      "[step: 891] loss: 0.0005354236927814782\n",
      "[step: 892] loss: 0.0005347505793906748\n",
      "[step: 893] loss: 0.0005376790068112314\n",
      "[step: 894] loss: 0.0005365977413021028\n",
      "[step: 895] loss: 0.0005337799666449428\n",
      "[step: 896] loss: 0.0005340835778042674\n",
      "[step: 897] loss: 0.0005359517526812851\n",
      "[step: 898] loss: 0.0005353506421670318\n",
      "[step: 899] loss: 0.0005345474928617477\n",
      "[step: 900] loss: 0.0005365376709960401\n",
      "[step: 901] loss: 0.0005397664499469101\n",
      "[step: 902] loss: 0.0005426738644018769\n",
      "[step: 903] loss: 0.0005484663415700197\n",
      "[step: 904] loss: 0.0005607104976661503\n",
      "[step: 905] loss: 0.0005787659320048988\n",
      "[step: 906] loss: 0.0006101871840655804\n",
      "[step: 907] loss: 0.0006537436274811625\n",
      "[step: 908] loss: 0.0007230598712339997\n",
      "[step: 909] loss: 0.0007991172606125474\n",
      "[step: 910] loss: 0.0008530397317372262\n",
      "[step: 911] loss: 0.000866143440362066\n",
      "[step: 912] loss: 0.0007818975136615336\n",
      "[step: 913] loss: 0.0006426812033168972\n",
      "[step: 914] loss: 0.0005572987138293684\n",
      "[step: 915] loss: 0.0006224919925443828\n",
      "[step: 916] loss: 0.00072313635610044\n",
      "[step: 917] loss: 0.000677659991197288\n",
      "[step: 918] loss: 0.0005949964397586882\n",
      "[step: 919] loss: 0.0005969846388325095\n",
      "[step: 920] loss: 0.0005879803793504834\n",
      "[step: 921] loss: 0.0005545610329136252\n",
      "[step: 922] loss: 0.0006039757281541824\n",
      "[step: 923] loss: 0.0006254893960431218\n",
      "[step: 924] loss: 0.0005615988629870117\n",
      "[step: 925] loss: 0.0005708745447918773\n",
      "[step: 926] loss: 0.000564874499104917\n",
      "[step: 927] loss: 0.000538892054464668\n",
      "[step: 928] loss: 0.0005701201735064387\n",
      "[step: 929] loss: 0.0005664906348101795\n",
      "[step: 930] loss: 0.0005420802044682205\n",
      "[step: 931] loss: 0.0005540181300602853\n",
      "[step: 932] loss: 0.0005426030838862062\n",
      "[step: 933] loss: 0.0005305517115630209\n",
      "[step: 934] loss: 0.0005494982469826937\n",
      "[step: 935] loss: 0.0005461527616716921\n",
      "[step: 936] loss: 0.0005371816805563867\n",
      "[step: 937] loss: 0.0005460991524159908\n",
      "[step: 938] loss: 0.000533457612618804\n",
      "[step: 939] loss: 0.0005281530902720988\n",
      "[step: 940] loss: 0.0005348193808458745\n",
      "[step: 941] loss: 0.000527893309481442\n",
      "[step: 942] loss: 0.0005291650886647403\n",
      "[step: 943] loss: 0.0005352255539037287\n",
      "[step: 944] loss: 0.0005299114855006337\n",
      "[step: 945] loss: 0.0005311740096658468\n",
      "[step: 946] loss: 0.0005355309695005417\n",
      "[step: 947] loss: 0.0005308527615852654\n",
      "[step: 948] loss: 0.0005310516571626067\n",
      "[step: 949] loss: 0.0005344162927940488\n",
      "[step: 950] loss: 0.0005329935229383409\n",
      "[step: 951] loss: 0.0005357548361644149\n",
      "[step: 952] loss: 0.0005419013905338943\n",
      "[step: 953] loss: 0.0005461796536110342\n",
      "[step: 954] loss: 0.0005530111957341433\n",
      "[step: 955] loss: 0.0005659122834913433\n",
      "[step: 956] loss: 0.0005763659719377756\n",
      "[step: 957] loss: 0.0005901252734474838\n",
      "[step: 958] loss: 0.0006028497591614723\n",
      "[step: 959] loss: 0.0006123756757006049\n",
      "[step: 960] loss: 0.0006099510937929153\n",
      "[step: 961] loss: 0.0005985096795484424\n",
      "[step: 962] loss: 0.0005767248221673071\n",
      "[step: 963] loss: 0.0005485300207510591\n",
      "[step: 964] loss: 0.0005269927787594497\n",
      "[step: 965] loss: 0.0005187134374864399\n",
      "[step: 966] loss: 0.000521675159689039\n",
      "[step: 967] loss: 0.0005318940384313464\n",
      "[step: 968] loss: 0.0005432141479104757\n",
      "[step: 969] loss: 0.0005497875972650945\n",
      "[step: 970] loss: 0.0005510802147909999\n",
      "[step: 971] loss: 0.0005495447549037635\n",
      "[step: 972] loss: 0.0005496863159351051\n",
      "[step: 973] loss: 0.0005460195825435221\n",
      "[step: 974] loss: 0.0005394459003582597\n",
      "[step: 975] loss: 0.0005295005976222456\n",
      "[step: 976] loss: 0.0005206834757700562\n",
      "[step: 977] loss: 0.0005157088744454086\n",
      "[step: 978] loss: 0.0005156091647222638\n",
      "[step: 979] loss: 0.0005182531895115972\n",
      "[step: 980] loss: 0.000520734756719321\n",
      "[step: 981] loss: 0.0005217735306359828\n",
      "[step: 982] loss: 0.0005223423358984292\n",
      "[step: 983] loss: 0.0005244362400844693\n",
      "[step: 984] loss: 0.0005280286422930658\n",
      "[step: 985] loss: 0.0005326234386302531\n",
      "[step: 986] loss: 0.0005372202722355723\n",
      "[step: 987] loss: 0.0005410314188338816\n",
      "[step: 988] loss: 0.000545262941159308\n",
      "[step: 989] loss: 0.0005516965175047517\n",
      "[step: 990] loss: 0.0005623024189844728\n",
      "[step: 991] loss: 0.0005839047371409833\n",
      "[step: 992] loss: 0.0006061961757950485\n",
      "[step: 993] loss: 0.0006339302635751665\n",
      "[step: 994] loss: 0.0006305866991169751\n",
      "[step: 995] loss: 0.0006232491577975452\n",
      "[step: 996] loss: 0.0006383153377100825\n",
      "[step: 997] loss: 0.0006574423750862479\n",
      "[step: 998] loss: 0.0006425244500860572\n",
      "[step: 999] loss: 0.0006012837984599173\n",
      "[step: 1000] loss: 0.0005481467233039439\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "#load data\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/taxi_data/City_Counted_TaxiMach_Link_Dataset_Full_201704 -201708.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data preprocessing\n",
    "train_size = int(len(xy)*0.7)\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 27\n",
    "output_dim = 25\n",
    "learning_rate = 0.1\n",
    "iterations = 1001\n",
    "\n",
    "xy= xy[:,:27]\n",
    "test_set = xy[train_size:]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "data_max = np.max(b,1)\n",
    "data_min = np.min(b,1)\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "train_set = xy[:train_size]\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/Users/yeseo/ML_with_Taxidata/code/save_model_code/model/my_model.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"/Users/yeseo/ML_with_Taxidata/code/save_model_code/model/my_model.ckpt\")\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    Y = graph.get_tensor_by_name(\"Y:0\")\n",
    "   # rnn = graph.get_tensor_by_name(\"rnn:0\")\n",
    "    Y_pred = graph.get_tensor_by_name(\"rnn/fully_connected/BiasAdd:0\")\n",
    "    loss = graph.get_tensor_by_name(\"rnn/Mean:0\")\n",
    "    \n",
    "    train = graph.get_operation_by_name(\"rnn/Adam\")\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train,loss],feed_dict={X:trainX, Y:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss))\n",
    "\n",
    "\n",
    "    \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X:validationX})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
