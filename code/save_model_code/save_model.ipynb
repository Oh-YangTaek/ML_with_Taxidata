{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.22916409373283386\n",
      "[step: 1] loss: 0.16104911267757416\n",
      "[step: 2] loss: 0.11711139976978302\n",
      "[step: 3] loss: 0.08661767095327377\n",
      "[step: 4] loss: 0.0655556246638298\n",
      "[step: 5] loss: 0.050411976873874664\n",
      "[step: 6] loss: 0.04113463684916496\n",
      "[step: 7] loss: 0.03385419026017189\n",
      "[step: 8] loss: 0.03009498305618763\n",
      "[step: 9] loss: 0.027184823527932167\n",
      "[step: 10] loss: 0.02454414777457714\n",
      "[step: 11] loss: 0.022536572068929672\n",
      "[step: 12] loss: 0.021073197945952415\n",
      "[step: 13] loss: 0.020195741206407547\n",
      "[step: 14] loss: 0.018596967682242393\n",
      "[step: 15] loss: 0.01751396618783474\n",
      "[step: 16] loss: 0.016156751662492752\n",
      "[step: 17] loss: 0.015727002173662186\n",
      "[step: 18] loss: 0.015159176662564278\n",
      "[step: 19] loss: 0.01455342024564743\n",
      "[step: 20] loss: 0.014658471569418907\n",
      "[step: 21] loss: 0.013619748875498772\n",
      "[step: 22] loss: 0.013438323512673378\n",
      "[step: 23] loss: 0.01246720552444458\n",
      "[step: 24] loss: 0.012137931771576405\n",
      "[step: 25] loss: 0.011427147313952446\n",
      "[step: 26] loss: 0.011033736169338226\n",
      "[step: 27] loss: 0.01052689366042614\n",
      "[step: 28] loss: 0.010128366760909557\n",
      "[step: 29] loss: 0.009730488061904907\n",
      "[step: 30] loss: 0.00946920644491911\n",
      "[step: 31] loss: 0.009154037572443485\n",
      "[step: 32] loss: 0.00906218122690916\n",
      "[step: 33] loss: 0.008713096380233765\n",
      "[step: 34] loss: 0.008484279736876488\n",
      "[step: 35] loss: 0.008034974336624146\n",
      "[step: 36] loss: 0.007841079495847225\n",
      "[step: 37] loss: 0.007459227927029133\n",
      "[step: 38] loss: 0.0072902487590909\n",
      "[step: 39] loss: 0.006943580228835344\n",
      "[step: 40] loss: 0.006786709651350975\n",
      "[step: 41] loss: 0.006519977003335953\n",
      "[step: 42] loss: 0.0063341655768454075\n",
      "[step: 43] loss: 0.006175559479743242\n",
      "[step: 44] loss: 0.006096415221691132\n",
      "[step: 45] loss: 0.005809547379612923\n",
      "[step: 46] loss: 0.005806508474051952\n",
      "[step: 47] loss: 0.005469572264701128\n",
      "[step: 48] loss: 0.005446034017950296\n",
      "[step: 49] loss: 0.005210670176893473\n",
      "[step: 50] loss: 0.005110647063702345\n",
      "[step: 51] loss: 0.004963571205735207\n",
      "[step: 52] loss: 0.004840341862291098\n",
      "[step: 53] loss: 0.004788043908774853\n",
      "[step: 54] loss: 0.004641582258045673\n",
      "[step: 55] loss: 0.004609862342476845\n",
      "[step: 56] loss: 0.004472598899155855\n",
      "[step: 57] loss: 0.004417296964675188\n",
      "[step: 58] loss: 0.004341341555118561\n",
      "[step: 59] loss: 0.0042642876505851746\n",
      "[step: 60] loss: 0.00419333903118968\n",
      "[step: 61] loss: 0.004123060032725334\n",
      "[step: 62] loss: 0.004067153204232454\n",
      "[step: 63] loss: 0.003998235333710909\n",
      "[step: 64] loss: 0.003942417912185192\n",
      "[step: 65] loss: 0.0038852058351039886\n",
      "[step: 66] loss: 0.003829844295978546\n",
      "[step: 67] loss: 0.0037938335444778204\n",
      "[step: 68] loss: 0.0037356168031692505\n",
      "[step: 69] loss: 0.0037151514552533627\n",
      "[step: 70] loss: 0.003656527027487755\n",
      "[step: 71] loss: 0.003632744774222374\n",
      "[step: 72] loss: 0.0035979191306978464\n",
      "[step: 73] loss: 0.0035528242588043213\n",
      "[step: 74] loss: 0.0035300219897180796\n",
      "[step: 75] loss: 0.0034857732243835926\n",
      "[step: 76] loss: 0.003459702478721738\n",
      "[step: 77] loss: 0.0034295283257961273\n",
      "[step: 78] loss: 0.0033913343213498592\n",
      "[step: 79] loss: 0.0033707749098539352\n",
      "[step: 80] loss: 0.0033384852577000856\n",
      "[step: 81] loss: 0.0033081185538321733\n",
      "[step: 82] loss: 0.0032866825349628925\n",
      "[step: 83] loss: 0.003255304880440235\n",
      "[step: 84] loss: 0.003231320297345519\n",
      "[step: 85] loss: 0.003212899202480912\n",
      "[step: 86] loss: 0.003185585141181946\n",
      "[step: 87] loss: 0.0031631719321012497\n",
      "[step: 88] loss: 0.003146054456010461\n",
      "[step: 89] loss: 0.0031247299630194902\n",
      "[step: 90] loss: 0.0031072169076651335\n",
      "[step: 91] loss: 0.0031026615761220455\n",
      "[step: 92] loss: 0.003107338910922408\n",
      "[step: 93] loss: 0.003162023378536105\n",
      "[step: 94] loss: 0.003212246112525463\n",
      "[step: 95] loss: 0.0033023422583937645\n",
      "[step: 96] loss: 0.0032141786068677902\n",
      "[step: 97] loss: 0.0030803822446614504\n",
      "[step: 98] loss: 0.0029786766972392797\n",
      "[step: 99] loss: 0.0030406895093619823\n",
      "[step: 100] loss: 0.0031219625379890203\n",
      "[step: 101] loss: 0.003036247566342354\n",
      "[step: 102] loss: 0.0029477188363671303\n",
      "[step: 103] loss: 0.0029337978921830654\n",
      "[step: 104] loss: 0.0029836848843842745\n",
      "[step: 105] loss: 0.0029937876388430595\n",
      "[step: 106] loss: 0.002894192235544324\n",
      "[step: 107] loss: 0.002875551814213395\n",
      "[step: 108] loss: 0.002909668954089284\n",
      "[step: 109] loss: 0.0028968455735594034\n",
      "[step: 110] loss: 0.002874598605558276\n",
      "[step: 111] loss: 0.0028244773857295513\n",
      "[step: 112] loss: 0.0028094470035284758\n",
      "[step: 113] loss: 0.002836622064933181\n",
      "[step: 114] loss: 0.0028281884733587503\n",
      "[step: 115] loss: 0.002799013629555702\n",
      "[step: 116] loss: 0.0027590098325163126\n",
      "[step: 117] loss: 0.002763141645118594\n",
      "[step: 118] loss: 0.0027917250990867615\n",
      "[step: 119] loss: 0.002772235544398427\n",
      "[step: 120] loss: 0.0027487522456794977\n",
      "[step: 121] loss: 0.002741806674748659\n",
      "[step: 122] loss: 0.002736140275374055\n",
      "[step: 123] loss: 0.0027232817374169827\n",
      "[step: 124] loss: 0.002716755960136652\n",
      "[step: 125] loss: 0.0027312440797686577\n",
      "[step: 126] loss: 0.00272119278088212\n",
      "[step: 127] loss: 0.002686617663130164\n",
      "[step: 128] loss: 0.002658687997609377\n",
      "[step: 129] loss: 0.002693086862564087\n",
      "[step: 130] loss: 0.002660902217030525\n",
      "[step: 131] loss: 0.0026470315642654896\n",
      "[step: 132] loss: 0.0026210148353129625\n",
      "[step: 133] loss: 0.0026173340156674385\n",
      "[step: 134] loss: 0.002597775077447295\n",
      "[step: 135] loss: 0.002591882599517703\n",
      "[step: 136] loss: 0.0025946006644517183\n",
      "[step: 137] loss: 0.002587169175967574\n",
      "[step: 138] loss: 0.0026205864269286394\n",
      "[step: 139] loss: 0.00270638894289732\n",
      "[step: 140] loss: 0.0029778636526316404\n",
      "[step: 141] loss: 0.002916685538366437\n",
      "[step: 142] loss: 0.002798821311444044\n",
      "[step: 143] loss: 0.0029511484317481518\n",
      "[step: 144] loss: 0.0029457963537424803\n",
      "[step: 145] loss: 0.002693489193916321\n",
      "[step: 146] loss: 0.002820710651576519\n",
      "[step: 147] loss: 0.0026915522757917643\n",
      "[step: 148] loss: 0.0027396725490689278\n",
      "[step: 149] loss: 0.002785610733553767\n",
      "[step: 150] loss: 0.002648405497893691\n",
      "[step: 151] loss: 0.002630998846143484\n",
      "[step: 152] loss: 0.002622984116896987\n",
      "[step: 153] loss: 0.0026704135816544294\n",
      "[step: 154] loss: 0.0026201775763183832\n",
      "[step: 155] loss: 0.0026293820701539516\n",
      "[step: 156] loss: 0.002539153676480055\n",
      "[step: 157] loss: 0.002553215017542243\n",
      "[step: 158] loss: 0.002538609318435192\n",
      "[step: 159] loss: 0.0025535477325320244\n",
      "[step: 160] loss: 0.002520739333704114\n",
      "[step: 161] loss: 0.0024743094108998775\n",
      "[step: 162] loss: 0.00251235393807292\n",
      "[step: 163] loss: 0.0024730036966502666\n",
      "[step: 164] loss: 0.0024959221482276917\n",
      "[step: 165] loss: 0.002446046331897378\n",
      "[step: 166] loss: 0.002459541894495487\n",
      "[step: 167] loss: 0.0024571302346885204\n",
      "[step: 168] loss: 0.0024451466742902994\n",
      "[step: 169] loss: 0.002438548021018505\n",
      "[step: 170] loss: 0.0024111634120345116\n",
      "[step: 171] loss: 0.0024241425562649965\n",
      "[step: 172] loss: 0.0024105648044496775\n",
      "[step: 173] loss: 0.0024310173466801643\n",
      "[step: 174] loss: 0.002424962818622589\n",
      "[step: 175] loss: 0.0024123291950672865\n",
      "[step: 176] loss: 0.002406422048807144\n",
      "[step: 177] loss: 0.0023817343171685934\n",
      "[step: 178] loss: 0.00237710471265018\n",
      "[step: 179] loss: 0.0023729701060801744\n",
      "[step: 180] loss: 0.0023658687714487314\n",
      "[step: 181] loss: 0.0023753331042826176\n",
      "[step: 182] loss: 0.002358123194426298\n",
      "[step: 183] loss: 0.0023627423215657473\n",
      "[step: 184] loss: 0.002348196692764759\n",
      "[step: 185] loss: 0.002338187536224723\n",
      "[step: 186] loss: 0.002335440833121538\n",
      "[step: 187] loss: 0.0023280910681933165\n",
      "[step: 188] loss: 0.002319524995982647\n",
      "[step: 189] loss: 0.0023192313965409994\n",
      "[step: 190] loss: 0.002307233400642872\n",
      "[step: 191] loss: 0.002303557936102152\n",
      "[step: 192] loss: 0.002300261054188013\n",
      "[step: 193] loss: 0.002294631442055106\n",
      "[step: 194] loss: 0.002292992314323783\n",
      "[step: 195] loss: 0.0022968025878071785\n",
      "[step: 196] loss: 0.0023000070359557867\n",
      "[step: 197] loss: 0.0023276580031961203\n",
      "[step: 198] loss: 0.002398258075118065\n",
      "[step: 199] loss: 0.0025407234206795692\n",
      "[step: 200] loss: 0.0028882361948490143\n",
      "[step: 201] loss: 0.0030598321463912725\n",
      "[step: 202] loss: 0.0031206056009978056\n",
      "[step: 203] loss: 0.0024247742258012295\n",
      "[step: 204] loss: 0.0023881811648607254\n",
      "[step: 205] loss: 0.0027771980967372656\n",
      "[step: 206] loss: 0.0023825932294130325\n",
      "[step: 207] loss: 0.002363736042752862\n",
      "[step: 208] loss: 0.0025732845533639193\n",
      "[step: 209] loss: 0.002291209762915969\n",
      "[step: 210] loss: 0.0023745635990053415\n",
      "[step: 211] loss: 0.0024087277706712484\n",
      "[step: 212] loss: 0.002277425490319729\n",
      "[step: 213] loss: 0.002315702149644494\n",
      "[step: 214] loss: 0.002308465540409088\n",
      "[step: 215] loss: 0.002251978497952223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 216] loss: 0.0022598837967962027\n",
      "[step: 217] loss: 0.002278328873217106\n",
      "[step: 218] loss: 0.002217439003288746\n",
      "[step: 219] loss: 0.0022839675657451153\n",
      "[step: 220] loss: 0.002250839490443468\n",
      "[step: 221] loss: 0.0022127798292785883\n",
      "[step: 222] loss: 0.002264333888888359\n",
      "[step: 223] loss: 0.002201850526034832\n",
      "[step: 224] loss: 0.0022113961167633533\n",
      "[step: 225] loss: 0.0022134867031127214\n",
      "[step: 226] loss: 0.002191923325881362\n",
      "[step: 227] loss: 0.0021826718002557755\n",
      "[step: 228] loss: 0.0021869658958166838\n",
      "[step: 229] loss: 0.0021774820052087307\n",
      "[step: 230] loss: 0.002156891394406557\n",
      "[step: 231] loss: 0.002171956468373537\n",
      "[step: 232] loss: 0.0021579409949481487\n",
      "[step: 233] loss: 0.002141704550012946\n",
      "[step: 234] loss: 0.0021533865947276354\n",
      "[step: 235] loss: 0.002141553210094571\n",
      "[step: 236] loss: 0.002129362663254142\n",
      "[step: 237] loss: 0.0021339519880712032\n",
      "[step: 238] loss: 0.002123965648934245\n",
      "[step: 239] loss: 0.0021216163877397776\n",
      "[step: 240] loss: 0.0021176880691200495\n",
      "[step: 241] loss: 0.0021054467651993036\n",
      "[step: 242] loss: 0.0021104542538523674\n",
      "[step: 243] loss: 0.0021131148096174\n",
      "[step: 244] loss: 0.0020965624134987593\n",
      "[step: 245] loss: 0.00209353887476027\n",
      "[step: 246] loss: 0.0020922573748975992\n",
      "[step: 247] loss: 0.002088613575324416\n",
      "[step: 248] loss: 0.0020931630861014128\n",
      "[step: 249] loss: 0.0020860128570348024\n",
      "[step: 250] loss: 0.0020779669284820557\n",
      "[step: 251] loss: 0.00207238900475204\n",
      "[step: 252] loss: 0.002070177812129259\n",
      "[step: 253] loss: 0.002059266436845064\n",
      "[step: 254] loss: 0.0020570300985127687\n",
      "[step: 255] loss: 0.0020559837576001883\n",
      "[step: 256] loss: 0.002057909034192562\n",
      "[step: 257] loss: 0.0020647170022130013\n",
      "[step: 258] loss: 0.002118948381394148\n",
      "[step: 259] loss: 0.0021046807523816824\n",
      "[step: 260] loss: 0.002145725069567561\n",
      "[step: 261] loss: 0.002081651706248522\n",
      "[step: 262] loss: 0.0020489762537181377\n",
      "[step: 263] loss: 0.0020349035039544106\n",
      "[step: 264] loss: 0.002062405226752162\n",
      "[step: 265] loss: 0.0021299924701452255\n",
      "[step: 266] loss: 0.0021540664602071047\n",
      "[step: 267] loss: 0.002224355237558484\n",
      "[step: 268] loss: 0.0020722453482449055\n",
      "[step: 269] loss: 0.00205660006031394\n",
      "[step: 270] loss: 0.0021754468325525522\n",
      "[step: 271] loss: 0.002210028003901243\n",
      "[step: 272] loss: 0.0021481395233422518\n",
      "[step: 273] loss: 0.0020468621514737606\n",
      "[step: 274] loss: 0.0020497972145676613\n",
      "[step: 275] loss: 0.0020739638712257147\n",
      "[step: 276] loss: 0.0021042199805378914\n",
      "[step: 277] loss: 0.002030777744948864\n",
      "[step: 278] loss: 0.0020084737334400415\n",
      "[step: 279] loss: 0.0020077431108802557\n",
      "[step: 280] loss: 0.0019755796529352665\n",
      "[step: 281] loss: 0.0020059181842952967\n",
      "[step: 282] loss: 0.001997910672798753\n",
      "[step: 283] loss: 0.0019800695590674877\n",
      "[step: 284] loss: 0.0019554283935576677\n",
      "[step: 285] loss: 0.0019628519657999277\n",
      "[step: 286] loss: 0.0019524188246577978\n",
      "[step: 287] loss: 0.0019611618481576443\n",
      "[step: 288] loss: 0.00197254354134202\n",
      "[step: 289] loss: 0.001985938288271427\n",
      "[step: 290] loss: 0.0019920370541512966\n",
      "[step: 291] loss: 0.00206967000849545\n",
      "[step: 292] loss: 0.0019764387980103493\n",
      "[step: 293] loss: 0.0019769829232245684\n",
      "[step: 294] loss: 0.0019760201685130596\n",
      "[step: 295] loss: 0.002044296357780695\n",
      "[step: 296] loss: 0.0020449559669941664\n",
      "[step: 297] loss: 0.0020697868894785643\n",
      "[step: 298] loss: 0.0020954133942723274\n",
      "[step: 299] loss: 0.001990231918171048\n",
      "[step: 300] loss: 0.001994833117350936\n",
      "[step: 301] loss: 0.0020034865010529757\n",
      "[step: 302] loss: 0.0021436228416860104\n",
      "[step: 303] loss: 0.0021331822499632835\n",
      "[step: 304] loss: 0.002022842410951853\n",
      "[step: 305] loss: 0.001997889718040824\n",
      "[step: 306] loss: 0.0019459921168163419\n",
      "[step: 307] loss: 0.0019924426451325417\n",
      "[step: 308] loss: 0.001979171996936202\n",
      "[step: 309] loss: 0.0019556183833628893\n",
      "[step: 310] loss: 0.0019193593179807067\n",
      "[step: 311] loss: 0.00190771056804806\n",
      "[step: 312] loss: 0.0019219444366171956\n",
      "[step: 313] loss: 0.001937352935783565\n",
      "[step: 314] loss: 0.0019292101496830583\n",
      "[step: 315] loss: 0.0018723903922364116\n",
      "[step: 316] loss: 0.0018870551139116287\n",
      "[step: 317] loss: 0.0018801884725689888\n",
      "[step: 318] loss: 0.0019098195480182767\n",
      "[step: 319] loss: 0.001884309109300375\n",
      "[step: 320] loss: 0.0019173630280420184\n",
      "[step: 321] loss: 0.001853441703133285\n",
      "[step: 322] loss: 0.0018738694489002228\n",
      "[step: 323] loss: 0.0018679074710235\n",
      "[step: 324] loss: 0.0019047377863898873\n",
      "[step: 325] loss: 0.0019158180803060532\n",
      "[step: 326] loss: 0.0019359525758773088\n",
      "[step: 327] loss: 0.0019299095729365945\n",
      "[step: 328] loss: 0.0018910098588094115\n",
      "[step: 329] loss: 0.0018387174932286143\n",
      "[step: 330] loss: 0.0018252048175781965\n",
      "[step: 331] loss: 0.0018706748960539699\n",
      "[step: 332] loss: 0.001897393143735826\n",
      "[step: 333] loss: 0.001984331989660859\n",
      "[step: 334] loss: 0.002153877168893814\n",
      "[step: 335] loss: 0.0021015587262809277\n",
      "[step: 336] loss: 0.001982440473511815\n",
      "[step: 337] loss: 0.0018731581512838602\n",
      "[step: 338] loss: 0.0019927015528082848\n",
      "[step: 339] loss: 0.0020328592509031296\n",
      "[step: 340] loss: 0.0020499948877841234\n",
      "[step: 341] loss: 0.001970872050151229\n",
      "[step: 342] loss: 0.0019966778345406055\n",
      "[step: 343] loss: 0.0018324470147490501\n",
      "[step: 344] loss: 0.0019340690923854709\n",
      "[step: 345] loss: 0.001899485127069056\n",
      "[step: 346] loss: 0.001919067115522921\n",
      "[step: 347] loss: 0.001829855260439217\n",
      "[step: 348] loss: 0.0018864150624722242\n",
      "[step: 349] loss: 0.0019005571957677603\n",
      "[step: 350] loss: 0.001982123591005802\n",
      "[step: 351] loss: 0.0018826181767508388\n",
      "[step: 352] loss: 0.0018428018083795905\n",
      "[step: 353] loss: 0.0018240439239889383\n",
      "[step: 354] loss: 0.0018487132620066404\n",
      "[step: 355] loss: 0.0018338019726797938\n",
      "[step: 356] loss: 0.0018206145614385605\n",
      "[step: 357] loss: 0.001779781887307763\n",
      "[step: 358] loss: 0.0018021947471424937\n",
      "[step: 359] loss: 0.0017972671194002032\n",
      "[step: 360] loss: 0.0017930022440850735\n",
      "[step: 361] loss: 0.0017757382011041045\n",
      "[step: 362] loss: 0.0017502551199868321\n",
      "[step: 363] loss: 0.001756144454702735\n",
      "[step: 364] loss: 0.001755995792336762\n",
      "[step: 365] loss: 0.001761595718562603\n",
      "[step: 366] loss: 0.0017561014974489808\n",
      "[step: 367] loss: 0.001739894738420844\n",
      "[step: 368] loss: 0.0017330722184851766\n",
      "[step: 369] loss: 0.0017301262123510242\n",
      "[step: 370] loss: 0.0017348764231428504\n",
      "[step: 371] loss: 0.0017400938086211681\n",
      "[step: 372] loss: 0.0017418934730812907\n",
      "[step: 373] loss: 0.0017391941510140896\n",
      "[step: 374] loss: 0.001747222151607275\n",
      "[step: 375] loss: 0.0017447923310101032\n",
      "[step: 376] loss: 0.0017505051800981164\n",
      "[step: 377] loss: 0.0017674458213150501\n",
      "[step: 378] loss: 0.001781678176485002\n",
      "[step: 379] loss: 0.0018021983560174704\n",
      "[step: 380] loss: 0.0018148693488910794\n",
      "[step: 381] loss: 0.0018691838486120105\n",
      "[step: 382] loss: 0.001850264729000628\n",
      "[step: 383] loss: 0.001827050931751728\n",
      "[step: 384] loss: 0.0017746351659297943\n",
      "[step: 385] loss: 0.0017569467891007662\n",
      "[step: 386] loss: 0.0016990294679999352\n",
      "[step: 387] loss: 0.0016931503778323531\n",
      "[step: 388] loss: 0.0016986783593893051\n",
      "[step: 389] loss: 0.0017559017287567258\n",
      "[step: 390] loss: 0.0017998088151216507\n",
      "[step: 391] loss: 0.001868108636699617\n",
      "[step: 392] loss: 0.0020022185053676367\n",
      "[step: 393] loss: 0.0019672741182148457\n",
      "[step: 394] loss: 0.0018166068475693464\n",
      "[step: 395] loss: 0.0017179712885990739\n",
      "[step: 396] loss: 0.0017100831028074026\n",
      "[step: 397] loss: 0.0017488161101937294\n",
      "[step: 398] loss: 0.0017594500677660108\n",
      "[step: 399] loss: 0.0017575857928022742\n",
      "[step: 400] loss: 0.0017344896914437413\n",
      "[step: 401] loss: 0.0016621396644040942\n",
      "[step: 402] loss: 0.0016717350808903575\n",
      "[step: 403] loss: 0.0016678014071658254\n",
      "[step: 404] loss: 0.0017048993613570929\n",
      "[step: 405] loss: 0.0016915911110118032\n",
      "[step: 406] loss: 0.0016969821881502867\n",
      "[step: 407] loss: 0.0016471610870212317\n",
      "[step: 408] loss: 0.001650809426791966\n",
      "[step: 409] loss: 0.001633868901990354\n",
      "[step: 410] loss: 0.0016538618365302682\n",
      "[step: 411] loss: 0.0016721361316740513\n",
      "[step: 412] loss: 0.0017113527283072472\n",
      "[step: 413] loss: 0.0017124187434092164\n",
      "[step: 414] loss: 0.0017162984004244208\n",
      "[step: 415] loss: 0.0017387049738317728\n",
      "[step: 416] loss: 0.0017096727387979627\n",
      "[step: 417] loss: 0.0016678513493388891\n",
      "[step: 418] loss: 0.0016336622647941113\n",
      "[step: 419] loss: 0.0016308591002598405\n",
      "[step: 420] loss: 0.001606629346497357\n",
      "[step: 421] loss: 0.0016208643792197108\n",
      "[step: 422] loss: 0.0016316481633111835\n",
      "[step: 423] loss: 0.0016842313343659043\n",
      "[step: 424] loss: 0.0017141714924946427\n",
      "[step: 425] loss: 0.0017491646576672792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 426] loss: 0.0018468416528776288\n",
      "[step: 427] loss: 0.0018554345006123185\n",
      "[step: 428] loss: 0.0017953369533643126\n",
      "[step: 429] loss: 0.0016767618944868445\n",
      "[step: 430] loss: 0.0016181806568056345\n",
      "[step: 431] loss: 0.0016014951979741454\n",
      "[step: 432] loss: 0.0016513486625626683\n",
      "[step: 433] loss: 0.0016783202299848199\n",
      "[step: 434] loss: 0.0017041877144947648\n",
      "[step: 435] loss: 0.001662593800574541\n",
      "[step: 436] loss: 0.0016639282694086432\n",
      "[step: 437] loss: 0.0016650699544698\n",
      "[step: 438] loss: 0.0016017014859244227\n",
      "[step: 439] loss: 0.0015744863776490092\n",
      "[step: 440] loss: 0.0016318721463903785\n",
      "[step: 441] loss: 0.0016117618652060628\n",
      "[step: 442] loss: 0.0015853274380788207\n",
      "[step: 443] loss: 0.0015991973923519254\n",
      "[step: 444] loss: 0.001601541880518198\n",
      "[step: 445] loss: 0.0015638954937458038\n",
      "[step: 446] loss: 0.0015549898380413651\n",
      "[step: 447] loss: 0.0015769472811371088\n",
      "[step: 448] loss: 0.0015540665481239557\n",
      "[step: 449] loss: 0.0015401220880448818\n",
      "[step: 450] loss: 0.0015505686169490218\n",
      "[step: 451] loss: 0.0015595286386087537\n",
      "[step: 452] loss: 0.0015368747990578413\n",
      "[step: 453] loss: 0.0015323362313210964\n",
      "[step: 454] loss: 0.0015498611610382795\n",
      "[step: 455] loss: 0.0015573709970340133\n",
      "[step: 456] loss: 0.0015678174095228314\n",
      "[step: 457] loss: 0.0016319125425070524\n",
      "[step: 458] loss: 0.0017399780917912722\n",
      "[step: 459] loss: 0.0020902452524751425\n",
      "[step: 460] loss: 0.002185794757679105\n",
      "[step: 461] loss: 0.002258146181702614\n",
      "[step: 462] loss: 0.0017094475915655494\n",
      "[step: 463] loss: 0.0016750621143728495\n",
      "[step: 464] loss: 0.0017546560848131776\n",
      "[step: 465] loss: 0.0018874218221753836\n",
      "[step: 466] loss: 0.001973849255591631\n",
      "[step: 467] loss: 0.0017274155979976058\n",
      "[step: 468] loss: 0.0017330761766061187\n",
      "[step: 469] loss: 0.001744909561239183\n",
      "[step: 470] loss: 0.0017952645430341363\n",
      "[step: 471] loss: 0.0016463693464174867\n",
      "[step: 472] loss: 0.0016865450888872147\n",
      "[step: 473] loss: 0.0016995243495330215\n",
      "[step: 474] loss: 0.0016440399922430515\n",
      "[step: 475] loss: 0.001566163613460958\n",
      "[step: 476] loss: 0.0016557194758206606\n",
      "[step: 477] loss: 0.001583049539476633\n",
      "[step: 478] loss: 0.001606449601240456\n",
      "[step: 479] loss: 0.001524913008324802\n",
      "[step: 480] loss: 0.0015833128709346056\n",
      "[step: 481] loss: 0.0015353300841525197\n",
      "[step: 482] loss: 0.0015398846007883549\n",
      "[step: 483] loss: 0.0015044116880744696\n",
      "[step: 484] loss: 0.0015417683171108365\n",
      "[step: 485] loss: 0.0014993849908933043\n",
      "[step: 486] loss: 0.0015007053734734654\n",
      "[step: 487] loss: 0.001500324928201735\n",
      "[step: 488] loss: 0.0015017376281321049\n",
      "[step: 489] loss: 0.00149092439096421\n",
      "[step: 490] loss: 0.001476061763241887\n",
      "[step: 491] loss: 0.0014798538759350777\n",
      "[step: 492] loss: 0.0014764557126909494\n",
      "[step: 493] loss: 0.0014685587957501411\n",
      "[step: 494] loss: 0.0014495928771793842\n",
      "[step: 495] loss: 0.001459715305827558\n",
      "[step: 496] loss: 0.0014647720381617546\n",
      "[step: 497] loss: 0.0014567631296813488\n",
      "[step: 498] loss: 0.0014421524247154593\n",
      "[step: 499] loss: 0.0014414581237360835\n",
      "[step: 500] loss: 0.0014393663732334971\n",
      "[step: 501] loss: 0.0014370545977726579\n",
      "[step: 502] loss: 0.0014287007506936789\n",
      "[step: 503] loss: 0.0014313184656202793\n",
      "[step: 504] loss: 0.001423371839337051\n",
      "[step: 505] loss: 0.0014178280252963305\n",
      "[step: 506] loss: 0.0014085550792515278\n",
      "[step: 507] loss: 0.001406636438332498\n",
      "[step: 508] loss: 0.001407786039635539\n",
      "[step: 509] loss: 0.00140814995393157\n",
      "[step: 510] loss: 0.001406466355547309\n",
      "[step: 511] loss: 0.001404906390234828\n",
      "[step: 512] loss: 0.0014080106047913432\n",
      "[step: 513] loss: 0.0014112868811935186\n",
      "[step: 514] loss: 0.0014248201623558998\n",
      "[step: 515] loss: 0.0014417215716093779\n",
      "[step: 516] loss: 0.0014955095248296857\n",
      "[step: 517] loss: 0.0015252854209393263\n",
      "[step: 518] loss: 0.0015938415890559554\n",
      "[step: 519] loss: 0.0015976405702531338\n",
      "[step: 520] loss: 0.0016149412840604782\n",
      "[step: 521] loss: 0.0014892937615513802\n",
      "[step: 522] loss: 0.001428516348823905\n",
      "[step: 523] loss: 0.0014705209759995341\n",
      "[step: 524] loss: 0.0015533658443018794\n",
      "[step: 525] loss: 0.0014474556082859635\n",
      "[step: 526] loss: 0.001367754302918911\n",
      "[step: 527] loss: 0.0014175468822941184\n",
      "[step: 528] loss: 0.0014571105130016804\n",
      "[step: 529] loss: 0.0013782000169157982\n",
      "[step: 530] loss: 0.0013897896278649569\n",
      "[step: 531] loss: 0.0014495099894702435\n",
      "[step: 532] loss: 0.00142636988312006\n",
      "[step: 533] loss: 0.0013975752517580986\n",
      "[step: 534] loss: 0.001457776641473174\n",
      "[step: 535] loss: 0.0014217236312106252\n",
      "[step: 536] loss: 0.001409156946465373\n",
      "[step: 537] loss: 0.001447306014597416\n",
      "[step: 538] loss: 0.0014052160549908876\n",
      "[step: 539] loss: 0.0014197275741025805\n",
      "[step: 540] loss: 0.001410335418768227\n",
      "[step: 541] loss: 0.0013642337871715426\n",
      "[step: 542] loss: 0.0013375976122915745\n",
      "[step: 543] loss: 0.0013516278704628348\n",
      "[step: 544] loss: 0.0013038815231993794\n",
      "[step: 545] loss: 0.0013453229330480099\n",
      "[step: 546] loss: 0.0013438405003398657\n",
      "[step: 547] loss: 0.001371423015370965\n",
      "[step: 548] loss: 0.0013844426721334457\n",
      "[step: 549] loss: 0.0013799458974972367\n",
      "[step: 550] loss: 0.001411867793649435\n",
      "[step: 551] loss: 0.0013799458974972367\n",
      "[step: 552] loss: 0.0013406595680862665\n",
      "[step: 553] loss: 0.0013131704181432724\n",
      "[step: 554] loss: 0.0012902829330414534\n",
      "[step: 555] loss: 0.0012839093105867505\n",
      "[step: 556] loss: 0.0013025833759456873\n",
      "[step: 557] loss: 0.0013028724351897836\n",
      "[step: 558] loss: 0.0013605969725176692\n",
      "[step: 559] loss: 0.0013648027088493109\n",
      "[step: 560] loss: 0.0014020323287695646\n",
      "[step: 561] loss: 0.0014268120285123587\n",
      "[step: 562] loss: 0.001420586952008307\n",
      "[step: 563] loss: 0.0013709597988054156\n",
      "[step: 564] loss: 0.0013091576984152198\n",
      "[step: 565] loss: 0.0012689883587881923\n",
      "[step: 566] loss: 0.0012377346865832806\n",
      "[step: 567] loss: 0.0012354744831100106\n",
      "[step: 568] loss: 0.0012552918633446097\n",
      "[step: 569] loss: 0.0012859802227467299\n",
      "[step: 570] loss: 0.001292917411774397\n",
      "[step: 571] loss: 0.001283847144804895\n",
      "[step: 572] loss: 0.0012635781895369291\n",
      "[step: 573] loss: 0.0012377589009702206\n",
      "[step: 574] loss: 0.0012086142087355256\n",
      "[step: 575] loss: 0.0011997848050668836\n",
      "[step: 576] loss: 0.0011967971222475171\n",
      "[step: 577] loss: 0.001205693231895566\n",
      "[step: 578] loss: 0.001213239273056388\n",
      "[step: 579] loss: 0.001228185254149139\n",
      "[step: 580] loss: 0.0012363693676888943\n",
      "[step: 581] loss: 0.001247662235982716\n",
      "[step: 582] loss: 0.0012508375803008676\n",
      "[step: 583] loss: 0.0012588275130838156\n",
      "[step: 584] loss: 0.001260559307411313\n",
      "[step: 585] loss: 0.0012629819102585316\n",
      "[step: 586] loss: 0.0012465758481994271\n",
      "[step: 587] loss: 0.0012311734026297927\n",
      "[step: 588] loss: 0.001205470529384911\n",
      "[step: 589] loss: 0.001184202148579061\n",
      "[step: 590] loss: 0.0011649789521470666\n",
      "[step: 591] loss: 0.0011519660474732518\n",
      "[step: 592] loss: 0.0011475334176793694\n",
      "[step: 593] loss: 0.001147816888988018\n",
      "[step: 594] loss: 0.0011518789688125253\n",
      "[step: 595] loss: 0.0011586351320147514\n",
      "[step: 596] loss: 0.00116947281640023\n",
      "[step: 597] loss: 0.0011817754711955786\n",
      "[step: 598] loss: 0.0012053370010107756\n",
      "[step: 599] loss: 0.0012297731591388583\n",
      "[step: 600] loss: 0.0012713723117485642\n",
      "[step: 601] loss: 0.0013002646155655384\n",
      "[step: 602] loss: 0.0013417950831353664\n",
      "[step: 603] loss: 0.0013219703687354922\n",
      "[step: 604] loss: 0.0012789441971108317\n",
      "[step: 605] loss: 0.0011937228264287114\n",
      "[step: 606] loss: 0.0011298005701974034\n",
      "[step: 607] loss: 0.0011145995231345296\n",
      "[step: 608] loss: 0.0011443799594417214\n",
      "[step: 609] loss: 0.0011875395430251956\n",
      "[step: 610] loss: 0.0011980307754129171\n",
      "[step: 611] loss: 0.0011782646179199219\n",
      "[step: 612] loss: 0.0011338156182318926\n",
      "[step: 613] loss: 0.0011018902296200395\n",
      "[step: 614] loss: 0.0010984549298882484\n",
      "[step: 615] loss: 0.0011161689180880785\n",
      "[step: 616] loss: 0.0011368339182808995\n",
      "[step: 617] loss: 0.0011424485128372908\n",
      "[step: 618] loss: 0.0011346437968313694\n",
      "[step: 619] loss: 0.0011124414158985019\n",
      "[step: 620] loss: 0.0010920437052845955\n",
      "[step: 621] loss: 0.0010788211366161704\n",
      "[step: 622] loss: 0.0010754121467471123\n",
      "[step: 623] loss: 0.0010788447689265013\n",
      "[step: 624] loss: 0.001086411182768643\n",
      "[step: 625] loss: 0.0010975265176966786\n",
      "[step: 626] loss: 0.001107055926695466\n",
      "[step: 627] loss: 0.0011205285554751754\n",
      "[step: 628] loss: 0.0011302384082227945\n",
      "[step: 629] loss: 0.0011469221208244562\n",
      "[step: 630] loss: 0.001153885037638247\n",
      "[step: 631] loss: 0.0011653637047857046\n",
      "[step: 632] loss: 0.0011563411680981517\n",
      "[step: 633] loss: 0.0011421272065490484\n",
      "[step: 634] loss: 0.0011101709678769112\n",
      "[step: 635] loss: 0.00108129414729774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 636] loss: 0.0010566558921709657\n",
      "[step: 637] loss: 0.0010448717512190342\n",
      "[step: 638] loss: 0.0010433330899104476\n",
      "[step: 639] loss: 0.0010482316138222814\n",
      "[step: 640] loss: 0.0010570118902251124\n",
      "[step: 641] loss: 0.001066853990778327\n",
      "[step: 642] loss: 0.0010815846035256982\n",
      "[step: 643] loss: 0.0010914960876107216\n",
      "[step: 644] loss: 0.0011083773570135236\n",
      "[step: 645] loss: 0.0011060165707021952\n",
      "[step: 646] loss: 0.0011066475417464972\n",
      "[step: 647] loss: 0.0010909934062510729\n",
      "[step: 648] loss: 0.0010805731872096658\n",
      "[step: 649] loss: 0.0010666247690096498\n",
      "[step: 650] loss: 0.001052709762006998\n",
      "[step: 651] loss: 0.0010366588830947876\n",
      "[step: 652] loss: 0.0010219081304967403\n",
      "[step: 653] loss: 0.0010128453141078353\n",
      "[step: 654] loss: 0.001010125852189958\n",
      "[step: 655] loss: 0.0010099868522956967\n",
      "[step: 656] loss: 0.001009350293315947\n",
      "[step: 657] loss: 0.0010069760028272867\n",
      "[step: 658] loss: 0.0010054833255708218\n",
      "[step: 659] loss: 0.0010076977778226137\n",
      "[step: 660] loss: 0.0010156307835131884\n",
      "[step: 661] loss: 0.0010349355870857835\n",
      "[step: 662] loss: 0.0010670197661966085\n",
      "[step: 663] loss: 0.0011458820663392544\n",
      "[step: 664] loss: 0.0012459263671189547\n",
      "[step: 665] loss: 0.0014557349495589733\n",
      "[step: 666] loss: 0.0015466135228052735\n",
      "[step: 667] loss: 0.0015186922391876578\n",
      "[step: 668] loss: 0.0012362246634438634\n",
      "[step: 669] loss: 0.0010121879167854786\n",
      "[step: 670] loss: 0.001116970437578857\n",
      "[step: 671] loss: 0.0012736934004351497\n",
      "[step: 672] loss: 0.001182303880341351\n",
      "[step: 673] loss: 0.001046181540004909\n",
      "[step: 674] loss: 0.0010784751502797008\n",
      "[step: 675] loss: 0.001081362017430365\n",
      "[step: 676] loss: 0.0010674030054360628\n",
      "[step: 677] loss: 0.0011059644166380167\n",
      "[step: 678] loss: 0.0010543613461777568\n",
      "[step: 679] loss: 0.000996280461549759\n",
      "[step: 680] loss: 0.0010206337319687009\n",
      "[step: 681] loss: 0.0010420246981084347\n",
      "[step: 682] loss: 0.0010063688969239593\n",
      "[step: 683] loss: 0.0010012113489210606\n",
      "[step: 684] loss: 0.0010039524640887976\n",
      "[step: 685] loss: 0.0009731617756187916\n",
      "[step: 686] loss: 0.0009891832014545798\n",
      "[step: 687] loss: 0.00100145791657269\n",
      "[step: 688] loss: 0.000977949588559568\n",
      "[step: 689] loss: 0.0009822457795962691\n",
      "[step: 690] loss: 0.0009665315737947822\n",
      "[step: 691] loss: 0.0009576624142937362\n",
      "[step: 692] loss: 0.0009690636652521789\n",
      "[step: 693] loss: 0.0009632432484067976\n",
      "[step: 694] loss: 0.0009667715057730675\n",
      "[step: 695] loss: 0.0009658440831117332\n",
      "[step: 696] loss: 0.0009576722513884306\n",
      "[step: 697] loss: 0.0009573322022333741\n",
      "[step: 698] loss: 0.0009490188676863909\n",
      "[step: 699] loss: 0.0009454041137360036\n",
      "[step: 700] loss: 0.0009464645409025252\n",
      "[step: 701] loss: 0.0009390016202814877\n",
      "[step: 702] loss: 0.0009395850938744843\n",
      "[step: 703] loss: 0.0009403960430063307\n",
      "[step: 704] loss: 0.0009361754637211561\n",
      "[step: 705] loss: 0.0009379939874634147\n",
      "[step: 706] loss: 0.0009393715881742537\n",
      "[step: 707] loss: 0.0009420895366929471\n",
      "[step: 708] loss: 0.0009521183092147112\n",
      "[step: 709] loss: 0.0009735482162795961\n",
      "[step: 710] loss: 0.0010044033406302333\n",
      "[step: 711] loss: 0.001081633847206831\n",
      "[step: 712] loss: 0.0011651742970570922\n",
      "[step: 713] loss: 0.00130978983361274\n",
      "[step: 714] loss: 0.0012524245539680123\n",
      "[step: 715] loss: 0.0011078956304118037\n",
      "[step: 716] loss: 0.0009671163279563189\n",
      "[step: 717] loss: 0.0009498336003161967\n",
      "[step: 718] loss: 0.0010499069467186928\n",
      "[step: 719] loss: 0.0010785048361867666\n",
      "[step: 720] loss: 0.0010043276706710458\n",
      "[step: 721] loss: 0.0009333757916465402\n",
      "[step: 722] loss: 0.0009471702505834401\n",
      "[step: 723] loss: 0.0009953085100278258\n",
      "[step: 724] loss: 0.0009833509102463722\n",
      "[step: 725] loss: 0.0009365234873257577\n",
      "[step: 726] loss: 0.0009203627123497427\n",
      "[step: 727] loss: 0.0009454188402742147\n",
      "[step: 728] loss: 0.0009586295345798135\n",
      "[step: 729] loss: 0.0009455875842832029\n",
      "[step: 730] loss: 0.0009264573454856873\n",
      "[step: 731] loss: 0.0009131355909630656\n",
      "[step: 732] loss: 0.0009151120902970433\n",
      "[step: 733] loss: 0.0009279574151150882\n",
      "[step: 734] loss: 0.0009340174729004502\n",
      "[step: 735] loss: 0.0009259536163881421\n",
      "[step: 736] loss: 0.0009084369521588087\n",
      "[step: 737] loss: 0.0008993697119876742\n",
      "[step: 738] loss: 0.0009023051825352013\n",
      "[step: 739] loss: 0.0009069177322089672\n",
      "[step: 740] loss: 0.0009099997114390135\n",
      "[step: 741] loss: 0.000912621442694217\n",
      "[step: 742] loss: 0.0009180020424537361\n",
      "[step: 743] loss: 0.0009179599583148956\n",
      "[step: 744] loss: 0.0009137832093983889\n",
      "[step: 745] loss: 0.0009076044661924243\n",
      "[step: 746] loss: 0.0009052393143065274\n",
      "[step: 747] loss: 0.0009023413294926286\n",
      "[step: 748] loss: 0.0008997608674690127\n",
      "[step: 749] loss: 0.000896093319170177\n",
      "[step: 750] loss: 0.000895093020517379\n",
      "[step: 751] loss: 0.0008963131112977862\n",
      "[step: 752] loss: 0.0009002477745525539\n",
      "[step: 753] loss: 0.000904960441403091\n",
      "[step: 754] loss: 0.0009167546522803605\n",
      "[step: 755] loss: 0.0009363905992358923\n",
      "[step: 756] loss: 0.0009814053773880005\n",
      "[step: 757] loss: 0.001032938715070486\n",
      "[step: 758] loss: 0.0011209654621779919\n",
      "[step: 759] loss: 0.0011471313191577792\n",
      "[step: 760] loss: 0.0011414611944928765\n",
      "[step: 761] loss: 0.001015040441416204\n",
      "[step: 762] loss: 0.000901203544344753\n",
      "[step: 763] loss: 0.0008909564930945635\n",
      "[step: 764] loss: 0.0009613112779334188\n",
      "[step: 765] loss: 0.0010119915241375566\n",
      "[step: 766] loss: 0.0009639798663556576\n",
      "[step: 767] loss: 0.000891891133505851\n",
      "[step: 768] loss: 0.0008793189190328121\n",
      "[step: 769] loss: 0.0009214204037562013\n",
      "[step: 770] loss: 0.0009468343923799694\n",
      "[step: 771] loss: 0.0009147062082774937\n",
      "[step: 772] loss: 0.0008748606196604669\n",
      "[step: 773] loss: 0.00086965411901474\n",
      "[step: 774] loss: 0.0008944128057919443\n",
      "[step: 775] loss: 0.0009120667236857116\n",
      "[step: 776] loss: 0.0008988332701846957\n",
      "[step: 777] loss: 0.0008744143415242434\n",
      "[step: 778] loss: 0.0008587737684138119\n",
      "[step: 779] loss: 0.0008634330006316304\n",
      "[step: 780] loss: 0.0008788499981164932\n",
      "[step: 781] loss: 0.0008870664751157165\n",
      "[step: 782] loss: 0.0008848783327266574\n",
      "[step: 783] loss: 0.0008711479022167623\n",
      "[step: 784] loss: 0.0008578944252803922\n",
      "[step: 785] loss: 0.0008502318523824215\n",
      "[step: 786] loss: 0.0008494583889842033\n",
      "[step: 787] loss: 0.0008538524270989001\n",
      "[step: 788] loss: 0.0008605828043073416\n",
      "[step: 789] loss: 0.0008680332684889436\n",
      "[step: 790] loss: 0.0008746447274461389\n",
      "[step: 791] loss: 0.0008842415409162641\n",
      "[step: 792] loss: 0.0008925256552174687\n",
      "[step: 793] loss: 0.0009084090706892312\n",
      "[step: 794] loss: 0.0009219574858434498\n",
      "[step: 795] loss: 0.0009442329173907638\n",
      "[step: 796] loss: 0.0009564474457874894\n",
      "[step: 797] loss: 0.0009638915071263909\n",
      "[step: 798] loss: 0.0009439093410037458\n",
      "[step: 799] loss: 0.0009124896605499089\n",
      "[step: 800] loss: 0.0008712031412869692\n",
      "[step: 801] loss: 0.0008453840273432434\n",
      "[step: 802] loss: 0.0008403859683312476\n",
      "[step: 803] loss: 0.0008523741853423417\n",
      "[step: 804] loss: 0.0008688431698828936\n",
      "[step: 805] loss: 0.0008768231491558254\n",
      "[step: 806] loss: 0.0008768402622081339\n",
      "[step: 807] loss: 0.0008668221416883171\n",
      "[step: 808] loss: 0.0008569878991693258\n",
      "[step: 809] loss: 0.0008482714183628559\n",
      "[step: 810] loss: 0.0008401067461818457\n",
      "[step: 811] loss: 0.0008345675305463374\n",
      "[step: 812] loss: 0.0008312222198583186\n",
      "[step: 813] loss: 0.0008320342749357224\n",
      "[step: 814] loss: 0.0008372715674340725\n",
      "[step: 815] loss: 0.0008467569132335484\n",
      "[step: 816] loss: 0.0008600431610830128\n",
      "[step: 817] loss: 0.0008794716559350491\n",
      "[step: 818] loss: 0.0009034907561726868\n",
      "[step: 819] loss: 0.0009425364551134408\n",
      "[step: 820] loss: 0.0009742729016579688\n",
      "[step: 821] loss: 0.001019632793031633\n",
      "[step: 822] loss: 0.0010357144055888057\n",
      "[step: 823] loss: 0.001025665202178061\n",
      "[step: 824] loss: 0.0009757061488926411\n",
      "[step: 825] loss: 0.0009064561454579234\n",
      "[step: 826] loss: 0.0008590181241743267\n",
      "[step: 827] loss: 0.0008756528259254992\n",
      "[step: 828] loss: 0.0009246055851690471\n",
      "[step: 829] loss: 0.0009282025275751948\n",
      "[step: 830] loss: 0.0008737262105569243\n",
      "[step: 831] loss: 0.0008175899856723845\n",
      "[step: 832] loss: 0.0008267147932201624\n",
      "[step: 833] loss: 0.0008672098629176617\n",
      "[step: 834] loss: 0.0008731618872843683\n",
      "[step: 835] loss: 0.0008569685160182416\n",
      "[step: 836] loss: 0.0008574064122512937\n",
      "[step: 837] loss: 0.0008627393399365246\n",
      "[step: 838] loss: 0.0008461337420158088\n",
      "[step: 839] loss: 0.0008142259903252125\n",
      "[step: 840] loss: 0.0008083704160526395\n",
      "[step: 841] loss: 0.0008192945388145745\n",
      "[step: 842] loss: 0.0008198419236578047\n",
      "[step: 843] loss: 0.0008165915496647358\n",
      "[step: 844] loss: 0.0008253612322732806\n",
      "[step: 845] loss: 0.0008341989596374333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 846] loss: 0.0008396649500355124\n",
      "[step: 847] loss: 0.0008433747570961714\n",
      "[step: 848] loss: 0.0008560072164982557\n",
      "[step: 849] loss: 0.0008648925577290356\n",
      "[step: 850] loss: 0.0008628757786937058\n",
      "[step: 851] loss: 0.0008544012671336532\n",
      "[step: 852] loss: 0.0008527419995516539\n",
      "[step: 853] loss: 0.0008402870735153556\n",
      "[step: 854] loss: 0.0008226358913816512\n",
      "[step: 855] loss: 0.0008007965516299009\n",
      "[step: 856] loss: 0.0007904600352048874\n",
      "[step: 857] loss: 0.0007911825668998063\n",
      "[step: 858] loss: 0.0007948575657792389\n",
      "[step: 859] loss: 0.0007991709862835705\n",
      "[step: 860] loss: 0.0008033637423068285\n",
      "[step: 861] loss: 0.0008107558242045343\n",
      "[step: 862] loss: 0.0008172052330337465\n",
      "[step: 863] loss: 0.0008280651527456939\n",
      "[step: 864] loss: 0.0008494883659295738\n",
      "[step: 865] loss: 0.000885836489032954\n",
      "[step: 866] loss: 0.0009332750923931599\n",
      "[step: 867] loss: 0.000959084602072835\n",
      "[step: 868] loss: 0.0009393020300194621\n",
      "[step: 869] loss: 0.0008778161136433482\n",
      "[step: 870] loss: 0.000844032852910459\n",
      "[step: 871] loss: 0.0008794074528850615\n",
      "[step: 872] loss: 0.0008834762847982347\n",
      "[step: 873] loss: 0.0008240575552918017\n",
      "[step: 874] loss: 0.0007813932606950402\n",
      "[step: 875] loss: 0.0007930179126560688\n",
      "[step: 876] loss: 0.0008331153658218682\n",
      "[step: 877] loss: 0.000838491425383836\n",
      "[step: 878] loss: 0.0008257504086941481\n",
      "[step: 879] loss: 0.0008409502333961427\n",
      "[step: 880] loss: 0.0008652267861180007\n",
      "[step: 881] loss: 0.0008582024020142853\n",
      "[step: 882] loss: 0.0008473924244754016\n",
      "[step: 883] loss: 0.0008395736804232001\n",
      "[step: 884] loss: 0.0008229978266172111\n",
      "[step: 885] loss: 0.0007844387437216938\n",
      "[step: 886] loss: 0.0007728997152298689\n",
      "[step: 887] loss: 0.0007830610848031938\n",
      "[step: 888] loss: 0.0007886564708314836\n",
      "[step: 889] loss: 0.000794366467744112\n",
      "[step: 890] loss: 0.0008046358707360923\n",
      "[step: 891] loss: 0.0008160627912729979\n",
      "[step: 892] loss: 0.0008126673637889326\n",
      "[step: 893] loss: 0.0008028964512050152\n",
      "[step: 894] loss: 0.0007925251265987754\n",
      "[step: 895] loss: 0.0007882635109126568\n",
      "[step: 896] loss: 0.0007739362772554159\n",
      "[step: 897] loss: 0.0007617823430337012\n",
      "[step: 898] loss: 0.0007584127597510815\n",
      "[step: 899] loss: 0.0007585144485346973\n",
      "[step: 900] loss: 0.0007567157736048102\n",
      "[step: 901] loss: 0.0007550634327344596\n",
      "[step: 902] loss: 0.0007551416638307273\n",
      "[step: 903] loss: 0.0007605659193359315\n",
      "[step: 904] loss: 0.0007711839862167835\n",
      "[step: 905] loss: 0.000790599558968097\n",
      "[step: 906] loss: 0.0008324534283019602\n",
      "[step: 907] loss: 0.0009199983323924243\n",
      "[step: 908] loss: 0.0011251427931711078\n",
      "[step: 909] loss: 0.0013305586762726307\n",
      "[step: 910] loss: 0.0014858287759125233\n",
      "[step: 911] loss: 0.0012535727582871914\n",
      "[step: 912] loss: 0.0008689701789990067\n",
      "[step: 913] loss: 0.0008692528936080635\n",
      "[step: 914] loss: 0.0011027840664610267\n",
      "[step: 915] loss: 0.0010578486835584044\n",
      "[step: 916] loss: 0.0008468375308439136\n",
      "[step: 917] loss: 0.0009015851537697017\n",
      "[step: 918] loss: 0.0009910794906318188\n",
      "[step: 919] loss: 0.0008897855877876282\n",
      "[step: 920] loss: 0.0008447194122709334\n",
      "[step: 921] loss: 0.0009005573228932917\n",
      "[step: 922] loss: 0.0009200579370371997\n",
      "[step: 923] loss: 0.00086348777404055\n",
      "[step: 924] loss: 0.0008102383580990136\n",
      "[step: 925] loss: 0.0008945479057729244\n",
      "[step: 926] loss: 0.0008815601468086243\n",
      "[step: 927] loss: 0.0007842257036827505\n",
      "[step: 928] loss: 0.0008377458434551954\n",
      "[step: 929] loss: 0.0008501440170221031\n",
      "[step: 930] loss: 0.0007659640978090465\n",
      "[step: 931] loss: 0.0008063008426688612\n",
      "[step: 932] loss: 0.0008183276513591409\n",
      "[step: 933] loss: 0.0007757181883789599\n",
      "[step: 934] loss: 0.0007807166548445821\n",
      "[step: 935] loss: 0.0007679818663746119\n",
      "[step: 936] loss: 0.0007724835304543376\n",
      "[step: 937] loss: 0.0007852366543374956\n",
      "[step: 938] loss: 0.0007414508727379143\n",
      "[step: 939] loss: 0.0007537222118116915\n",
      "[step: 940] loss: 0.0007659950060769916\n",
      "[step: 941] loss: 0.0007561632664874196\n",
      "[step: 942] loss: 0.0007508958224207163\n",
      "[step: 943] loss: 0.000746132165659219\n",
      "[step: 944] loss: 0.000731827924028039\n",
      "[step: 945] loss: 0.0007463583024218678\n",
      "[step: 946] loss: 0.0007445158553309739\n",
      "[step: 947] loss: 0.0007397340377792716\n",
      "[step: 948] loss: 0.0007316249539144337\n",
      "[step: 949] loss: 0.0007308346685022116\n",
      "[step: 950] loss: 0.0007244688458740711\n",
      "[step: 951] loss: 0.000733999302610755\n",
      "[step: 952] loss: 0.0007335268892347813\n",
      "[step: 953] loss: 0.0007270277128554881\n",
      "[step: 954] loss: 0.0007264064624905586\n",
      "[step: 955] loss: 0.0007228775648400187\n",
      "[step: 956] loss: 0.0007165612769313157\n",
      "[step: 957] loss: 0.0007176492945291102\n",
      "[step: 958] loss: 0.000716925656888634\n",
      "[step: 959] loss: 0.0007151701720431447\n",
      "[step: 960] loss: 0.0007178770611062646\n",
      "[step: 961] loss: 0.0007213426870293915\n",
      "[step: 962] loss: 0.0007248485344462097\n",
      "[step: 963] loss: 0.0007322835735976696\n",
      "[step: 964] loss: 0.0007476608152501285\n",
      "[step: 965] loss: 0.000774148793425411\n",
      "[step: 966] loss: 0.0008118725963868201\n",
      "[step: 967] loss: 0.0008786708349362016\n",
      "[step: 968] loss: 0.0009203577064909041\n",
      "[step: 969] loss: 0.0009331826586276293\n",
      "[step: 970] loss: 0.000844020105432719\n",
      "[step: 971] loss: 0.0007408487726934254\n",
      "[step: 972] loss: 0.0007234315271489322\n",
      "[step: 973] loss: 0.0007826574146747589\n",
      "[step: 974] loss: 0.0008245172211900353\n",
      "[step: 975] loss: 0.0007633629720658064\n",
      "[step: 976] loss: 0.0007225695881061256\n",
      "[step: 977] loss: 0.0007499450002796948\n",
      "[step: 978] loss: 0.0007892581052146852\n",
      "[step: 979] loss: 0.0007666086894460022\n",
      "[step: 980] loss: 0.0007155947387218475\n",
      "[step: 981] loss: 0.0007119415095075965\n",
      "[step: 982] loss: 0.0007487458060495555\n",
      "[step: 983] loss: 0.000757659669034183\n",
      "[step: 984] loss: 0.0007324574398808181\n",
      "[step: 985] loss: 0.000705720332916826\n",
      "[step: 986] loss: 0.0007075004396028817\n",
      "[step: 987] loss: 0.0007275027455762029\n",
      "[step: 988] loss: 0.0007319656433537602\n",
      "[step: 989] loss: 0.0007230589399114251\n",
      "[step: 990] loss: 0.0007034546579234302\n",
      "[step: 991] loss: 0.0006948882946744561\n",
      "[step: 992] loss: 0.0007016872987151146\n",
      "[step: 993] loss: 0.0007146139396354556\n",
      "[step: 994] loss: 0.0007259569247253239\n",
      "[step: 995] loss: 0.0007243528380058706\n",
      "[step: 996] loss: 0.0007194125792011619\n",
      "[step: 997] loss: 0.000714316382072866\n",
      "[step: 998] loss: 0.00070970319211483\n",
      "[step: 999] loss: 0.0007072617881931365\n",
      "[step: 1000] loss: 0.0007029234548099339\n",
      "RMSE: 0.033973123878240585 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/taxi_data/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1001\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,data_dim], name = \"X\")\n",
    "Y = tf.placeholder(tf.float32,[None,25], name = \"Y\")\n",
    "\n",
    "#LSTM CELL만들기\n",
    "with tf.variable_scope(\"rnn\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs,_states = tf.nn.dynamic_rnn(cell,X,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss =tf.reduce_mean(tf.square(Y_pred-Y))\n",
    "    train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train,loss],feed_dict={X:trainX, Y:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/Users/yeseo/ML_with_Taxidata/code/save_model_code/model/my_model.ckpt\")\n",
    "    \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X:validationX})\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    \n",
    "    print(\"RMSE: {} \".format(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.519983  , 0.5357133 , 0.7311876 , ..., 0.62834936, 0.8134205 ,\n",
       "        0.77985823],\n",
       "       [0.48948288, 0.5064425 , 0.7059424 , ..., 0.59972465, 0.8103568 ,\n",
       "        0.7986638 ],\n",
       "       [0.46200874, 0.4708667 , 0.6635073 , ..., 0.5821145 , 0.80588466,\n",
       "        0.7984813 ],\n",
       "       ...,\n",
       "       [0.65016776, 0.6609901 , 0.62372655, ..., 0.61395335, 0.3835152 ,\n",
       "        0.27562097],\n",
       "       [0.6763084 , 0.6765678 , 0.64603007, ..., 0.64307237, 0.43519932,\n",
       "        0.31744656],\n",
       "       [0.69684863, 0.6880646 , 0.6709062 , ..., 0.67722094, 0.46239683,\n",
       "        0.3378895 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
