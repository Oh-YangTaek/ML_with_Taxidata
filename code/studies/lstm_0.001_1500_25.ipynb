{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 469987.0625\n",
      "[step: 0] loss: 0.3875851035118103\n",
      "[step: 1] loss: 316266.90625\n",
      "[step: 1] loss: 0.26090964674949646\n",
      "[step: 2] loss: 225293.53125\n",
      "[step: 2] loss: 0.17596584558486938\n",
      "[step: 3] loss: 170295.875\n",
      "[step: 3] loss: 0.11450070142745972\n",
      "[step: 4] loss: 131991.21875\n",
      "[step: 4] loss: 0.07579457014799118\n",
      "[step: 5] loss: 102887.40625\n",
      "[step: 5] loss: 0.05900811776518822\n",
      "[step: 6] loss: 86077.28125\n",
      "[step: 6] loss: 0.052881937474012375\n",
      "[step: 7] loss: 76386.359375\n",
      "[step: 7] loss: 0.049006182700395584\n",
      "[step: 8] loss: 69736.484375\n",
      "[step: 8] loss: 0.0450737401843071\n",
      "[step: 9] loss: 64988.41796875\n",
      "[step: 9] loss: 0.04124637693166733\n",
      "[step: 10] loss: 62862.7109375\n",
      "[step: 10] loss: 0.03778216242790222\n",
      "[step: 11] loss: 62969.17578125\n",
      "[step: 11] loss: 0.03484201431274414\n",
      "[step: 12] loss: 63200.4765625\n",
      "[step: 12] loss: 0.03269193321466446\n",
      "[step: 13] loss: 62203.609375\n",
      "[step: 13] loss: 0.03143831714987755\n",
      "[step: 14] loss: 60241.1640625\n",
      "[step: 14] loss: 0.030806541442871094\n",
      "[step: 15] loss: 57920.0859375\n",
      "[step: 15] loss: 0.03033052384853363\n",
      "[step: 16] loss: 55516.640625\n",
      "[step: 16] loss: 0.029717011377215385\n",
      "[step: 17] loss: 53019.00390625\n",
      "[step: 17] loss: 0.028931831941008568\n",
      "[step: 18] loss: 50417.296875\n",
      "[step: 18] loss: 0.028074871748685837\n",
      "[step: 19] loss: 47825.19140625\n",
      "[step: 19] loss: 0.02724362537264824\n",
      "[step: 20] loss: 45450.30078125\n",
      "[step: 20] loss: 0.026486126706004143\n",
      "[step: 21] loss: 43525.1953125\n",
      "[step: 21] loss: 0.02581128291785717\n",
      "[step: 22] loss: 42207.27734375\n",
      "[step: 22] loss: 0.025215594097971916\n",
      "[step: 23] loss: 41416.78125\n",
      "[step: 23] loss: 0.024698909372091293\n",
      "[step: 24] loss: 40877.23828125\n",
      "[step: 24] loss: 0.02428371086716652\n",
      "[step: 25] loss: 40406.3671875\n",
      "[step: 25] loss: 0.023998931050300598\n",
      "[step: 26] loss: 39928.82421875\n",
      "[step: 26] loss: 0.023828189820051193\n",
      "[step: 27] loss: 39431.6875\n",
      "[step: 27] loss: 0.023683110252022743\n",
      "[step: 28] loss: 38983.88671875\n",
      "[step: 28] loss: 0.02347581833600998\n",
      "[step: 29] loss: 38596.5\n",
      "[step: 29] loss: 0.023198582231998444\n",
      "[step: 30] loss: 38197.5078125\n",
      "[step: 30] loss: 0.022913705557584763\n",
      "[step: 31] loss: 37665.2734375\n",
      "[step: 31] loss: 0.022688036784529686\n",
      "[step: 32] loss: 36991.49609375\n",
      "[step: 32] loss: 0.02252894639968872\n",
      "[step: 33] loss: 36263.640625\n",
      "[step: 33] loss: 0.022386163473129272\n",
      "[step: 34] loss: 35583.87890625\n",
      "[step: 34] loss: 0.02220890112221241\n",
      "[step: 35] loss: 34982.46875\n",
      "[step: 35] loss: 0.021997351199388504\n",
      "[step: 36] loss: 34433.41796875\n",
      "[step: 36] loss: 0.021797291934490204\n",
      "[step: 37] loss: 33870.171875\n",
      "[step: 37] loss: 0.021652501076459885\n",
      "[step: 38] loss: 33308.0546875\n",
      "[step: 38] loss: 0.02156473882496357\n",
      "[step: 39] loss: 32834.14453125\n",
      "[step: 39] loss: 0.021497337147593498\n",
      "[step: 40] loss: 32485.44140625\n",
      "[step: 40] loss: 0.0214129239320755\n",
      "[step: 41] loss: 32145.890625\n",
      "[step: 41] loss: 0.021305900067090988\n",
      "[step: 42] loss: 31764.525390625\n",
      "[step: 42] loss: 0.021195348352193832\n",
      "[step: 43] loss: 31349.828125\n",
      "[step: 43] loss: 0.021097533404827118\n",
      "[step: 44] loss: 30923.66015625\n",
      "[step: 44] loss: 0.021009501069784164\n",
      "[step: 45] loss: 30539.263671875\n",
      "[step: 45] loss: 0.020919447764754295\n",
      "[step: 46] loss: 30190.89453125\n",
      "[step: 46] loss: 0.020824681967496872\n",
      "[step: 47] loss: 29813.529296875\n",
      "[step: 47] loss: 0.02073862962424755\n",
      "[step: 48] loss: 29467.48828125\n",
      "[step: 48] loss: 0.02067410759627819\n",
      "[step: 49] loss: 29116.23046875\n",
      "[step: 49] loss: 0.020627768710255623\n",
      "[step: 50] loss: 28719.818359375\n",
      "[step: 50] loss: 0.02058199979364872\n",
      "[step: 51] loss: 28322.7734375\n",
      "[step: 51] loss: 0.020520800724625587\n",
      "[step: 52] loss: 27937.79296875\n",
      "[step: 52] loss: 0.020442556589841843\n",
      "[step: 53] loss: 27605.142578125\n",
      "[step: 53] loss: 0.020358597859740257\n",
      "[step: 54] loss: 27262.85546875\n",
      "[step: 54] loss: 0.020278647541999817\n",
      "[step: 55] loss: 26892.9609375\n",
      "[step: 55] loss: 0.02020677924156189\n",
      "[step: 56] loss: 26545.900390625\n",
      "[step: 56] loss: 0.020140230655670166\n",
      "[step: 57] loss: 26206.88671875\n",
      "[step: 57] loss: 0.02007763646543026\n",
      "[step: 58] loss: 25907.626953125\n",
      "[step: 58] loss: 0.02001841366291046\n",
      "[step: 59] loss: 25551.349609375\n",
      "[step: 59] loss: 0.01996208354830742\n",
      "[step: 60] loss: 25147.140625\n",
      "[step: 60] loss: 0.01990610733628273\n",
      "[step: 61] loss: 24739.80859375\n",
      "[step: 61] loss: 0.019848912954330444\n",
      "[step: 62] loss: 24249.173828125\n",
      "[step: 62] loss: 0.019790800288319588\n",
      "[step: 63] loss: 23907.990234375\n",
      "[step: 63] loss: 0.019732236862182617\n",
      "[step: 64] loss: 23595.275390625\n",
      "[step: 64] loss: 0.019673705101013184\n",
      "[step: 65] loss: 23125.1875\n",
      "[step: 65] loss: 0.019614778459072113\n",
      "[step: 66] loss: 22885.232421875\n",
      "[step: 66] loss: 0.019555168226361275\n",
      "[step: 67] loss: 22602.638671875\n",
      "[step: 67] loss: 0.019495198503136635\n",
      "[step: 68] loss: 22109.333984375\n",
      "[step: 68] loss: 0.019434746354818344\n",
      "[step: 69] loss: 21840.396484375\n",
      "[step: 69] loss: 0.019376056268811226\n",
      "[step: 70] loss: 21644.38671875\n",
      "[step: 70] loss: 0.01932058483362198\n",
      "[step: 71] loss: 21219.25\n",
      "[step: 71] loss: 0.019251933321356773\n",
      "[step: 72] loss: 20738.130859375\n",
      "[step: 72] loss: 0.01918858103454113\n",
      "[step: 73] loss: 20543.03125\n",
      "[step: 73] loss: 0.019136112183332443\n",
      "[step: 74] loss: 20516.052734375\n",
      "[step: 74] loss: 0.019079433754086494\n",
      "[step: 75] loss: 20081.650390625\n",
      "[step: 75] loss: 0.019016940146684647\n",
      "[step: 76] loss: 19527.060546875\n",
      "[step: 76] loss: 0.01896027848124504\n",
      "[step: 77] loss: 19350.482421875\n",
      "[step: 77] loss: 0.018909066915512085\n",
      "[step: 78] loss: 19358.060546875\n",
      "[step: 78] loss: 0.01884782500565052\n",
      "[step: 79] loss: 19120.0703125\n",
      "[step: 79] loss: 0.018787475302815437\n",
      "[step: 80] loss: 18592.43359375\n",
      "[step: 80] loss: 0.018732354044914246\n",
      "[step: 81] loss: 18297.431640625\n",
      "[step: 81] loss: 0.018675459548830986\n",
      "[step: 82] loss: 18282.658203125\n",
      "[step: 82] loss: 0.018616721034049988\n",
      "[step: 83] loss: 18023.57421875\n",
      "[step: 83] loss: 0.018559858202934265\n",
      "[step: 84] loss: 17663.33203125\n",
      "[step: 84] loss: 0.018506065011024475\n",
      "[step: 85] loss: 17480.197265625\n",
      "[step: 85] loss: 0.018449552357196808\n",
      "[step: 86] loss: 17230.76953125\n",
      "[step: 86] loss: 0.018390359356999397\n",
      "[step: 87] loss: 17039.90234375\n",
      "[step: 87] loss: 0.01833203062415123\n",
      "[step: 88] loss: 16881.912109375\n",
      "[step: 88] loss: 0.01827446185052395\n",
      "[step: 89] loss: 16556.75390625\n",
      "[step: 89] loss: 0.01821517013013363\n",
      "[step: 90] loss: 16264.9111328125\n",
      "[step: 90] loss: 0.01815788820385933\n",
      "[step: 91] loss: 16095.5654296875\n",
      "[step: 91] loss: 0.01810206100344658\n",
      "[step: 92] loss: 15985.20703125\n",
      "[step: 92] loss: 0.01804306171834469\n",
      "[step: 93] loss: 15774.7646484375\n",
      "[step: 93] loss: 0.017986571416258812\n",
      "[step: 94] loss: 15419.4482421875\n",
      "[step: 94] loss: 0.017928214743733406\n",
      "[step: 95] loss: 15262.7109375\n",
      "[step: 95] loss: 0.017869051545858383\n",
      "[step: 96] loss: 15194.115234375\n",
      "[step: 96] loss: 0.017812088131904602\n",
      "[step: 97] loss: 14926.9404296875\n",
      "[step: 97] loss: 0.017752697691321373\n",
      "[step: 98] loss: 14749.0966796875\n",
      "[step: 98] loss: 0.017696019262075424\n",
      "[step: 99] loss: 14615.201171875\n",
      "[step: 99] loss: 0.017636995762586594\n",
      "[step: 100] loss: 14350.05859375\n",
      "[step: 100] loss: 0.017579345032572746\n",
      "[step: 101] loss: 14077.6943359375\n",
      "[step: 101] loss: 0.01752026379108429\n",
      "[step: 102] loss: 13901.1201171875\n",
      "[step: 102] loss: 0.017461391165852547\n",
      "[step: 103] loss: 13801.5478515625\n",
      "[step: 103] loss: 0.017401807010173798\n",
      "[step: 104] loss: 13682.216796875\n",
      "[step: 104] loss: 0.017341718077659607\n",
      "[step: 105] loss: 13466.9248046875\n",
      "[step: 105] loss: 0.017280982807278633\n",
      "[step: 106] loss: 13244.6279296875\n",
      "[step: 106] loss: 0.01722012273967266\n",
      "[step: 107] loss: 13065.98828125\n",
      "[step: 107] loss: 0.017157787457108498\n",
      "[step: 108] loss: 12929.40234375\n",
      "[step: 108] loss: 0.01709633134305477\n",
      "[step: 109] loss: 12834.4814453125\n",
      "[step: 109] loss: 0.01703554578125477\n",
      "[step: 110] loss: 12656.2880859375\n",
      "[step: 110] loss: 0.01698748767375946\n",
      "[step: 111] loss: 12492.396484375\n",
      "[step: 111] loss: 0.016939770430326462\n",
      "[step: 112] loss: 12289.3916015625\n",
      "[step: 112] loss: 0.016889454796910286\n",
      "[step: 113] loss: 12140.025390625\n",
      "[step: 113] loss: 0.01678469590842724\n",
      "[step: 114] loss: 12062.2509765625\n",
      "[step: 114] loss: 0.016790861263871193\n",
      "[step: 115] loss: 12095.67578125\n",
      "[step: 115] loss: 0.016666186973452568\n",
      "[step: 116] loss: 12530.5751953125\n",
      "[step: 116] loss: 0.016641439869999886\n",
      "[step: 117] loss: 13378.58203125\n",
      "[step: 117] loss: 0.016517093405127525\n",
      "[step: 118] loss: 13242.5087890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 118] loss: 0.016528910025954247\n",
      "[step: 119] loss: 11509.841796875\n",
      "[step: 119] loss: 0.016426198184490204\n",
      "[step: 120] loss: 12172.3837890625\n",
      "[step: 120] loss: 0.01639014482498169\n",
      "[step: 121] loss: 11781.498046875\n",
      "[step: 121] loss: 0.016248589381575584\n",
      "[step: 122] loss: 11566.7861328125\n",
      "[step: 122] loss: 0.01626085303723812\n",
      "[step: 123] loss: 11795.318359375\n",
      "[step: 123] loss: 0.016120901331305504\n",
      "[step: 124] loss: 11061.666015625\n",
      "[step: 124] loss: 0.016089826822280884\n",
      "[step: 125] loss: 11540.5673828125\n",
      "[step: 125] loss: 0.015970956534147263\n",
      "[step: 126] loss: 11113.482421875\n",
      "[step: 126] loss: 0.01595638506114483\n",
      "[step: 127] loss: 11252.390625\n",
      "[step: 127] loss: 0.01582198776304722\n",
      "[step: 128] loss: 10680.20703125\n",
      "[step: 128] loss: 0.015784751623868942\n",
      "[step: 129] loss: 11125.0986328125\n",
      "[step: 129] loss: 0.01567051000893116\n",
      "[step: 130] loss: 10602.2138671875\n",
      "[step: 130] loss: 0.015643516555428505\n",
      "[step: 131] loss: 10601.580078125\n",
      "[step: 131] loss: 0.015520959161221981\n",
      "[step: 132] loss: 10627.9912109375\n",
      "[step: 132] loss: 0.015455547720193863\n",
      "[step: 133] loss: 10339.25390625\n",
      "[step: 133] loss: 0.015350966714322567\n",
      "[step: 134] loss: 10406.5986328125\n",
      "[step: 134] loss: 0.015281201340258121\n",
      "[step: 135] loss: 10136.6484375\n",
      "[step: 135] loss: 0.015208466909825802\n",
      "[step: 136] loss: 10283.875\n",
      "[step: 136] loss: 0.015105143189430237\n",
      "[step: 137] loss: 10016.47265625\n",
      "[step: 137] loss: 0.015044549480080605\n",
      "[step: 138] loss: 9992.515625\n",
      "[step: 138] loss: 0.014930465258657932\n",
      "[step: 139] loss: 9994.734375\n",
      "[step: 139] loss: 0.014868768863379955\n",
      "[step: 140] loss: 9822.111328125\n",
      "[step: 140] loss: 0.014772718772292137\n",
      "[step: 141] loss: 9855.7373046875\n",
      "[step: 141] loss: 0.014691888354718685\n",
      "[step: 142] loss: 9680.2470703125\n",
      "[step: 142] loss: 0.014664334245026112\n",
      "[step: 143] loss: 9689.3798828125\n",
      "[step: 143] loss: 0.014563984237611294\n",
      "[step: 144] loss: 9655.580078125\n",
      "[step: 144] loss: 0.014504699036478996\n",
      "[step: 145] loss: 9478.19140625\n",
      "[step: 145] loss: 0.01436686422675848\n",
      "[step: 146] loss: 9500.6533203125\n",
      "[step: 146] loss: 0.014290552586317062\n",
      "[step: 147] loss: 9433.7177734375\n",
      "[step: 147] loss: 0.014298280701041222\n",
      "[step: 148] loss: 9349.0458984375\n",
      "[step: 148] loss: 0.014133518561720848\n",
      "[step: 149] loss: 9344.3056640625\n",
      "[step: 149] loss: 0.01400897465646267\n",
      "[step: 150] loss: 9239.8544921875\n",
      "[step: 150] loss: 0.013991900719702244\n",
      "[step: 151] loss: 9161.9619140625\n",
      "[step: 151] loss: 0.014048950746655464\n",
      "[step: 152] loss: 9176.427734375\n",
      "[step: 152] loss: 0.014172496274113655\n",
      "[step: 153] loss: 9115.595703125\n",
      "[step: 153] loss: 0.013738995417952538\n",
      "[step: 154] loss: 9023.8076171875\n",
      "[step: 154] loss: 0.013903658837080002\n",
      "[step: 155] loss: 9004.208984375\n",
      "[step: 155] loss: 0.01375177875161171\n",
      "[step: 156] loss: 8958.322265625\n",
      "[step: 156] loss: 0.013626438565552235\n",
      "[step: 157] loss: 8872.4072265625\n",
      "[step: 157] loss: 0.013659792952239513\n",
      "[step: 158] loss: 8829.56640625\n",
      "[step: 158] loss: 0.013406210578978062\n",
      "[step: 159] loss: 8822.7392578125\n",
      "[step: 159] loss: 0.013448033481836319\n",
      "[step: 160] loss: 8790.1845703125\n",
      "[step: 160] loss: 0.013209283351898193\n",
      "[step: 161] loss: 8729.453125\n",
      "[step: 161] loss: 0.013301176950335503\n",
      "[step: 162] loss: 8691.998046875\n",
      "[step: 162] loss: 0.01311182975769043\n",
      "[step: 163] loss: 8666.662109375\n",
      "[step: 163] loss: 0.012976464815437794\n",
      "[step: 164] loss: 8672.576171875\n",
      "[step: 164] loss: 0.012891938909888268\n",
      "[step: 165] loss: 8622.4150390625\n",
      "[step: 165] loss: 0.012898099608719349\n",
      "[step: 166] loss: 8590.1181640625\n",
      "[step: 166] loss: 0.01283592451363802\n",
      "[step: 167] loss: 8506.9541015625\n",
      "[step: 167] loss: 0.012669717893004417\n",
      "[step: 168] loss: 8445.78125\n",
      "[step: 168] loss: 0.012616352178156376\n",
      "[step: 169] loss: 8383.8681640625\n",
      "[step: 169] loss: 0.012624206952750683\n",
      "[step: 170] loss: 8336.1708984375\n",
      "[step: 170] loss: 0.012670174241065979\n",
      "[step: 171] loss: 8295.396484375\n",
      "[step: 171] loss: 0.012639333494007587\n",
      "[step: 172] loss: 8259.0654296875\n",
      "[step: 172] loss: 0.012398741208016872\n",
      "[step: 173] loss: 8227.1884765625\n",
      "[step: 173] loss: 0.012424258515238762\n",
      "[step: 174] loss: 8204.6328125\n",
      "[step: 174] loss: 0.012266063131392002\n",
      "[step: 175] loss: 8214.822265625\n",
      "[step: 175] loss: 0.01220843568444252\n",
      "[step: 176] loss: 8343.568359375\n",
      "[step: 176] loss: 0.012259362265467644\n",
      "[step: 177] loss: 8983.1552734375\n",
      "[step: 177] loss: 0.012371326796710491\n",
      "[step: 178] loss: 10802.84375\n",
      "[step: 178] loss: 0.012093965895473957\n",
      "[step: 179] loss: 13034.4970703125\n",
      "[step: 179] loss: 0.012061267159879208\n",
      "[step: 180] loss: 8705.6982421875\n",
      "[step: 180] loss: 0.012048883363604546\n",
      "[step: 181] loss: 9853.5009765625\n",
      "[step: 181] loss: 0.011821603402495384\n",
      "[step: 182] loss: 9651.6279296875\n",
      "[step: 182] loss: 0.011961687356233597\n",
      "[step: 183] loss: 9180.578125\n",
      "[step: 183] loss: 0.011709245853126049\n",
      "[step: 184] loss: 9627.6396484375\n",
      "[step: 184] loss: 0.011743062175810337\n",
      "[step: 185] loss: 9049.5615234375\n",
      "[step: 185] loss: 0.011708804406225681\n",
      "[step: 186] loss: 9478.650390625\n",
      "[step: 186] loss: 0.011552263051271439\n",
      "[step: 187] loss: 8914.333984375\n",
      "[step: 187] loss: 0.011627338826656342\n",
      "[step: 188] loss: 8681.580078125\n",
      "[step: 188] loss: 0.011438914574682713\n",
      "[step: 189] loss: 9519.9208984375\n",
      "[step: 189] loss: 0.011454859748482704\n",
      "[step: 190] loss: 8511.01953125\n",
      "[step: 190] loss: 0.011424445547163486\n",
      "[step: 191] loss: 8724.708984375\n",
      "[step: 191] loss: 0.011281440034508705\n",
      "[step: 192] loss: 8181.607421875\n",
      "[step: 192] loss: 0.011295292526483536\n",
      "[step: 193] loss: 8712.2255859375\n",
      "[step: 193] loss: 0.011221442371606827\n",
      "[step: 194] loss: 8011.0419921875\n",
      "[step: 194] loss: 0.011160160414874554\n",
      "[step: 195] loss: 8433.79296875\n",
      "[step: 195] loss: 0.011154460720717907\n",
      "[step: 196] loss: 7964.4765625\n",
      "[step: 196] loss: 0.011064168065786362\n",
      "[step: 197] loss: 8426.74609375\n",
      "[step: 197] loss: 0.011036381125450134\n",
      "[step: 198] loss: 7823.67333984375\n",
      "[step: 198] loss: 0.011017657816410065\n",
      "[step: 199] loss: 8197.2353515625\n",
      "[step: 199] loss: 0.01094345934689045\n",
      "[step: 200] loss: 7931.6728515625\n",
      "[step: 200] loss: 0.010914009064435959\n",
      "[step: 201] loss: 8052.0029296875\n",
      "[step: 201] loss: 0.01088541466742754\n",
      "[step: 202] loss: 7749.00244140625\n",
      "[step: 202] loss: 0.010827461257576942\n",
      "[step: 203] loss: 8028.02490234375\n",
      "[step: 203] loss: 0.010796223767101765\n",
      "[step: 204] loss: 7723.404296875\n",
      "[step: 204] loss: 0.010775245726108551\n",
      "[step: 205] loss: 7802.8662109375\n",
      "[step: 205] loss: 0.010730430483818054\n",
      "[step: 206] loss: 7755.8642578125\n",
      "[step: 206] loss: 0.010682312771677971\n",
      "[step: 207] loss: 7715.7900390625\n",
      "[step: 207] loss: 0.010658049955964088\n",
      "[step: 208] loss: 7647.373046875\n",
      "[step: 208] loss: 0.010636157356202602\n",
      "[step: 209] loss: 7611.7177734375\n",
      "[step: 209] loss: 0.010592374950647354\n",
      "[step: 210] loss: 7658.8896484375\n",
      "[step: 210] loss: 0.010549982078373432\n",
      "[step: 211] loss: 7508.2890625\n",
      "[step: 211] loss: 0.010526728816330433\n",
      "[step: 212] loss: 7580.794921875\n",
      "[step: 212] loss: 0.010504701174795628\n",
      "[step: 213] loss: 7466.01171875\n",
      "[step: 213] loss: 0.010472681373357773\n",
      "[step: 214] loss: 7534.650390625\n",
      "[step: 214] loss: 0.010438533499836922\n",
      "[step: 215] loss: 7437.07275390625\n",
      "[step: 215] loss: 0.010405591689050198\n",
      "[step: 216] loss: 7420.7080078125\n",
      "[step: 216] loss: 0.010378769598901272\n",
      "[step: 217] loss: 7440.23291015625\n",
      "[step: 217] loss: 0.010357880033552647\n",
      "[step: 218] loss: 7366.21484375\n",
      "[step: 218] loss: 0.010335181839764118\n",
      "[step: 219] loss: 7386.9404296875\n",
      "[step: 219] loss: 0.0103111881762743\n",
      "[step: 220] loss: 7307.0673828125\n",
      "[step: 220] loss: 0.01028809417039156\n",
      "[step: 221] loss: 7316.41943359375\n",
      "[step: 221] loss: 0.010266167111694813\n",
      "[step: 222] loss: 7307.2880859375\n",
      "[step: 222] loss: 0.010247386060655117\n",
      "[step: 223] loss: 7251.376953125\n",
      "[step: 223] loss: 0.010239657014608383\n",
      "[step: 224] loss: 7256.99853515625\n",
      "[step: 224] loss: 0.010240115225315094\n",
      "[step: 225] loss: 7206.32470703125\n",
      "[step: 225] loss: 0.010265298187732697\n",
      "[step: 226] loss: 7189.90478515625\n",
      "[step: 226] loss: 0.010296735912561417\n",
      "[step: 227] loss: 7195.6240234375\n",
      "[step: 227] loss: 0.010351711884140968\n",
      "[step: 228] loss: 7149.9716796875\n",
      "[step: 228] loss: 0.010312560945749283\n",
      "[step: 229] loss: 7137.3193359375\n",
      "[step: 229] loss: 0.010192301124334335\n",
      "[step: 230] loss: 7123.1865234375\n",
      "[step: 230] loss: 0.010071557946503162\n",
      "[step: 231] loss: 7080.8017578125\n",
      "[step: 231] loss: 0.010082432068884373\n",
      "[step: 232] loss: 7070.5859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 232] loss: 0.010193495079874992\n",
      "[step: 233] loss: 7065.83203125\n",
      "[step: 233] loss: 0.010292790830135345\n",
      "[step: 234] loss: 7034.7578125\n",
      "[step: 234] loss: 0.010295883752405643\n",
      "[step: 235] loss: 7013.8330078125\n",
      "[step: 235] loss: 0.01005904097110033\n",
      "[step: 236] loss: 7007.10595703125\n",
      "[step: 236] loss: 0.01007386576384306\n",
      "[step: 237] loss: 6983.6552734375\n",
      "[step: 237] loss: 0.010240846313536167\n",
      "[step: 238] loss: 6952.5087890625\n",
      "[step: 238] loss: 0.010210087522864342\n",
      "[step: 239] loss: 6933.3994140625\n",
      "[step: 239] loss: 0.009963992983102798\n",
      "[step: 240] loss: 6922.4775390625\n",
      "[step: 240] loss: 0.01002119667828083\n",
      "[step: 241] loss: 6903.357421875\n",
      "[step: 241] loss: 0.010030782781541348\n",
      "[step: 242] loss: 6877.48095703125\n",
      "[step: 242] loss: 0.010043852962553501\n",
      "[step: 243] loss: 6854.833984375\n",
      "[step: 243] loss: 0.009885935112833977\n",
      "[step: 244] loss: 6839.63232421875\n",
      "[step: 244] loss: 0.009940887801349163\n",
      "[step: 245] loss: 6827.658203125\n",
      "[step: 245] loss: 0.009956274181604385\n",
      "[step: 246] loss: 6814.830078125\n",
      "[step: 246] loss: 0.00985332578420639\n",
      "[step: 247] loss: 6800.29052734375\n",
      "[step: 247] loss: 0.009849075227975845\n",
      "[step: 248] loss: 6786.3603515625\n",
      "[step: 248] loss: 0.009881134144961834\n",
      "[step: 249] loss: 6777.751953125\n",
      "[step: 249] loss: 0.009872011840343475\n",
      "[step: 250] loss: 6781.18359375\n",
      "[step: 250] loss: 0.00976848229765892\n",
      "[step: 251] loss: 6823.421875\n",
      "[step: 251] loss: 0.009787977673113346\n",
      "[step: 252] loss: 6932.3203125\n",
      "[step: 252] loss: 0.009794531390070915\n",
      "[step: 253] loss: 7310.4013671875\n",
      "[step: 253] loss: 0.009806415997445583\n",
      "[step: 254] loss: 7860.029296875\n",
      "[step: 254] loss: 0.009732765145599842\n",
      "[step: 255] loss: 9165.1259765625\n",
      "[step: 255] loss: 0.009702135808765888\n",
      "[step: 256] loss: 10271.1181640625\n",
      "[step: 256] loss: 0.009719837456941605\n",
      "[step: 257] loss: 7965.30810546875\n",
      "[step: 257] loss: 0.009713143110275269\n",
      "[step: 258] loss: 7280.42041015625\n",
      "[step: 258] loss: 0.00970530230551958\n",
      "[step: 259] loss: 8417.80078125\n",
      "[step: 259] loss: 0.009647942148149014\n",
      "[step: 260] loss: 7109.236328125\n",
      "[step: 260] loss: 0.009641085751354694\n",
      "[step: 261] loss: 7665.77197265625\n",
      "[step: 261] loss: 0.009636064060032368\n",
      "[step: 262] loss: 7328.7255859375\n",
      "[step: 262] loss: 0.009648886509239674\n",
      "[step: 263] loss: 7506.568359375\n",
      "[step: 263] loss: 0.009656124748289585\n",
      "[step: 264] loss: 7431.67578125\n",
      "[step: 264] loss: 0.009637312963604927\n",
      "[step: 265] loss: 6973.6513671875\n",
      "[step: 265] loss: 0.009614242240786552\n",
      "[step: 266] loss: 7258.5166015625\n",
      "[step: 266] loss: 0.009565290994942188\n",
      "[step: 267] loss: 6764.94189453125\n",
      "[step: 267] loss: 0.009552877396345139\n",
      "[step: 268] loss: 7260.7099609375\n",
      "[step: 268] loss: 0.009554917924106121\n",
      "[step: 269] loss: 6751.9833984375\n",
      "[step: 269] loss: 0.009573620744049549\n",
      "[step: 270] loss: 7010.2255859375\n",
      "[step: 270] loss: 0.009616968221962452\n",
      "[step: 271] loss: 6759.609375\n",
      "[step: 271] loss: 0.009632488712668419\n",
      "[step: 272] loss: 6800.22314453125\n",
      "[step: 272] loss: 0.009628895670175552\n",
      "[step: 273] loss: 6885.3125\n",
      "[step: 273] loss: 0.009540869854390621\n",
      "[step: 274] loss: 6608.53125\n",
      "[step: 274] loss: 0.009479059837758541\n",
      "[step: 275] loss: 6841.6982421875\n",
      "[step: 275] loss: 0.009476527571678162\n",
      "[step: 276] loss: 6607.93310546875\n",
      "[step: 276] loss: 0.009513705037534237\n",
      "[step: 277] loss: 6680.72412109375\n",
      "[step: 277] loss: 0.009607878513634205\n",
      "[step: 278] loss: 6689.724609375\n",
      "[step: 278] loss: 0.009653681889176369\n",
      "[step: 279] loss: 6494.21875\n",
      "[step: 279] loss: 0.009642720222473145\n",
      "[step: 280] loss: 6629.603515625\n",
      "[step: 280] loss: 0.009482278488576412\n",
      "[step: 281] loss: 6526.90625\n",
      "[step: 281] loss: 0.009429716505110264\n",
      "[step: 282] loss: 6485.91015625\n",
      "[step: 282] loss: 0.009558381512761116\n",
      "[step: 283] loss: 6558.2138671875\n",
      "[step: 283] loss: 0.009665083140134811\n",
      "[step: 284] loss: 6427.10791015625\n",
      "[step: 284] loss: 0.009754781611263752\n",
      "[step: 285] loss: 6444.4599609375\n",
      "[step: 285] loss: 0.00949751865118742\n",
      "[step: 286] loss: 6493.64453125\n",
      "[step: 286] loss: 0.00945090688765049\n",
      "[step: 287] loss: 6386.7080078125\n",
      "[step: 287] loss: 0.009561415761709213\n",
      "[step: 288] loss: 6384.94677734375\n",
      "[step: 288] loss: 0.009491882286965847\n",
      "[step: 289] loss: 6427.2998046875\n",
      "[step: 289] loss: 0.009351310320198536\n",
      "[step: 290] loss: 6356.38525390625\n",
      "[step: 290] loss: 0.009438294917345047\n",
      "[step: 291] loss: 6317.2685546875\n",
      "[step: 291] loss: 0.009473562240600586\n",
      "[step: 292] loss: 6353.0791015625\n",
      "[step: 292] loss: 0.00945610087364912\n",
      "[step: 293] loss: 6340.51123046875\n",
      "[step: 293] loss: 0.009326430968940258\n",
      "[step: 294] loss: 6277.6494140625\n",
      "[step: 294] loss: 0.009379331022500992\n",
      "[step: 295] loss: 6250.916015625\n",
      "[step: 295] loss: 0.009387071244418621\n",
      "[step: 296] loss: 6273.41357421875\n",
      "[step: 296] loss: 0.009351872839033604\n",
      "[step: 297] loss: 6282.96044921875\n",
      "[step: 297] loss: 0.009280509315431118\n",
      "[step: 298] loss: 6248.27294921875\n",
      "[step: 298] loss: 0.009336251765489578\n",
      "[step: 299] loss: 6203.84375\n",
      "[step: 299] loss: 0.009325343184173107\n",
      "[step: 300] loss: 6178.6611328125\n",
      "[step: 300] loss: 0.009321501478552818\n",
      "[step: 301] loss: 6176.67578125\n",
      "[step: 301] loss: 0.009263026528060436\n",
      "[step: 302] loss: 6187.47412109375\n",
      "[step: 302] loss: 0.009268933907151222\n",
      "[step: 303] loss: 6199.05908203125\n",
      "[step: 303] loss: 0.009254426695406437\n",
      "[step: 304] loss: 6215.416015625\n",
      "[step: 304] loss: 0.009254812262952328\n",
      "[step: 305] loss: 6236.3095703125\n",
      "[step: 305] loss: 0.009230762720108032\n",
      "[step: 306] loss: 6291.48681640625\n",
      "[step: 306] loss: 0.009226636029779911\n",
      "[step: 307] loss: 6386.0634765625\n",
      "[step: 307] loss: 0.009208137169480324\n",
      "[step: 308] loss: 6618.916015625\n",
      "[step: 308] loss: 0.00920123141258955\n",
      "[step: 309] loss: 6900.24658203125\n",
      "[step: 309] loss: 0.009200521744787693\n",
      "[step: 310] loss: 7354.8623046875\n",
      "[step: 310] loss: 0.00919499434530735\n",
      "[step: 311] loss: 7095.15478515625\n",
      "[step: 311] loss: 0.009183608926832676\n",
      "[step: 312] loss: 6532.853515625\n",
      "[step: 312] loss: 0.009166446514427662\n",
      "[step: 313] loss: 6213.384765625\n",
      "[step: 313] loss: 0.009166188538074493\n",
      "[step: 314] loss: 6500.93505859375\n",
      "[step: 314] loss: 0.009151162579655647\n",
      "[step: 315] loss: 6563.599609375\n",
      "[step: 315] loss: 0.009148097597062588\n",
      "[step: 316] loss: 6100.49853515625\n",
      "[step: 316] loss: 0.009135075844824314\n",
      "[step: 317] loss: 6249.0830078125\n",
      "[step: 317] loss: 0.009138712659478188\n",
      "[step: 318] loss: 6535.9150390625\n",
      "[step: 318] loss: 0.009130400605499744\n",
      "[step: 319] loss: 6191.22021484375\n",
      "[step: 319] loss: 0.009137381799519062\n",
      "[step: 320] loss: 6153.826171875\n",
      "[step: 320] loss: 0.009151176549494267\n",
      "[step: 321] loss: 6364.869140625\n",
      "[step: 321] loss: 0.009212980978190899\n",
      "[step: 322] loss: 6202.87646484375\n",
      "[step: 322] loss: 0.009314958937466145\n",
      "[step: 323] loss: 5987.84521484375\n",
      "[step: 323] loss: 0.00953369215130806\n",
      "[step: 324] loss: 6070.15966796875\n",
      "[step: 324] loss: 0.009482610039412975\n",
      "[step: 325] loss: 6203.28369140625\n",
      "[step: 325] loss: 0.009267990477383137\n",
      "[step: 326] loss: 6149.98095703125\n",
      "[step: 326] loss: 0.009123928844928741\n",
      "[step: 327] loss: 5999.0126953125\n",
      "[step: 327] loss: 0.009271223098039627\n",
      "[step: 328] loss: 5975.8525390625\n",
      "[step: 328] loss: 0.009361319243907928\n",
      "[step: 329] loss: 6047.6279296875\n",
      "[step: 329] loss: 0.009205630980432034\n",
      "[step: 330] loss: 6032.193359375\n",
      "[step: 330] loss: 0.009075717069208622\n",
      "[step: 331] loss: 5971.80615234375\n",
      "[step: 331] loss: 0.00912744365632534\n",
      "[step: 332] loss: 5935.2939453125\n",
      "[step: 332] loss: 0.009172752499580383\n",
      "[step: 333] loss: 5930.8583984375\n",
      "[step: 333] loss: 0.0091161597520113\n",
      "[step: 334] loss: 5956.62255859375\n",
      "[step: 334] loss: 0.009042956866323948\n",
      "[step: 335] loss: 5913.22900390625\n",
      "[step: 335] loss: 0.00908873975276947\n",
      "[step: 336] loss: 5899.89208984375\n",
      "[step: 336] loss: 0.009114397689700127\n",
      "[step: 337] loss: 5950.2578125\n",
      "[step: 337] loss: 0.009064001962542534\n",
      "[step: 338] loss: 6144.701171875\n",
      "[step: 338] loss: 0.009010257199406624\n",
      "[step: 339] loss: 6612.71875\n",
      "[step: 339] loss: 0.009045513346791267\n",
      "[step: 340] loss: 7804.27099609375\n",
      "[step: 340] loss: 0.00905877910554409\n",
      "[step: 341] loss: 9011.251953125\n",
      "[step: 341] loss: 0.00903234165161848\n",
      "[step: 342] loss: 9508.6455078125\n",
      "[step: 342] loss: 0.008987244218587875\n",
      "[step: 343] loss: 6091.66748046875\n",
      "[step: 343] loss: 0.008998689241707325\n",
      "[step: 344] loss: 7817.224609375\n",
      "[step: 344] loss: 0.009013143368065357\n",
      "[step: 345] loss: 7358.361328125\n",
      "[step: 345] loss: 0.009012735448777676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 346] loss: 6882.10888671875\n",
      "[step: 346] loss: 0.008985836058855057\n",
      "[step: 347] loss: 7484.82763671875\n",
      "[step: 347] loss: 0.008963492698967457\n",
      "[step: 348] loss: 6362.30419921875\n",
      "[step: 348] loss: 0.008951921947300434\n",
      "[step: 349] loss: 7150.6103515625\n",
      "[step: 349] loss: 0.008955421857535839\n",
      "[step: 350] loss: 6299.36865234375\n",
      "[step: 350] loss: 0.00895559974014759\n",
      "[step: 351] loss: 7052.51953125\n",
      "[step: 351] loss: 0.008952982723712921\n",
      "[step: 352] loss: 6216.04833984375\n",
      "[step: 352] loss: 0.008943301625549793\n",
      "[step: 353] loss: 6941.9130859375\n",
      "[step: 353] loss: 0.00892813503742218\n",
      "[step: 354] loss: 6100.67626953125\n",
      "[step: 354] loss: 0.008918296545743942\n",
      "[step: 355] loss: 6774.83056640625\n",
      "[step: 355] loss: 0.008910102769732475\n",
      "[step: 356] loss: 6331.203125\n",
      "[step: 356] loss: 0.0089093754068017\n",
      "[step: 357] loss: 6686.6318359375\n",
      "[step: 357] loss: 0.008905316703021526\n",
      "[step: 358] loss: 5988.68212890625\n",
      "[step: 358] loss: 0.008907457813620567\n",
      "[step: 359] loss: 6555.365234375\n",
      "[step: 359] loss: 0.008907578885555267\n",
      "[step: 360] loss: 6122.66650390625\n",
      "[step: 360] loss: 0.00891520269215107\n",
      "[step: 361] loss: 6302.93505859375\n",
      "[step: 361] loss: 0.00892217829823494\n",
      "[step: 362] loss: 5964.1865234375\n",
      "[step: 362] loss: 0.008939195424318314\n",
      "[step: 363] loss: 6218.2529296875\n",
      "[step: 363] loss: 0.008951529860496521\n",
      "[step: 364] loss: 6227.6171875\n",
      "[step: 364] loss: 0.008970539085566998\n",
      "[step: 365] loss: 5941.7392578125\n",
      "[step: 365] loss: 0.008962499909102917\n",
      "[step: 366] loss: 6180.24609375\n",
      "[step: 366] loss: 0.008940346539020538\n",
      "[step: 367] loss: 6165.232421875\n",
      "[step: 367] loss: 0.008895117789506912\n",
      "[step: 368] loss: 6090.2880859375\n",
      "[step: 368] loss: 0.008857015520334244\n",
      "[step: 369] loss: 5891.6201171875\n",
      "[step: 369] loss: 0.008838510140776634\n",
      "[step: 370] loss: 6321.52392578125\n",
      "[step: 370] loss: 0.008838275447487831\n",
      "[step: 371] loss: 5956.34716796875\n",
      "[step: 371] loss: 0.008854023180902004\n",
      "[step: 372] loss: 5850.83837890625\n",
      "[step: 372] loss: 0.0088789202272892\n",
      "[step: 373] loss: 6246.51953125\n",
      "[step: 373] loss: 0.008921366184949875\n",
      "[step: 374] loss: 5782.09375\n",
      "[step: 374] loss: 0.008951676078140736\n",
      "[step: 375] loss: 5891.08837890625\n",
      "[step: 375] loss: 0.00898691825568676\n",
      "[step: 376] loss: 6177.42041015625\n",
      "[step: 376] loss: 0.008950828574597836\n",
      "[step: 377] loss: 5710.6298828125\n",
      "[step: 377] loss: 0.008877616375684738\n",
      "[step: 378] loss: 5896.18896484375\n",
      "[step: 378] loss: 0.008807629346847534\n",
      "[step: 379] loss: 6104.412109375\n",
      "[step: 379] loss: 0.00880429521203041\n",
      "[step: 380] loss: 5671.03564453125\n",
      "[step: 380] loss: 0.00885718036442995\n",
      "[step: 381] loss: 5884.47509765625\n",
      "[step: 381] loss: 0.008917958475649357\n",
      "[step: 382] loss: 6087.5791015625\n",
      "[step: 382] loss: 0.008974742144346237\n",
      "[step: 383] loss: 5702.1923828125\n",
      "[step: 383] loss: 0.00892812293022871\n",
      "[step: 384] loss: 5912.404296875\n",
      "[step: 384] loss: 0.00882887002080679\n",
      "[step: 385] loss: 6020.7685546875\n",
      "[step: 385] loss: 0.0087692029774189\n",
      "[step: 386] loss: 5797.97265625\n",
      "[step: 386] loss: 0.008810759522020817\n",
      "[step: 387] loss: 5911.77978515625\n",
      "[step: 387] loss: 0.008894241414964199\n",
      "[step: 388] loss: 5794.14794921875\n",
      "[step: 388] loss: 0.008931384421885014\n",
      "[step: 389] loss: 5876.2607421875\n",
      "[step: 389] loss: 0.008897362276911736\n",
      "[step: 390] loss: 5851.513671875\n",
      "[step: 390] loss: 0.008780685253441334\n",
      "[step: 391] loss: 5547.029296875\n",
      "[step: 391] loss: 0.00875505618751049\n",
      "[step: 392] loss: 5772.052734375\n",
      "[step: 392] loss: 0.008837937377393246\n",
      "[step: 393] loss: 5741.68310546875\n",
      "[step: 393] loss: 0.00889229029417038\n",
      "[step: 394] loss: 5529.32421875\n",
      "[step: 394] loss: 0.008876669220626354\n",
      "[step: 395] loss: 5592.38232421875\n",
      "[step: 395] loss: 0.008783829398453236\n",
      "[step: 396] loss: 5563.8427734375\n",
      "[step: 396] loss: 0.00872527901083231\n",
      "[step: 397] loss: 5627.591796875\n",
      "[step: 397] loss: 0.008766635321080685\n",
      "[step: 398] loss: 5626.29541015625\n",
      "[step: 398] loss: 0.0088318707421422\n",
      "[step: 399] loss: 5460.3935546875\n",
      "[step: 399] loss: 0.008838312700390816\n",
      "[step: 400] loss: 5509.8623046875\n",
      "[step: 400] loss: 0.008770760148763657\n",
      "[step: 401] loss: 5533.80908203125\n",
      "[step: 401] loss: 0.008713904768228531\n",
      "[step: 402] loss: 5521.56103515625\n",
      "[step: 402] loss: 0.008717767894268036\n",
      "[step: 403] loss: 5616.955078125\n",
      "[step: 403] loss: 0.008763785474002361\n",
      "[step: 404] loss: 5573.3037109375\n",
      "[step: 404] loss: 0.008800897747278214\n",
      "[step: 405] loss: 5524.18359375\n",
      "[step: 405] loss: 0.008775888942182064\n",
      "[step: 406] loss: 5546.52197265625\n",
      "[step: 406] loss: 0.008728457614779472\n",
      "[step: 407] loss: 5460.38232421875\n",
      "[step: 407] loss: 0.00868354830890894\n",
      "[step: 408] loss: 5442.984375\n",
      "[step: 408] loss: 0.008690902963280678\n",
      "[step: 409] loss: 5448.787109375\n",
      "[step: 409] loss: 0.008735669776797295\n",
      "[step: 410] loss: 5391.47705078125\n",
      "[step: 410] loss: 0.00876129511743784\n",
      "[step: 411] loss: 5400.0927734375\n",
      "[step: 411] loss: 0.008753053843975067\n",
      "[step: 412] loss: 5413.564453125\n",
      "[step: 412] loss: 0.008707313798367977\n",
      "[step: 413] loss: 5407.01806640625\n",
      "[step: 413] loss: 0.008660203777253628\n",
      "[step: 414] loss: 5502.70654296875\n",
      "[step: 414] loss: 0.008654165081679821\n",
      "[step: 415] loss: 5701.587890625\n",
      "[step: 415] loss: 0.008689447306096554\n",
      "[step: 416] loss: 6266.89697265625\n",
      "[step: 416] loss: 0.008722969330847263\n",
      "[step: 417] loss: 7140.08251953125\n",
      "[step: 417] loss: 0.008730257861316204\n",
      "[step: 418] loss: 8421.564453125\n",
      "[step: 418] loss: 0.008712830021977425\n",
      "[step: 419] loss: 6569.546875\n",
      "[step: 419] loss: 0.008656403049826622\n",
      "[step: 420] loss: 5496.02783203125\n",
      "[step: 420] loss: 0.008622348308563232\n",
      "[step: 421] loss: 6453.5390625\n",
      "[step: 421] loss: 0.008640661835670471\n",
      "[step: 422] loss: 5927.4404296875\n",
      "[step: 422] loss: 0.008677969686686993\n",
      "[step: 423] loss: 5770.40576171875\n",
      "[step: 423] loss: 0.008716044016182423\n",
      "[step: 424] loss: 5977.76025390625\n",
      "[step: 424] loss: 0.008724982850253582\n",
      "[step: 425] loss: 5682.251953125\n",
      "[step: 425] loss: 0.008687197230756283\n",
      "[step: 426] loss: 5818.71728515625\n",
      "[step: 426] loss: 0.00862254947423935\n",
      "[step: 427] loss: 5625.4296875\n",
      "[step: 427] loss: 0.00860070064663887\n",
      "[step: 428] loss: 5763.6240234375\n",
      "[step: 428] loss: 0.008618015795946121\n",
      "[step: 429] loss: 5583.35400390625\n",
      "[step: 429] loss: 0.00865553691983223\n",
      "[step: 430] loss: 5797.03564453125\n",
      "[step: 430] loss: 0.008693495765328407\n",
      "[step: 431] loss: 5804.30517578125\n",
      "[step: 431] loss: 0.008689392358064651\n",
      "[step: 432] loss: 5507.4609375\n",
      "[step: 432] loss: 0.008650180883705616\n",
      "[step: 433] loss: 5858.021484375\n",
      "[step: 433] loss: 0.00860185083001852\n",
      "[step: 434] loss: 5504.51171875\n",
      "[step: 434] loss: 0.008575353771448135\n",
      "[step: 435] loss: 5518.92236328125\n",
      "[step: 435] loss: 0.008575888350605965\n",
      "[step: 436] loss: 5604.4892578125\n",
      "[step: 436] loss: 0.008601495996117592\n",
      "[step: 437] loss: 5331.767578125\n",
      "[step: 437] loss: 0.008626173250377178\n",
      "[step: 438] loss: 5545.767578125\n",
      "[step: 438] loss: 0.008638902567327023\n",
      "[step: 439] loss: 5337.189453125\n",
      "[step: 439] loss: 0.008646884933114052\n",
      "[step: 440] loss: 5376.06298828125\n",
      "[step: 440] loss: 0.008634204976260662\n",
      "[step: 441] loss: 5420.91845703125\n",
      "[step: 441] loss: 0.008602237328886986\n",
      "[step: 442] loss: 5273.8759765625\n",
      "[step: 442] loss: 0.008560885675251484\n",
      "[step: 443] loss: 5417.20458984375\n",
      "[step: 443] loss: 0.008539865724742413\n",
      "[step: 444] loss: 5326.8037109375\n",
      "[step: 444] loss: 0.008547414094209671\n",
      "[step: 445] loss: 5278.76806640625\n",
      "[step: 445] loss: 0.008575796149671078\n",
      "[step: 446] loss: 5345.67919921875\n",
      "[step: 446] loss: 0.008626824244856834\n",
      "[step: 447] loss: 5231.2001953125\n",
      "[step: 447] loss: 0.008693019859492779\n",
      "[step: 448] loss: 5236.83203125\n",
      "[step: 448] loss: 0.008744684979319572\n",
      "[step: 449] loss: 5256.70947265625\n",
      "[step: 449] loss: 0.008682223968207836\n",
      "[step: 450] loss: 5184.44970703125\n",
      "[step: 450] loss: 0.008561782538890839\n",
      "[step: 451] loss: 5231.10546875\n",
      "[step: 451] loss: 0.008524823933839798\n",
      "[step: 452] loss: 5197.21435546875\n",
      "[step: 452] loss: 0.008591102436184883\n",
      "[step: 453] loss: 5177.51611328125\n",
      "[step: 453] loss: 0.008689960464835167\n",
      "[step: 454] loss: 5197.923828125\n",
      "[step: 454] loss: 0.008735744282603264\n",
      "[step: 455] loss: 5169.57861328125\n",
      "[step: 455] loss: 0.008675926364958286\n",
      "[step: 456] loss: 5188.50244140625\n",
      "[step: 456] loss: 0.00854305736720562\n",
      "[step: 457] loss: 5265.02001953125\n",
      "[step: 457] loss: 0.008524568751454353\n",
      "[step: 458] loss: 5415.95751953125\n",
      "[step: 458] loss: 0.008621159940958023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 459] loss: 6045.4072265625\n",
      "[step: 459] loss: 0.00866563431918621\n",
      "[step: 460] loss: 7324.35498046875\n",
      "[step: 460] loss: 0.008604130707681179\n",
      "[step: 461] loss: 9857.91015625\n",
      "[step: 461] loss: 0.008511973544955254\n",
      "[step: 462] loss: 7036.3037109375\n",
      "[step: 462] loss: 0.00849924422800541\n",
      "[step: 463] loss: 5472.5458984375\n",
      "[step: 463] loss: 0.008574300445616245\n",
      "[step: 464] loss: 6793.130859375\n",
      "[step: 464] loss: 0.008599601686000824\n",
      "[step: 465] loss: 5678.609375\n",
      "[step: 465] loss: 0.008555568754673004\n",
      "[step: 466] loss: 6259.15478515625\n",
      "[step: 466] loss: 0.00848702434450388\n",
      "[step: 467] loss: 5797.5107421875\n",
      "[step: 467] loss: 0.008483028039336205\n",
      "[step: 468] loss: 5924.64697265625\n",
      "[step: 468] loss: 0.008525741286575794\n",
      "[step: 469] loss: 5720.7333984375\n",
      "[step: 469] loss: 0.008566569536924362\n",
      "[step: 470] loss: 5780.232421875\n",
      "[step: 470] loss: 0.008556346409022808\n",
      "[step: 471] loss: 5895.7158203125\n",
      "[step: 471] loss: 0.008508148603141308\n",
      "[step: 472] loss: 5670.3564453125\n",
      "[step: 472] loss: 0.008461368270218372\n",
      "[step: 473] loss: 5905.5947265625\n",
      "[step: 473] loss: 0.008458517491817474\n",
      "[step: 474] loss: 5343.98388671875\n",
      "[step: 474] loss: 0.008496198803186417\n",
      "[step: 475] loss: 5679.98193359375\n",
      "[step: 475] loss: 0.008523941040039062\n",
      "[step: 476] loss: 5304.78564453125\n",
      "[step: 476] loss: 0.008521475829184055\n",
      "[step: 477] loss: 5593.34814453125\n",
      "[step: 477] loss: 0.008487201295793056\n",
      "[step: 478] loss: 5356.859375\n",
      "[step: 478] loss: 0.008444718085229397\n",
      "[step: 479] loss: 5422.123046875\n",
      "[step: 479] loss: 0.008429423905909061\n",
      "[step: 480] loss: 5354.58203125\n",
      "[step: 480] loss: 0.008454118855297565\n",
      "[step: 481] loss: 5282.15625\n",
      "[step: 481] loss: 0.008482245728373528\n",
      "[step: 482] loss: 5371.5458984375\n",
      "[step: 482] loss: 0.008502659387886524\n",
      "[step: 483] loss: 5183.21875\n",
      "[step: 483] loss: 0.008492883294820786\n",
      "[step: 484] loss: 5356.1416015625\n",
      "[step: 484] loss: 0.008453389629721642\n",
      "[step: 485] loss: 5184.83154296875\n",
      "[step: 485] loss: 0.00841632392257452\n",
      "[step: 486] loss: 5213.81494140625\n",
      "[step: 486] loss: 0.008407766930758953\n",
      "[step: 487] loss: 5241.3251953125\n",
      "[step: 487] loss: 0.008420392870903015\n",
      "[step: 488] loss: 5114.9755859375\n",
      "[step: 488] loss: 0.008447743952274323\n",
      "[step: 489] loss: 5172.216796875\n",
      "[step: 489] loss: 0.00847769994288683\n",
      "[step: 490] loss: 5126.02392578125\n",
      "[step: 490] loss: 0.008491706103086472\n",
      "[step: 491] loss: 5118.154296875\n",
      "[step: 491] loss: 0.008490454405546188\n",
      "[step: 492] loss: 5092.927734375\n",
      "[step: 492] loss: 0.00846282672137022\n",
      "[step: 493] loss: 5073.69970703125\n",
      "[step: 493] loss: 0.008420329540967941\n",
      "[step: 494] loss: 5104.73291015625\n",
      "[step: 494] loss: 0.008386394940316677\n",
      "[step: 495] loss: 5033.41015625\n",
      "[step: 495] loss: 0.008380329236388206\n",
      "[step: 496] loss: 5028.14990234375\n",
      "[step: 496] loss: 0.00840156339108944\n",
      "[step: 497] loss: 5046.73095703125\n",
      "[step: 497] loss: 0.008434674702584743\n",
      "[step: 498] loss: 5004.12158203125\n",
      "[step: 498] loss: 0.008484401740133762\n",
      "[step: 499] loss: 5018.47265625\n",
      "[step: 499] loss: 0.008533813059329987\n",
      "[step: 500] loss: 4992.1025390625\n",
      "[step: 500] loss: 0.008532613515853882\n",
      "[step: 501] loss: 4970.09130859375\n",
      "[step: 501] loss: 0.008457114920020103\n",
      "[step: 502] loss: 4990.5595703125\n",
      "[step: 502] loss: 0.008378748781979084\n",
      "[step: 503] loss: 4973.27001953125\n",
      "[step: 503] loss: 0.008364877663552761\n",
      "[step: 504] loss: 4958.8603515625\n",
      "[step: 504] loss: 0.008420084603130817\n",
      "[step: 505] loss: 4974.28173828125\n",
      "[step: 505] loss: 0.00849131029099226\n",
      "[step: 506] loss: 4964.953125\n",
      "[step: 506] loss: 0.008538092486560345\n",
      "[step: 507] loss: 4960.6787109375\n",
      "[step: 507] loss: 0.008526349440217018\n",
      "[step: 508] loss: 4995.630859375\n",
      "[step: 508] loss: 0.008421333506703377\n",
      "[step: 509] loss: 5033.060546875\n",
      "[step: 509] loss: 0.008354163728654385\n",
      "[step: 510] loss: 5135.17236328125\n",
      "[step: 510] loss: 0.008397514000535011\n",
      "[step: 511] loss: 5358.609375\n",
      "[step: 511] loss: 0.008452046662569046\n",
      "[step: 512] loss: 5913.1318359375\n",
      "[step: 512] loss: 0.00848008319735527\n",
      "[step: 513] loss: 6426.333984375\n",
      "[step: 513] loss: 0.00844502355903387\n",
      "[step: 514] loss: 7115.771484375\n",
      "[step: 514] loss: 0.008356336504220963\n",
      "[step: 515] loss: 5850.861328125\n",
      "[step: 515] loss: 0.008345518261194229\n",
      "[step: 516] loss: 5015.40283203125\n",
      "[step: 516] loss: 0.008380191400647163\n",
      "[step: 517] loss: 5525.8193359375\n",
      "[step: 517] loss: 0.008409885689616203\n",
      "[step: 518] loss: 5555.76123046875\n",
      "[step: 518] loss: 0.008424279280006886\n",
      "[step: 519] loss: 5161.591796875\n",
      "[step: 519] loss: 0.00837741605937481\n",
      "[step: 520] loss: 5158.5400390625\n",
      "[step: 520] loss: 0.00832224078476429\n",
      "[step: 521] loss: 5381.755859375\n",
      "[step: 521] loss: 0.008325432427227497\n",
      "[step: 522] loss: 5435.998046875\n",
      "[step: 522] loss: 0.008333228528499603\n",
      "[step: 523] loss: 5082.341796875\n",
      "[step: 523] loss: 0.008346695452928543\n",
      "[step: 524] loss: 5265.9755859375\n",
      "[step: 524] loss: 0.008360733278095722\n",
      "[step: 525] loss: 5390.72216796875\n",
      "[step: 525] loss: 0.008337140083312988\n",
      "[step: 526] loss: 5001.07861328125\n",
      "[step: 526] loss: 0.00830500666052103\n",
      "[step: 527] loss: 5134.9873046875\n",
      "[step: 527] loss: 0.00830191932618618\n",
      "[step: 528] loss: 5118.76025390625\n",
      "[step: 528] loss: 0.008299343287944794\n",
      "[step: 529] loss: 4953.5927734375\n",
      "[step: 529] loss: 0.008294153958559036\n",
      "[step: 530] loss: 5019.037109375\n",
      "[step: 530] loss: 0.00831572711467743\n",
      "[step: 531] loss: 5001.66845703125\n",
      "[step: 531] loss: 0.008330890908837318\n",
      "[step: 532] loss: 4980.5966796875\n",
      "[step: 532] loss: 0.008328189142048359\n",
      "[step: 533] loss: 4956.7998046875\n",
      "[step: 533] loss: 0.008336685597896576\n",
      "[step: 534] loss: 4890.63818359375\n",
      "[step: 534] loss: 0.0083440151065588\n",
      "[step: 535] loss: 5001.6904296875\n",
      "[step: 535] loss: 0.008320695720613003\n",
      "[step: 536] loss: 4940.640625\n",
      "[step: 536] loss: 0.008305207826197147\n",
      "[step: 537] loss: 4862.50927734375\n",
      "[step: 537] loss: 0.00831043440848589\n",
      "[step: 538] loss: 4884.9814453125\n",
      "[step: 538] loss: 0.008294316940009594\n",
      "[step: 539] loss: 4874.66943359375\n",
      "[step: 539] loss: 0.008273829706013203\n",
      "[step: 540] loss: 4907.595703125\n",
      "[step: 540] loss: 0.008275299333035946\n",
      "[step: 541] loss: 4946.6279296875\n",
      "[step: 541] loss: 0.008266674354672432\n",
      "[step: 542] loss: 4856.8828125\n",
      "[step: 542] loss: 0.008248888887465\n",
      "[step: 543] loss: 4841.4169921875\n",
      "[step: 543] loss: 0.008245349861681461\n",
      "[step: 544] loss: 4824.283203125\n",
      "[step: 544] loss: 0.008249114267528057\n",
      "[step: 545] loss: 4772.20068359375\n",
      "[step: 545] loss: 0.008250734768807888\n",
      "[step: 546] loss: 4783.46875\n",
      "[step: 546] loss: 0.008248888887465\n",
      "[step: 547] loss: 4795.6416015625\n",
      "[step: 547] loss: 0.008262977935373783\n",
      "[step: 548] loss: 4798.0673828125\n",
      "[step: 548] loss: 0.008309401571750641\n",
      "[step: 549] loss: 4831.92919921875\n",
      "[step: 549] loss: 0.008416553027927876\n",
      "[step: 550] loss: 4963.33837890625\n",
      "[step: 550] loss: 0.008652782998979092\n",
      "[step: 551] loss: 5138.05078125\n",
      "[step: 551] loss: 0.00905083492398262\n",
      "[step: 552] loss: 5666.11083984375\n",
      "[step: 552] loss: 0.008954420685768127\n",
      "[step: 553] loss: 6321.8720703125\n",
      "[step: 553] loss: 0.008476805873215199\n",
      "[step: 554] loss: 7327.5576171875\n",
      "[step: 554] loss: 0.008516634814441204\n",
      "[step: 555] loss: 6071.2978515625\n",
      "[step: 555] loss: 0.008520416915416718\n",
      "[step: 556] loss: 5015.54150390625\n",
      "[step: 556] loss: 0.008671511895954609\n",
      "[step: 557] loss: 5304.41943359375\n",
      "[step: 557] loss: 0.008647792041301727\n",
      "[step: 558] loss: 5528.04443359375\n",
      "[step: 558] loss: 0.008420655503869057\n",
      "[step: 559] loss: 5196.310546875\n",
      "[step: 559] loss: 0.008626719005405903\n",
      "[step: 560] loss: 5031.27783203125\n",
      "[step: 560] loss: 0.008327433839440346\n",
      "[step: 561] loss: 5259.2705078125\n",
      "[step: 561] loss: 0.008501552976667881\n",
      "[step: 562] loss: 5448.658203125\n",
      "[step: 562] loss: 0.00835197139531374\n",
      "[step: 563] loss: 4899.3154296875\n",
      "[step: 563] loss: 0.008455608040094376\n",
      "[step: 564] loss: 5172.3095703125\n",
      "[step: 564] loss: 0.008263370022177696\n",
      "[step: 565] loss: 5267.25390625\n",
      "[step: 565] loss: 0.00846187025308609\n",
      "[step: 566] loss: 4847.8642578125\n",
      "[step: 566] loss: 0.008303677663207054\n",
      "[step: 567] loss: 5102.900390625\n",
      "[step: 567] loss: 0.008370485156774521\n",
      "[step: 568] loss: 4973.73291015625\n",
      "[step: 568] loss: 0.008276411332190037\n",
      "[step: 569] loss: 4865.5205078125\n",
      "[step: 569] loss: 0.008322285488247871\n",
      "[step: 570] loss: 4941.0947265625\n",
      "[step: 570] loss: 0.008271107450127602\n",
      "[step: 571] loss: 4882.72607421875\n",
      "[step: 571] loss: 0.008252741768956184\n",
      "[step: 572] loss: 4900.2236328125\n",
      "[step: 572] loss: 0.008298085071146488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 573] loss: 4770.6455078125\n",
      "[step: 573] loss: 0.008225817233324051\n",
      "[step: 574] loss: 4876.1767578125\n",
      "[step: 574] loss: 0.008268485777080059\n",
      "[step: 575] loss: 4846.1875\n",
      "[step: 575] loss: 0.008211257867515087\n",
      "[step: 576] loss: 4724.85986328125\n",
      "[step: 576] loss: 0.00826515443623066\n",
      "[step: 577] loss: 4754.142578125\n",
      "[step: 577] loss: 0.008192842826247215\n",
      "[step: 578] loss: 4790.8212890625\n",
      "[step: 578] loss: 0.008235974237322807\n",
      "[step: 579] loss: 4743.30078125\n",
      "[step: 579] loss: 0.008200755342841148\n",
      "[step: 580] loss: 4753.3994140625\n",
      "[step: 580] loss: 0.008198263123631477\n",
      "[step: 581] loss: 4678.3134765625\n",
      "[step: 581] loss: 0.00820200890302658\n",
      "[step: 582] loss: 4672.43408203125\n",
      "[step: 582] loss: 0.008170933462679386\n",
      "[step: 583] loss: 4703.533203125\n",
      "[step: 583] loss: 0.008200997486710548\n",
      "[step: 584] loss: 4694.0703125\n",
      "[step: 584] loss: 0.00816341582685709\n",
      "[step: 585] loss: 4685.9580078125\n",
      "[step: 585] loss: 0.008172468282282352\n",
      "[step: 586] loss: 4703.853515625\n",
      "[step: 586] loss: 0.008167649619281292\n",
      "[step: 587] loss: 4656.798828125\n",
      "[step: 587] loss: 0.008154879324138165\n",
      "[step: 588] loss: 4663.4677734375\n",
      "[step: 588] loss: 0.008166326209902763\n",
      "[step: 589] loss: 4681.12255859375\n",
      "[step: 589] loss: 0.008143984712660313\n",
      "[step: 590] loss: 4713.0439453125\n",
      "[step: 590] loss: 0.008146218955516815\n",
      "[step: 591] loss: 4747.564453125\n",
      "[step: 591] loss: 0.008148694410920143\n",
      "[step: 592] loss: 4913.87548828125\n",
      "[step: 592] loss: 0.008132269605994225\n",
      "[step: 593] loss: 5169.8193359375\n",
      "[step: 593] loss: 0.008135169744491577\n",
      "[step: 594] loss: 5675.11962890625\n",
      "[step: 594] loss: 0.008127677254378796\n",
      "[step: 595] loss: 5967.85400390625\n",
      "[step: 595] loss: 0.008119923062622547\n",
      "[step: 596] loss: 6301.8427734375\n",
      "[step: 596] loss: 0.00812453217804432\n",
      "[step: 597] loss: 5300.380859375\n",
      "[step: 597] loss: 0.008115950040519238\n",
      "[step: 598] loss: 4659.79052734375\n",
      "[step: 598] loss: 0.008108558133244514\n",
      "[step: 599] loss: 5178.73779296875\n",
      "[step: 599] loss: 0.00811067782342434\n",
      "[step: 600] loss: 5302.87060546875\n",
      "[step: 600] loss: 0.008103260770440102\n",
      "[step: 601] loss: 4846.03173828125\n",
      "[step: 601] loss: 0.008097520098090172\n",
      "[step: 602] loss: 4702.24609375\n",
      "[step: 602] loss: 0.008100172504782677\n",
      "[step: 603] loss: 5157.48876953125\n",
      "[step: 603] loss: 0.008096287958323956\n",
      "[step: 604] loss: 5306.39404296875\n",
      "[step: 604] loss: 0.008090866729617119\n",
      "[step: 605] loss: 4703.8701171875\n",
      "[step: 605] loss: 0.008089432492852211\n",
      "[step: 606] loss: 4946.90478515625\n",
      "[step: 606] loss: 0.008087129332125187\n",
      "[step: 607] loss: 5201.890625\n",
      "[step: 607] loss: 0.00808227714151144\n",
      "[step: 608] loss: 4831.958984375\n",
      "[step: 608] loss: 0.008078616112470627\n",
      "[step: 609] loss: 4830.51025390625\n",
      "[step: 609] loss: 0.008080665953457355\n",
      "[step: 610] loss: 5036.8857421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-759f66f7d028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[step: {}] loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_loss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mloss_for_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_loss1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX_with_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainY_with_noise\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[step: {}] loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_loss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/taxi_data/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/taxi_data/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =48\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1500\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + 24, :]\n",
    "        _y = time_series[i+24:i+48,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, (seq_length/2),data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None, (seq_length/2),output_dim])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, (seq_length/2),data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None, (seq_length/2),output_dim])\n",
    "\n",
    "#LSTM CELL\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,:], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_mean(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,:], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE \n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
