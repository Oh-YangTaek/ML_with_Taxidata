{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-666fc2204338>:90: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 12110.4833984375\n",
      "[step: 1] loss: 4323.2626953125\n",
      "[step: 2] loss: 9444.328125\n",
      "[step: 3] loss: 3525.119384765625\n",
      "[step: 4] loss: 3382.76416015625\n",
      "[step: 5] loss: 3057.71630859375\n",
      "[step: 6] loss: 2941.58935546875\n",
      "[step: 7] loss: 2655.846435546875\n",
      "[step: 8] loss: 2220.332275390625\n",
      "[step: 9] loss: 1879.392822265625\n",
      "[step: 10] loss: 1669.26708984375\n",
      "[step: 11] loss: 1581.265625\n",
      "[step: 12] loss: 1586.117431640625\n",
      "[step: 13] loss: 1601.7105712890625\n",
      "[step: 14] loss: 1586.043212890625\n",
      "[step: 15] loss: 1589.8408203125\n",
      "[step: 16] loss: 1520.392578125\n",
      "[step: 17] loss: 1475.1068115234375\n",
      "[step: 18] loss: 1452.75927734375\n",
      "[step: 19] loss: 1391.042724609375\n",
      "[step: 20] loss: 1364.839111328125\n",
      "[step: 21] loss: 1338.134521484375\n",
      "[step: 22] loss: 1290.3062744140625\n",
      "[step: 23] loss: 1259.8321533203125\n",
      "[step: 24] loss: 1244.4052734375\n",
      "[step: 25] loss: 1226.07568359375\n",
      "[step: 26] loss: 1196.284912109375\n",
      "[step: 27] loss: 1171.42626953125\n",
      "[step: 28] loss: 1135.5577392578125\n",
      "[step: 29] loss: 1100.563720703125\n",
      "[step: 30] loss: 1063.7799072265625\n",
      "[step: 31] loss: 1025.2294921875\n",
      "[step: 32] loss: 988.2506713867188\n",
      "[step: 33] loss: 950.12744140625\n",
      "[step: 34] loss: 897.67529296875\n",
      "[step: 35] loss: 849.3388671875\n",
      "[step: 36] loss: 809.7522583007812\n",
      "[step: 37] loss: 773.2401123046875\n",
      "[step: 38] loss: 732.0884399414062\n",
      "[step: 39] loss: 743.3843994140625\n",
      "[step: 40] loss: 749.0299072265625\n",
      "[step: 41] loss: 694.9358520507812\n",
      "[step: 42] loss: 643.322509765625\n",
      "[step: 43] loss: 654.256591796875\n",
      "[step: 44] loss: 587.6539306640625\n",
      "[step: 45] loss: 571.7556762695312\n",
      "[step: 46] loss: 558.7850952148438\n",
      "[step: 47] loss: 550.0348510742188\n",
      "[step: 48] loss: 540.5823974609375\n",
      "[step: 49] loss: 512.2808837890625\n",
      "[step: 50] loss: 526.3060913085938\n",
      "[step: 51] loss: 489.1324462890625\n",
      "[step: 52] loss: 497.8471374511719\n",
      "[step: 53] loss: 469.72369384765625\n",
      "[step: 54] loss: 465.05908203125\n",
      "[step: 55] loss: 444.3367919921875\n",
      "[step: 56] loss: 453.74310302734375\n",
      "[step: 57] loss: 444.95269775390625\n",
      "[step: 58] loss: 424.9780578613281\n",
      "[step: 59] loss: 426.5540466308594\n",
      "[step: 60] loss: 409.8597412109375\n",
      "[step: 61] loss: 418.42584228515625\n",
      "[step: 62] loss: 400.9400939941406\n",
      "[step: 63] loss: 398.822998046875\n",
      "[step: 64] loss: 393.05316162109375\n",
      "[step: 65] loss: 384.02801513671875\n",
      "[step: 66] loss: 385.13287353515625\n",
      "[step: 67] loss: 368.9176025390625\n",
      "[step: 68] loss: 394.9317932128906\n",
      "[step: 69] loss: 375.0220642089844\n",
      "[step: 70] loss: 384.1787109375\n",
      "[step: 71] loss: 370.23858642578125\n",
      "[step: 72] loss: 366.7572021484375\n",
      "[step: 73] loss: 368.91082763671875\n",
      "[step: 74] loss: 358.08428955078125\n",
      "[step: 75] loss: 366.6488342285156\n",
      "[step: 76] loss: 351.88922119140625\n",
      "[step: 77] loss: 354.08587646484375\n",
      "[step: 78] loss: 352.877197265625\n",
      "[step: 79] loss: 355.77178955078125\n",
      "[step: 80] loss: 339.4713439941406\n",
      "[step: 81] loss: 348.02783203125\n",
      "[step: 82] loss: 334.8774108886719\n",
      "[step: 83] loss: 340.535400390625\n",
      "[step: 84] loss: 324.45184326171875\n",
      "[step: 85] loss: 332.66961669921875\n",
      "[step: 86] loss: 329.35211181640625\n",
      "[step: 87] loss: 326.7210693359375\n",
      "[step: 88] loss: 323.87933349609375\n",
      "[step: 89] loss: 322.2219543457031\n",
      "[step: 90] loss: 321.6227722167969\n",
      "[step: 91] loss: 319.269775390625\n",
      "[step: 92] loss: 315.88287353515625\n",
      "[step: 93] loss: 323.76324462890625\n",
      "[step: 94] loss: 320.20416259765625\n",
      "[step: 95] loss: 312.4456481933594\n",
      "[step: 96] loss: 316.10235595703125\n",
      "[step: 97] loss: 315.3240966796875\n",
      "[step: 98] loss: 309.2221984863281\n",
      "[step: 99] loss: 315.1489562988281\n",
      "[step: 100] loss: 310.800048828125\n",
      "[step: 101] loss: 314.09832763671875\n",
      "[step: 102] loss: 305.1573791503906\n",
      "[step: 103] loss: 301.1098327636719\n",
      "[step: 104] loss: 303.07281494140625\n",
      "[step: 105] loss: 298.69927978515625\n",
      "[step: 106] loss: 293.5738525390625\n",
      "[step: 107] loss: 294.1418151855469\n",
      "[step: 108] loss: 299.6427001953125\n",
      "[step: 109] loss: 294.669921875\n",
      "[step: 110] loss: 291.18951416015625\n",
      "[step: 111] loss: 295.30511474609375\n",
      "[step: 112] loss: 286.30810546875\n",
      "[step: 113] loss: 287.38287353515625\n",
      "[step: 114] loss: 287.4344787597656\n",
      "[step: 115] loss: 284.241455078125\n",
      "[step: 116] loss: 286.62548828125\n",
      "[step: 117] loss: 286.37054443359375\n",
      "[step: 118] loss: 282.48382568359375\n",
      "[step: 119] loss: 286.3681640625\n",
      "[step: 120] loss: 286.7635498046875\n",
      "[step: 121] loss: 280.01507568359375\n",
      "[step: 122] loss: 276.3109130859375\n",
      "[step: 123] loss: 277.88916015625\n",
      "[step: 124] loss: 276.3536376953125\n",
      "[step: 125] loss: 271.359130859375\n",
      "[step: 126] loss: 275.4975891113281\n",
      "[step: 127] loss: 273.12939453125\n",
      "[step: 128] loss: 271.22430419921875\n",
      "[step: 129] loss: 271.46893310546875\n",
      "[step: 130] loss: 263.32421875\n",
      "[step: 131] loss: 269.4466857910156\n",
      "[step: 132] loss: 267.73028564453125\n",
      "[step: 133] loss: 268.6523742675781\n",
      "[step: 134] loss: 259.79266357421875\n",
      "[step: 135] loss: 259.33868408203125\n",
      "[step: 136] loss: 260.6598815917969\n",
      "[step: 137] loss: 264.1390380859375\n",
      "[step: 138] loss: 259.2218933105469\n",
      "[step: 139] loss: 263.6067199707031\n",
      "[step: 140] loss: 258.2530212402344\n",
      "[step: 141] loss: 262.62017822265625\n",
      "[step: 142] loss: 257.54248046875\n",
      "[step: 143] loss: 263.84161376953125\n",
      "[step: 144] loss: 258.4850769042969\n",
      "[step: 145] loss: 250.5469970703125\n",
      "[step: 146] loss: 248.19290161132812\n",
      "[step: 147] loss: 257.2842102050781\n",
      "[step: 148] loss: 245.11309814453125\n",
      "[step: 149] loss: 252.38348388671875\n",
      "[step: 150] loss: 256.052490234375\n",
      "[step: 151] loss: 253.6596221923828\n",
      "[step: 152] loss: 247.06320190429688\n",
      "[step: 153] loss: 258.3995361328125\n",
      "[step: 154] loss: 243.9107666015625\n",
      "[step: 155] loss: 250.4700927734375\n",
      "[step: 156] loss: 248.45347595214844\n",
      "[step: 157] loss: 244.19366455078125\n",
      "[step: 158] loss: 243.83372497558594\n",
      "[step: 159] loss: 253.53433227539062\n",
      "[step: 160] loss: 247.51211547851562\n",
      "[step: 161] loss: 245.3209686279297\n",
      "[step: 162] loss: 246.0845947265625\n",
      "[step: 163] loss: 250.93856811523438\n",
      "[step: 164] loss: 250.5760040283203\n",
      "[step: 165] loss: 239.81434631347656\n",
      "[step: 166] loss: 248.20318603515625\n",
      "[step: 167] loss: 244.14718627929688\n",
      "[step: 168] loss: 245.2101287841797\n",
      "[step: 169] loss: 239.1015625\n",
      "[step: 170] loss: 238.73426818847656\n",
      "[step: 171] loss: 248.2958984375\n",
      "[step: 172] loss: 236.9336700439453\n",
      "[step: 173] loss: 243.05645751953125\n",
      "[step: 174] loss: 240.39166259765625\n",
      "[step: 175] loss: 228.0335235595703\n",
      "[step: 176] loss: 232.5639190673828\n",
      "[step: 177] loss: 244.28781127929688\n",
      "[step: 178] loss: 231.35150146484375\n",
      "[step: 179] loss: 235.91302490234375\n",
      "[step: 180] loss: 239.1883544921875\n",
      "[step: 181] loss: 236.59674072265625\n",
      "[step: 182] loss: 237.6165771484375\n",
      "[step: 183] loss: 230.99081420898438\n",
      "[step: 184] loss: 235.42269897460938\n",
      "[step: 185] loss: 230.82901000976562\n",
      "[step: 186] loss: 227.17446899414062\n",
      "[step: 187] loss: 233.51947021484375\n",
      "[step: 188] loss: 227.602783203125\n",
      "[step: 189] loss: 226.77651977539062\n",
      "[step: 190] loss: 225.71356201171875\n",
      "[step: 191] loss: 229.5353240966797\n",
      "[step: 192] loss: 227.7053985595703\n",
      "[step: 193] loss: 226.96009826660156\n",
      "[step: 194] loss: 229.0233612060547\n",
      "[step: 195] loss: 220.611328125\n",
      "[step: 196] loss: 236.8285675048828\n",
      "[step: 197] loss: 218.99322509765625\n",
      "[step: 198] loss: 224.07241821289062\n",
      "[step: 199] loss: 226.8922882080078\n",
      "[step: 200] loss: 226.29698181152344\n",
      "[step: 201] loss: 222.80783081054688\n",
      "[step: 202] loss: 220.85821533203125\n",
      "[step: 203] loss: 222.9119873046875\n",
      "[step: 204] loss: 221.62484741210938\n",
      "[step: 205] loss: 217.82070922851562\n",
      "[step: 206] loss: 224.2556915283203\n",
      "[step: 207] loss: 225.23043823242188\n",
      "[step: 208] loss: 222.19119262695312\n",
      "[step: 209] loss: 225.63218688964844\n",
      "[step: 210] loss: 216.567626953125\n",
      "[step: 211] loss: 217.8416748046875\n",
      "[step: 212] loss: 231.69961547851562\n",
      "[step: 213] loss: 221.87353515625\n",
      "[step: 214] loss: 215.69439697265625\n",
      "[step: 215] loss: 221.32046508789062\n",
      "[step: 216] loss: 224.75515747070312\n",
      "[step: 217] loss: 216.85516357421875\n",
      "[step: 218] loss: 222.28094482421875\n",
      "[step: 219] loss: 220.13665771484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 220] loss: 213.1904754638672\n",
      "[step: 221] loss: 212.73211669921875\n",
      "[step: 222] loss: 217.82501220703125\n",
      "[step: 223] loss: 224.25827026367188\n",
      "[step: 224] loss: 227.3611297607422\n",
      "[step: 225] loss: 222.15284729003906\n",
      "[step: 226] loss: 212.20867919921875\n",
      "[step: 227] loss: 224.48007202148438\n",
      "[step: 228] loss: 223.7059326171875\n",
      "[step: 229] loss: 202.72767639160156\n",
      "[step: 230] loss: 217.775634765625\n",
      "[step: 231] loss: 212.9490509033203\n",
      "[step: 232] loss: 209.51400756835938\n",
      "[step: 233] loss: 213.2264862060547\n",
      "[step: 234] loss: 213.2304229736328\n",
      "[step: 235] loss: 204.075927734375\n",
      "[step: 236] loss: 209.33070373535156\n",
      "[step: 237] loss: 217.2452392578125\n",
      "[step: 238] loss: 212.04708862304688\n",
      "[step: 239] loss: 202.80963134765625\n",
      "[step: 240] loss: 216.0485076904297\n",
      "[step: 241] loss: 204.15435791015625\n",
      "[step: 242] loss: 210.071044921875\n",
      "[step: 243] loss: 225.77755737304688\n",
      "[step: 244] loss: 211.52488708496094\n",
      "[step: 245] loss: 202.7921905517578\n",
      "[step: 246] loss: 217.59681701660156\n",
      "[step: 247] loss: 214.06016540527344\n",
      "[step: 248] loss: 204.2333984375\n",
      "[step: 249] loss: 207.20632934570312\n",
      "[step: 250] loss: 211.50540161132812\n",
      "[step: 251] loss: 205.2465057373047\n",
      "[step: 252] loss: 207.89984130859375\n",
      "[step: 253] loss: 207.32998657226562\n",
      "[step: 254] loss: 206.23568725585938\n",
      "[step: 255] loss: 204.79124450683594\n",
      "[step: 256] loss: 211.25396728515625\n",
      "[step: 257] loss: 211.58155822753906\n",
      "[step: 258] loss: 205.84140014648438\n",
      "[step: 259] loss: 210.3050079345703\n",
      "[step: 260] loss: 204.2857666015625\n",
      "[step: 261] loss: 201.67645263671875\n",
      "[step: 262] loss: 210.74893188476562\n",
      "[step: 263] loss: 200.86178588867188\n",
      "[step: 264] loss: 213.07225036621094\n",
      "[step: 265] loss: 203.27622985839844\n",
      "[step: 266] loss: 195.557861328125\n",
      "[step: 267] loss: 202.59681701660156\n",
      "[step: 268] loss: 197.94927978515625\n",
      "[step: 269] loss: 197.11737060546875\n",
      "[step: 270] loss: 194.04983520507812\n",
      "[step: 271] loss: 201.0667724609375\n",
      "[step: 272] loss: 195.90213012695312\n",
      "[step: 273] loss: 202.8536834716797\n",
      "[step: 274] loss: 201.95462036132812\n",
      "[step: 275] loss: 207.267333984375\n",
      "[step: 276] loss: 196.25491333007812\n",
      "[step: 277] loss: 207.61474609375\n",
      "[step: 278] loss: 202.04116821289062\n",
      "[step: 279] loss: 199.23641967773438\n",
      "[step: 280] loss: 207.89263916015625\n",
      "[step: 281] loss: 203.27963256835938\n",
      "[step: 282] loss: 203.53021240234375\n",
      "[step: 283] loss: 199.93145751953125\n",
      "[step: 284] loss: 195.69326782226562\n",
      "[step: 285] loss: 203.63404846191406\n",
      "[step: 286] loss: 199.99172973632812\n",
      "[step: 287] loss: 194.33926391601562\n",
      "[step: 288] loss: 198.69302368164062\n",
      "[step: 289] loss: 197.57418823242188\n",
      "[step: 290] loss: 199.6476287841797\n",
      "[step: 291] loss: 206.3813018798828\n",
      "[step: 292] loss: 197.25421142578125\n",
      "[step: 293] loss: 195.49444580078125\n",
      "[step: 294] loss: 199.1540069580078\n",
      "[step: 295] loss: 193.16619873046875\n",
      "[step: 296] loss: 194.40768432617188\n",
      "[step: 297] loss: 192.06166076660156\n",
      "[step: 298] loss: 190.38331604003906\n",
      "[step: 299] loss: 188.54067993164062\n",
      "[step: 300] loss: 195.5506591796875\n",
      "[step: 301] loss: 189.45355224609375\n",
      "[step: 302] loss: 196.21347045898438\n",
      "[step: 303] loss: 192.6650390625\n",
      "[step: 304] loss: 190.12271118164062\n",
      "[step: 305] loss: 192.17169189453125\n",
      "[step: 306] loss: 186.37796020507812\n",
      "[step: 307] loss: 190.371337890625\n",
      "[step: 308] loss: 189.1945343017578\n",
      "[step: 309] loss: 181.05874633789062\n",
      "[step: 310] loss: 186.04971313476562\n",
      "[step: 311] loss: 184.55416870117188\n",
      "[step: 312] loss: 191.64999389648438\n",
      "[step: 313] loss: 196.92950439453125\n",
      "[step: 314] loss: 184.66659545898438\n",
      "[step: 315] loss: 186.12637329101562\n",
      "[step: 316] loss: 184.9906005859375\n",
      "[step: 317] loss: 185.79905700683594\n",
      "[step: 318] loss: 188.2730712890625\n",
      "[step: 319] loss: 184.3417510986328\n",
      "[step: 320] loss: 192.96023559570312\n",
      "[step: 321] loss: 182.43634033203125\n",
      "[step: 322] loss: 186.16395568847656\n",
      "[step: 323] loss: 183.22598266601562\n",
      "[step: 324] loss: 183.98373413085938\n",
      "[step: 325] loss: 182.7240753173828\n",
      "[step: 326] loss: 186.15768432617188\n",
      "[step: 327] loss: 185.1028289794922\n",
      "[step: 328] loss: 180.53289794921875\n",
      "[step: 329] loss: 189.2177276611328\n",
      "[step: 330] loss: 182.472900390625\n",
      "[step: 331] loss: 182.25057983398438\n",
      "[step: 332] loss: 189.62158203125\n",
      "[step: 333] loss: 189.0423583984375\n",
      "[step: 334] loss: 183.48028564453125\n",
      "[step: 335] loss: 181.04302978515625\n",
      "[step: 336] loss: 186.6887664794922\n",
      "[step: 337] loss: 181.4632568359375\n",
      "[step: 338] loss: 187.208740234375\n",
      "[step: 339] loss: 180.81158447265625\n",
      "[step: 340] loss: 185.32174682617188\n",
      "[step: 341] loss: 192.78823852539062\n",
      "[step: 342] loss: 185.3328857421875\n",
      "[step: 343] loss: 197.32205200195312\n",
      "[step: 344] loss: 187.21826171875\n",
      "[step: 345] loss: 180.30364990234375\n",
      "[step: 346] loss: 195.8271484375\n",
      "[step: 347] loss: 197.10328674316406\n",
      "[step: 348] loss: 180.61366271972656\n",
      "[step: 349] loss: 182.8150634765625\n",
      "[step: 350] loss: 191.05902099609375\n",
      "[step: 351] loss: 181.07052612304688\n",
      "[step: 352] loss: 186.4437255859375\n",
      "[step: 353] loss: 183.9591522216797\n",
      "[step: 354] loss: 191.55648803710938\n",
      "[step: 355] loss: 193.0792694091797\n",
      "[step: 356] loss: 179.97122192382812\n",
      "[step: 357] loss: 182.81472778320312\n",
      "[step: 358] loss: 187.50308227539062\n",
      "[step: 359] loss: 182.94729614257812\n",
      "[step: 360] loss: 176.3809051513672\n",
      "[step: 361] loss: 183.77719116210938\n",
      "[step: 362] loss: 182.2593536376953\n",
      "[step: 363] loss: 178.96421813964844\n",
      "[step: 364] loss: 180.23875427246094\n",
      "[step: 365] loss: 180.9484405517578\n",
      "[step: 366] loss: 177.02755737304688\n",
      "[step: 367] loss: 177.68624877929688\n",
      "[step: 368] loss: 179.93927001953125\n",
      "[step: 369] loss: 179.371826171875\n",
      "[step: 370] loss: 179.40777587890625\n",
      "[step: 371] loss: 177.73370361328125\n",
      "[step: 372] loss: 172.4342041015625\n",
      "[step: 373] loss: 179.3449249267578\n",
      "[step: 374] loss: 171.44638061523438\n",
      "[step: 375] loss: 175.68934631347656\n",
      "[step: 376] loss: 174.97459411621094\n",
      "[step: 377] loss: 169.33123779296875\n",
      "[step: 378] loss: 184.46554565429688\n",
      "[step: 379] loss: 173.25381469726562\n",
      "[step: 380] loss: 172.78350830078125\n",
      "[step: 381] loss: 179.31301879882812\n",
      "[step: 382] loss: 182.00303649902344\n",
      "[step: 383] loss: 175.73165893554688\n",
      "[step: 384] loss: 177.8209686279297\n",
      "[step: 385] loss: 170.92575073242188\n",
      "[step: 386] loss: 166.43618774414062\n",
      "[step: 387] loss: 171.19313049316406\n",
      "[step: 388] loss: 174.314453125\n",
      "[step: 389] loss: 170.27337646484375\n",
      "[step: 390] loss: 176.17247009277344\n",
      "[step: 391] loss: 182.26417541503906\n",
      "[step: 392] loss: 186.10772705078125\n",
      "[step: 393] loss: 177.5851593017578\n",
      "[step: 394] loss: 176.0702667236328\n",
      "[step: 395] loss: 191.42401123046875\n",
      "[step: 396] loss: 188.628173828125\n",
      "[step: 397] loss: 175.75579833984375\n",
      "[step: 398] loss: 184.85597229003906\n",
      "[step: 399] loss: 187.3273468017578\n",
      "[step: 400] loss: 173.3629608154297\n",
      "[step: 401] loss: 184.38442993164062\n",
      "[step: 402] loss: 190.41709899902344\n",
      "[step: 403] loss: 173.26918029785156\n",
      "[step: 404] loss: 179.72647094726562\n",
      "[step: 405] loss: 174.41329956054688\n",
      "[step: 406] loss: 167.175048828125\n",
      "[step: 407] loss: 169.5901336669922\n",
      "[step: 408] loss: 172.90692138671875\n",
      "[step: 409] loss: 173.55288696289062\n",
      "[step: 410] loss: 176.85848999023438\n",
      "[step: 411] loss: 171.85986328125\n",
      "[step: 412] loss: 172.08816528320312\n",
      "[step: 413] loss: 166.17401123046875\n",
      "[step: 414] loss: 168.35836791992188\n",
      "[step: 415] loss: 164.74533081054688\n",
      "[step: 416] loss: 172.3525390625\n",
      "[step: 417] loss: 164.98587036132812\n",
      "[step: 418] loss: 172.30609130859375\n",
      "[step: 419] loss: 169.7621612548828\n",
      "[step: 420] loss: 161.80502319335938\n",
      "[step: 421] loss: 163.1968536376953\n",
      "[step: 422] loss: 164.58859252929688\n",
      "[step: 423] loss: 162.0802459716797\n",
      "[step: 424] loss: 167.66378784179688\n",
      "[step: 425] loss: 164.61935424804688\n",
      "[step: 426] loss: 170.49237060546875\n",
      "[step: 427] loss: 171.53286743164062\n",
      "[step: 428] loss: 165.00924682617188\n",
      "[step: 429] loss: 165.48216247558594\n",
      "[step: 430] loss: 165.380126953125\n",
      "[step: 431] loss: 170.36361694335938\n",
      "[step: 432] loss: 161.6444091796875\n",
      "[step: 433] loss: 170.80674743652344\n",
      "[step: 434] loss: 175.5806884765625\n",
      "[step: 435] loss: 166.5365753173828\n",
      "[step: 436] loss: 157.7470703125\n",
      "[step: 437] loss: 161.9302215576172\n",
      "[step: 438] loss: 174.2316436767578\n",
      "[step: 439] loss: 167.5110321044922\n",
      "[step: 440] loss: 161.34283447265625\n",
      "[step: 441] loss: 167.74916076660156\n",
      "[step: 442] loss: 179.59596252441406\n",
      "[step: 443] loss: 176.01449584960938\n",
      "[step: 444] loss: 159.63473510742188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 445] loss: 181.9324493408203\n",
      "[step: 446] loss: 172.34848022460938\n",
      "[step: 447] loss: 158.4947967529297\n",
      "[step: 448] loss: 175.555908203125\n",
      "[step: 449] loss: 175.35586547851562\n",
      "[step: 450] loss: 176.8648681640625\n",
      "[step: 451] loss: 173.82424926757812\n",
      "[step: 452] loss: 162.99688720703125\n",
      "[step: 453] loss: 177.96673583984375\n",
      "[step: 454] loss: 165.3805389404297\n",
      "[step: 455] loss: 166.404052734375\n",
      "[step: 456] loss: 188.97506713867188\n",
      "[step: 457] loss: 157.56295776367188\n",
      "[step: 458] loss: 184.6947479248047\n",
      "[step: 459] loss: 170.88040161132812\n",
      "[step: 460] loss: 173.56024169921875\n",
      "[step: 461] loss: 175.531982421875\n",
      "[step: 462] loss: 168.9223175048828\n",
      "[step: 463] loss: 175.78175354003906\n",
      "[step: 464] loss: 163.26498413085938\n",
      "[step: 465] loss: 172.740478515625\n",
      "[step: 466] loss: 164.76626586914062\n",
      "[step: 467] loss: 167.82351684570312\n",
      "[step: 468] loss: 163.45611572265625\n",
      "[step: 469] loss: 168.13851928710938\n",
      "[step: 470] loss: 166.94915771484375\n",
      "[step: 471] loss: 161.62399291992188\n",
      "[step: 472] loss: 165.33212280273438\n",
      "[step: 473] loss: 163.588623046875\n",
      "[step: 474] loss: 169.28659057617188\n",
      "[step: 475] loss: 161.9064178466797\n",
      "[step: 476] loss: 169.4500732421875\n",
      "[step: 477] loss: 161.5820770263672\n",
      "[step: 478] loss: 173.87640380859375\n",
      "[step: 479] loss: 164.62451171875\n",
      "[step: 480] loss: 158.87344360351562\n",
      "[step: 481] loss: 171.5364990234375\n",
      "[step: 482] loss: 158.13772583007812\n",
      "[step: 483] loss: 157.95492553710938\n",
      "[step: 484] loss: 162.54888916015625\n",
      "[step: 485] loss: 157.42596435546875\n",
      "[step: 486] loss: 162.58328247070312\n",
      "[step: 487] loss: 162.94619750976562\n",
      "[step: 488] loss: 159.52915954589844\n",
      "[step: 489] loss: 159.380859375\n",
      "[step: 490] loss: 156.5640106201172\n",
      "[step: 491] loss: 162.72877502441406\n",
      "[step: 492] loss: 156.90582275390625\n",
      "[step: 493] loss: 168.57305908203125\n",
      "[step: 494] loss: 167.2813720703125\n",
      "[step: 495] loss: 163.29473876953125\n",
      "[step: 496] loss: 169.08213806152344\n",
      "[step: 497] loss: 156.33859252929688\n",
      "[step: 498] loss: 163.36569213867188\n",
      "[step: 499] loss: 164.37030029296875\n",
      "[step: 500] loss: 154.34515380859375\n",
      "[step: 501] loss: 159.69476318359375\n",
      "[step: 502] loss: 158.44508361816406\n",
      "[step: 503] loss: 156.02688598632812\n",
      "[step: 504] loss: 159.55459594726562\n",
      "[step: 505] loss: 155.133056640625\n",
      "[step: 506] loss: 155.1055908203125\n",
      "[step: 507] loss: 154.01792907714844\n",
      "[step: 508] loss: 158.38339233398438\n",
      "[step: 509] loss: 155.48182678222656\n",
      "[step: 510] loss: 148.47012329101562\n",
      "[step: 511] loss: 155.2198944091797\n",
      "[step: 512] loss: 157.228759765625\n",
      "[step: 513] loss: 151.6072998046875\n",
      "[step: 514] loss: 152.62452697753906\n",
      "[step: 515] loss: 153.54635620117188\n",
      "[step: 516] loss: 154.52638244628906\n",
      "[step: 517] loss: 151.02630615234375\n",
      "[step: 518] loss: 151.49642944335938\n",
      "[step: 519] loss: 157.62350463867188\n",
      "[step: 520] loss: 155.738525390625\n",
      "[step: 521] loss: 155.66331481933594\n",
      "[step: 522] loss: 149.38827514648438\n",
      "[step: 523] loss: 155.0431365966797\n",
      "[step: 524] loss: 154.52252197265625\n",
      "[step: 525] loss: 155.93557739257812\n",
      "[step: 526] loss: 149.9979705810547\n",
      "[step: 527] loss: 149.73313903808594\n",
      "[step: 528] loss: 148.34652709960938\n",
      "[step: 529] loss: 151.35745239257812\n",
      "[step: 530] loss: 149.34478759765625\n",
      "[step: 531] loss: 148.9388885498047\n",
      "[step: 532] loss: 150.06832885742188\n",
      "[step: 533] loss: 159.7233428955078\n",
      "[step: 534] loss: 152.5542755126953\n",
      "[step: 535] loss: 150.45516967773438\n",
      "[step: 536] loss: 148.38380432128906\n",
      "[step: 537] loss: 150.76817321777344\n",
      "[step: 538] loss: 148.29505920410156\n",
      "[step: 539] loss: 149.4678955078125\n",
      "[step: 540] loss: 149.3783416748047\n",
      "[step: 541] loss: 146.75999450683594\n",
      "[step: 542] loss: 145.35928344726562\n",
      "[step: 543] loss: 145.22402954101562\n",
      "[step: 544] loss: 145.1421661376953\n",
      "[step: 545] loss: 148.35848999023438\n",
      "[step: 546] loss: 148.3079833984375\n",
      "[step: 547] loss: 153.81008911132812\n",
      "[step: 548] loss: 149.93777465820312\n",
      "[step: 549] loss: 157.27528381347656\n",
      "[step: 550] loss: 159.20523071289062\n",
      "[step: 551] loss: 149.02120971679688\n",
      "[step: 552] loss: 145.265625\n",
      "[step: 553] loss: 147.8724822998047\n",
      "[step: 554] loss: 160.51275634765625\n",
      "[step: 555] loss: 146.85084533691406\n",
      "[step: 556] loss: 150.84300231933594\n",
      "[step: 557] loss: 151.74322509765625\n",
      "[step: 558] loss: 152.77395629882812\n",
      "[step: 559] loss: 147.4842071533203\n",
      "[step: 560] loss: 156.49295043945312\n",
      "[step: 561] loss: 151.02305603027344\n",
      "[step: 562] loss: 148.95144653320312\n",
      "[step: 563] loss: 147.41058349609375\n",
      "[step: 564] loss: 147.96905517578125\n",
      "[step: 565] loss: 149.37374877929688\n",
      "[step: 566] loss: 159.75132751464844\n",
      "[step: 567] loss: 150.17318725585938\n",
      "[step: 568] loss: 148.65206909179688\n",
      "[step: 569] loss: 166.06320190429688\n",
      "[step: 570] loss: 156.02890014648438\n",
      "[step: 571] loss: 147.05474853515625\n",
      "[step: 572] loss: 153.2067413330078\n",
      "[step: 573] loss: 152.57528686523438\n",
      "[step: 574] loss: 146.8550567626953\n",
      "[step: 575] loss: 149.79428100585938\n",
      "[step: 576] loss: 139.4811553955078\n",
      "[step: 577] loss: 146.87222290039062\n",
      "[step: 578] loss: 141.50845336914062\n",
      "[step: 579] loss: 147.33407592773438\n",
      "[step: 580] loss: 141.79966735839844\n",
      "[step: 581] loss: 147.052490234375\n",
      "[step: 582] loss: 141.16348266601562\n",
      "[step: 583] loss: 142.99891662597656\n",
      "[step: 584] loss: 148.87884521484375\n",
      "[step: 585] loss: 143.00930786132812\n",
      "[step: 586] loss: 143.58999633789062\n",
      "[step: 587] loss: 149.09671020507812\n",
      "[step: 588] loss: 144.5048065185547\n",
      "[step: 589] loss: 144.45156860351562\n",
      "[step: 590] loss: 143.72567749023438\n",
      "[step: 591] loss: 141.38424682617188\n",
      "[step: 592] loss: 142.05850219726562\n",
      "[step: 593] loss: 138.17674255371094\n",
      "[step: 594] loss: 144.95562744140625\n",
      "[step: 595] loss: 142.0753173828125\n",
      "[step: 596] loss: 136.90328979492188\n",
      "[step: 597] loss: 140.78158569335938\n",
      "[step: 598] loss: 140.41343688964844\n",
      "[step: 599] loss: 137.33724975585938\n",
      "[step: 600] loss: 138.31158447265625\n",
      "[step: 601] loss: 146.58413696289062\n",
      "[step: 602] loss: 156.2387237548828\n",
      "[step: 603] loss: 158.15286254882812\n",
      "[step: 604] loss: 156.72552490234375\n",
      "[step: 605] loss: 156.2582244873047\n",
      "[step: 606] loss: 147.59628295898438\n",
      "[step: 607] loss: 145.26812744140625\n",
      "[step: 608] loss: 156.09027099609375\n",
      "[step: 609] loss: 155.07748413085938\n",
      "[step: 610] loss: 146.54129028320312\n",
      "[step: 611] loss: 137.06773376464844\n",
      "[step: 612] loss: 149.22776794433594\n",
      "[step: 613] loss: 141.88739013671875\n",
      "[step: 614] loss: 141.95101928710938\n",
      "[step: 615] loss: 140.04489135742188\n",
      "[step: 616] loss: 152.67567443847656\n",
      "[step: 617] loss: 149.11473083496094\n",
      "[step: 618] loss: 143.64657592773438\n",
      "[step: 619] loss: 140.58224487304688\n",
      "[step: 620] loss: 153.1599578857422\n",
      "[step: 621] loss: 147.64048767089844\n",
      "[step: 622] loss: 136.37075805664062\n",
      "[step: 623] loss: 149.71270751953125\n",
      "[step: 624] loss: 155.09078979492188\n",
      "[step: 625] loss: 146.30828857421875\n",
      "[step: 626] loss: 140.09652709960938\n",
      "[step: 627] loss: 148.52899169921875\n",
      "[step: 628] loss: 146.88247680664062\n",
      "[step: 629] loss: 139.85987854003906\n",
      "[step: 630] loss: 140.1813507080078\n",
      "[step: 631] loss: 142.19833374023438\n",
      "[step: 632] loss: 144.61129760742188\n",
      "[step: 633] loss: 137.1478729248047\n",
      "[step: 634] loss: 133.10289001464844\n",
      "[step: 635] loss: 153.35289001464844\n",
      "[step: 636] loss: 145.49691772460938\n",
      "[step: 637] loss: 135.84298706054688\n",
      "[step: 638] loss: 144.75306701660156\n",
      "[step: 639] loss: 149.22914123535156\n",
      "[step: 640] loss: 146.4197998046875\n",
      "[step: 641] loss: 138.97994995117188\n",
      "[step: 642] loss: 147.94937133789062\n",
      "[step: 643] loss: 136.5787811279297\n",
      "[step: 644] loss: 141.30996704101562\n",
      "[step: 645] loss: 146.72836303710938\n",
      "[step: 646] loss: 138.45663452148438\n",
      "[step: 647] loss: 139.83084106445312\n",
      "[step: 648] loss: 149.8299560546875\n",
      "[step: 649] loss: 141.0750732421875\n",
      "[step: 650] loss: 144.77719116210938\n",
      "[step: 651] loss: 143.98501586914062\n",
      "[step: 652] loss: 137.40011596679688\n",
      "[step: 653] loss: 140.57144165039062\n",
      "[step: 654] loss: 130.01840209960938\n",
      "[step: 655] loss: 137.23165893554688\n",
      "[step: 656] loss: 139.52035522460938\n",
      "[step: 657] loss: 130.5242919921875\n",
      "[step: 658] loss: 134.9918212890625\n",
      "[step: 659] loss: 139.55810546875\n",
      "[step: 660] loss: 135.69744873046875\n",
      "[step: 661] loss: 131.3091278076172\n",
      "[step: 662] loss: 135.26565551757812\n",
      "[step: 663] loss: 138.6275634765625\n",
      "[step: 664] loss: 137.33212280273438\n",
      "[step: 665] loss: 137.592041015625\n",
      "[step: 666] loss: 135.13987731933594\n",
      "[step: 667] loss: 134.9969940185547\n",
      "[step: 668] loss: 137.38226318359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 669] loss: 135.29293823242188\n",
      "[step: 670] loss: 136.96336364746094\n",
      "[step: 671] loss: 137.88858032226562\n",
      "[step: 672] loss: 141.46804809570312\n",
      "[step: 673] loss: 140.7540740966797\n",
      "[step: 674] loss: 148.2294921875\n",
      "[step: 675] loss: 145.49868774414062\n",
      "[step: 676] loss: 137.28915405273438\n",
      "[step: 677] loss: 143.3187713623047\n",
      "[step: 678] loss: 155.2915496826172\n",
      "[step: 679] loss: 144.67449951171875\n",
      "[step: 680] loss: 136.89822387695312\n",
      "[step: 681] loss: 158.85667419433594\n",
      "[step: 682] loss: 146.02444458007812\n",
      "[step: 683] loss: 145.28973388671875\n",
      "[step: 684] loss: 155.63998413085938\n",
      "[step: 685] loss: 141.19387817382812\n",
      "[step: 686] loss: 150.6055908203125\n",
      "[step: 687] loss: 144.66453552246094\n",
      "[step: 688] loss: 139.82925415039062\n",
      "[step: 689] loss: 138.3298797607422\n",
      "[step: 690] loss: 138.29522705078125\n",
      "[step: 691] loss: 142.5030517578125\n",
      "[step: 692] loss: 135.52194213867188\n",
      "[step: 693] loss: 139.72975158691406\n",
      "[step: 694] loss: 140.71864318847656\n",
      "[step: 695] loss: 142.36422729492188\n",
      "[step: 696] loss: 141.19808959960938\n",
      "[step: 697] loss: 142.74063110351562\n",
      "[step: 698] loss: 139.06980895996094\n",
      "[step: 699] loss: 134.53070068359375\n",
      "[step: 700] loss: 141.3245849609375\n",
      "[step: 701] loss: 128.9178466796875\n",
      "[step: 702] loss: 134.03121948242188\n",
      "[step: 703] loss: 139.5830078125\n",
      "[step: 704] loss: 131.04978942871094\n",
      "[step: 705] loss: 140.93177795410156\n",
      "[step: 706] loss: 133.7486572265625\n",
      "[step: 707] loss: 133.58688354492188\n",
      "[step: 708] loss: 134.84494018554688\n",
      "[step: 709] loss: 136.05381774902344\n",
      "[step: 710] loss: 131.80184936523438\n",
      "[step: 711] loss: 133.25460815429688\n",
      "[step: 712] loss: 131.91854858398438\n",
      "[step: 713] loss: 135.31585693359375\n",
      "[step: 714] loss: 131.2222442626953\n",
      "[step: 715] loss: 132.88238525390625\n",
      "[step: 716] loss: 129.8838653564453\n",
      "[step: 717] loss: 135.60305786132812\n",
      "[step: 718] loss: 126.49797821044922\n",
      "[step: 719] loss: 127.00289916992188\n",
      "[step: 720] loss: 126.90648651123047\n",
      "[step: 721] loss: 129.29884338378906\n",
      "[step: 722] loss: 126.46387481689453\n",
      "[step: 723] loss: 128.02088928222656\n",
      "[step: 724] loss: 128.2772674560547\n",
      "[step: 725] loss: 132.802734375\n",
      "[step: 726] loss: 128.28834533691406\n",
      "[step: 727] loss: 129.08470153808594\n",
      "[step: 728] loss: 128.76751708984375\n",
      "[step: 729] loss: 130.42015075683594\n",
      "[step: 730] loss: 126.52053833007812\n",
      "[step: 731] loss: 129.4073944091797\n",
      "[step: 732] loss: 132.01397705078125\n",
      "[step: 733] loss: 127.03057098388672\n",
      "[step: 734] loss: 124.71994018554688\n",
      "[step: 735] loss: 133.81781005859375\n",
      "[step: 736] loss: 136.55560302734375\n",
      "[step: 737] loss: 126.50883483886719\n",
      "[step: 738] loss: 130.14669799804688\n",
      "[step: 739] loss: 133.8294677734375\n",
      "[step: 740] loss: 129.22776794433594\n",
      "[step: 741] loss: 125.87435913085938\n",
      "[step: 742] loss: 130.85943603515625\n",
      "[step: 743] loss: 131.32362365722656\n",
      "[step: 744] loss: 131.20433044433594\n",
      "[step: 745] loss: 138.26705932617188\n",
      "[step: 746] loss: 145.79620361328125\n",
      "[step: 747] loss: 144.96478271484375\n",
      "[step: 748] loss: 135.49383544921875\n",
      "[step: 749] loss: 130.60055541992188\n",
      "[step: 750] loss: 128.57916259765625\n",
      "[step: 751] loss: 131.38706970214844\n",
      "[step: 752] loss: 128.986328125\n",
      "[step: 753] loss: 135.39718627929688\n",
      "[step: 754] loss: 126.75723266601562\n",
      "[step: 755] loss: 126.95093536376953\n",
      "[step: 756] loss: 127.09935760498047\n",
      "[step: 757] loss: 134.05657958984375\n",
      "[step: 758] loss: 130.77676391601562\n",
      "[step: 759] loss: 126.64553833007812\n",
      "[step: 760] loss: 130.6683349609375\n",
      "[step: 761] loss: 130.98553466796875\n",
      "[step: 762] loss: 131.58346557617188\n",
      "[step: 763] loss: 129.2754669189453\n",
      "[step: 764] loss: 123.34041595458984\n",
      "[step: 765] loss: 127.21684265136719\n",
      "[step: 766] loss: 127.75283813476562\n",
      "[step: 767] loss: 129.11190795898438\n",
      "[step: 768] loss: 126.87338256835938\n",
      "[step: 769] loss: 130.83389282226562\n",
      "[step: 770] loss: 129.9209442138672\n",
      "[step: 771] loss: 132.18072509765625\n",
      "[step: 772] loss: 138.2975311279297\n",
      "[step: 773] loss: 135.6794891357422\n",
      "[step: 774] loss: 134.30810546875\n",
      "[step: 775] loss: 130.02178955078125\n",
      "[step: 776] loss: 125.66728210449219\n",
      "[step: 777] loss: 128.89828491210938\n",
      "[step: 778] loss: 130.35826110839844\n",
      "[step: 779] loss: 130.09814453125\n",
      "[step: 780] loss: 122.9092788696289\n",
      "[step: 781] loss: 125.9935531616211\n",
      "[step: 782] loss: 121.09561157226562\n",
      "[step: 783] loss: 126.95011901855469\n",
      "[step: 784] loss: 125.06968688964844\n",
      "[step: 785] loss: 126.57005310058594\n",
      "[step: 786] loss: 131.68844604492188\n",
      "[step: 787] loss: 132.15481567382812\n",
      "[step: 788] loss: 121.2798080444336\n",
      "[step: 789] loss: 126.43588256835938\n",
      "[step: 790] loss: 128.29867553710938\n",
      "[step: 791] loss: 138.43130493164062\n",
      "[step: 792] loss: 129.92098999023438\n",
      "[step: 793] loss: 139.83554077148438\n",
      "[step: 794] loss: 130.57803344726562\n",
      "[step: 795] loss: 121.94845581054688\n",
      "[step: 796] loss: 121.64508056640625\n",
      "[step: 797] loss: 133.36801147460938\n",
      "[step: 798] loss: 135.81961059570312\n",
      "[step: 799] loss: 127.04974365234375\n",
      "[step: 800] loss: 132.35858154296875\n",
      "[step: 801] loss: 131.22720336914062\n",
      "[step: 802] loss: 122.59318542480469\n",
      "[step: 803] loss: 133.65296936035156\n",
      "[step: 804] loss: 131.75820922851562\n",
      "[step: 805] loss: 126.56632995605469\n",
      "[step: 806] loss: 130.39804077148438\n",
      "[step: 807] loss: 128.35684204101562\n",
      "[step: 808] loss: 131.83810424804688\n",
      "[step: 809] loss: 129.47837829589844\n",
      "[step: 810] loss: 118.61277770996094\n",
      "[step: 811] loss: 122.47695922851562\n",
      "[step: 812] loss: 122.65802001953125\n",
      "[step: 813] loss: 121.21153259277344\n",
      "[step: 814] loss: 117.84196472167969\n",
      "[step: 815] loss: 123.00007629394531\n",
      "[step: 816] loss: 120.20267486572266\n",
      "[step: 817] loss: 121.8415298461914\n",
      "[step: 818] loss: 124.92710876464844\n",
      "[step: 819] loss: 119.67265319824219\n",
      "[step: 820] loss: 118.75971984863281\n",
      "[step: 821] loss: 121.54696655273438\n",
      "[step: 822] loss: 121.43731689453125\n",
      "[step: 823] loss: 119.13843536376953\n",
      "[step: 824] loss: 126.62342071533203\n",
      "[step: 825] loss: 124.73831176757812\n",
      "[step: 826] loss: 127.7098388671875\n",
      "[step: 827] loss: 129.76153564453125\n",
      "[step: 828] loss: 139.99050903320312\n",
      "[step: 829] loss: 142.42135620117188\n",
      "[step: 830] loss: 148.8939666748047\n",
      "[step: 831] loss: 128.09384155273438\n",
      "[step: 832] loss: 127.30299377441406\n",
      "[step: 833] loss: 134.12217712402344\n",
      "[step: 834] loss: 125.4462890625\n",
      "[step: 835] loss: 134.3203125\n",
      "[step: 836] loss: 122.7078628540039\n",
      "[step: 837] loss: 126.28900146484375\n",
      "[step: 838] loss: 123.5682373046875\n",
      "[step: 839] loss: 120.62010192871094\n",
      "[step: 840] loss: 127.03836059570312\n",
      "[step: 841] loss: 122.66744995117188\n",
      "[step: 842] loss: 129.08932495117188\n",
      "[step: 843] loss: 125.05377960205078\n",
      "[step: 844] loss: 123.0765380859375\n",
      "[step: 845] loss: 123.12681579589844\n",
      "[step: 846] loss: 135.82492065429688\n",
      "[step: 847] loss: 126.06352996826172\n",
      "[step: 848] loss: 120.86095428466797\n",
      "[step: 849] loss: 122.77606201171875\n",
      "[step: 850] loss: 124.52574157714844\n",
      "[step: 851] loss: 123.55952453613281\n",
      "[step: 852] loss: 134.20291137695312\n",
      "[step: 853] loss: 120.75068664550781\n",
      "[step: 854] loss: 119.67617797851562\n",
      "[step: 855] loss: 121.48710632324219\n",
      "[step: 856] loss: 119.69342041015625\n",
      "[step: 857] loss: 120.1043701171875\n",
      "[step: 858] loss: 124.9774398803711\n",
      "[step: 859] loss: 121.8624267578125\n",
      "[step: 860] loss: 129.77761840820312\n",
      "[step: 861] loss: 150.50038146972656\n",
      "[step: 862] loss: 147.53477478027344\n",
      "[step: 863] loss: 129.22650146484375\n",
      "[step: 864] loss: 124.93106842041016\n",
      "[step: 865] loss: 129.974365234375\n",
      "[step: 866] loss: 129.84368896484375\n",
      "[step: 867] loss: 136.01513671875\n",
      "[step: 868] loss: 126.2204360961914\n",
      "[step: 869] loss: 121.34429168701172\n",
      "[step: 870] loss: 126.30596923828125\n",
      "[step: 871] loss: 122.67615509033203\n",
      "[step: 872] loss: 114.62841796875\n",
      "[step: 873] loss: 117.07083892822266\n",
      "[step: 874] loss: 124.55797576904297\n",
      "[step: 875] loss: 112.32675170898438\n",
      "[step: 876] loss: 119.48011016845703\n",
      "[step: 877] loss: 116.07161712646484\n",
      "[step: 878] loss: 119.45401763916016\n",
      "[step: 879] loss: 115.09459686279297\n",
      "[step: 880] loss: 116.04194641113281\n",
      "[step: 881] loss: 115.7732162475586\n",
      "[step: 882] loss: 115.68556213378906\n",
      "[step: 883] loss: 116.01195526123047\n",
      "[step: 884] loss: 117.29975891113281\n",
      "[step: 885] loss: 115.96723937988281\n",
      "[step: 886] loss: 117.95988464355469\n",
      "[step: 887] loss: 120.66433715820312\n",
      "[step: 888] loss: 116.03269958496094\n",
      "[step: 889] loss: 112.91128540039062\n",
      "[step: 890] loss: 114.6807632446289\n",
      "[step: 891] loss: 115.96839141845703\n",
      "[step: 892] loss: 122.36788940429688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 893] loss: 118.92091369628906\n",
      "[step: 894] loss: 115.5360336303711\n",
      "[step: 895] loss: 114.76068115234375\n",
      "[step: 896] loss: 117.41609191894531\n",
      "[step: 897] loss: 115.60414123535156\n",
      "[step: 898] loss: 119.20248413085938\n",
      "[step: 899] loss: 118.38764953613281\n",
      "[step: 900] loss: 119.1675033569336\n",
      "[step: 901] loss: 125.68148803710938\n",
      "[step: 902] loss: 127.80007934570312\n",
      "[step: 903] loss: 127.52796936035156\n",
      "[step: 904] loss: 131.97088623046875\n",
      "[step: 905] loss: 150.35711669921875\n",
      "[step: 906] loss: 131.76507568359375\n",
      "[step: 907] loss: 123.17677307128906\n",
      "[step: 908] loss: 128.64564514160156\n",
      "[step: 909] loss: 127.42538452148438\n",
      "[step: 910] loss: 135.19442749023438\n",
      "[step: 911] loss: 120.28553771972656\n",
      "[step: 912] loss: 129.35423278808594\n",
      "[step: 913] loss: 125.74923706054688\n",
      "[step: 914] loss: 128.26365661621094\n",
      "[step: 915] loss: 128.0562744140625\n",
      "[step: 916] loss: 124.94207000732422\n",
      "[step: 917] loss: 130.23135375976562\n",
      "[step: 918] loss: 122.34615325927734\n",
      "[step: 919] loss: 126.76829528808594\n",
      "[step: 920] loss: 119.1607666015625\n",
      "[step: 921] loss: 119.82535552978516\n",
      "[step: 922] loss: 124.83146667480469\n",
      "[step: 923] loss: 117.97676086425781\n",
      "[step: 924] loss: 122.33197021484375\n",
      "[step: 925] loss: 123.15921020507812\n",
      "[step: 926] loss: 122.66270446777344\n",
      "[step: 927] loss: 123.94415283203125\n",
      "[step: 928] loss: 131.96133422851562\n",
      "[step: 929] loss: 115.26261138916016\n",
      "[step: 930] loss: 131.2870330810547\n",
      "[step: 931] loss: 127.82269287109375\n",
      "[step: 932] loss: 132.98094177246094\n",
      "[step: 933] loss: 124.3304443359375\n",
      "[step: 934] loss: 120.00738525390625\n",
      "[step: 935] loss: 127.75006866455078\n",
      "[step: 936] loss: 110.9249496459961\n",
      "[step: 937] loss: 128.44839477539062\n",
      "[step: 938] loss: 122.83799743652344\n",
      "[step: 939] loss: 129.0795135498047\n",
      "[step: 940] loss: 118.07818603515625\n",
      "[step: 941] loss: 118.92025756835938\n",
      "[step: 942] loss: 120.97589111328125\n",
      "[step: 943] loss: 115.94735717773438\n",
      "[step: 944] loss: 126.9979476928711\n",
      "[step: 945] loss: 120.16919708251953\n",
      "[step: 946] loss: 120.4281005859375\n",
      "[step: 947] loss: 125.25855255126953\n",
      "[step: 948] loss: 114.13955688476562\n",
      "[step: 949] loss: 123.91007995605469\n",
      "[step: 950] loss: 127.572998046875\n",
      "[step: 951] loss: 116.07023620605469\n",
      "[step: 952] loss: 112.30867004394531\n",
      "[step: 953] loss: 115.79536437988281\n",
      "[step: 954] loss: 123.14125061035156\n",
      "[step: 955] loss: 119.89604187011719\n",
      "[step: 956] loss: 114.46351623535156\n",
      "[step: 957] loss: 116.23462677001953\n",
      "[step: 958] loss: 109.57588195800781\n",
      "[step: 959] loss: 111.24969482421875\n",
      "[step: 960] loss: 118.29443359375\n",
      "[step: 961] loss: 116.61224365234375\n",
      "[step: 962] loss: 117.46969604492188\n",
      "[step: 963] loss: 108.31991577148438\n",
      "[step: 964] loss: 114.46542358398438\n",
      "[step: 965] loss: 112.15788269042969\n",
      "[step: 966] loss: 112.87162017822266\n",
      "[step: 967] loss: 111.39642333984375\n",
      "[step: 968] loss: 111.00657653808594\n",
      "[step: 969] loss: 114.11643981933594\n",
      "[step: 970] loss: 119.72398376464844\n",
      "[step: 971] loss: 114.24630737304688\n",
      "[step: 972] loss: 121.54739379882812\n",
      "[step: 973] loss: 121.23173522949219\n",
      "[step: 974] loss: 110.77392578125\n",
      "[step: 975] loss: 107.13440704345703\n",
      "[step: 976] loss: 117.77003479003906\n",
      "[step: 977] loss: 114.63389587402344\n",
      "[step: 978] loss: 117.14437103271484\n",
      "[step: 979] loss: 133.86944580078125\n",
      "[step: 980] loss: 128.8492431640625\n",
      "[step: 981] loss: 124.61186981201172\n",
      "[step: 982] loss: 120.81910705566406\n",
      "[step: 983] loss: 116.4924087524414\n",
      "[step: 984] loss: 122.90570068359375\n",
      "[step: 985] loss: 138.8604278564453\n",
      "[step: 986] loss: 123.88468933105469\n",
      "[step: 987] loss: 112.83316040039062\n",
      "[step: 988] loss: 119.46968078613281\n",
      "[step: 989] loss: 115.48347473144531\n",
      "[step: 990] loss: 120.7799072265625\n",
      "[step: 991] loss: 125.4830322265625\n",
      "[step: 992] loss: 111.94783020019531\n",
      "[step: 993] loss: 119.10209655761719\n",
      "[step: 994] loss: 119.66561889648438\n",
      "[step: 995] loss: 115.47877502441406\n",
      "[step: 996] loss: 124.6098403930664\n",
      "[step: 997] loss: 123.17106628417969\n",
      "[step: 998] loss: 116.26057434082031\n",
      "[step: 999] loss: 120.8165512084961\n",
      "[step: 1000] loss: 127.31065368652344\n",
      "[step: 1001] loss: 129.83828735351562\n",
      "[step: 1002] loss: 111.34125518798828\n",
      "[step: 1003] loss: 128.4837646484375\n",
      "[step: 1004] loss: 124.84496307373047\n",
      "[step: 1005] loss: 112.57310485839844\n",
      "[step: 1006] loss: 119.08995056152344\n",
      "[step: 1007] loss: 114.61030578613281\n",
      "[step: 1008] loss: 115.9988021850586\n",
      "[step: 1009] loss: 123.06880187988281\n",
      "[step: 1010] loss: 110.49591064453125\n",
      "[step: 1011] loss: 116.03176879882812\n",
      "[step: 1012] loss: 119.05844116210938\n",
      "[step: 1013] loss: 110.78531646728516\n",
      "[step: 1014] loss: 118.44552612304688\n",
      "[step: 1015] loss: 112.09388732910156\n",
      "[step: 1016] loss: 109.78070068359375\n",
      "[step: 1017] loss: 120.56511688232422\n",
      "[step: 1018] loss: 110.71420288085938\n",
      "[step: 1019] loss: 111.36813354492188\n",
      "[step: 1020] loss: 113.85596466064453\n",
      "[step: 1021] loss: 106.03717803955078\n",
      "[step: 1022] loss: 113.90029907226562\n",
      "[step: 1023] loss: 115.28997039794922\n",
      "[step: 1024] loss: 106.95710754394531\n",
      "[step: 1025] loss: 110.71965026855469\n",
      "[step: 1026] loss: 109.76106262207031\n",
      "[step: 1027] loss: 110.71726989746094\n",
      "[step: 1028] loss: 108.73577880859375\n",
      "[step: 1029] loss: 105.76652526855469\n",
      "[step: 1030] loss: 106.3184814453125\n",
      "[step: 1031] loss: 112.74291229248047\n",
      "[step: 1032] loss: 107.5416259765625\n",
      "[step: 1033] loss: 109.33170318603516\n",
      "[step: 1034] loss: 115.45673370361328\n",
      "[step: 1035] loss: 114.5821304321289\n",
      "[step: 1036] loss: 123.87013244628906\n",
      "[step: 1037] loss: 135.1335906982422\n",
      "[step: 1038] loss: 131.83056640625\n",
      "[step: 1039] loss: 127.77655029296875\n",
      "[step: 1040] loss: 114.78701782226562\n",
      "[step: 1041] loss: 142.32412719726562\n",
      "[step: 1042] loss: 148.4585418701172\n",
      "[step: 1043] loss: 117.5984115600586\n",
      "[step: 1044] loss: 139.55291748046875\n",
      "[step: 1045] loss: 130.40977478027344\n",
      "[step: 1046] loss: 120.8694076538086\n",
      "[step: 1047] loss: 124.44774627685547\n",
      "[step: 1048] loss: 127.52625274658203\n",
      "[step: 1049] loss: 121.5970687866211\n",
      "[step: 1050] loss: 126.00698852539062\n",
      "[step: 1051] loss: 126.39161682128906\n",
      "[step: 1052] loss: 113.4305419921875\n",
      "[step: 1053] loss: 126.59761047363281\n",
      "[step: 1054] loss: 120.2020263671875\n",
      "[step: 1055] loss: 112.31967163085938\n",
      "[step: 1056] loss: 115.00161743164062\n",
      "[step: 1057] loss: 119.9721450805664\n",
      "[step: 1058] loss: 110.06564331054688\n",
      "[step: 1059] loss: 110.66751098632812\n",
      "[step: 1060] loss: 111.73863220214844\n",
      "[step: 1061] loss: 108.2333755493164\n",
      "[step: 1062] loss: 116.5929183959961\n",
      "[step: 1063] loss: 113.13136291503906\n",
      "[step: 1064] loss: 108.70301055908203\n",
      "[step: 1065] loss: 108.71047973632812\n",
      "[step: 1066] loss: 108.22421264648438\n",
      "[step: 1067] loss: 108.69036102294922\n",
      "[step: 1068] loss: 111.55683898925781\n",
      "[step: 1069] loss: 114.67481994628906\n",
      "[step: 1070] loss: 105.609375\n",
      "[step: 1071] loss: 110.24070739746094\n",
      "[step: 1072] loss: 108.43846130371094\n",
      "[step: 1073] loss: 106.75271606445312\n",
      "[step: 1074] loss: 115.86131286621094\n",
      "[step: 1075] loss: 116.108642578125\n",
      "[step: 1076] loss: 110.79179382324219\n",
      "[step: 1077] loss: 106.95789337158203\n",
      "[step: 1078] loss: 105.89790344238281\n",
      "[step: 1079] loss: 111.90454864501953\n",
      "[step: 1080] loss: 105.21121978759766\n",
      "[step: 1081] loss: 111.08518981933594\n",
      "[step: 1082] loss: 106.97264099121094\n",
      "[step: 1083] loss: 107.29634094238281\n",
      "[step: 1084] loss: 113.44041442871094\n",
      "[step: 1085] loss: 109.18252563476562\n",
      "[step: 1086] loss: 108.13577270507812\n",
      "[step: 1087] loss: 113.25110626220703\n",
      "[step: 1088] loss: 111.50521850585938\n",
      "[step: 1089] loss: 116.47018432617188\n",
      "[step: 1090] loss: 116.9758071899414\n",
      "[step: 1091] loss: 118.50394439697266\n",
      "[step: 1092] loss: 119.88861083984375\n",
      "[step: 1093] loss: 110.52698516845703\n",
      "[step: 1094] loss: 111.37416076660156\n",
      "[step: 1095] loss: 114.446533203125\n",
      "[step: 1096] loss: 105.24479675292969\n",
      "[step: 1097] loss: 115.06089782714844\n",
      "[step: 1098] loss: 110.58360290527344\n",
      "[step: 1099] loss: 106.48165893554688\n",
      "[step: 1100] loss: 109.09481811523438\n",
      "[step: 1101] loss: 108.86092376708984\n",
      "[step: 1102] loss: 113.89015197753906\n",
      "[step: 1103] loss: 107.33296966552734\n",
      "[step: 1104] loss: 103.48385620117188\n",
      "[step: 1105] loss: 111.7608642578125\n",
      "[step: 1106] loss: 107.04142761230469\n",
      "[step: 1107] loss: 110.86807250976562\n",
      "[step: 1108] loss: 110.48008728027344\n",
      "[step: 1109] loss: 105.06735229492188\n",
      "[step: 1110] loss: 109.82142639160156\n",
      "[step: 1111] loss: 106.49571990966797\n",
      "[step: 1112] loss: 107.1776123046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1113] loss: 100.78087615966797\n",
      "[step: 1114] loss: 102.36605834960938\n",
      "[step: 1115] loss: 107.64424133300781\n",
      "[step: 1116] loss: 101.42916870117188\n",
      "[step: 1117] loss: 113.10551452636719\n",
      "[step: 1118] loss: 127.37864685058594\n",
      "[step: 1119] loss: 109.26360321044922\n",
      "[step: 1120] loss: 110.87616729736328\n",
      "[step: 1121] loss: 114.26235961914062\n",
      "[step: 1122] loss: 111.85792541503906\n",
      "[step: 1123] loss: 114.23446655273438\n",
      "[step: 1124] loss: 114.13851928710938\n",
      "[step: 1125] loss: 113.09051513671875\n",
      "[step: 1126] loss: 103.87846374511719\n",
      "[step: 1127] loss: 104.9677734375\n",
      "[step: 1128] loss: 108.00553131103516\n",
      "[step: 1129] loss: 100.78653717041016\n",
      "[step: 1130] loss: 105.76690673828125\n",
      "[step: 1131] loss: 99.39640808105469\n",
      "[step: 1132] loss: 103.08558654785156\n",
      "[step: 1133] loss: 100.27616882324219\n",
      "[step: 1134] loss: 102.70660400390625\n",
      "[step: 1135] loss: 99.2724609375\n",
      "[step: 1136] loss: 101.89788818359375\n",
      "[step: 1137] loss: 102.8473892211914\n",
      "[step: 1138] loss: 100.32023620605469\n",
      "[step: 1139] loss: 101.31684875488281\n",
      "[step: 1140] loss: 101.49715423583984\n",
      "[step: 1141] loss: 102.45806884765625\n",
      "[step: 1142] loss: 102.19456481933594\n",
      "[step: 1143] loss: 104.32136535644531\n",
      "[step: 1144] loss: 103.14816284179688\n",
      "[step: 1145] loss: 112.65937805175781\n",
      "[step: 1146] loss: 117.25564575195312\n",
      "[step: 1147] loss: 112.37898254394531\n",
      "[step: 1148] loss: 104.16133117675781\n",
      "[step: 1149] loss: 105.24457550048828\n",
      "[step: 1150] loss: 111.05615997314453\n",
      "[step: 1151] loss: 116.10917663574219\n",
      "[step: 1152] loss: 103.89964294433594\n",
      "[step: 1153] loss: 102.02345275878906\n",
      "[step: 1154] loss: 104.31588745117188\n",
      "[step: 1155] loss: 108.61373901367188\n",
      "[step: 1156] loss: 104.00962829589844\n",
      "[step: 1157] loss: 111.92765808105469\n",
      "[step: 1158] loss: 104.07354736328125\n",
      "[step: 1159] loss: 105.54703521728516\n",
      "[step: 1160] loss: 112.80326843261719\n",
      "[step: 1161] loss: 102.2398681640625\n",
      "[step: 1162] loss: 103.75053405761719\n",
      "[step: 1163] loss: 102.24697875976562\n",
      "[step: 1164] loss: 102.69015502929688\n",
      "[step: 1165] loss: 104.5225601196289\n",
      "[step: 1166] loss: 101.62889099121094\n",
      "[step: 1167] loss: 101.43067932128906\n",
      "[step: 1168] loss: 105.65650177001953\n",
      "[step: 1169] loss: 107.81813049316406\n",
      "[step: 1170] loss: 107.23263549804688\n",
      "[step: 1171] loss: 107.73851013183594\n",
      "[step: 1172] loss: 100.50118255615234\n",
      "[step: 1173] loss: 103.72203826904297\n",
      "[step: 1174] loss: 106.27523040771484\n",
      "[step: 1175] loss: 105.96939086914062\n",
      "[step: 1176] loss: 104.98070526123047\n",
      "[step: 1177] loss: 96.23733520507812\n",
      "[step: 1178] loss: 99.17266845703125\n",
      "[step: 1179] loss: 106.27421569824219\n",
      "[step: 1180] loss: 101.5418701171875\n",
      "[step: 1181] loss: 102.37596893310547\n",
      "[step: 1182] loss: 100.63809204101562\n",
      "[step: 1183] loss: 103.39085388183594\n",
      "[step: 1184] loss: 108.04837036132812\n",
      "[step: 1185] loss: 109.7197265625\n",
      "[step: 1186] loss: 102.99552917480469\n",
      "[step: 1187] loss: 107.74273681640625\n",
      "[step: 1188] loss: 99.35855102539062\n",
      "[step: 1189] loss: 102.17579650878906\n",
      "[step: 1190] loss: 101.71990966796875\n",
      "[step: 1191] loss: 105.55868530273438\n",
      "[step: 1192] loss: 100.42013549804688\n",
      "[step: 1193] loss: 107.21151733398438\n",
      "[step: 1194] loss: 106.8884506225586\n",
      "[step: 1195] loss: 95.35458374023438\n",
      "[step: 1196] loss: 101.03231048583984\n",
      "[step: 1197] loss: 102.22259521484375\n",
      "[step: 1198] loss: 106.10693359375\n",
      "[step: 1199] loss: 98.49171447753906\n",
      "[step: 1200] loss: 102.71371459960938\n",
      "[step: 1201] loss: 97.7874526977539\n",
      "[step: 1202] loss: 103.16891479492188\n",
      "[step: 1203] loss: 110.64142608642578\n",
      "[step: 1204] loss: 107.3875503540039\n",
      "[step: 1205] loss: 99.41313171386719\n",
      "[step: 1206] loss: 100.02914428710938\n",
      "[step: 1207] loss: 108.54191589355469\n",
      "[step: 1208] loss: 104.47248840332031\n",
      "[step: 1209] loss: 101.65180969238281\n",
      "[step: 1210] loss: 100.73560333251953\n",
      "[step: 1211] loss: 106.8703384399414\n",
      "[step: 1212] loss: 129.126708984375\n",
      "[step: 1213] loss: 105.22117614746094\n",
      "[step: 1214] loss: 100.49090576171875\n",
      "[step: 1215] loss: 108.05435180664062\n",
      "[step: 1216] loss: 99.77464294433594\n",
      "[step: 1217] loss: 97.98735809326172\n",
      "[step: 1218] loss: 99.35282897949219\n",
      "[step: 1219] loss: 104.4743881225586\n",
      "[step: 1220] loss: 103.23495483398438\n",
      "[step: 1221] loss: 104.67202758789062\n",
      "[step: 1222] loss: 110.97500610351562\n",
      "[step: 1223] loss: 104.28019714355469\n",
      "[step: 1224] loss: 98.43888854980469\n",
      "[step: 1225] loss: 97.35348510742188\n",
      "[step: 1226] loss: 104.09146118164062\n",
      "[step: 1227] loss: 97.02630615234375\n",
      "[step: 1228] loss: 99.68531799316406\n",
      "[step: 1229] loss: 99.8883285522461\n",
      "[step: 1230] loss: 97.9462890625\n",
      "[step: 1231] loss: 100.00849914550781\n",
      "[step: 1232] loss: 107.94908905029297\n",
      "[step: 1233] loss: 119.2191162109375\n",
      "[step: 1234] loss: 148.449951171875\n",
      "[step: 1235] loss: 105.97695922851562\n",
      "[step: 1236] loss: 143.8730926513672\n",
      "[step: 1237] loss: 130.5726318359375\n",
      "[step: 1238] loss: 130.28973388671875\n",
      "[step: 1239] loss: 122.12617492675781\n",
      "[step: 1240] loss: 131.60337829589844\n",
      "[step: 1241] loss: 116.61636352539062\n",
      "[step: 1242] loss: 129.9310302734375\n",
      "[step: 1243] loss: 109.10234069824219\n",
      "[step: 1244] loss: 125.07676696777344\n",
      "[step: 1245] loss: 111.39837646484375\n",
      "[step: 1246] loss: 121.88044738769531\n",
      "[step: 1247] loss: 107.88395690917969\n",
      "[step: 1248] loss: 118.75331115722656\n",
      "[step: 1249] loss: 103.87590789794922\n",
      "[step: 1250] loss: 118.9027099609375\n",
      "[step: 1251] loss: 107.32976531982422\n",
      "[step: 1252] loss: 109.83457946777344\n",
      "[step: 1253] loss: 99.26725769042969\n",
      "[step: 1254] loss: 112.06216430664062\n",
      "[step: 1255] loss: 107.52348327636719\n",
      "[step: 1256] loss: 108.49726867675781\n",
      "[step: 1257] loss: 106.43403625488281\n",
      "[step: 1258] loss: 106.27714538574219\n",
      "[step: 1259] loss: 112.58740997314453\n",
      "[step: 1260] loss: 100.69386291503906\n",
      "[step: 1261] loss: 110.72752380371094\n",
      "[step: 1262] loss: 99.2265396118164\n",
      "[step: 1263] loss: 101.86555480957031\n",
      "[step: 1264] loss: 98.40943908691406\n",
      "[step: 1265] loss: 98.54463958740234\n",
      "[step: 1266] loss: 98.73271179199219\n",
      "[step: 1267] loss: 100.94276428222656\n",
      "[step: 1268] loss: 104.40696716308594\n",
      "[step: 1269] loss: 98.43614196777344\n",
      "[step: 1270] loss: 101.0953369140625\n",
      "[step: 1271] loss: 100.27970886230469\n",
      "[step: 1272] loss: 100.71524047851562\n",
      "[step: 1273] loss: 103.18973541259766\n",
      "[step: 1274] loss: 94.50267028808594\n",
      "[step: 1275] loss: 101.00154113769531\n",
      "[step: 1276] loss: 98.59078216552734\n",
      "[step: 1277] loss: 103.18283081054688\n",
      "[step: 1278] loss: 102.18180847167969\n",
      "[step: 1279] loss: 97.23146057128906\n",
      "[step: 1280] loss: 102.10894012451172\n",
      "[step: 1281] loss: 95.82972717285156\n",
      "[step: 1282] loss: 104.29776000976562\n",
      "[step: 1283] loss: 109.4261474609375\n",
      "[step: 1284] loss: 110.24392700195312\n",
      "[step: 1285] loss: 103.03353118896484\n",
      "[step: 1286] loss: 99.65913391113281\n",
      "[step: 1287] loss: 99.80301666259766\n",
      "[step: 1288] loss: 100.81275939941406\n",
      "[step: 1289] loss: 98.22559356689453\n",
      "[step: 1290] loss: 104.88314819335938\n",
      "[step: 1291] loss: 103.33250427246094\n",
      "[step: 1292] loss: 94.95439910888672\n",
      "[step: 1293] loss: 107.49203491210938\n",
      "[step: 1294] loss: 100.2204818725586\n",
      "[step: 1295] loss: 102.52535247802734\n",
      "[step: 1296] loss: 101.79478454589844\n",
      "[step: 1297] loss: 110.53401947021484\n",
      "[step: 1298] loss: 113.67593383789062\n",
      "[step: 1299] loss: 94.49372100830078\n",
      "[step: 1300] loss: 116.70730590820312\n",
      "[step: 1301] loss: 108.65457153320312\n",
      "[step: 1302] loss: 108.43280029296875\n",
      "[step: 1303] loss: 107.70889282226562\n",
      "[step: 1304] loss: 112.09332275390625\n",
      "[step: 1305] loss: 107.61628723144531\n",
      "[step: 1306] loss: 102.25286865234375\n",
      "[step: 1307] loss: 108.31446838378906\n",
      "[step: 1308] loss: 101.16596984863281\n",
      "[step: 1309] loss: 102.66030883789062\n",
      "[step: 1310] loss: 100.49195861816406\n",
      "[step: 1311] loss: 100.969970703125\n",
      "[step: 1312] loss: 96.54891967773438\n",
      "[step: 1313] loss: 100.06414794921875\n",
      "[step: 1314] loss: 100.3255615234375\n",
      "[step: 1315] loss: 96.33953857421875\n",
      "[step: 1316] loss: 100.31233215332031\n",
      "[step: 1317] loss: 94.91087341308594\n",
      "[step: 1318] loss: 98.4239501953125\n",
      "[step: 1319] loss: 97.94612884521484\n",
      "[step: 1320] loss: 99.14276123046875\n",
      "[step: 1321] loss: 96.21115112304688\n",
      "[step: 1322] loss: 96.24778747558594\n",
      "[step: 1323] loss: 97.6741943359375\n",
      "[step: 1324] loss: 97.63423156738281\n",
      "[step: 1325] loss: 95.86614227294922\n",
      "[step: 1326] loss: 99.83717346191406\n",
      "[step: 1327] loss: 107.08573913574219\n",
      "[step: 1328] loss: 101.61778259277344\n",
      "[step: 1329] loss: 94.7696533203125\n",
      "[step: 1330] loss: 93.96891784667969\n",
      "[step: 1331] loss: 100.74976348876953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1332] loss: 93.24102020263672\n",
      "[step: 1333] loss: 94.1957015991211\n",
      "[step: 1334] loss: 98.72422790527344\n",
      "[step: 1335] loss: 99.88102722167969\n",
      "[step: 1336] loss: 93.5025634765625\n",
      "[step: 1337] loss: 99.31450653076172\n",
      "[step: 1338] loss: 112.08280944824219\n",
      "[step: 1339] loss: 103.07140350341797\n",
      "[step: 1340] loss: 94.17097473144531\n",
      "[step: 1341] loss: 98.59234619140625\n",
      "[step: 1342] loss: 111.58668518066406\n",
      "[step: 1343] loss: 101.76875305175781\n",
      "[step: 1344] loss: 91.9926986694336\n",
      "[step: 1345] loss: 103.6667251586914\n",
      "[step: 1346] loss: 101.00128936767578\n",
      "[step: 1347] loss: 96.64269256591797\n",
      "[step: 1348] loss: 98.6021728515625\n",
      "[step: 1349] loss: 98.6787109375\n",
      "[step: 1350] loss: 92.12442779541016\n",
      "[step: 1351] loss: 100.81007385253906\n",
      "[step: 1352] loss: 105.78516387939453\n",
      "[step: 1353] loss: 94.54171752929688\n",
      "[step: 1354] loss: 99.36588287353516\n",
      "[step: 1355] loss: 105.94279479980469\n",
      "[step: 1356] loss: 97.16036987304688\n",
      "[step: 1357] loss: 98.78160095214844\n",
      "[step: 1358] loss: 109.54652404785156\n",
      "[step: 1359] loss: 101.34544372558594\n",
      "[step: 1360] loss: 97.52552032470703\n",
      "[step: 1361] loss: 96.71711730957031\n",
      "[step: 1362] loss: 95.71981811523438\n",
      "[step: 1363] loss: 92.81141662597656\n",
      "[step: 1364] loss: 95.04910278320312\n",
      "[step: 1365] loss: 94.55984497070312\n",
      "[step: 1366] loss: 96.21786499023438\n",
      "[step: 1367] loss: 94.68466186523438\n",
      "[step: 1368] loss: 95.50875854492188\n",
      "[step: 1369] loss: 92.04315185546875\n",
      "[step: 1370] loss: 95.31524658203125\n",
      "[step: 1371] loss: 92.64940643310547\n",
      "[step: 1372] loss: 88.7187271118164\n",
      "[step: 1373] loss: 94.29911804199219\n",
      "[step: 1374] loss: 95.89453125\n",
      "[step: 1375] loss: 106.69551086425781\n",
      "[step: 1376] loss: 98.4798812866211\n",
      "[step: 1377] loss: 106.9442367553711\n",
      "[step: 1378] loss: 127.15312194824219\n",
      "[step: 1379] loss: 102.66434478759766\n",
      "[step: 1380] loss: 123.54127502441406\n",
      "[step: 1381] loss: 127.18783569335938\n",
      "[step: 1382] loss: 121.71321105957031\n",
      "[step: 1383] loss: 113.36080932617188\n",
      "[step: 1384] loss: 122.5467529296875\n",
      "[step: 1385] loss: 116.40715026855469\n",
      "[step: 1386] loss: 100.90196228027344\n",
      "[step: 1387] loss: 116.56321716308594\n",
      "[step: 1388] loss: 101.5007095336914\n",
      "[step: 1389] loss: 104.55509948730469\n",
      "[step: 1390] loss: 103.0736083984375\n",
      "[step: 1391] loss: 111.37306213378906\n",
      "[step: 1392] loss: 96.03764343261719\n",
      "[step: 1393] loss: 102.40036010742188\n",
      "[step: 1394] loss: 96.8843994140625\n",
      "[step: 1395] loss: 99.36439514160156\n",
      "[step: 1396] loss: 101.08442687988281\n",
      "[step: 1397] loss: 94.59744262695312\n",
      "[step: 1398] loss: 104.37004852294922\n",
      "[step: 1399] loss: 93.31446838378906\n",
      "[step: 1400] loss: 97.94808959960938\n",
      "[step: 1401] loss: 95.40767669677734\n",
      "[step: 1402] loss: 100.92564392089844\n",
      "[step: 1403] loss: 94.72500610351562\n",
      "[step: 1404] loss: 96.71588134765625\n",
      "[step: 1405] loss: 97.33820343017578\n",
      "[step: 1406] loss: 94.62034606933594\n",
      "[step: 1407] loss: 99.98906707763672\n",
      "[step: 1408] loss: 97.65154266357422\n",
      "[step: 1409] loss: 93.14044189453125\n",
      "[step: 1410] loss: 98.99457550048828\n",
      "[step: 1411] loss: 93.28605651855469\n",
      "[step: 1412] loss: 94.37834167480469\n",
      "[step: 1413] loss: 90.56230926513672\n",
      "[step: 1414] loss: 91.91853332519531\n",
      "[step: 1415] loss: 93.06148529052734\n",
      "[step: 1416] loss: 93.96142578125\n",
      "[step: 1417] loss: 96.23725891113281\n",
      "[step: 1418] loss: 95.05815887451172\n",
      "[step: 1419] loss: 91.7993392944336\n",
      "[step: 1420] loss: 94.75711059570312\n",
      "[step: 1421] loss: 95.75834655761719\n",
      "[step: 1422] loss: 97.68103790283203\n",
      "[step: 1423] loss: 91.71526336669922\n",
      "[step: 1424] loss: 89.58018493652344\n",
      "[step: 1425] loss: 94.97994995117188\n",
      "[step: 1426] loss: 92.4752197265625\n",
      "[step: 1427] loss: 93.16169738769531\n",
      "[step: 1428] loss: 91.93022918701172\n",
      "[step: 1429] loss: 92.2287826538086\n",
      "[step: 1430] loss: 94.15846252441406\n",
      "[step: 1431] loss: 93.18409729003906\n",
      "[step: 1432] loss: 97.64016723632812\n",
      "[step: 1433] loss: 101.7924575805664\n",
      "[step: 1434] loss: 100.43718719482422\n",
      "[step: 1435] loss: 101.54048919677734\n",
      "[step: 1436] loss: 94.73282623291016\n",
      "[step: 1437] loss: 91.53892517089844\n",
      "[step: 1438] loss: 97.09138488769531\n",
      "[step: 1439] loss: 94.2735595703125\n",
      "[step: 1440] loss: 91.26094055175781\n",
      "[step: 1441] loss: 92.0223388671875\n",
      "[step: 1442] loss: 95.63665008544922\n",
      "[step: 1443] loss: 91.04334259033203\n",
      "[step: 1444] loss: 93.08953094482422\n",
      "[step: 1445] loss: 94.71295928955078\n",
      "[step: 1446] loss: 102.87629699707031\n",
      "[step: 1447] loss: 101.62674713134766\n",
      "[step: 1448] loss: 101.17108154296875\n",
      "[step: 1449] loss: 94.54238891601562\n",
      "[step: 1450] loss: 90.35173034667969\n",
      "[step: 1451] loss: 95.6020736694336\n",
      "[step: 1452] loss: 95.06549835205078\n",
      "[step: 1453] loss: 92.43061828613281\n",
      "[step: 1454] loss: 93.85870361328125\n",
      "[step: 1455] loss: 98.15784454345703\n",
      "[step: 1456] loss: 95.17214965820312\n",
      "[step: 1457] loss: 92.45821380615234\n",
      "[step: 1458] loss: 97.97889709472656\n",
      "[step: 1459] loss: 97.01515197753906\n",
      "[step: 1460] loss: 92.62669372558594\n",
      "[step: 1461] loss: 88.93617248535156\n",
      "[step: 1462] loss: 90.93307495117188\n",
      "[step: 1463] loss: 98.16853332519531\n",
      "[step: 1464] loss: 96.78108215332031\n",
      "[step: 1465] loss: 92.86636352539062\n",
      "[step: 1466] loss: 94.05658721923828\n",
      "[step: 1467] loss: 98.25260162353516\n",
      "[step: 1468] loss: 94.66728973388672\n",
      "[step: 1469] loss: 90.96614837646484\n",
      "[step: 1470] loss: 98.60728454589844\n",
      "[step: 1471] loss: 95.93984985351562\n",
      "[step: 1472] loss: 87.46279907226562\n",
      "[step: 1473] loss: 100.70390319824219\n",
      "[step: 1474] loss: 99.39134979248047\n",
      "[step: 1475] loss: 92.35169219970703\n",
      "[step: 1476] loss: 100.26293182373047\n",
      "[step: 1477] loss: 107.08389282226562\n",
      "[step: 1478] loss: 94.8902587890625\n",
      "[step: 1479] loss: 95.28401947021484\n",
      "[step: 1480] loss: 98.80145263671875\n",
      "[step: 1481] loss: 107.76398468017578\n",
      "[step: 1482] loss: 98.74479675292969\n",
      "[step: 1483] loss: 92.82560729980469\n",
      "[step: 1484] loss: 103.00057983398438\n",
      "[step: 1485] loss: 95.66557312011719\n",
      "[step: 1486] loss: 90.6302490234375\n",
      "[step: 1487] loss: 94.60647583007812\n",
      "[step: 1488] loss: 91.66267395019531\n",
      "[step: 1489] loss: 92.38736724853516\n",
      "[step: 1490] loss: 103.20604705810547\n",
      "[step: 1491] loss: 99.53190612792969\n",
      "[step: 1492] loss: 90.73432922363281\n",
      "[step: 1493] loss: 91.58976745605469\n",
      "[step: 1494] loss: 91.5616455078125\n",
      "[step: 1495] loss: 89.72622680664062\n",
      "[step: 1496] loss: 88.43864440917969\n",
      "[step: 1497] loss: 89.89892578125\n",
      "[step: 1498] loss: 95.57809448242188\n",
      "[step: 1499] loss: 94.33419799804688\n",
      "RMSE: 0.13878527283668518\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "##xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "##xy_with_noise = xy_with_noise[:,:27]\n",
    "##a_with_noise = xy_with_noise[:,:2]\n",
    "##b_with_noise = xy_with_noise[:,2:]\n",
    "##b_with_noise = MinMaxScaler(b_with_noise)\n",
    "##xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 54\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 1500\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "##train_set_with_noise = xy_with_noise[:train_size]\n",
    "##validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "##test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "##trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "##validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "##testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "##X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "##Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    lstm_1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout1 = tf.contrib.rnn.DropoutWrapper(lstm_1,keep_prob)\n",
    "    lstm_2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim*2, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout2 = tf.contrib.rnn.DropoutWrapper(lstm_2,keep_prob)\n",
    "    lstm_3 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout3 = tf.contrib.rnn.DropoutWrapper(lstm_3,keep_prob)\n",
    "    cell1 = tf.contrib.rnn.MultiRNNCell([dropout1,dropout2,dropout3])\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "##with tf.variable_scope(\"rnn2\"):\n",
    "    ##cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    ##outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    ##Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    ##loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    ##train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE \n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY, keep_prob : 0.7})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "       ## _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        ##print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    validation_predict = sess.run(Y_pred, feed_dict = {X1:validationX, keep_prob : 1})\n",
    "    ##validation_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:testX, keep_prob : 1})\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: validation_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
