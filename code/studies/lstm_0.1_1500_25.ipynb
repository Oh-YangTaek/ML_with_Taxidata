{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-4ce416823641>:89: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 19893.150390625\n",
      "[step: 0] loss: 0.3186686038970947\n",
      "[step: 1] loss: 95134.9375\n",
      "[step: 1] loss: 0.18458418548107147\n",
      "[step: 2] loss: 17048.6171875\n",
      "[step: 2] loss: 0.07859673351049423\n",
      "[step: 3] loss: 11760.109375\n",
      "[step: 3] loss: 0.07895123213529587\n",
      "[step: 4] loss: 11369.0849609375\n",
      "[step: 4] loss: 0.046017102897167206\n",
      "[step: 5] loss: 10146.640625\n",
      "[step: 5] loss: 0.035002149641513824\n",
      "[step: 6] loss: 9582.294921875\n",
      "[step: 6] loss: 0.03080286830663681\n",
      "[step: 7] loss: 7760.5107421875\n",
      "[step: 7] loss: 0.02879856899380684\n",
      "[step: 8] loss: 5162.228515625\n",
      "[step: 8] loss: 0.028716888278722763\n",
      "[step: 9] loss: 3604.385498046875\n",
      "[step: 9] loss: 0.027990786358714104\n",
      "[step: 10] loss: 3516.51220703125\n",
      "[step: 10] loss: 0.026484794914722443\n",
      "[step: 11] loss: 3786.32275390625\n",
      "[step: 11] loss: 0.027458975091576576\n",
      "[step: 12] loss: 4168.5673828125\n",
      "[step: 12] loss: 0.026245959103107452\n",
      "[step: 13] loss: 4046.844970703125\n",
      "[step: 13] loss: 0.024720607325434685\n",
      "[step: 14] loss: 3764.78515625\n",
      "[step: 14] loss: 0.02388216368854046\n",
      "[step: 15] loss: 3854.63671875\n",
      "[step: 15] loss: 0.022580048069357872\n",
      "[step: 16] loss: 3375.732177734375\n",
      "[step: 16] loss: 0.02130291983485222\n",
      "[step: 17] loss: 2908.1376953125\n",
      "[step: 17] loss: 0.01918693073093891\n",
      "[step: 18] loss: 2899.103515625\n",
      "[step: 18] loss: 0.020174197852611542\n",
      "[step: 19] loss: 2604.15087890625\n",
      "[step: 19] loss: 0.01964751072227955\n",
      "[step: 20] loss: 2264.65869140625\n",
      "[step: 20] loss: 0.01933075487613678\n",
      "[step: 21] loss: 2261.600341796875\n",
      "[step: 21] loss: 0.018481401726603508\n",
      "[step: 22] loss: 2191.64111328125\n",
      "[step: 22] loss: 0.019950946792960167\n",
      "[step: 23] loss: 2024.5523681640625\n",
      "[step: 23] loss: 0.018342452123761177\n",
      "[step: 24] loss: 1906.432373046875\n",
      "[step: 24] loss: 0.018427465111017227\n",
      "[step: 25] loss: 1959.876953125\n",
      "[step: 25] loss: 0.018557647243142128\n",
      "[step: 26] loss: 1887.556640625\n",
      "[step: 26] loss: 0.017859885469079018\n",
      "[step: 27] loss: 1730.620849609375\n",
      "[step: 27] loss: 0.017054354771971703\n",
      "[step: 28] loss: 1716.093505859375\n",
      "[step: 28] loss: 0.017370261251926422\n",
      "[step: 29] loss: 1742.1988525390625\n",
      "[step: 29] loss: 0.016976960003376007\n",
      "[step: 30] loss: 1660.9375\n",
      "[step: 30] loss: 0.01663140393793583\n",
      "[step: 31] loss: 1586.680419921875\n",
      "[step: 31] loss: 0.016926737502217293\n",
      "[step: 32] loss: 1565.8167724609375\n",
      "[step: 32] loss: 0.016968674957752228\n",
      "[step: 33] loss: 1528.8582763671875\n",
      "[step: 33] loss: 0.016704337671399117\n",
      "[step: 34] loss: 1475.421630859375\n",
      "[step: 34] loss: 0.016679780557751656\n",
      "[step: 35] loss: 1438.166748046875\n",
      "[step: 35] loss: 0.016888586804270744\n",
      "[step: 36] loss: 1375.310791015625\n",
      "[step: 36] loss: 0.01656322181224823\n",
      "[step: 37] loss: 1341.4989013671875\n",
      "[step: 37] loss: 0.016481805592775345\n",
      "[step: 38] loss: 1327.9027099609375\n",
      "[step: 38] loss: 0.016558583825826645\n",
      "[step: 39] loss: 1266.3492431640625\n",
      "[step: 39] loss: 0.016446178779006004\n",
      "[step: 40] loss: 1216.962646484375\n",
      "[step: 40] loss: 0.016291342675685883\n",
      "[step: 41] loss: 1210.58837890625\n",
      "[step: 41] loss: 0.016394538804888725\n",
      "[step: 42] loss: 1166.0931396484375\n",
      "[step: 42] loss: 0.01640489138662815\n",
      "[step: 43] loss: 1112.785400390625\n",
      "[step: 43] loss: 0.016278279945254326\n",
      "[step: 44] loss: 1090.8916015625\n",
      "[step: 44] loss: 0.01632869988679886\n",
      "[step: 45] loss: 1062.7392578125\n",
      "[step: 45] loss: 0.016323424875736237\n",
      "[step: 46] loss: 1020.2606201171875\n",
      "[step: 46] loss: 0.016190430149435997\n",
      "[step: 47] loss: 990.019287109375\n",
      "[step: 47] loss: 0.016150683164596558\n",
      "[step: 48] loss: 973.8384399414062\n",
      "[step: 48] loss: 0.01616457849740982\n",
      "[step: 49] loss: 947.79541015625\n",
      "[step: 49] loss: 0.01606142148375511\n",
      "[step: 50] loss: 919.5736694335938\n",
      "[step: 50] loss: 0.016061294823884964\n",
      "[step: 51] loss: 901.093017578125\n",
      "[step: 51] loss: 0.016097672283649445\n",
      "[step: 52] loss: 880.6213989257812\n",
      "[step: 52] loss: 0.01606016606092453\n",
      "[step: 53] loss: 849.8552856445312\n",
      "[step: 53] loss: 0.016056030988693237\n",
      "[step: 54] loss: 819.2926635742188\n",
      "[step: 54] loss: 0.01609151065349579\n",
      "[step: 55] loss: 797.5114135742188\n",
      "[step: 55] loss: 0.01604584790766239\n",
      "[step: 56] loss: 772.991455078125\n",
      "[step: 56] loss: 0.01603037305176258\n",
      "[step: 57] loss: 745.4168701171875\n",
      "[step: 57] loss: 0.01603740081191063\n",
      "[step: 58] loss: 726.5052490234375\n",
      "[step: 58] loss: 0.016000092029571533\n",
      "[step: 59] loss: 710.201416015625\n",
      "[step: 59] loss: 0.01598593406379223\n",
      "[step: 60] loss: 688.003662109375\n",
      "[step: 60] loss: 0.016002142801880836\n",
      "[step: 61] loss: 666.4196166992188\n",
      "[step: 61] loss: 0.01598121039569378\n",
      "[step: 62] loss: 649.5093994140625\n",
      "[step: 62] loss: 0.01598602719604969\n",
      "[step: 63] loss: 627.0038452148438\n",
      "[step: 63] loss: 0.015999360010027885\n",
      "[step: 64] loss: 604.9929809570312\n",
      "[step: 64] loss: 0.01598331145942211\n",
      "[step: 65] loss: 588.6921997070312\n",
      "[step: 65] loss: 0.015983842313289642\n",
      "[step: 66] loss: 569.476318359375\n",
      "[step: 66] loss: 0.01598425768315792\n",
      "[step: 67] loss: 551.048828125\n",
      "[step: 67] loss: 0.015963509678840637\n",
      "[step: 68] loss: 537.2008666992188\n",
      "[step: 68] loss: 0.015962760895490646\n",
      "[step: 69] loss: 519.2732543945312\n",
      "[step: 69] loss: 0.015957100316882133\n",
      "[step: 70] loss: 504.03057861328125\n",
      "[step: 70] loss: 0.015945175662636757\n",
      "[step: 71] loss: 490.0489196777344\n",
      "[step: 71] loss: 0.015950793400406837\n",
      "[step: 72] loss: 473.9627990722656\n",
      "[step: 72] loss: 0.01594836451113224\n",
      "[step: 73] loss: 462.03314208984375\n",
      "[step: 73] loss: 0.015945298597216606\n",
      "[step: 74] loss: 448.47406005859375\n",
      "[step: 74] loss: 0.015950670465826988\n",
      "[step: 75] loss: 436.4852294921875\n",
      "[step: 75] loss: 0.0159455556422472\n",
      "[step: 76] loss: 425.620849609375\n",
      "[step: 76] loss: 0.01594228483736515\n",
      "[step: 77] loss: 413.68927001953125\n",
      "[step: 77] loss: 0.015942716971039772\n",
      "[step: 78] loss: 404.269775390625\n",
      "[step: 78] loss: 0.015935581177473068\n",
      "[step: 79] loss: 393.50018310546875\n",
      "[step: 79] loss: 0.015934690833091736\n",
      "[step: 80] loss: 384.8357238769531\n",
      "[step: 80] loss: 0.015933997929096222\n",
      "[step: 81] loss: 375.6099853515625\n",
      "[step: 81] loss: 0.0159302968531847\n",
      "[step: 82] loss: 367.60772705078125\n",
      "[step: 82] loss: 0.01593232899904251\n",
      "[step: 83] loss: 359.3525390625\n",
      "[step: 83] loss: 0.01593109965324402\n",
      "[step: 84] loss: 351.9643249511719\n",
      "[step: 84] loss: 0.01592986471951008\n",
      "[step: 85] loss: 344.376708984375\n",
      "[step: 85] loss: 0.01593087613582611\n",
      "[step: 86] loss: 337.749267578125\n",
      "[step: 86] loss: 0.015927953645586967\n",
      "[step: 87] loss: 330.8477783203125\n",
      "[step: 87] loss: 0.015927137807011604\n",
      "[step: 88] loss: 325.11456298828125\n",
      "[step: 88] loss: 0.01592588610947132\n",
      "[step: 89] loss: 318.977294921875\n",
      "[step: 89] loss: 0.01592329517006874\n",
      "[step: 90] loss: 313.947998046875\n",
      "[step: 90] loss: 0.0159233957529068\n",
      "[step: 91] loss: 308.474609375\n",
      "[step: 91] loss: 0.015921883285045624\n",
      "[step: 92] loss: 303.84814453125\n",
      "[step: 92] loss: 0.01592128537595272\n",
      "[step: 93] loss: 299.0235900878906\n",
      "[step: 93] loss: 0.015921272337436676\n",
      "[step: 94] loss: 294.6907958984375\n",
      "[step: 94] loss: 0.015919717028737068\n",
      "[step: 95] loss: 290.5796813964844\n",
      "[step: 95] loss: 0.015919489786028862\n",
      "[step: 96] loss: 286.58868408203125\n",
      "[step: 96] loss: 0.015918325632810593\n",
      "[step: 97] loss: 283.09661865234375\n",
      "[step: 97] loss: 0.015917276963591576\n",
      "[step: 98] loss: 279.52838134765625\n",
      "[step: 98] loss: 0.015916995704174042\n",
      "[step: 99] loss: 276.314208984375\n",
      "[step: 99] loss: 0.015915846452116966\n",
      "[step: 100] loss: 273.24737548828125\n",
      "[step: 100] loss: 0.015915576368570328\n",
      "[step: 101] loss: 270.19805908203125\n",
      "[step: 101] loss: 0.015914857387542725\n",
      "[step: 102] loss: 267.4795227050781\n",
      "[step: 102] loss: 0.01591399870812893\n",
      "[step: 103] loss: 264.8359680175781\n",
      "[step: 103] loss: 0.015913553535938263\n",
      "[step: 104] loss: 262.248291015625\n",
      "[step: 104] loss: 0.01591251790523529\n",
      "[step: 105] loss: 259.97265625\n",
      "[step: 105] loss: 0.01591205596923828\n",
      "[step: 106] loss: 257.6558837890625\n",
      "[step: 106] loss: 0.015911312773823738\n",
      "[step: 107] loss: 255.44793701171875\n",
      "[step: 107] loss: 0.015910619869828224\n",
      "[step: 108] loss: 253.40225219726562\n",
      "[step: 108] loss: 0.015910150483250618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 109] loss: 251.46743774414062\n",
      "[step: 109] loss: 0.015909329056739807\n",
      "[step: 110] loss: 249.52658081054688\n",
      "[step: 110] loss: 0.01590881310403347\n",
      "[step: 111] loss: 247.75567626953125\n",
      "[step: 111] loss: 0.015908051282167435\n",
      "[step: 112] loss: 246.08767700195312\n",
      "[step: 112] loss: 0.01590735651552677\n",
      "[step: 113] loss: 244.48402404785156\n",
      "[step: 113] loss: 0.01590673252940178\n",
      "[step: 114] loss: 242.86795043945312\n",
      "[step: 114] loss: 0.01590598002076149\n",
      "[step: 115] loss: 241.33001708984375\n",
      "[step: 115] loss: 0.015905452892184258\n",
      "[step: 116] loss: 239.8590087890625\n",
      "[step: 116] loss: 0.015904711559414864\n",
      "[step: 117] loss: 238.48040771484375\n",
      "[step: 117] loss: 0.01590409129858017\n",
      "[step: 118] loss: 237.13572692871094\n",
      "[step: 118] loss: 0.015903349965810776\n",
      "[step: 119] loss: 235.84036254882812\n",
      "[step: 119] loss: 0.015902595594525337\n",
      "[step: 120] loss: 234.56808471679688\n",
      "[step: 120] loss: 0.01590186357498169\n",
      "[step: 121] loss: 233.34722900390625\n",
      "[step: 121] loss: 0.015901023522019386\n",
      "[step: 122] loss: 232.16482543945312\n",
      "[step: 122] loss: 0.015900280326604843\n",
      "[step: 123] loss: 231.04116821289062\n",
      "[step: 123] loss: 0.015899380668997765\n",
      "[step: 124] loss: 229.9792022705078\n",
      "[step: 124] loss: 0.01589851640164852\n",
      "[step: 125] loss: 229.03387451171875\n",
      "[step: 125] loss: 0.01589752733707428\n",
      "[step: 126] loss: 228.36831665039062\n",
      "[step: 126] loss: 0.015896454453468323\n",
      "[step: 127] loss: 228.40081787109375\n",
      "[step: 127] loss: 0.01589527167379856\n",
      "[step: 128] loss: 230.90670776367188\n",
      "[step: 128] loss: 0.015893908217549324\n",
      "[step: 129] loss: 239.67300415039062\n",
      "[step: 129] loss: 0.015892332419753075\n",
      "[step: 130] loss: 274.916748046875\n",
      "[step: 130] loss: 0.015890415757894516\n",
      "[step: 131] loss: 323.4627685546875\n",
      "[step: 131] loss: 0.01588802970945835\n",
      "[step: 132] loss: 448.152587890625\n",
      "[step: 132] loss: 0.01588483713567257\n",
      "[step: 133] loss: 278.3438720703125\n",
      "[step: 133] loss: 0.015880532562732697\n",
      "[step: 134] loss: 250.02847290039062\n",
      "[step: 134] loss: 0.01587444171309471\n",
      "[step: 135] loss: 419.8815612792969\n",
      "[step: 135] loss: 0.01586552895605564\n",
      "[step: 136] loss: 313.19873046875\n",
      "[step: 136] loss: 0.015852268785238266\n",
      "[step: 137] loss: 242.36839294433594\n",
      "[step: 137] loss: 0.01583096943795681\n",
      "[step: 138] loss: 304.0994567871094\n",
      "[step: 138] loss: 0.01578802615404129\n",
      "[step: 139] loss: 273.36572265625\n",
      "[step: 139] loss: 0.01569247432053089\n",
      "[step: 140] loss: 225.96356201171875\n",
      "[step: 140] loss: 0.015801385045051575\n",
      "[step: 141] loss: 258.094482421875\n",
      "[step: 141] loss: 0.016389155760407448\n",
      "[step: 142] loss: 225.54415893554688\n",
      "[step: 142] loss: 0.015975836664438248\n",
      "[step: 143] loss: 239.12734985351562\n",
      "[step: 143] loss: 0.016327351331710815\n",
      "[step: 144] loss: 243.695068359375\n",
      "[step: 144] loss: 0.01585935428738594\n",
      "[step: 145] loss: 231.27810668945312\n",
      "[step: 145] loss: 0.016081275418400764\n",
      "[step: 146] loss: 234.56869506835938\n",
      "[step: 146] loss: 0.015244823880493641\n",
      "[step: 147] loss: 240.51048278808594\n",
      "[step: 147] loss: 0.016858482733368874\n",
      "[step: 148] loss: 218.1171417236328\n",
      "[step: 148] loss: 0.019254913553595543\n",
      "[step: 149] loss: 236.4718780517578\n",
      "[step: 149] loss: 0.021799610927700996\n",
      "[step: 150] loss: 223.675048828125\n",
      "[step: 150] loss: 0.0216007549315691\n",
      "[step: 151] loss: 220.33665466308594\n",
      "[step: 151] loss: 0.020444100722670555\n",
      "[step: 152] loss: 232.63442993164062\n",
      "[step: 152] loss: 0.019572842866182327\n",
      "[step: 153] loss: 222.04762268066406\n",
      "[step: 153] loss: 0.019396940246224403\n",
      "[step: 154] loss: 217.70834350585938\n",
      "[step: 154] loss: 0.019741831347346306\n",
      "[step: 155] loss: 227.17074584960938\n",
      "[step: 155] loss: 0.020248763263225555\n",
      "[step: 156] loss: 219.696533203125\n",
      "[step: 156] loss: 0.01962391287088394\n",
      "[step: 157] loss: 210.73434448242188\n",
      "[step: 157] loss: 0.018379835411906242\n",
      "[step: 158] loss: 221.36322021484375\n",
      "[step: 158] loss: 0.018042946234345436\n",
      "[step: 159] loss: 215.3816375732422\n",
      "[step: 159] loss: 0.01805034466087818\n",
      "[step: 160] loss: 209.0319061279297\n",
      "[step: 160] loss: 0.01752089336514473\n",
      "[step: 161] loss: 213.67349243164062\n",
      "[step: 161] loss: 0.016812574118375778\n",
      "[step: 162] loss: 216.25445556640625\n",
      "[step: 162] loss: 0.01739054173231125\n",
      "[step: 163] loss: 205.50860595703125\n",
      "[step: 163] loss: 0.016706453636288643\n",
      "[step: 164] loss: 210.2493896484375\n",
      "[step: 164] loss: 0.016769621521234512\n",
      "[step: 165] loss: 214.16000366210938\n",
      "[step: 165] loss: 0.016740987077355385\n",
      "[step: 166] loss: 208.2235107421875\n",
      "[step: 166] loss: 0.015890823677182198\n",
      "[step: 167] loss: 203.04278564453125\n",
      "[step: 167] loss: 0.017007840797305107\n",
      "[step: 168] loss: 209.49725341796875\n",
      "[step: 168] loss: 0.01706440933048725\n",
      "[step: 169] loss: 208.44992065429688\n",
      "[step: 169] loss: 0.018588868901133537\n",
      "[step: 170] loss: 204.12451171875\n",
      "[step: 170] loss: 0.017722396180033684\n",
      "[step: 171] loss: 200.77027893066406\n",
      "[step: 171] loss: 0.01577388308942318\n",
      "[step: 172] loss: 205.43991088867188\n",
      "[step: 172] loss: 0.017310606315732002\n",
      "[step: 173] loss: 205.8843994140625\n",
      "[step: 173] loss: 0.015920603647828102\n",
      "[step: 174] loss: 202.11192321777344\n",
      "[step: 174] loss: 0.015829797834157944\n",
      "[step: 175] loss: 198.33786010742188\n",
      "[step: 175] loss: 0.016496695578098297\n",
      "[step: 176] loss: 201.33103942871094\n",
      "[step: 176] loss: 0.016412388533353806\n",
      "[step: 177] loss: 202.28460693359375\n",
      "[step: 177] loss: 0.015637964010238647\n",
      "[step: 178] loss: 201.1674041748047\n",
      "[step: 178] loss: 0.015661031007766724\n",
      "[step: 179] loss: 196.74574279785156\n",
      "[step: 179] loss: 0.015925932675600052\n",
      "[step: 180] loss: 196.92831420898438\n",
      "[step: 180] loss: 0.01525110099464655\n",
      "[step: 181] loss: 198.28921508789062\n",
      "[step: 181] loss: 0.01589677669107914\n",
      "[step: 182] loss: 199.26344299316406\n",
      "[step: 182] loss: 0.014850026927888393\n",
      "[step: 183] loss: 196.78558349609375\n",
      "[step: 183] loss: 0.01828385517001152\n",
      "[step: 184] loss: 194.68238830566406\n",
      "[step: 184] loss: 0.019672539085149765\n",
      "[step: 185] loss: 193.5045928955078\n",
      "[step: 185] loss: 0.022895053029060364\n",
      "[step: 186] loss: 194.45706176757812\n",
      "[step: 186] loss: 0.02232467383146286\n",
      "[step: 187] loss: 194.7545623779297\n",
      "[step: 187] loss: 0.02049378864467144\n",
      "[step: 188] loss: 194.5828857421875\n",
      "[step: 188] loss: 0.019349435344338417\n",
      "[step: 189] loss: 193.1336669921875\n",
      "[step: 189] loss: 0.01948564127087593\n",
      "[step: 190] loss: 191.71090698242188\n",
      "[step: 190] loss: 0.020311739295721054\n",
      "[step: 191] loss: 190.7256317138672\n",
      "[step: 191] loss: 0.020906509831547737\n",
      "[step: 192] loss: 190.47274780273438\n",
      "[step: 192] loss: 0.020653165876865387\n",
      "[step: 193] loss: 190.74046325683594\n",
      "[step: 193] loss: 0.019690563902258873\n",
      "[step: 194] loss: 190.80667114257812\n",
      "[step: 194] loss: 0.018939733505249023\n",
      "[step: 195] loss: 190.8129119873047\n",
      "[step: 195] loss: 0.018856346607208252\n",
      "[step: 196] loss: 190.2442626953125\n",
      "[step: 196] loss: 0.018962308764457703\n",
      "[step: 197] loss: 189.74098205566406\n",
      "[step: 197] loss: 0.018704451620578766\n",
      "[step: 198] loss: 188.7692413330078\n",
      "[step: 198] loss: 0.018135031685233116\n",
      "[step: 199] loss: 188.0692138671875\n",
      "[step: 199] loss: 0.01787823624908924\n",
      "[step: 200] loss: 187.26536560058594\n",
      "[step: 200] loss: 0.018093138933181763\n",
      "[step: 201] loss: 186.6947784423828\n",
      "[step: 201] loss: 0.0178065188229084\n",
      "[step: 202] loss: 186.10592651367188\n",
      "[step: 202] loss: 0.017195452004671097\n",
      "[step: 203] loss: 185.629638671875\n",
      "[step: 203] loss: 0.016979258507490158\n",
      "[step: 204] loss: 185.197509765625\n",
      "[step: 204] loss: 0.017015768215060234\n",
      "[step: 205] loss: 184.75152587890625\n",
      "[step: 205] loss: 0.017010554671287537\n",
      "[step: 206] loss: 184.388671875\n",
      "[step: 206] loss: 0.016899587586522102\n",
      "[step: 207] loss: 183.95736694335938\n",
      "[step: 207] loss: 0.0168084017932415\n",
      "[step: 208] loss: 183.61981201171875\n",
      "[step: 208] loss: 0.016812017187476158\n",
      "[step: 209] loss: 183.2301483154297\n",
      "[step: 209] loss: 0.016763826832175255\n",
      "[step: 210] loss: 182.92782592773438\n",
      "[step: 210] loss: 0.016587885096669197\n",
      "[step: 211] loss: 182.679443359375\n",
      "[step: 211] loss: 0.016450142487883568\n",
      "[step: 212] loss: 182.61880493164062\n",
      "[step: 212] loss: 0.016404064372181892\n",
      "[step: 213] loss: 183.0083770751953\n",
      "[step: 213] loss: 0.016310498118400574\n",
      "[step: 214] loss: 184.5854034423828\n",
      "[step: 214] loss: 0.016069594770669937\n",
      "[step: 215] loss: 189.02581787109375\n",
      "[step: 215] loss: 0.01577228493988514\n",
      "[step: 216] loss: 201.94027709960938\n",
      "[step: 216] loss: 0.015623321756720543\n",
      "[step: 217] loss: 230.91549682617188\n",
      "[step: 217] loss: 0.015344231389462948\n",
      "[step: 218] loss: 309.043212890625\n",
      "[step: 218] loss: 0.014983086846768856\n",
      "[step: 219] loss: 371.62078857421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 219] loss: 0.014916723594069481\n",
      "[step: 220] loss: 425.25567626953125\n",
      "[step: 220] loss: 0.01472877524793148\n",
      "[step: 221] loss: 254.10108947753906\n",
      "[step: 221] loss: 0.01468508131802082\n",
      "[step: 222] loss: 186.93753051757812\n",
      "[step: 222] loss: 0.014666436240077019\n",
      "[step: 223] loss: 293.3839416503906\n",
      "[step: 223] loss: 0.01421319879591465\n",
      "[step: 224] loss: 297.58648681640625\n",
      "[step: 224] loss: 0.014248916879296303\n",
      "[step: 225] loss: 204.66769409179688\n",
      "[step: 225] loss: 0.014653025195002556\n",
      "[step: 226] loss: 219.4003448486328\n",
      "[step: 226] loss: 0.013931499794125557\n",
      "[step: 227] loss: 281.90728759765625\n",
      "[step: 227] loss: 0.014665095135569572\n",
      "[step: 228] loss: 219.70106506347656\n",
      "[step: 228] loss: 0.017267493531107903\n",
      "[step: 229] loss: 212.09161376953125\n",
      "[step: 229] loss: 0.014887530356645584\n",
      "[step: 230] loss: 246.6709747314453\n",
      "[step: 230] loss: 0.028513319790363312\n",
      "[step: 231] loss: 197.5556640625\n",
      "[step: 231] loss: 0.02986745722591877\n",
      "[step: 232] loss: 202.618896484375\n",
      "[step: 232] loss: 0.03581054508686066\n",
      "[step: 233] loss: 216.35986328125\n",
      "[step: 233] loss: 0.03086492419242859\n",
      "[step: 234] loss: 184.7003173828125\n",
      "[step: 234] loss: 0.024334242567420006\n",
      "[step: 235] loss: 209.93719482421875\n",
      "[step: 235] loss: 0.021976331248879433\n",
      "[step: 236] loss: 195.30862426757812\n",
      "[step: 236] loss: 0.024347927421331406\n",
      "[step: 237] loss: 189.0202178955078\n",
      "[step: 237] loss: 0.02813926339149475\n",
      "[step: 238] loss: 201.173583984375\n",
      "[step: 238] loss: 0.02988193929195404\n",
      "[step: 239] loss: 184.822265625\n",
      "[step: 239] loss: 0.02864217571914196\n",
      "[step: 240] loss: 188.30067443847656\n",
      "[step: 240] loss: 0.02591070719063282\n",
      "[step: 241] loss: 190.85128784179688\n",
      "[step: 241] loss: 0.023777471855282784\n",
      "[step: 242] loss: 179.85537719726562\n",
      "[step: 242] loss: 0.02324756048619747\n",
      "[step: 243] loss: 188.67250061035156\n",
      "[step: 243] loss: 0.0238659605383873\n",
      "[step: 244] loss: 186.09957885742188\n",
      "[step: 244] loss: 0.024548152461647987\n",
      "[step: 245] loss: 178.48541259765625\n",
      "[step: 245] loss: 0.024589240550994873\n",
      "[step: 246] loss: 188.3622283935547\n",
      "[step: 246] loss: 0.024028008803725243\n",
      "[step: 247] loss: 180.93064880371094\n",
      "[step: 247] loss: 0.023330477997660637\n",
      "[step: 248] loss: 178.16204833984375\n",
      "[step: 248] loss: 0.022872136905789375\n",
      "[step: 249] loss: 183.00616455078125\n",
      "[step: 249] loss: 0.02268640697002411\n",
      "[step: 250] loss: 179.2360076904297\n",
      "[step: 250] loss: 0.022564807906746864\n",
      "[step: 251] loss: 175.1201171875\n",
      "[step: 251] loss: 0.022253813222050667\n",
      "[step: 252] loss: 181.03704833984375\n",
      "[step: 252] loss: 0.02162489853799343\n",
      "[step: 253] loss: 176.11587524414062\n",
      "[step: 253] loss: 0.021995412185788155\n",
      "[step: 254] loss: 174.890625\n",
      "[step: 254] loss: 0.020599570125341415\n",
      "[step: 255] loss: 177.4370574951172\n",
      "[step: 255] loss: 0.0212252140045166\n",
      "[step: 256] loss: 176.26593017578125\n",
      "[step: 256] loss: 0.021233029663562775\n",
      "[step: 257] loss: 172.60848999023438\n",
      "[step: 257] loss: 0.01984899304807186\n",
      "[step: 258] loss: 176.14622497558594\n",
      "[step: 258] loss: 0.01965075545012951\n",
      "[step: 259] loss: 174.8654022216797\n",
      "[step: 259] loss: 0.017999451607465744\n",
      "[step: 260] loss: 172.3271484375\n",
      "[step: 260] loss: 0.018264969810843468\n",
      "[step: 261] loss: 173.20849609375\n",
      "[step: 261] loss: 0.017246758565306664\n",
      "[step: 262] loss: 174.28070068359375\n",
      "[step: 262] loss: 0.017579970881342888\n",
      "[step: 263] loss: 171.66053771972656\n",
      "[step: 263] loss: 0.016518210992217064\n",
      "[step: 264] loss: 171.12777709960938\n",
      "[step: 264] loss: 0.016570257022976875\n",
      "[step: 265] loss: 172.6744384765625\n",
      "[step: 265] loss: 0.015811264514923096\n",
      "[step: 266] loss: 171.2912139892578\n",
      "[step: 266] loss: 0.015381629578769207\n",
      "[step: 267] loss: 169.9871063232422\n",
      "[step: 267] loss: 0.015215089544653893\n",
      "[step: 268] loss: 170.30093383789062\n",
      "[step: 268] loss: 0.015261937864124775\n",
      "[step: 269] loss: 170.87255859375\n",
      "[step: 269] loss: 0.02406201884150505\n",
      "[step: 270] loss: 169.29559326171875\n",
      "[step: 270] loss: 0.027740808203816414\n",
      "[step: 271] loss: 168.82333374023438\n",
      "[step: 271] loss: 0.03154689073562622\n",
      "[step: 272] loss: 169.13917541503906\n",
      "[step: 272] loss: 0.028987526893615723\n",
      "[step: 273] loss: 169.14022827148438\n",
      "[step: 273] loss: 0.024810295552015305\n",
      "[step: 274] loss: 168.08230590820312\n",
      "[step: 274] loss: 0.022250324487686157\n",
      "[step: 275] loss: 167.60572814941406\n",
      "[step: 275] loss: 0.02241814136505127\n",
      "[step: 276] loss: 167.94479370117188\n",
      "[step: 276] loss: 0.024255525320768356\n",
      "[step: 277] loss: 167.7237548828125\n",
      "[step: 277] loss: 0.025844447314739227\n",
      "[step: 278] loss: 167.08251953125\n",
      "[step: 278] loss: 0.025978848338127136\n",
      "[step: 279] loss: 166.48582458496094\n",
      "[step: 279] loss: 0.024797560647130013\n",
      "[step: 280] loss: 166.65213012695312\n",
      "[step: 280] loss: 0.02329585701227188\n",
      "[step: 281] loss: 166.55694580078125\n",
      "[step: 281] loss: 0.02239149622619152\n",
      "[step: 282] loss: 166.13751220703125\n",
      "[step: 282] loss: 0.02231650799512863\n",
      "[step: 283] loss: 165.56491088867188\n",
      "[step: 283] loss: 0.02264847792685032\n",
      "[step: 284] loss: 165.34475708007812\n",
      "[step: 284] loss: 0.022724628448486328\n",
      "[step: 285] loss: 165.3731689453125\n",
      "[step: 285] loss: 0.021817345172166824\n",
      "[step: 286] loss: 165.1662139892578\n",
      "[step: 286] loss: 0.02223370037972927\n",
      "[step: 287] loss: 164.80581665039062\n",
      "[step: 287] loss: 0.020609138533473015\n",
      "[step: 288] loss: 164.3441619873047\n",
      "[step: 288] loss: 0.02102987840771675\n",
      "[step: 289] loss: 164.13990783691406\n",
      "[step: 289] loss: 0.020465722307562828\n",
      "[step: 290] loss: 164.01422119140625\n",
      "[step: 290] loss: 0.021325135603547096\n",
      "[step: 291] loss: 163.89479064941406\n",
      "[step: 291] loss: 0.021084370091557503\n",
      "[step: 292] loss: 163.66387939453125\n",
      "[step: 292] loss: 0.02156680077314377\n",
      "[step: 293] loss: 163.31494140625\n",
      "[step: 293] loss: 0.02166595496237278\n",
      "[step: 294] loss: 163.014404296875\n",
      "[step: 294] loss: 0.021693529561161995\n",
      "[step: 295] loss: 162.7432403564453\n",
      "[step: 295] loss: 0.021733300760388374\n",
      "[step: 296] loss: 162.58338928222656\n",
      "[step: 296] loss: 0.021785663440823555\n",
      "[step: 297] loss: 162.4283905029297\n",
      "[step: 297] loss: 0.02181376703083515\n",
      "[step: 298] loss: 162.2735137939453\n",
      "[step: 298] loss: 0.021787654608488083\n",
      "[step: 299] loss: 162.09593200683594\n",
      "[step: 299] loss: 0.021715465933084488\n",
      "[step: 300] loss: 161.86898803710938\n",
      "[step: 300] loss: 0.021639961749315262\n",
      "[step: 301] loss: 161.64581298828125\n",
      "[step: 301] loss: 0.021605193614959717\n",
      "[step: 302] loss: 161.39886474609375\n",
      "[step: 302] loss: 0.02162233740091324\n",
      "[step: 303] loss: 161.17144775390625\n",
      "[step: 303] loss: 0.02166374772787094\n",
      "[step: 304] loss: 160.94822692871094\n",
      "[step: 304] loss: 0.02168804034590721\n",
      "[step: 305] loss: 160.7320556640625\n",
      "[step: 305] loss: 0.021672964096069336\n",
      "[step: 306] loss: 160.5380859375\n",
      "[step: 306] loss: 0.021629229187965393\n",
      "[step: 307] loss: 160.34494018554688\n",
      "[step: 307] loss: 0.021586714312434196\n",
      "[step: 308] loss: 160.18377685546875\n",
      "[step: 308] loss: 0.021568957716226578\n",
      "[step: 309] loss: 160.05361938476562\n",
      "[step: 309] loss: 0.021577056497335434\n",
      "[step: 310] loss: 159.99765014648438\n",
      "[step: 310] loss: 0.021594161167740822\n",
      "[step: 311] loss: 160.1030731201172\n",
      "[step: 311] loss: 0.021602142602205276\n",
      "[step: 312] loss: 160.57659912109375\n",
      "[step: 312] loss: 0.02159501425921917\n",
      "[step: 313] loss: 161.9244384765625\n",
      "[step: 313] loss: 0.021579615771770477\n",
      "[step: 314] loss: 165.49240112304688\n",
      "[step: 314] loss: 0.02156638912856579\n",
      "[step: 315] loss: 174.44932556152344\n",
      "[step: 315] loss: 0.0215605478733778\n",
      "[step: 316] loss: 198.13453674316406\n",
      "[step: 316] loss: 0.02156015858054161\n",
      "[step: 317] loss: 251.8720703125\n",
      "[step: 317] loss: 0.021560557186603546\n",
      "[step: 318] loss: 372.947265625\n",
      "[step: 318] loss: 0.021559158340096474\n",
      "[step: 319] loss: 486.64080810546875\n",
      "[step: 319] loss: 0.021556664258241653\n",
      "[step: 320] loss: 475.7966003417969\n",
      "[step: 320] loss: 0.02155463956296444\n",
      "[step: 321] loss: 215.19793701171875\n",
      "[step: 321] loss: 0.02155337668955326\n",
      "[step: 322] loss: 212.56021118164062\n",
      "[step: 322] loss: 0.021551750600337982\n",
      "[step: 323] loss: 383.6337585449219\n",
      "[step: 323] loss: 0.021548956632614136\n",
      "[step: 324] loss: 263.83551025390625\n",
      "[step: 324] loss: 0.02154567278921604\n",
      "[step: 325] loss: 191.85328674316406\n",
      "[step: 325] loss: 0.021543312817811966\n",
      "[step: 326] loss: 334.4155578613281\n",
      "[step: 326] loss: 0.021542681381106377\n",
      "[step: 327] loss: 194.64651489257812\n",
      "[step: 327] loss: 0.021543225273489952\n",
      "[step: 328] loss: 226.44369506835938\n",
      "[step: 328] loss: 0.021543456241488457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 329] loss: 237.35440063476562\n",
      "[step: 329] loss: 0.02154260128736496\n",
      "[step: 330] loss: 177.5238800048828\n",
      "[step: 330] loss: 0.02154085971415043\n",
      "[step: 331] loss: 243.19723510742188\n",
      "[step: 331] loss: 0.021539131179451942\n",
      "[step: 332] loss: 190.27392578125\n",
      "[step: 332] loss: 0.021538155153393745\n",
      "[step: 333] loss: 210.43580627441406\n",
      "[step: 333] loss: 0.021537845954298973\n",
      "[step: 334] loss: 200.07501220703125\n",
      "[step: 334] loss: 0.021537693217396736\n",
      "[step: 335] loss: 177.21853637695312\n",
      "[step: 335] loss: 0.021537363529205322\n",
      "[step: 336] loss: 205.46478271484375\n",
      "[step: 336] loss: 0.02153681218624115\n",
      "[step: 337] loss: 170.89732360839844\n",
      "[step: 337] loss: 0.021536286920309067\n",
      "[step: 338] loss: 202.2667236328125\n",
      "[step: 338] loss: 0.02153589390218258\n",
      "[step: 339] loss: 170.05059814453125\n",
      "[step: 339] loss: 0.021535595878958702\n",
      "[step: 340] loss: 185.92852783203125\n",
      "[step: 340] loss: 0.02153521217405796\n",
      "[step: 341] loss: 175.77728271484375\n",
      "[step: 341] loss: 0.02153473161160946\n",
      "[step: 342] loss: 173.5530242919922\n",
      "[step: 342] loss: 0.021534321829676628\n",
      "[step: 343] loss: 181.857177734375\n",
      "[step: 343] loss: 0.02153412625193596\n",
      "[step: 344] loss: 164.3452911376953\n",
      "[step: 344] loss: 0.0215340293943882\n",
      "[step: 345] loss: 181.42294311523438\n",
      "[step: 345] loss: 0.021533934399485588\n",
      "[step: 346] loss: 162.75741577148438\n",
      "[step: 346] loss: 0.021533731371164322\n",
      "[step: 347] loss: 175.1109161376953\n",
      "[step: 347] loss: 0.021533412858843803\n",
      "[step: 348] loss: 166.4512939453125\n",
      "[step: 348] loss: 0.02153310924768448\n",
      "[step: 349] loss: 166.8282470703125\n",
      "[step: 349] loss: 0.02153291180729866\n",
      "[step: 350] loss: 169.21923828125\n",
      "[step: 350] loss: 0.021532807499170303\n",
      "[step: 351] loss: 162.61587524414062\n",
      "[step: 351] loss: 0.021532749757170677\n",
      "[step: 352] loss: 168.447998046875\n",
      "[step: 352] loss: 0.021532636135816574\n",
      "[step: 353] loss: 163.74423217773438\n",
      "[step: 353] loss: 0.021532485261559486\n",
      "[step: 354] loss: 163.7476806640625\n",
      "[step: 354] loss: 0.021532297134399414\n",
      "[step: 355] loss: 166.0991973876953\n",
      "[step: 355] loss: 0.021532170474529266\n",
      "[step: 356] loss: 160.18765258789062\n",
      "[step: 356] loss: 0.021532058715820312\n",
      "[step: 357] loss: 165.07522583007812\n",
      "[step: 357] loss: 0.021531959995627403\n",
      "[step: 358] loss: 160.77975463867188\n",
      "[step: 358] loss: 0.021531859412789345\n",
      "[step: 359] loss: 161.86489868164062\n",
      "[step: 359] loss: 0.02153177745640278\n",
      "[step: 360] loss: 162.52896118164062\n",
      "[step: 360] loss: 0.02153169922530651\n",
      "[step: 361] loss: 159.67788696289062\n",
      "[step: 361] loss: 0.0215316042304039\n",
      "[step: 362] loss: 161.74459838867188\n",
      "[step: 362] loss: 0.021531520411372185\n",
      "[step: 363] loss: 160.31060791015625\n",
      "[step: 363] loss: 0.021531399339437485\n",
      "[step: 364] loss: 159.26773071289062\n",
      "[step: 364] loss: 0.021531295031309128\n",
      "[step: 365] loss: 160.9676513671875\n",
      "[step: 365] loss: 0.021531222388148308\n",
      "[step: 366] loss: 158.53269958496094\n",
      "[step: 366] loss: 0.02153119444847107\n",
      "[step: 367] loss: 159.4967041015625\n",
      "[step: 367] loss: 0.021531151607632637\n",
      "[step: 368] loss: 159.3424835205078\n",
      "[step: 368] loss: 0.021531090140342712\n",
      "[step: 369] loss: 157.984375\n",
      "[step: 369] loss: 0.021531010046601295\n",
      "[step: 370] loss: 158.88658142089844\n",
      "[step: 370] loss: 0.021530896425247192\n",
      "[step: 371] loss: 158.1884765625\n",
      "[step: 371] loss: 0.02153083123266697\n",
      "[step: 372] loss: 157.50668334960938\n",
      "[step: 372] loss: 0.021530799567699432\n",
      "[step: 373] loss: 158.25344848632812\n",
      "[step: 373] loss: 0.021530721336603165\n",
      "[step: 374] loss: 157.25271606445312\n",
      "[step: 374] loss: 0.02153070829808712\n",
      "[step: 375] loss: 157.21630859375\n",
      "[step: 375] loss: 0.02153063379228115\n",
      "[step: 376] loss: 157.45254516601562\n",
      "[step: 376] loss: 0.02153056301176548\n",
      "[step: 377] loss: 156.7117919921875\n",
      "[step: 377] loss: 0.021530481055378914\n",
      "[step: 378] loss: 156.67579650878906\n",
      "[step: 378] loss: 0.021530425176024437\n",
      "[step: 379] loss: 156.8468017578125\n",
      "[step: 379] loss: 0.021530384197831154\n",
      "[step: 380] loss: 156.1239776611328\n",
      "[step: 380] loss: 0.02153034508228302\n",
      "[step: 381] loss: 156.22825622558594\n",
      "[step: 381] loss: 0.021530279889702797\n",
      "[step: 382] loss: 156.1844482421875\n",
      "[step: 382] loss: 0.02153024636209011\n",
      "[step: 383] loss: 155.7184600830078\n",
      "[step: 383] loss: 0.02153020352125168\n",
      "[step: 384] loss: 155.65521240234375\n",
      "[step: 384] loss: 0.0215301476418972\n",
      "[step: 385] loss: 155.70721435546875\n",
      "[step: 385] loss: 0.021530084311962128\n",
      "[step: 386] loss: 155.24081420898438\n",
      "[step: 386] loss: 0.021530019119381905\n",
      "[step: 387] loss: 155.20150756835938\n",
      "[step: 387] loss: 0.02153000235557556\n",
      "[step: 388] loss: 155.17263793945312\n",
      "[step: 388] loss: 0.021529946476221085\n",
      "[step: 389] loss: 154.88595581054688\n",
      "[step: 389] loss: 0.0215299129486084\n",
      "[step: 390] loss: 154.68394470214844\n",
      "[step: 390] loss: 0.02152985706925392\n",
      "[step: 391] loss: 154.72598266601562\n",
      "[step: 391] loss: 0.021529817953705788\n",
      "[step: 392] loss: 154.4775390625\n",
      "[step: 392] loss: 0.021529750898480415\n",
      "[step: 393] loss: 154.26846313476562\n",
      "[step: 393] loss: 0.021529724821448326\n",
      "[step: 394] loss: 154.21963500976562\n",
      "[step: 394] loss: 0.02152969129383564\n",
      "[step: 395] loss: 154.11129760742188\n",
      "[step: 395] loss: 0.021529627963900566\n",
      "[step: 396] loss: 153.86630249023438\n",
      "[step: 396] loss: 0.021529601886868477\n",
      "[step: 397] loss: 153.759033203125\n",
      "[step: 397] loss: 0.021529564633965492\n",
      "[step: 398] loss: 153.69345092773438\n",
      "[step: 398] loss: 0.021529508754611015\n",
      "[step: 399] loss: 153.51931762695312\n",
      "[step: 399] loss: 0.02152947150170803\n",
      "[step: 400] loss: 153.34249877929688\n",
      "[step: 400] loss: 0.021529443562030792\n",
      "[step: 401] loss: 153.25186157226562\n",
      "[step: 401] loss: 0.021529389545321465\n",
      "[step: 402] loss: 153.1608123779297\n",
      "[step: 402] loss: 0.021529339253902435\n",
      "[step: 403] loss: 152.98536682128906\n",
      "[step: 403] loss: 0.021529318764805794\n",
      "[step: 404] loss: 152.84466552734375\n",
      "[step: 404] loss: 0.021529261022806168\n",
      "[step: 405] loss: 152.74749755859375\n",
      "[step: 405] loss: 0.021529225632548332\n",
      "[step: 406] loss: 152.648193359375\n",
      "[step: 406] loss: 0.02152918465435505\n",
      "[step: 407] loss: 152.4925537109375\n",
      "[step: 407] loss: 0.021529143676161766\n",
      "[step: 408] loss: 152.35914611816406\n",
      "[step: 408] loss: 0.02152913436293602\n",
      "[step: 409] loss: 152.255126953125\n",
      "[step: 409] loss: 0.02152908407151699\n",
      "[step: 410] loss: 152.15658569335938\n",
      "[step: 410] loss: 0.02152904123067856\n",
      "[step: 411] loss: 152.02554321289062\n",
      "[step: 411] loss: 0.021529007703065872\n",
      "[step: 412] loss: 151.89093017578125\n",
      "[step: 412] loss: 0.02152896486222744\n",
      "[step: 413] loss: 151.77923583984375\n",
      "[step: 413] loss: 0.021528933197259903\n",
      "[step: 414] loss: 151.67935180664062\n",
      "[step: 414] loss: 0.021528903394937515\n",
      "[step: 415] loss: 151.57176208496094\n",
      "[step: 415] loss: 0.021528862416744232\n",
      "[step: 416] loss: 151.44598388671875\n",
      "[step: 416] loss: 0.021528827026486397\n",
      "[step: 417] loss: 151.32652282714844\n",
      "[step: 417] loss: 0.021528784185647964\n",
      "[step: 418] loss: 151.21658325195312\n",
      "[step: 418] loss: 0.021528739482164383\n",
      "[step: 419] loss: 151.11795043945312\n",
      "[step: 419] loss: 0.021528733894228935\n",
      "[step: 420] loss: 151.01217651367188\n",
      "[step: 420] loss: 0.021528687328100204\n",
      "[step: 421] loss: 150.90036010742188\n",
      "[step: 421] loss: 0.02152865380048752\n",
      "[step: 422] loss: 150.78533935546875\n",
      "[step: 422] loss: 0.021528618410229683\n",
      "[step: 423] loss: 150.67564392089844\n",
      "[step: 423] loss: 0.021528583019971848\n",
      "[step: 424] loss: 150.57325744628906\n",
      "[step: 424] loss: 0.02152854949235916\n",
      "[step: 425] loss: 150.4727020263672\n",
      "[step: 425] loss: 0.021528523415327072\n",
      "[step: 426] loss: 150.37286376953125\n",
      "[step: 426] loss: 0.021528489887714386\n",
      "[step: 427] loss: 150.26824951171875\n",
      "[step: 427] loss: 0.021528467535972595\n",
      "[step: 428] loss: 150.16323852539062\n",
      "[step: 428] loss: 0.021528426557779312\n",
      "[step: 429] loss: 150.05740356445312\n",
      "[step: 429] loss: 0.02152838557958603\n",
      "[step: 430] loss: 149.9542999267578\n",
      "[step: 430] loss: 0.02152835763990879\n",
      "[step: 431] loss: 149.85333251953125\n",
      "[step: 431] loss: 0.021528327837586403\n",
      "[step: 432] loss: 149.75482177734375\n",
      "[step: 432] loss: 0.021528298035264015\n",
      "[step: 433] loss: 149.65826416015625\n",
      "[step: 433] loss: 0.021528257057070732\n",
      "[step: 434] loss: 149.56192016601562\n",
      "[step: 434] loss: 0.021528232842683792\n",
      "[step: 435] loss: 149.46690368652344\n",
      "[step: 435] loss: 0.021528229117393494\n",
      "[step: 436] loss: 149.37210083007812\n",
      "[step: 436] loss: 0.021528184413909912\n",
      "[step: 437] loss: 149.278564453125\n",
      "[step: 437] loss: 0.02152814343571663\n",
      "[step: 438] loss: 149.18650817871094\n",
      "[step: 438] loss: 0.02152811549603939\n",
      "[step: 439] loss: 149.0968017578125\n",
      "[step: 439] loss: 0.021528081968426704\n",
      "[step: 440] loss: 149.0115966796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 440] loss: 0.021528054028749466\n",
      "[step: 441] loss: 148.93359375\n",
      "[step: 441] loss: 0.02152802236378193\n",
      "[step: 442] loss: 148.87013244628906\n",
      "[step: 442] loss: 0.021528001874685287\n",
      "[step: 443] loss: 148.83462524414062\n",
      "[step: 443] loss: 0.021527981385588646\n",
      "[step: 444] loss: 148.8587646484375\n",
      "[step: 444] loss: 0.021527955308556557\n",
      "[step: 445] loss: 149.0079345703125\n",
      "[step: 445] loss: 0.021527914330363274\n",
      "[step: 446] loss: 149.45394897460938\n",
      "[step: 446] loss: 0.021527905017137527\n",
      "[step: 447] loss: 150.55596923828125\n",
      "[step: 447] loss: 0.021527869626879692\n",
      "[step: 448] loss: 153.36813354492188\n",
      "[step: 448] loss: 0.021527830511331558\n",
      "[step: 449] loss: 159.97315979003906\n",
      "[step: 449] loss: 0.021527796983718872\n",
      "[step: 450] loss: 177.53712463378906\n",
      "[step: 450] loss: 0.02152777463197708\n",
      "[step: 451] loss: 215.48193359375\n",
      "[step: 451] loss: 0.0215277299284935\n",
      "[step: 452] loss: 313.54803466796875\n",
      "[step: 452] loss: 0.021527716889977455\n",
      "[step: 453] loss: 423.2911376953125\n",
      "[step: 453] loss: 0.021527685225009918\n",
      "[step: 454] loss: 547.1032104492188\n",
      "[step: 454] loss: 0.021527668461203575\n",
      "[step: 455] loss: 314.1874084472656\n",
      "[step: 455] loss: 0.021527636796236038\n",
      "[step: 456] loss: 156.41012573242188\n",
      "[step: 456] loss: 0.021527612581849098\n",
      "[step: 457] loss: 298.59368896484375\n",
      "[step: 457] loss: 0.021527588367462158\n",
      "[step: 458] loss: 321.6454772949219\n",
      "[step: 458] loss: 0.021527554839849472\n",
      "[step: 459] loss: 180.6600341796875\n",
      "[step: 459] loss: 0.021527530625462532\n",
      "[step: 460] loss: 242.49269104003906\n",
      "[step: 460] loss: 0.021527506411075592\n",
      "[step: 461] loss: 279.5338134765625\n",
      "[step: 461] loss: 0.021527476608753204\n",
      "[step: 462] loss: 171.01193237304688\n",
      "[step: 462] loss: 0.021527446806430817\n",
      "[step: 463] loss: 230.607421875\n",
      "[step: 463] loss: 0.021527420729398727\n",
      "[step: 464] loss: 187.54217529296875\n",
      "[step: 464] loss: 0.02152739278972149\n",
      "[step: 465] loss: 185.3738555908203\n",
      "[step: 465] loss: 0.021527359262108803\n",
      "[step: 466] loss: 207.81463623046875\n",
      "[step: 466] loss: 0.02152733877301216\n",
      "[step: 467] loss: 175.15394592285156\n",
      "[step: 467] loss: 0.02152731828391552\n",
      "[step: 468] loss: 194.48948669433594\n",
      "[step: 468] loss: 0.021527286618947983\n",
      "[step: 469] loss: 177.53750610351562\n",
      "[step: 469] loss: 0.02152726799249649\n",
      "[step: 470] loss: 174.34593200683594\n",
      "[step: 470] loss: 0.021527238190174103\n",
      "[step: 471] loss: 184.1138916015625\n",
      "[step: 471] loss: 0.021527206525206566\n",
      "[step: 472] loss: 161.38568115234375\n",
      "[step: 472] loss: 0.021527182310819626\n",
      "[step: 473] loss: 184.60504150390625\n",
      "[step: 473] loss: 0.021527161821722984\n",
      "[step: 474] loss: 156.65499877929688\n",
      "[step: 474] loss: 0.021527137607336044\n",
      "[step: 475] loss: 175.8046875\n",
      "[step: 475] loss: 0.021527107805013657\n",
      "[step: 476] loss: 159.46878051757812\n",
      "[step: 476] loss: 0.021527083590626717\n",
      "[step: 477] loss: 166.08572387695312\n",
      "[step: 477] loss: 0.02152705378830433\n",
      "[step: 478] loss: 163.758056640625\n",
      "[step: 478] loss: 0.02152702771127224\n",
      "[step: 479] loss: 161.58151245117188\n",
      "[step: 479] loss: 0.021526999771595\n",
      "[step: 480] loss: 160.50314331054688\n",
      "[step: 480] loss: 0.02152697555720806\n",
      "[step: 481] loss: 162.95965576171875\n",
      "[step: 481] loss: 0.021526949480175972\n",
      "[step: 482] loss: 154.31398010253906\n",
      "[step: 482] loss: 0.02152692712843418\n",
      "[step: 483] loss: 162.87176513671875\n",
      "[step: 483] loss: 0.02152690477669239\n",
      "[step: 484] loss: 153.50054931640625\n",
      "[step: 484] loss: 0.021526876837015152\n",
      "[step: 485] loss: 159.36306762695312\n",
      "[step: 485] loss: 0.021526852622628212\n",
      "[step: 486] loss: 156.4582061767578\n",
      "[step: 486] loss: 0.021526826545596123\n",
      "[step: 487] loss: 155.07742309570312\n",
      "[step: 487] loss: 0.021526798605918884\n",
      "[step: 488] loss: 155.78273010253906\n",
      "[step: 488] loss: 0.021526774391531944\n",
      "[step: 489] loss: 155.79800415039062\n",
      "[step: 489] loss: 0.021526746451854706\n",
      "[step: 490] loss: 152.11605834960938\n",
      "[step: 490] loss: 0.021526718512177467\n",
      "[step: 491] loss: 156.25262451171875\n",
      "[step: 491] loss: 0.021526696160435677\n",
      "[step: 492] loss: 152.03927612304688\n",
      "[step: 492] loss: 0.02152666635811329\n",
      "[step: 493] loss: 153.44761657714844\n",
      "[step: 493] loss: 0.02152663841843605\n",
      "[step: 494] loss: 153.45834350585938\n",
      "[step: 494] loss: 0.021526610478758812\n",
      "[step: 495] loss: 152.31427001953125\n",
      "[step: 495] loss: 0.021526582539081573\n",
      "[step: 496] loss: 151.81289672851562\n",
      "[step: 496] loss: 0.021526554599404335\n",
      "[step: 497] loss: 153.2584686279297\n",
      "[step: 497] loss: 0.021526526659727097\n",
      "[step: 498] loss: 150.6583251953125\n",
      "[step: 498] loss: 0.021526500582695007\n",
      "[step: 499] loss: 151.84339904785156\n",
      "[step: 499] loss: 0.02152647078037262\n",
      "[step: 500] loss: 151.3802490234375\n",
      "[step: 500] loss: 0.021526440978050232\n",
      "[step: 501] loss: 150.735595703125\n",
      "[step: 501] loss: 0.021526413038372993\n",
      "[step: 502] loss: 150.51412963867188\n",
      "[step: 502] loss: 0.021526377648115158\n",
      "[step: 503] loss: 151.13742065429688\n",
      "[step: 503] loss: 0.02152635157108307\n",
      "[step: 504] loss: 149.744384765625\n",
      "[step: 504] loss: 0.02152632363140583\n",
      "[step: 505] loss: 150.28143310546875\n",
      "[step: 505] loss: 0.021526288241147995\n",
      "[step: 506] loss: 150.09130859375\n",
      "[step: 506] loss: 0.02152625471353531\n",
      "[step: 507] loss: 149.74539184570312\n",
      "[step: 507] loss: 0.021526223048567772\n",
      "[step: 508] loss: 149.27011108398438\n",
      "[step: 508] loss: 0.02152618207037449\n",
      "[step: 509] loss: 149.84756469726562\n",
      "[step: 509] loss: 0.021526142954826355\n",
      "[step: 510] loss: 149.1422119140625\n",
      "[step: 510] loss: 0.021526101976633072\n",
      "[step: 511] loss: 149.01771545410156\n",
      "[step: 511] loss: 0.02152605727314949\n",
      "[step: 512] loss: 148.96170043945312\n",
      "[step: 512] loss: 0.021526014432311058\n",
      "[step: 513] loss: 149.05380249023438\n",
      "[step: 513] loss: 0.021525967866182327\n",
      "[step: 514] loss: 148.4327392578125\n",
      "[step: 514] loss: 0.021525913849473\n",
      "[step: 515] loss: 148.53765869140625\n",
      "[step: 515] loss: 0.02152586169540882\n",
      "[step: 516] loss: 148.458984375\n",
      "[step: 516] loss: 0.021525802090764046\n",
      "[step: 517] loss: 148.30801391601562\n",
      "[step: 517] loss: 0.02152574434876442\n",
      "[step: 518] loss: 147.93820190429688\n",
      "[step: 518] loss: 0.021525681018829346\n",
      "[step: 519] loss: 148.02467346191406\n",
      "[step: 519] loss: 0.021525608375668526\n",
      "[step: 520] loss: 147.90084838867188\n",
      "[step: 520] loss: 0.021525535732507706\n",
      "[step: 521] loss: 147.7081298828125\n",
      "[step: 521] loss: 0.02152545563876629\n",
      "[step: 522] loss: 147.49319458007812\n",
      "[step: 522] loss: 0.021525364369153976\n",
      "[step: 523] loss: 147.49356079101562\n",
      "[step: 523] loss: 0.021525269374251366\n",
      "[step: 524] loss: 147.40225219726562\n",
      "[step: 524] loss: 0.02152515947818756\n",
      "[step: 525] loss: 147.20545959472656\n",
      "[step: 525] loss: 0.021525045856833458\n",
      "[step: 526] loss: 147.05426025390625\n",
      "[step: 526] loss: 0.02152492105960846\n",
      "[step: 527] loss: 146.98464965820312\n",
      "[step: 527] loss: 0.021524786949157715\n",
      "[step: 528] loss: 146.9288787841797\n",
      "[step: 528] loss: 0.021524643525481224\n",
      "[step: 529] loss: 146.75418090820312\n",
      "[step: 529] loss: 0.021524492651224136\n",
      "[step: 530] loss: 146.62274169921875\n",
      "[step: 530] loss: 0.021524326875805855\n",
      "[step: 531] loss: 146.50198364257812\n",
      "[step: 531] loss: 0.021524162963032722\n",
      "[step: 532] loss: 146.46011352539062\n",
      "[step: 532] loss: 0.02152399532496929\n",
      "[step: 533] loss: 146.3296356201172\n",
      "[step: 533] loss: 0.02152380160987377\n",
      "[step: 534] loss: 146.21145629882812\n",
      "[step: 534] loss: 0.021523606032133102\n",
      "[step: 535] loss: 146.0640869140625\n",
      "[step: 535] loss: 0.02152339555323124\n",
      "[step: 536] loss: 145.99075317382812\n",
      "[step: 536] loss: 0.02152317762374878\n",
      "[step: 537] loss: 145.8979034423828\n",
      "[step: 537] loss: 0.02152293547987938\n",
      "[step: 538] loss: 145.80819702148438\n",
      "[step: 538] loss: 0.021522680297493935\n",
      "[step: 539] loss: 145.68075561523438\n",
      "[step: 539] loss: 0.02152240462601185\n",
      "[step: 540] loss: 145.5639190673828\n",
      "[step: 540] loss: 0.02152208238840103\n",
      "[step: 541] loss: 145.46212768554688\n",
      "[step: 541] loss: 0.021521737799048424\n",
      "[step: 542] loss: 145.37200927734375\n",
      "[step: 542] loss: 0.02152133360505104\n",
      "[step: 543] loss: 145.28932189941406\n",
      "[step: 543] loss: 0.02152087166905403\n",
      "[step: 544] loss: 145.18679809570312\n",
      "[step: 544] loss: 0.0215203445404768\n",
      "[step: 545] loss: 145.08688354492188\n",
      "[step: 545] loss: 0.021519698202610016\n",
      "[step: 546] loss: 144.97152709960938\n",
      "[step: 546] loss: 0.02151893451809883\n",
      "[step: 547] loss: 144.87356567382812\n",
      "[step: 547] loss: 0.02151801437139511\n",
      "[step: 548] loss: 144.77206420898438\n",
      "[step: 548] loss: 0.021516840904951096\n",
      "[step: 549] loss: 144.68515014648438\n",
      "[step: 549] loss: 0.021515363827347755\n",
      "[step: 550] loss: 144.5937957763672\n",
      "[step: 550] loss: 0.02151341550052166\n",
      "[step: 551] loss: 144.50588989257812\n",
      "[step: 551] loss: 0.021510792896151543\n",
      "[step: 552] loss: 144.41444396972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 552] loss: 0.021507082507014275\n",
      "[step: 553] loss: 144.3201904296875\n",
      "[step: 553] loss: 0.02150161750614643\n",
      "[step: 554] loss: 144.22723388671875\n",
      "[step: 554] loss: 0.021493123844265938\n",
      "[step: 555] loss: 144.13037109375\n",
      "[step: 555] loss: 0.021478882059454918\n",
      "[step: 556] loss: 144.03872680664062\n",
      "[step: 556] loss: 0.021453604102134705\n",
      "[step: 557] loss: 143.94366455078125\n",
      "[step: 557] loss: 0.02141435444355011\n",
      "[step: 558] loss: 143.854248046875\n",
      "[step: 558] loss: 0.02141708880662918\n",
      "[step: 559] loss: 143.76412963867188\n",
      "[step: 559] loss: 0.02125970646739006\n",
      "[step: 560] loss: 143.6798095703125\n",
      "[step: 560] loss: 0.02101156674325466\n",
      "[step: 561] loss: 143.60035705566406\n",
      "[step: 561] loss: 0.0202155914157629\n",
      "[step: 562] loss: 143.53158569335938\n",
      "[step: 562] loss: 0.02010667696595192\n",
      "[step: 563] loss: 143.4857177734375\n",
      "[step: 563] loss: 0.020057789981365204\n",
      "[step: 564] loss: 143.4813232421875\n",
      "[step: 564] loss: 0.022104980424046516\n",
      "[step: 565] loss: 143.57595825195312\n",
      "[step: 565] loss: 0.02259938232600689\n",
      "[step: 566] loss: 143.87728881835938\n",
      "[step: 566] loss: 0.02224542945623398\n",
      "[step: 567] loss: 144.69374084472656\n",
      "[step: 567] loss: 0.021734701469540596\n",
      "[step: 568] loss: 146.6335906982422\n",
      "[step: 568] loss: 0.021599221974611282\n",
      "[step: 569] loss: 151.68101501464844\n",
      "[step: 569] loss: 0.02183065004646778\n",
      "[step: 570] loss: 163.2599639892578\n",
      "[step: 570] loss: 0.022044247016310692\n",
      "[step: 571] loss: 194.87509155273438\n",
      "[step: 571] loss: 0.021993674337863922\n",
      "[step: 572] loss: 255.18930053710938\n",
      "[step: 572] loss: 0.021786099299788475\n",
      "[step: 573] loss: 406.4979248046875\n",
      "[step: 573] loss: 0.02165483683347702\n",
      "[step: 574] loss: 469.9574890136719\n",
      "[step: 574] loss: 0.021674051880836487\n",
      "[step: 575] loss: 456.0402526855469\n",
      "[step: 575] loss: 0.02174135111272335\n",
      "[step: 576] loss: 193.02101135253906\n",
      "[step: 576] loss: 0.021752115339040756\n",
      "[step: 577] loss: 213.23533630371094\n",
      "[step: 577] loss: 0.02170625887811184\n",
      "[step: 578] loss: 377.06488037109375\n",
      "[step: 578] loss: 0.02165951393544674\n",
      "[step: 579] loss: 243.932373046875\n",
      "[step: 579] loss: 0.02163892798125744\n",
      "[step: 580] loss: 195.88815307617188\n",
      "[step: 580] loss: 0.02163199707865715\n",
      "[step: 581] loss: 339.19635009765625\n",
      "[step: 581] loss: 0.02162582240998745\n",
      "[step: 582] loss: 175.1251220703125\n",
      "[step: 582] loss: 0.02162196673452854\n",
      "[step: 583] loss: 239.63873291015625\n",
      "[step: 583] loss: 0.021620027720928192\n",
      "[step: 584] loss: 222.01177978515625\n",
      "[step: 584] loss: 0.021610675379633904\n",
      "[step: 585] loss: 175.40048217773438\n",
      "[step: 585] loss: 0.02159152179956436\n",
      "[step: 586] loss: 238.11904907226562\n",
      "[step: 586] loss: 0.02157524973154068\n",
      "[step: 587] loss: 170.23895263671875\n",
      "[step: 587] loss: 0.021573506295681\n",
      "[step: 588] loss: 210.9382781982422\n",
      "[step: 588] loss: 0.021580560132861137\n",
      "[step: 589] loss: 178.2117156982422\n",
      "[step: 589] loss: 0.021580630913376808\n",
      "[step: 590] loss: 175.907958984375\n",
      "[step: 590] loss: 0.021568777039647102\n",
      "[step: 591] loss: 188.88577270507812\n",
      "[step: 591] loss: 0.02155562862753868\n",
      "[step: 592] loss: 161.68707275390625\n",
      "[step: 592] loss: 0.02155192196369171\n",
      "[step: 593] loss: 194.25108337402344\n",
      "[step: 593] loss: 0.021555466577410698\n",
      "[step: 594] loss: 152.56565856933594\n",
      "[step: 594] loss: 0.021556545048952103\n",
      "[step: 595] loss: 182.4225616455078\n",
      "[step: 595] loss: 0.02155127003788948\n",
      "[step: 596] loss: 155.1885986328125\n",
      "[step: 596] loss: 0.02154487557709217\n",
      "[step: 597] loss: 171.70858764648438\n",
      "[step: 597] loss: 0.021542590111494064\n",
      "[step: 598] loss: 164.5677490234375\n",
      "[step: 598] loss: 0.021543199196457863\n",
      "[step: 599] loss: 158.1387939453125\n",
      "[step: 599] loss: 0.021542323753237724\n",
      "[step: 600] loss: 169.21372985839844\n",
      "[step: 600] loss: 0.02153916470706463\n",
      "[step: 601] loss: 150.6321563720703\n",
      "[step: 601] loss: 0.021536603569984436\n",
      "[step: 602] loss: 165.660888671875\n",
      "[step: 602] loss: 0.021536320447921753\n",
      "[step: 603] loss: 152.27145385742188\n",
      "[step: 603] loss: 0.02153661474585533\n",
      "[step: 604] loss: 158.24551391601562\n",
      "[step: 604] loss: 0.021535256877541542\n",
      "[step: 605] loss: 156.86724853515625\n",
      "[step: 605] loss: 0.021532824262976646\n",
      "[step: 606] loss: 151.41238403320312\n",
      "[step: 606] loss: 0.021531451493501663\n",
      "[step: 607] loss: 158.84117126464844\n",
      "[step: 607] loss: 0.02153167687356472\n",
      "[step: 608] loss: 149.19876098632812\n",
      "[step: 608] loss: 0.02153199352324009\n",
      "[step: 609] loss: 155.1455078125\n",
      "[step: 609] loss: 0.02153106778860092\n",
      "[step: 610] loss: 151.5882568359375\n",
      "[step: 610] loss: 0.021529529243707657\n",
      "[step: 611] loss: 149.7408447265625\n",
      "[step: 611] loss: 0.021528765559196472\n",
      "[step: 612] loss: 153.401611328125\n",
      "[step: 612] loss: 0.021528972312808037\n",
      "[step: 613] loss: 147.73179626464844\n",
      "[step: 613] loss: 0.021529117599129677\n",
      "[step: 614] loss: 151.8521270751953\n",
      "[step: 614] loss: 0.021528448909521103\n",
      "[step: 615] loss: 149.30621337890625\n",
      "[step: 615] loss: 0.021527480334043503\n",
      "[step: 616] loss: 148.08871459960938\n",
      "[step: 616] loss: 0.021527061238884926\n",
      "[step: 617] loss: 150.55764770507812\n",
      "[step: 617] loss: 0.021527288481593132\n",
      "[step: 618] loss: 146.9216766357422\n",
      "[step: 618] loss: 0.021527426317334175\n",
      "[step: 619] loss: 148.36148071289062\n",
      "[step: 619] loss: 0.021527037024497986\n",
      "[step: 620] loss: 148.3904266357422\n",
      "[step: 620] loss: 0.021526379510760307\n",
      "[step: 621] loss: 146.02008056640625\n",
      "[step: 621] loss: 0.021526003256440163\n",
      "[step: 622] loss: 147.97061157226562\n",
      "[step: 622] loss: 0.021526096388697624\n",
      "[step: 623] loss: 146.53050231933594\n",
      "[step: 623] loss: 0.021526236087083817\n",
      "[step: 624] loss: 145.76795959472656\n",
      "[step: 624] loss: 0.021526111289858818\n",
      "[step: 625] loss: 146.8914794921875\n",
      "[step: 625] loss: 0.021525822579860687\n",
      "[step: 626] loss: 145.48336791992188\n",
      "[step: 626] loss: 0.021525533869862556\n",
      "[step: 627] loss: 145.30838012695312\n",
      "[step: 627] loss: 0.021525446325540543\n",
      "[step: 628] loss: 145.97274780273438\n",
      "[step: 628] loss: 0.021525472402572632\n",
      "[step: 629] loss: 144.70208740234375\n",
      "[step: 629] loss: 0.021525459364056587\n",
      "[step: 630] loss: 144.81236267089844\n",
      "[step: 630] loss: 0.021525369957089424\n",
      "[step: 631] loss: 145.11793518066406\n",
      "[step: 631] loss: 0.02152526192367077\n",
      "[step: 632] loss: 144.17269897460938\n",
      "[step: 632] loss: 0.021525129675865173\n",
      "[step: 633] loss: 144.1572265625\n",
      "[step: 633] loss: 0.021525053307414055\n",
      "[step: 634] loss: 144.4403076171875\n",
      "[step: 634] loss: 0.02152501605451107\n",
      "[step: 635] loss: 143.67288208007812\n",
      "[step: 635] loss: 0.021525003015995026\n",
      "[step: 636] loss: 143.53482055664062\n",
      "[step: 636] loss: 0.021525004878640175\n",
      "[step: 637] loss: 143.7649688720703\n",
      "[step: 637] loss: 0.02152492105960846\n",
      "[step: 638] loss: 143.27117919921875\n",
      "[step: 638] loss: 0.021524835377931595\n",
      "[step: 639] loss: 142.92527770996094\n",
      "[step: 639] loss: 0.021524783223867416\n",
      "[step: 640] loss: 143.10308837890625\n",
      "[step: 640] loss: 0.02152477204799652\n",
      "[step: 641] loss: 142.86178588867188\n",
      "[step: 641] loss: 0.021524790674448013\n",
      "[step: 642] loss: 142.43832397460938\n",
      "[step: 642] loss: 0.02152477577328682\n",
      "[step: 643] loss: 142.43043518066406\n",
      "[step: 643] loss: 0.021524691954255104\n",
      "[step: 644] loss: 142.41485595703125\n",
      "[step: 644] loss: 0.02152463048696518\n",
      "[step: 645] loss: 142.07119750976562\n",
      "[step: 645] loss: 0.021524619311094284\n",
      "[step: 646] loss: 141.846923828125\n",
      "[step: 646] loss: 0.021524647250771523\n",
      "[step: 647] loss: 141.86666870117188\n",
      "[step: 647] loss: 0.02152463048696518\n",
      "[step: 648] loss: 141.72470092773438\n",
      "[step: 648] loss: 0.0215245820581913\n",
      "[step: 649] loss: 141.4280548095703\n",
      "[step: 649] loss: 0.021524516865611076\n",
      "[step: 650] loss: 141.28948974609375\n",
      "[step: 650] loss: 0.02152453176677227\n",
      "[step: 651] loss: 141.26565551757812\n",
      "[step: 651] loss: 0.02152452990412712\n",
      "[step: 652] loss: 141.10084533691406\n",
      "[step: 652] loss: 0.021524516865611076\n",
      "[step: 653] loss: 140.86672973632812\n",
      "[step: 653] loss: 0.02152448147535324\n",
      "[step: 654] loss: 140.72772216796875\n",
      "[step: 654] loss: 0.0215244609862566\n",
      "[step: 655] loss: 140.6669921875\n",
      "[step: 655] loss: 0.02152443490922451\n",
      "[step: 656] loss: 140.53750610351562\n",
      "[step: 656] loss: 0.021524444222450256\n",
      "[step: 657] loss: 140.34365844726562\n",
      "[step: 657] loss: 0.021524427458643913\n",
      "[step: 658] loss: 140.17977905273438\n",
      "[step: 658] loss: 0.02152439020574093\n",
      "[step: 659] loss: 140.09002685546875\n",
      "[step: 659] loss: 0.021524367853999138\n",
      "[step: 660] loss: 139.99539184570312\n",
      "[step: 660] loss: 0.02152436599135399\n",
      "[step: 661] loss: 139.8490753173828\n",
      "[step: 661] loss: 0.021524367853999138\n",
      "[step: 662] loss: 139.68199157714844\n",
      "[step: 662] loss: 0.021524345502257347\n",
      "[step: 663] loss: 139.54861450195312\n",
      "[step: 663] loss: 0.02152434177696705\n",
      "[step: 664] loss: 139.44857788085938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 664] loss: 0.02152431569993496\n",
      "[step: 665] loss: 139.34759521484375\n",
      "[step: 665] loss: 0.02152431569993496\n",
      "[step: 666] loss: 139.22146606445312\n",
      "[step: 666] loss: 0.021524297073483467\n",
      "[step: 667] loss: 139.07571411132812\n",
      "[step: 667] loss: 0.02152429148554802\n",
      "[step: 668] loss: 138.93955993652344\n",
      "[step: 668] loss: 0.02152426354587078\n",
      "[step: 669] loss: 138.82342529296875\n",
      "[step: 669] loss: 0.02152426168322563\n",
      "[step: 670] loss: 138.7205352783203\n",
      "[step: 670] loss: 0.02152423933148384\n",
      "[step: 671] loss: 138.61529541015625\n",
      "[step: 671] loss: 0.02152424305677414\n",
      "[step: 672] loss: 138.50039672851562\n",
      "[step: 672] loss: 0.02152424119412899\n",
      "[step: 673] loss: 138.37472534179688\n",
      "[step: 673] loss: 0.021524211391806602\n",
      "[step: 674] loss: 138.2481689453125\n",
      "[step: 674] loss: 0.021524203941226006\n",
      "[step: 675] loss: 138.12649536132812\n",
      "[step: 675] loss: 0.02152419090270996\n",
      "[step: 676] loss: 138.011962890625\n",
      "[step: 676] loss: 0.021524176001548767\n",
      "[step: 677] loss: 137.90426635742188\n",
      "[step: 677] loss: 0.021524161100387573\n",
      "[step: 678] loss: 137.79974365234375\n",
      "[step: 678] loss: 0.02152414433658123\n",
      "[step: 679] loss: 137.69610595703125\n",
      "[step: 679] loss: 0.02152414433658123\n",
      "[step: 680] loss: 137.5919189453125\n",
      "[step: 680] loss: 0.02152414061129093\n",
      "[step: 681] loss: 137.4873046875\n",
      "[step: 681] loss: 0.02152412012219429\n",
      "[step: 682] loss: 137.38201904296875\n",
      "[step: 682] loss: 0.02152409963309765\n",
      "[step: 683] loss: 137.2780303955078\n",
      "[step: 683] loss: 0.02152409218251705\n",
      "[step: 684] loss: 137.17645263671875\n",
      "[step: 684] loss: 0.021524079144001007\n",
      "[step: 685] loss: 137.07858276367188\n",
      "[step: 685] loss: 0.021524077281355858\n",
      "[step: 686] loss: 136.98861694335938\n",
      "[step: 686] loss: 0.02152407169342041\n",
      "[step: 687] loss: 136.91073608398438\n",
      "[step: 687] loss: 0.02152407355606556\n",
      "[step: 688] loss: 136.85699462890625\n",
      "[step: 688] loss: 0.021524058654904366\n",
      "[step: 689] loss: 136.84620666503906\n",
      "[step: 689] loss: 0.021524015814065933\n",
      "[step: 690] loss: 136.92825317382812\n",
      "[step: 690] loss: 0.021524017676711082\n",
      "[step: 691] loss: 137.18853759765625\n",
      "[step: 691] loss: 0.021524015814065933\n",
      "[step: 692] loss: 137.8714599609375\n",
      "[step: 692] loss: 0.02152399905025959\n",
      "[step: 693] loss: 139.39163208007812\n",
      "[step: 693] loss: 0.021524006500840187\n",
      "[step: 694] loss: 143.12860107421875\n",
      "[step: 694] loss: 0.02152397856116295\n",
      "[step: 695] loss: 151.07510375976562\n",
      "[step: 695] loss: 0.021523961797356606\n",
      "[step: 696] loss: 171.6634521484375\n",
      "[step: 696] loss: 0.021523967385292053\n",
      "[step: 697] loss: 209.8056182861328\n",
      "[step: 697] loss: 0.021523965522646904\n",
      "[step: 698] loss: 307.87554931640625\n",
      "[step: 698] loss: 0.021523943170905113\n",
      "[step: 699] loss: 384.2973937988281\n",
      "[step: 699] loss: 0.021523937582969666\n",
      "[step: 700] loss: 484.03204345703125\n",
      "[step: 700] loss: 0.02152392640709877\n",
      "[step: 701] loss: 274.18377685546875\n",
      "[step: 701] loss: 0.02152390219271183\n",
      "[step: 702] loss: 144.86453247070312\n",
      "[step: 702] loss: 0.02152390591800213\n",
      "[step: 703] loss: 249.57687377929688\n",
      "[step: 703] loss: 0.021523894742131233\n",
      "[step: 704] loss: 274.8985595703125\n",
      "[step: 704] loss: 0.021523885428905487\n",
      "[step: 705] loss: 159.9306640625\n",
      "[step: 705] loss: 0.02152387425303459\n",
      "[step: 706] loss: 201.59378051757812\n",
      "[step: 706] loss: 0.021523866802453995\n",
      "[step: 707] loss: 237.7896728515625\n",
      "[step: 707] loss: 0.021523859351873398\n",
      "[step: 708] loss: 159.07040405273438\n",
      "[step: 708] loss: 0.021523846313357353\n",
      "[step: 709] loss: 193.30332946777344\n",
      "[step: 709] loss: 0.021523838862776756\n",
      "[step: 710] loss: 191.61993408203125\n",
      "[step: 710] loss: 0.02152382768690586\n",
      "[step: 711] loss: 148.31771850585938\n",
      "[step: 711] loss: 0.021523822098970413\n",
      "[step: 712] loss: 187.6402587890625\n",
      "[step: 712] loss: 0.021523812785744667\n",
      "[step: 713] loss: 153.828369140625\n",
      "[step: 713] loss: 0.021523797884583473\n",
      "[step: 714] loss: 165.92092895507812\n",
      "[step: 714] loss: 0.02152377925813198\n",
      "[step: 715] loss: 170.2389373779297\n",
      "[step: 715] loss: 0.021523769944906235\n",
      "[step: 716] loss: 148.37254333496094\n",
      "[step: 716] loss: 0.021523769944906235\n",
      "[step: 717] loss: 167.8049774169922\n",
      "[step: 717] loss: 0.02152375318109989\n",
      "[step: 718] loss: 150.38519287109375\n",
      "[step: 718] loss: 0.021523742005228996\n",
      "[step: 719] loss: 152.98780822753906\n",
      "[step: 719] loss: 0.02152373641729355\n",
      "[step: 720] loss: 159.36102294921875\n",
      "[step: 720] loss: 0.021523721516132355\n",
      "[step: 721] loss: 143.05429077148438\n",
      "[step: 721] loss: 0.021523714065551758\n",
      "[step: 722] loss: 157.98828125\n",
      "[step: 722] loss: 0.021523714065551758\n",
      "[step: 723] loss: 145.9425506591797\n",
      "[step: 723] loss: 0.021523697301745415\n",
      "[step: 724] loss: 147.56243896484375\n",
      "[step: 724] loss: 0.02152368426322937\n",
      "[step: 725] loss: 149.9433135986328\n",
      "[step: 725] loss: 0.021523676812648773\n",
      "[step: 726] loss: 143.78414916992188\n",
      "[step: 726] loss: 0.02152366004884243\n",
      "[step: 727] loss: 146.03787231445312\n",
      "[step: 727] loss: 0.021523652598261833\n",
      "[step: 728] loss: 147.14968872070312\n",
      "[step: 728] loss: 0.021523652598261833\n",
      "[step: 729] loss: 141.28506469726562\n",
      "[step: 729] loss: 0.021523645147681236\n",
      "[step: 730] loss: 146.8332977294922\n",
      "[step: 730] loss: 0.021523630246520042\n",
      "[step: 731] loss: 143.42904663085938\n",
      "[step: 731] loss: 0.0215236134827137\n",
      "[step: 732] loss: 140.98846435546875\n",
      "[step: 732] loss: 0.0215236134827137\n",
      "[step: 733] loss: 145.11798095703125\n",
      "[step: 733] loss: 0.0215236097574234\n",
      "[step: 734] loss: 140.70529174804688\n",
      "[step: 734] loss: 0.021523602306842804\n",
      "[step: 735] loss: 140.63577270507812\n",
      "[step: 735] loss: 0.021523576229810715\n",
      "[step: 736] loss: 142.20135498046875\n",
      "[step: 736] loss: 0.021523572504520416\n",
      "[step: 737] loss: 139.97702026367188\n",
      "[step: 737] loss: 0.02152355946600437\n",
      "[step: 738] loss: 138.9248809814453\n",
      "[step: 738] loss: 0.02152356132864952\n",
      "[step: 739] loss: 141.26776123046875\n",
      "[step: 739] loss: 0.021523544564843178\n",
      "[step: 740] loss: 138.43385314941406\n",
      "[step: 740] loss: 0.021523533388972282\n",
      "[step: 741] loss: 138.7599639892578\n",
      "[step: 741] loss: 0.02152351662516594\n",
      "[step: 742] loss: 139.56956481933594\n",
      "[step: 742] loss: 0.02152351289987564\n",
      "[step: 743] loss: 138.45281982421875\n",
      "[step: 743] loss: 0.021523503586649895\n",
      "[step: 744] loss: 137.435546875\n",
      "[step: 744] loss: 0.021523496136069298\n",
      "[step: 745] loss: 138.73715209960938\n",
      "[step: 745] loss: 0.021523483097553253\n",
      "[step: 746] loss: 137.91966247558594\n",
      "[step: 746] loss: 0.021523479372262955\n",
      "[step: 747] loss: 136.7587127685547\n",
      "[step: 747] loss: 0.02152346633374691\n",
      "[step: 748] loss: 137.42550659179688\n",
      "[step: 748] loss: 0.02152346260845661\n",
      "[step: 749] loss: 137.504638671875\n",
      "[step: 749] loss: 0.02152344398200512\n",
      "[step: 750] loss: 136.45892333984375\n",
      "[step: 750] loss: 0.02152343839406967\n",
      "[step: 751] loss: 136.11895751953125\n",
      "[step: 751] loss: 0.021523430943489075\n",
      "[step: 752] loss: 136.8602294921875\n",
      "[step: 752] loss: 0.021523423492908478\n",
      "[step: 753] loss: 136.23995971679688\n",
      "[step: 753] loss: 0.021523404866456985\n",
      "[step: 754] loss: 135.6490020751953\n",
      "[step: 754] loss: 0.021523404866456985\n",
      "[step: 755] loss: 135.56204223632812\n",
      "[step: 755] loss: 0.02152339555323124\n",
      "[step: 756] loss: 135.961669921875\n",
      "[step: 756] loss: 0.021523386240005493\n",
      "[step: 757] loss: 135.51486206054688\n",
      "[step: 757] loss: 0.021523378789424896\n",
      "[step: 758] loss: 134.98516845703125\n",
      "[step: 758] loss: 0.021523367613554\n",
      "[step: 759] loss: 134.9336700439453\n",
      "[step: 759] loss: 0.021523352712392807\n",
      "[step: 760] loss: 135.118408203125\n",
      "[step: 760] loss: 0.02152334339916706\n",
      "[step: 761] loss: 134.98545837402344\n",
      "[step: 761] loss: 0.021523334085941315\n",
      "[step: 762] loss: 134.4834747314453\n",
      "[step: 762] loss: 0.02152332104742527\n",
      "[step: 763] loss: 134.2931671142578\n",
      "[step: 763] loss: 0.021523315459489822\n",
      "[step: 764] loss: 134.29473876953125\n",
      "[step: 764] loss: 0.021523306146264076\n",
      "[step: 765] loss: 134.3967742919922\n",
      "[step: 765] loss: 0.021523304283618927\n",
      "[step: 766] loss: 134.13914489746094\n",
      "[step: 766] loss: 0.02152329869568348\n",
      "[step: 767] loss: 133.86453247070312\n",
      "[step: 767] loss: 0.021523291245102882\n",
      "[step: 768] loss: 133.64508056640625\n",
      "[step: 768] loss: 0.02152327261865139\n",
      "[step: 769] loss: 133.62489318847656\n",
      "[step: 769] loss: 0.021523263305425644\n",
      "[step: 770] loss: 133.63888549804688\n",
      "[step: 770] loss: 0.021523257717490196\n",
      "[step: 771] loss: 133.54437255859375\n",
      "[step: 771] loss: 0.021523239091038704\n",
      "[step: 772] loss: 133.38436889648438\n",
      "[step: 772] loss: 0.021523229777812958\n",
      "[step: 773] loss: 133.1468963623047\n",
      "[step: 773] loss: 0.02152322232723236\n",
      "[step: 774] loss: 133.0094451904297\n",
      "[step: 774] loss: 0.021523214876651764\n",
      "[step: 775] loss: 132.9031524658203\n",
      "[step: 775] loss: 0.021523209288716316\n",
      "[step: 776] loss: 132.879150390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 776] loss: 0.021523205563426018\n",
      "[step: 777] loss: 132.83468627929688\n",
      "[step: 777] loss: 0.021523194387555122\n",
      "[step: 778] loss: 132.77249145507812\n",
      "[step: 778] loss: 0.021523186936974525\n",
      "[step: 779] loss: 132.68017578125\n",
      "[step: 779] loss: 0.02152317203581333\n",
      "[step: 780] loss: 132.55120849609375\n",
      "[step: 780] loss: 0.021523160859942436\n",
      "[step: 781] loss: 132.4310302734375\n",
      "[step: 781] loss: 0.02152314968407154\n",
      "[step: 782] loss: 132.29476928710938\n",
      "[step: 782] loss: 0.021523140370845795\n",
      "[step: 783] loss: 132.18621826171875\n",
      "[step: 783] loss: 0.021523132920265198\n",
      "[step: 784] loss: 132.0720977783203\n",
      "[step: 784] loss: 0.02152312733232975\n",
      "[step: 785] loss: 131.9771728515625\n",
      "[step: 785] loss: 0.021523119881749153\n",
      "[step: 786] loss: 131.88369750976562\n",
      "[step: 786] loss: 0.021523114293813705\n",
      "[step: 787] loss: 131.79443359375\n",
      "[step: 787] loss: 0.02152310498058796\n",
      "[step: 788] loss: 131.71292114257812\n",
      "[step: 788] loss: 0.021523093804717064\n",
      "[step: 789] loss: 131.62588500976562\n",
      "[step: 789] loss: 0.021523084491491318\n",
      "[step: 790] loss: 131.548583984375\n",
      "[step: 790] loss: 0.021523071452975273\n",
      "[step: 791] loss: 131.46473693847656\n",
      "[step: 791] loss: 0.021523062139749527\n",
      "[step: 792] loss: 131.38771057128906\n",
      "[step: 792] loss: 0.02152305282652378\n",
      "[step: 793] loss: 131.30862426757812\n",
      "[step: 793] loss: 0.021523043513298035\n",
      "[step: 794] loss: 131.23089599609375\n",
      "[step: 794] loss: 0.02152303420007229\n",
      "[step: 795] loss: 131.1569366455078\n",
      "[step: 795] loss: 0.02152302674949169\n",
      "[step: 796] loss: 131.08200073242188\n",
      "[step: 796] loss: 0.021523017436265945\n",
      "[step: 797] loss: 131.01614379882812\n",
      "[step: 797] loss: 0.0215230081230402\n",
      "[step: 798] loss: 130.9586181640625\n",
      "[step: 798] loss: 0.021522998809814453\n",
      "[step: 799] loss: 130.93502807617188\n",
      "[step: 799] loss: 0.02152298577129841\n",
      "[step: 800] loss: 130.99720764160156\n",
      "[step: 800] loss: 0.02152297832071781\n",
      "[step: 801] loss: 131.30169677734375\n",
      "[step: 801] loss: 0.021522970870137215\n",
      "[step: 802] loss: 132.34326171875\n",
      "[step: 802] loss: 0.02152296155691147\n",
      "[step: 803] loss: 135.51153564453125\n",
      "[step: 803] loss: 0.02152295596897602\n",
      "[step: 804] loss: 145.6917724609375\n",
      "[step: 804] loss: 0.021522944793105125\n",
      "[step: 805] loss: 174.8323211669922\n",
      "[step: 805] loss: 0.02152293547987938\n",
      "[step: 806] loss: 271.4954528808594\n",
      "[step: 806] loss: 0.021522928029298782\n",
      "[step: 807] loss: 463.3841552734375\n",
      "[step: 807] loss: 0.021522916853427887\n",
      "[step: 808] loss: 886.9442138671875\n",
      "[step: 808] loss: 0.02152290940284729\n",
      "[step: 809] loss: 594.693359375\n",
      "[step: 809] loss: 0.021522898226976395\n",
      "[step: 810] loss: 151.87539672851562\n",
      "[step: 810] loss: 0.02152288891375065\n",
      "[step: 811] loss: 447.7986145019531\n",
      "[step: 811] loss: 0.021522879600524902\n",
      "[step: 812] loss: 401.7626953125\n",
      "[step: 812] loss: 0.021522868424654007\n",
      "[step: 813] loss: 202.8946075439453\n",
      "[step: 813] loss: 0.02152286097407341\n",
      "[step: 814] loss: 393.7793273925781\n",
      "[step: 814] loss: 0.021522851660847664\n",
      "[step: 815] loss: 180.73110961914062\n",
      "[step: 815] loss: 0.021522844210267067\n",
      "[step: 816] loss: 295.03375244140625\n",
      "[step: 816] loss: 0.02152283489704132\n",
      "[step: 817] loss: 211.74220275878906\n",
      "[step: 817] loss: 0.021522827446460724\n",
      "[step: 818] loss: 248.65322875976562\n",
      "[step: 818] loss: 0.02152281627058983\n",
      "[step: 819] loss: 214.98623657226562\n",
      "[step: 819] loss: 0.02152280882000923\n",
      "[step: 820] loss: 211.63619995117188\n",
      "[step: 820] loss: 0.021522795781493187\n",
      "[step: 821] loss: 201.8875732421875\n",
      "[step: 821] loss: 0.02152278646826744\n",
      "[step: 822] loss: 203.58984375\n",
      "[step: 822] loss: 0.021522779017686844\n",
      "[step: 823] loss: 188.60321044921875\n",
      "[step: 823] loss: 0.021522769704461098\n",
      "[step: 824] loss: 192.60723876953125\n",
      "[step: 824] loss: 0.02152276039123535\n",
      "[step: 825] loss: 181.8232421875\n",
      "[step: 825] loss: 0.021522749215364456\n",
      "[step: 826] loss: 179.12744140625\n",
      "[step: 826] loss: 0.02152274176478386\n",
      "[step: 827] loss: 189.80767822265625\n",
      "[step: 827] loss: 0.02152273617684841\n",
      "[step: 828] loss: 160.89202880859375\n",
      "[step: 828] loss: 0.021522723138332367\n",
      "[step: 829] loss: 182.16392517089844\n",
      "[step: 829] loss: 0.02152271382510662\n",
      "[step: 830] loss: 154.53921508789062\n",
      "[step: 830] loss: 0.021522704511880875\n",
      "[step: 831] loss: 175.5086669921875\n",
      "[step: 831] loss: 0.02152269519865513\n",
      "[step: 832] loss: 153.35733032226562\n",
      "[step: 832] loss: 0.021522685885429382\n",
      "[step: 833] loss: 163.55018615722656\n",
      "[step: 833] loss: 0.021522678434848785\n",
      "[step: 834] loss: 160.3470916748047\n",
      "[step: 834] loss: 0.02152267098426819\n",
      "[step: 835] loss: 155.21275329589844\n",
      "[step: 835] loss: 0.021522659808397293\n",
      "[step: 836] loss: 158.4790496826172\n",
      "[step: 836] loss: 0.02152264676988125\n",
      "[step: 837] loss: 156.3253173828125\n",
      "[step: 837] loss: 0.021522637456655502\n",
      "[step: 838] loss: 149.5477294921875\n",
      "[step: 838] loss: 0.021522628143429756\n",
      "[step: 839] loss: 154.9840087890625\n",
      "[step: 839] loss: 0.02152261883020401\n",
      "[step: 840] loss: 146.994873046875\n",
      "[step: 840] loss: 0.021522609516978264\n",
      "[step: 841] loss: 149.4503173828125\n",
      "[step: 841] loss: 0.021522600203752518\n",
      "[step: 842] loss: 147.59022521972656\n",
      "[step: 842] loss: 0.02152259089052677\n",
      "[step: 843] loss: 145.08184814453125\n",
      "[step: 843] loss: 0.021522581577301025\n",
      "[step: 844] loss: 148.78826904296875\n",
      "[step: 844] loss: 0.02152257412672043\n",
      "[step: 845] loss: 143.0819549560547\n",
      "[step: 845] loss: 0.021522564813494682\n",
      "[step: 846] loss: 146.9024658203125\n",
      "[step: 846] loss: 0.021522557362914085\n",
      "[step: 847] loss: 144.17848205566406\n",
      "[step: 847] loss: 0.02152254804968834\n",
      "[step: 848] loss: 142.92514038085938\n",
      "[step: 848] loss: 0.021522538736462593\n",
      "[step: 849] loss: 144.52520751953125\n",
      "[step: 849] loss: 0.021522529423236847\n",
      "[step: 850] loss: 140.54373168945312\n",
      "[step: 850] loss: 0.02152251824736595\n",
      "[step: 851] loss: 143.38299560546875\n",
      "[step: 851] loss: 0.021522507071495056\n",
      "[step: 852] loss: 140.32968139648438\n",
      "[step: 852] loss: 0.02152249775826931\n",
      "[step: 853] loss: 141.07247924804688\n",
      "[step: 853] loss: 0.021522486582398415\n",
      "[step: 854] loss: 140.82339477539062\n",
      "[step: 854] loss: 0.02152247726917267\n",
      "[step: 855] loss: 139.3568572998047\n",
      "[step: 855] loss: 0.021522466093301773\n",
      "[step: 856] loss: 140.34420776367188\n",
      "[step: 856] loss: 0.021522458642721176\n",
      "[step: 857] loss: 139.0240020751953\n",
      "[step: 857] loss: 0.02152244932949543\n",
      "[step: 858] loss: 138.65127563476562\n",
      "[step: 858] loss: 0.021522438153624535\n",
      "[step: 859] loss: 138.92803955078125\n",
      "[step: 859] loss: 0.021522430703043938\n",
      "[step: 860] loss: 137.4027099609375\n",
      "[step: 860] loss: 0.02152242138981819\n",
      "[step: 861] loss: 137.93502807617188\n",
      "[step: 861] loss: 0.021522412076592445\n",
      "[step: 862] loss: 136.91357421875\n",
      "[step: 862] loss: 0.0215224027633667\n",
      "[step: 863] loss: 136.8642578125\n",
      "[step: 863] loss: 0.021522391587495804\n",
      "[step: 864] loss: 136.49191284179688\n",
      "[step: 864] loss: 0.021522382274270058\n",
      "[step: 865] loss: 136.25933837890625\n",
      "[step: 865] loss: 0.02152237296104431\n",
      "[step: 866] loss: 135.82589721679688\n",
      "[step: 866] loss: 0.021522361785173416\n",
      "[step: 867] loss: 135.69216918945312\n",
      "[step: 867] loss: 0.02152235247194767\n",
      "[step: 868] loss: 135.5242919921875\n",
      "[step: 868] loss: 0.021522341296076775\n",
      "[step: 869] loss: 135.00933837890625\n",
      "[step: 869] loss: 0.02152233198285103\n",
      "[step: 870] loss: 134.99789428710938\n",
      "[step: 870] loss: 0.021522320806980133\n",
      "[step: 871] loss: 134.78025817871094\n",
      "[step: 871] loss: 0.021522311493754387\n",
      "[step: 872] loss: 134.27511596679688\n",
      "[step: 872] loss: 0.02152230218052864\n",
      "[step: 873] loss: 134.3868408203125\n",
      "[step: 873] loss: 0.021522292867302895\n",
      "[step: 874] loss: 133.919677734375\n",
      "[step: 874] loss: 0.02152228355407715\n",
      "[step: 875] loss: 133.79916381835938\n",
      "[step: 875] loss: 0.02152227610349655\n",
      "[step: 876] loss: 133.50604248046875\n",
      "[step: 876] loss: 0.021522264927625656\n",
      "[step: 877] loss: 133.43118286132812\n",
      "[step: 877] loss: 0.02152225561439991\n",
      "[step: 878] loss: 133.09951782226562\n",
      "[step: 878] loss: 0.021522244438529015\n",
      "[step: 879] loss: 132.9217529296875\n",
      "[step: 879] loss: 0.02152223326265812\n",
      "[step: 880] loss: 132.82083129882812\n",
      "[step: 880] loss: 0.02152222767472267\n",
      "[step: 881] loss: 132.5648956298828\n",
      "[step: 881] loss: 0.021522216498851776\n",
      "[step: 882] loss: 132.36790466308594\n",
      "[step: 882] loss: 0.02152220718562603\n",
      "[step: 883] loss: 132.23876953125\n",
      "[step: 883] loss: 0.021522197872400284\n",
      "[step: 884] loss: 132.06723022460938\n",
      "[step: 884] loss: 0.02152218669652939\n",
      "[step: 885] loss: 131.89149475097656\n",
      "[step: 885] loss: 0.021522175520658493\n",
      "[step: 886] loss: 131.6662139892578\n",
      "[step: 886] loss: 0.021522166207432747\n",
      "[step: 887] loss: 131.59066772460938\n",
      "[step: 887] loss: 0.02152215875685215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 888] loss: 131.37744140625\n",
      "[step: 888] loss: 0.021522149443626404\n",
      "[step: 889] loss: 131.23367309570312\n",
      "[step: 889] loss: 0.02152213826775551\n",
      "[step: 890] loss: 131.03759765625\n",
      "[step: 890] loss: 0.021522128954529762\n",
      "[step: 891] loss: 130.929931640625\n",
      "[step: 891] loss: 0.021522117778658867\n",
      "[step: 892] loss: 130.77423095703125\n",
      "[step: 892] loss: 0.02152210846543312\n",
      "[step: 893] loss: 130.60736083984375\n",
      "[step: 893] loss: 0.021522099152207375\n",
      "[step: 894] loss: 130.45852661132812\n",
      "[step: 894] loss: 0.021522091701626778\n",
      "[step: 895] loss: 130.3124237060547\n",
      "[step: 895] loss: 0.02152208238840103\n",
      "[step: 896] loss: 130.19515991210938\n",
      "[step: 896] loss: 0.021522071212530136\n",
      "[step: 897] loss: 130.03363037109375\n",
      "[step: 897] loss: 0.02152206003665924\n",
      "[step: 898] loss: 129.89022827148438\n",
      "[step: 898] loss: 0.021522050723433495\n",
      "[step: 899] loss: 129.73672485351562\n",
      "[step: 899] loss: 0.02152204141020775\n",
      "[step: 900] loss: 129.6100616455078\n",
      "[step: 900] loss: 0.02152203395962715\n",
      "[step: 901] loss: 129.4790802001953\n",
      "[step: 901] loss: 0.021522022783756256\n",
      "[step: 902] loss: 129.343505859375\n",
      "[step: 902] loss: 0.02152201160788536\n",
      "[step: 903] loss: 129.20327758789062\n",
      "[step: 903] loss: 0.021522002294659615\n",
      "[step: 904] loss: 129.05917358398438\n",
      "[step: 904] loss: 0.02152199111878872\n",
      "[step: 905] loss: 128.9254150390625\n",
      "[step: 905] loss: 0.021521983668208122\n",
      "[step: 906] loss: 128.79373168945312\n",
      "[step: 906] loss: 0.021521974354982376\n",
      "[step: 907] loss: 128.66732788085938\n",
      "[step: 907] loss: 0.02152196317911148\n",
      "[step: 908] loss: 128.53948974609375\n",
      "[step: 908] loss: 0.021521953865885735\n",
      "[step: 909] loss: 128.40859985351562\n",
      "[step: 909] loss: 0.02152194455265999\n",
      "[step: 910] loss: 128.27841186523438\n",
      "[step: 910] loss: 0.02152193896472454\n",
      "[step: 911] loss: 128.14454650878906\n",
      "[step: 911] loss: 0.021521931514143944\n",
      "[step: 912] loss: 128.01593017578125\n",
      "[step: 912] loss: 0.0215219184756279\n",
      "[step: 913] loss: 127.88533782958984\n",
      "[step: 913] loss: 0.021521911025047302\n",
      "[step: 914] loss: 127.76078796386719\n",
      "[step: 914] loss: 0.021521897986531258\n",
      "[step: 915] loss: 127.63482666015625\n",
      "[step: 915] loss: 0.021521886810660362\n",
      "[step: 916] loss: 127.51274871826172\n",
      "[step: 916] loss: 0.021521879360079765\n",
      "[step: 917] loss: 127.39069366455078\n",
      "[step: 917] loss: 0.02152186632156372\n",
      "[step: 918] loss: 127.26976776123047\n",
      "[step: 918] loss: 0.021521855145692825\n",
      "[step: 919] loss: 127.150390625\n",
      "[step: 919] loss: 0.02152184583246708\n",
      "[step: 920] loss: 127.031494140625\n",
      "[step: 920] loss: 0.021521838381886482\n",
      "[step: 921] loss: 126.91740417480469\n",
      "[step: 921] loss: 0.021521829068660736\n",
      "[step: 922] loss: 126.8084716796875\n",
      "[step: 922] loss: 0.02152181603014469\n",
      "[step: 923] loss: 126.71498107910156\n",
      "[step: 923] loss: 0.021521804854273796\n",
      "[step: 924] loss: 126.65003204345703\n",
      "[step: 924] loss: 0.0215217936784029\n",
      "[step: 925] loss: 126.65167999267578\n",
      "[step: 925] loss: 0.021521782502532005\n",
      "[step: 926] loss: 126.8062744140625\n",
      "[step: 926] loss: 0.02152177318930626\n",
      "[step: 927] loss: 127.30459594726562\n",
      "[step: 927] loss: 0.021521762013435364\n",
      "[step: 928] loss: 128.68569946289062\n",
      "[step: 928] loss: 0.021521752700209618\n",
      "[step: 929] loss: 132.05467224121094\n",
      "[step: 929] loss: 0.021521741524338722\n",
      "[step: 930] loss: 141.12759399414062\n",
      "[step: 930] loss: 0.021521732211112976\n",
      "[step: 931] loss: 161.86831665039062\n",
      "[step: 931] loss: 0.02152172103524208\n",
      "[step: 932] loss: 219.61007690429688\n",
      "[step: 932] loss: 0.021521707996726036\n",
      "[step: 933] loss: 315.7398986816406\n",
      "[step: 933] loss: 0.02152170054614544\n",
      "[step: 934] loss: 530.173583984375\n",
      "[step: 934] loss: 0.021521691232919693\n",
      "[step: 935] loss: 484.5195617675781\n",
      "[step: 935] loss: 0.021521681919693947\n",
      "[step: 936] loss: 325.8899230957031\n",
      "[step: 936] loss: 0.0215216726064682\n",
      "[step: 937] loss: 134.83950805664062\n",
      "[step: 937] loss: 0.021521657705307007\n",
      "[step: 938] loss: 278.4853515625\n",
      "[step: 938] loss: 0.02152164839208126\n",
      "[step: 939] loss: 335.0657958984375\n",
      "[step: 939] loss: 0.021521635353565216\n",
      "[step: 940] loss: 154.7397003173828\n",
      "[step: 940] loss: 0.02152162604033947\n",
      "[step: 941] loss: 286.0286865234375\n",
      "[step: 941] loss: 0.021521613001823425\n",
      "[step: 942] loss: 241.9358673095703\n",
      "[step: 942] loss: 0.02152160368859768\n",
      "[step: 943] loss: 180.96682739257812\n",
      "[step: 943] loss: 0.021521592512726784\n",
      "[step: 944] loss: 233.9526824951172\n",
      "[step: 944] loss: 0.02152158133685589\n",
      "[step: 945] loss: 167.8656768798828\n",
      "[step: 945] loss: 0.021521572023630142\n",
      "[step: 946] loss: 198.2611846923828\n",
      "[step: 946] loss: 0.021521560847759247\n",
      "[step: 947] loss: 200.05029296875\n",
      "[step: 947] loss: 0.02152154967188835\n",
      "[step: 948] loss: 159.4998321533203\n",
      "[step: 948] loss: 0.021521540358662605\n",
      "[step: 949] loss: 196.14315795898438\n",
      "[step: 949] loss: 0.02152153104543686\n",
      "[step: 950] loss: 147.1084442138672\n",
      "[step: 950] loss: 0.021521516144275665\n",
      "[step: 951] loss: 185.335205078125\n",
      "[step: 951] loss: 0.02152150496840477\n",
      "[step: 952] loss: 154.78286743164062\n",
      "[step: 952] loss: 0.021521497517824173\n",
      "[step: 953] loss: 162.2266845703125\n",
      "[step: 953] loss: 0.02152148447930813\n",
      "[step: 954] loss: 153.69903564453125\n",
      "[step: 954] loss: 0.02152147889137268\n",
      "[step: 955] loss: 149.55807495117188\n",
      "[step: 955] loss: 0.021521467715501785\n",
      "[step: 956] loss: 157.14353942871094\n",
      "[step: 956] loss: 0.02152145840227604\n",
      "[step: 957] loss: 143.98577880859375\n",
      "[step: 957] loss: 0.021521447226405144\n",
      "[step: 958] loss: 152.54238891601562\n",
      "[step: 958] loss: 0.02152143232524395\n",
      "[step: 959] loss: 142.06895446777344\n",
      "[step: 959] loss: 0.021521421149373055\n",
      "[step: 960] loss: 148.74935913085938\n",
      "[step: 960] loss: 0.02152140811085701\n",
      "[step: 961] loss: 141.08619689941406\n",
      "[step: 961] loss: 0.021521395072340965\n",
      "[step: 962] loss: 144.63638305664062\n",
      "[step: 962] loss: 0.02152138389647007\n",
      "[step: 963] loss: 139.0100860595703\n",
      "[step: 963] loss: 0.021521372720599174\n",
      "[step: 964] loss: 142.13624572753906\n",
      "[step: 964] loss: 0.02152135968208313\n",
      "[step: 965] loss: 134.8798065185547\n",
      "[step: 965] loss: 0.021521348506212234\n",
      "[step: 966] loss: 140.6235809326172\n",
      "[step: 966] loss: 0.02152133733034134\n",
      "[step: 967] loss: 133.06539916992188\n",
      "[step: 967] loss: 0.021521329879760742\n",
      "[step: 968] loss: 136.90341186523438\n",
      "[step: 968] loss: 0.021521316841244698\n",
      "[step: 969] loss: 133.38134765625\n",
      "[step: 969] loss: 0.021521305665373802\n",
      "[step: 970] loss: 132.626708984375\n",
      "[step: 970] loss: 0.021521292626857758\n",
      "[step: 971] loss: 132.9908447265625\n",
      "[step: 971] loss: 0.021521279588341713\n",
      "[step: 972] loss: 131.01712036132812\n",
      "[step: 972] loss: 0.021521268412470818\n",
      "[step: 973] loss: 130.18423461914062\n",
      "[step: 973] loss: 0.021521255373954773\n",
      "[step: 974] loss: 131.3795166015625\n",
      "[step: 974] loss: 0.02152123861014843\n",
      "[step: 975] loss: 128.0110321044922\n",
      "[step: 975] loss: 0.021521227434277534\n",
      "[step: 976] loss: 129.91004943847656\n",
      "[step: 976] loss: 0.02152121439576149\n",
      "[step: 977] loss: 127.52078247070312\n",
      "[step: 977] loss: 0.021521205082535744\n",
      "[step: 978] loss: 127.61271667480469\n",
      "[step: 978] loss: 0.02152119390666485\n",
      "[step: 979] loss: 127.0904541015625\n",
      "[step: 979] loss: 0.021521180868148804\n",
      "[step: 980] loss: 126.72334289550781\n",
      "[step: 980] loss: 0.02152116782963276\n",
      "[step: 981] loss: 125.48194122314453\n",
      "[step: 981] loss: 0.021521154791116714\n",
      "[step: 982] loss: 125.70912170410156\n",
      "[step: 982] loss: 0.02152114361524582\n",
      "[step: 983] loss: 125.0017318725586\n",
      "[step: 983] loss: 0.021521130576729774\n",
      "[step: 984] loss: 123.86503601074219\n",
      "[step: 984] loss: 0.021521123126149178\n",
      "[step: 985] loss: 124.41548156738281\n",
      "[step: 985] loss: 0.021521108224987984\n",
      "[step: 986] loss: 123.31961059570312\n",
      "[step: 986] loss: 0.021521098911762238\n",
      "[step: 987] loss: 123.21284484863281\n",
      "[step: 987] loss: 0.021521084010601044\n",
      "[step: 988] loss: 122.45728302001953\n",
      "[step: 988] loss: 0.02152107283473015\n",
      "[step: 989] loss: 122.9120101928711\n",
      "[step: 989] loss: 0.021521057933568954\n",
      "[step: 990] loss: 121.70062255859375\n",
      "[step: 990] loss: 0.02152104675769806\n",
      "[step: 991] loss: 121.85563659667969\n",
      "[step: 991] loss: 0.021521033719182014\n",
      "[step: 992] loss: 121.44406127929688\n",
      "[step: 992] loss: 0.02152102254331112\n",
      "[step: 993] loss: 121.35365295410156\n",
      "[step: 993] loss: 0.021521011367440224\n",
      "[step: 994] loss: 120.80352783203125\n",
      "[step: 994] loss: 0.02152100019156933\n",
      "[step: 995] loss: 120.52488708496094\n",
      "[step: 995] loss: 0.021520985290408134\n",
      "[step: 996] loss: 120.50508117675781\n",
      "[step: 996] loss: 0.02152097597718239\n",
      "[step: 997] loss: 120.24264526367188\n",
      "[step: 997] loss: 0.021520959213376045\n",
      "[step: 998] loss: 119.88011932373047\n",
      "[step: 998] loss: 0.02152094803750515\n",
      "[step: 999] loss: 119.66329956054688\n",
      "[step: 999] loss: 0.021520933136343956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1000] loss: 119.40397644042969\n",
      "[step: 1000] loss: 0.02152092009782791\n",
      "[step: 1001] loss: 119.41532897949219\n",
      "[step: 1001] loss: 0.021520905196666718\n",
      "[step: 1002] loss: 119.10696411132812\n",
      "[step: 1002] loss: 0.021520892158150673\n",
      "[step: 1003] loss: 118.9024887084961\n",
      "[step: 1003] loss: 0.02152087725698948\n",
      "[step: 1004] loss: 118.70751190185547\n",
      "[step: 1004] loss: 0.021520864218473434\n",
      "[step: 1005] loss: 118.53813171386719\n",
      "[step: 1005] loss: 0.02152084931731224\n",
      "[step: 1006] loss: 118.38594055175781\n",
      "[step: 1006] loss: 0.021520836278796196\n",
      "[step: 1007] loss: 118.31998443603516\n",
      "[step: 1007] loss: 0.02152082696557045\n",
      "[step: 1008] loss: 118.15583801269531\n",
      "[step: 1008] loss: 0.021520812064409256\n",
      "[step: 1009] loss: 118.00633239746094\n",
      "[step: 1009] loss: 0.02152080088853836\n",
      "[step: 1010] loss: 117.87873840332031\n",
      "[step: 1010] loss: 0.021520784124732018\n",
      "[step: 1011] loss: 117.71987915039062\n",
      "[step: 1011] loss: 0.021520771086215973\n",
      "[step: 1012] loss: 117.57400512695312\n",
      "[step: 1012] loss: 0.021520759910345078\n",
      "[step: 1013] loss: 117.4637680053711\n",
      "[step: 1013] loss: 0.021520746871829033\n",
      "[step: 1014] loss: 117.37315368652344\n",
      "[step: 1014] loss: 0.02152073197066784\n",
      "[step: 1015] loss: 117.23188781738281\n",
      "[step: 1015] loss: 0.021520717069506645\n",
      "[step: 1016] loss: 117.13238525390625\n",
      "[step: 1016] loss: 0.02152070216834545\n",
      "[step: 1017] loss: 117.0370864868164\n",
      "[step: 1017] loss: 0.021520687267184258\n",
      "[step: 1018] loss: 116.94888305664062\n",
      "[step: 1018] loss: 0.021520674228668213\n",
      "[step: 1019] loss: 116.8293685913086\n",
      "[step: 1019] loss: 0.02152065560221672\n",
      "[step: 1020] loss: 116.74119567871094\n",
      "[step: 1020] loss: 0.021520646288990974\n",
      "[step: 1021] loss: 116.65452575683594\n",
      "[step: 1021] loss: 0.02152063138782978\n",
      "[step: 1022] loss: 116.58029174804688\n",
      "[step: 1022] loss: 0.021520614624023438\n",
      "[step: 1023] loss: 116.48332214355469\n",
      "[step: 1023] loss: 0.021520601585507393\n",
      "[step: 1024] loss: 116.39833068847656\n",
      "[step: 1024] loss: 0.0215205866843462\n",
      "[step: 1025] loss: 116.32249450683594\n",
      "[step: 1025] loss: 0.021520571783185005\n",
      "[step: 1026] loss: 116.28134155273438\n",
      "[step: 1026] loss: 0.021520555019378662\n",
      "[step: 1027] loss: 116.32362365722656\n",
      "[step: 1027] loss: 0.021520540118217468\n",
      "[step: 1028] loss: 116.78214263916016\n",
      "[step: 1028] loss: 0.021520527079701424\n",
      "[step: 1029] loss: 118.63880157470703\n",
      "[step: 1029] loss: 0.02152051031589508\n",
      "[step: 1030] loss: 128.63064575195312\n",
      "[step: 1030] loss: 0.021520497277379036\n",
      "[step: 1031] loss: 130.2976531982422\n",
      "[step: 1031] loss: 0.021520478650927544\n",
      "[step: 1032] loss: 147.3043975830078\n",
      "[step: 1032] loss: 0.0215204656124115\n",
      "[step: 1033] loss: 158.5447540283203\n",
      "[step: 1033] loss: 0.021520448848605156\n",
      "[step: 1034] loss: 167.34542846679688\n",
      "[step: 1034] loss: 0.021520432084798813\n",
      "[step: 1035] loss: 209.6776123046875\n",
      "[step: 1035] loss: 0.021520420908927917\n",
      "[step: 1036] loss: 278.2463073730469\n",
      "[step: 1036] loss: 0.021520404145121574\n",
      "[step: 1037] loss: 303.28216552734375\n",
      "[step: 1037] loss: 0.02152038924396038\n",
      "[step: 1038] loss: 297.7858581542969\n",
      "[step: 1038] loss: 0.021520374342799187\n",
      "[step: 1039] loss: 211.54373168945312\n",
      "[step: 1039] loss: 0.021520359441637993\n",
      "[step: 1040] loss: 166.22767639160156\n",
      "[step: 1040] loss: 0.0215203445404768\n",
      "[step: 1041] loss: 188.51068115234375\n",
      "[step: 1041] loss: 0.021520325914025307\n",
      "[step: 1042] loss: 217.40122985839844\n",
      "[step: 1042] loss: 0.021520309150218964\n",
      "[step: 1043] loss: 186.530029296875\n",
      "[step: 1043] loss: 0.02152029238641262\n",
      "[step: 1044] loss: 152.495849609375\n",
      "[step: 1044] loss: 0.021520275622606277\n",
      "[step: 1045] loss: 193.36141967773438\n",
      "[step: 1045] loss: 0.021520258858799934\n",
      "[step: 1046] loss: 175.0114288330078\n",
      "[step: 1046] loss: 0.02152024395763874\n",
      "[step: 1047] loss: 152.2769317626953\n",
      "[step: 1047] loss: 0.021520225331187248\n",
      "[step: 1048] loss: 165.97775268554688\n",
      "[step: 1048] loss: 0.021520208567380905\n",
      "[step: 1049] loss: 169.23406982421875\n",
      "[step: 1049] loss: 0.021520191803574562\n",
      "[step: 1050] loss: 142.73086547851562\n",
      "[step: 1050] loss: 0.02152017503976822\n",
      "[step: 1051] loss: 153.23480224609375\n",
      "[step: 1051] loss: 0.021520156413316727\n",
      "[step: 1052] loss: 159.32089233398438\n",
      "[step: 1052] loss: 0.021520139649510384\n",
      "[step: 1053] loss: 139.54281616210938\n",
      "[step: 1053] loss: 0.02152012474834919\n",
      "[step: 1054] loss: 153.25230407714844\n",
      "[step: 1054] loss: 0.021520107984542847\n",
      "[step: 1055] loss: 149.75439453125\n",
      "[step: 1055] loss: 0.021520094946026802\n",
      "[step: 1056] loss: 138.71095275878906\n",
      "[step: 1056] loss: 0.021520080044865608\n",
      "[step: 1057] loss: 146.104736328125\n",
      "[step: 1057] loss: 0.021520061418414116\n",
      "[step: 1058] loss: 144.42506408691406\n",
      "[step: 1058] loss: 0.021520046517252922\n",
      "[step: 1059] loss: 136.64804077148438\n",
      "[step: 1059] loss: 0.02152002789080143\n",
      "[step: 1060] loss: 145.60182189941406\n",
      "[step: 1060] loss: 0.021520011126995087\n",
      "[step: 1061] loss: 141.99923706054688\n",
      "[step: 1061] loss: 0.021519996225833893\n",
      "[step: 1062] loss: 136.01336669921875\n",
      "[step: 1062] loss: 0.0215199775993824\n",
      "[step: 1063] loss: 143.01226806640625\n",
      "[step: 1063] loss: 0.021519962698221207\n",
      "[step: 1064] loss: 138.61854553222656\n",
      "[step: 1064] loss: 0.021519949659705162\n",
      "[step: 1065] loss: 135.31707763671875\n",
      "[step: 1065] loss: 0.02151992917060852\n",
      "[step: 1066] loss: 138.90896606445312\n",
      "[step: 1066] loss: 0.021519910544157028\n",
      "[step: 1067] loss: 138.13339233398438\n",
      "[step: 1067] loss: 0.021519897505640984\n",
      "[step: 1068] loss: 133.43167114257812\n",
      "[step: 1068] loss: 0.021519875153899193\n",
      "[step: 1069] loss: 136.99688720703125\n",
      "[step: 1069] loss: 0.021519860252738\n",
      "[step: 1070] loss: 136.04769897460938\n",
      "[step: 1070] loss: 0.021519841626286507\n",
      "[step: 1071] loss: 132.71945190429688\n",
      "[step: 1071] loss: 0.021519824862480164\n",
      "[step: 1072] loss: 134.5808868408203\n",
      "[step: 1072] loss: 0.02151980623602867\n",
      "[step: 1073] loss: 135.26058959960938\n",
      "[step: 1073] loss: 0.021519789472222328\n",
      "[step: 1074] loss: 132.79286193847656\n",
      "[step: 1074] loss: 0.021519772708415985\n",
      "[step: 1075] loss: 132.58425903320312\n",
      "[step: 1075] loss: 0.021519750356674194\n",
      "[step: 1076] loss: 134.48977661132812\n",
      "[step: 1076] loss: 0.021519729867577553\n",
      "[step: 1077] loss: 132.39981079101562\n",
      "[step: 1077] loss: 0.021519707515835762\n",
      "[step: 1078] loss: 131.73695373535156\n",
      "[step: 1078] loss: 0.021519694477319717\n",
      "[step: 1079] loss: 132.65066528320312\n",
      "[step: 1079] loss: 0.021519673988223076\n",
      "[step: 1080] loss: 132.79771423339844\n",
      "[step: 1080] loss: 0.021519653499126434\n",
      "[step: 1081] loss: 131.06768798828125\n",
      "[step: 1081] loss: 0.021519633010029793\n",
      "[step: 1082] loss: 131.3275909423828\n",
      "[step: 1082] loss: 0.0215196143835783\n",
      "[step: 1083] loss: 131.9448699951172\n",
      "[step: 1083] loss: 0.021519597619771957\n",
      "[step: 1084] loss: 131.46786499023438\n",
      "[step: 1084] loss: 0.021519575268030167\n",
      "[step: 1085] loss: 130.54730224609375\n",
      "[step: 1085] loss: 0.021519554778933525\n",
      "[step: 1086] loss: 130.6514892578125\n",
      "[step: 1086] loss: 0.021519536152482033\n",
      "[step: 1087] loss: 131.16156005859375\n",
      "[step: 1087] loss: 0.021519513800740242\n",
      "[step: 1088] loss: 130.68020629882812\n",
      "[step: 1088] loss: 0.02151949517428875\n",
      "[step: 1089] loss: 130.11080932617188\n",
      "[step: 1089] loss: 0.02151947282254696\n",
      "[step: 1090] loss: 129.93624877929688\n",
      "[step: 1090] loss: 0.021519456058740616\n",
      "[step: 1091] loss: 130.34210205078125\n",
      "[step: 1091] loss: 0.021519435569643974\n",
      "[step: 1092] loss: 130.27195739746094\n",
      "[step: 1092] loss: 0.021519413217902184\n",
      "[step: 1093] loss: 129.88026428222656\n",
      "[step: 1093] loss: 0.021519390866160393\n",
      "[step: 1094] loss: 129.49835205078125\n",
      "[step: 1094] loss: 0.021519368514418602\n",
      "[step: 1095] loss: 129.4898681640625\n",
      "[step: 1095] loss: 0.021519344300031662\n",
      "[step: 1096] loss: 129.6719512939453\n",
      "[step: 1096] loss: 0.02151932194828987\n",
      "[step: 1097] loss: 129.67449951171875\n",
      "[step: 1097] loss: 0.02151929959654808\n",
      "[step: 1098] loss: 129.48965454101562\n",
      "[step: 1098] loss: 0.02151927724480629\n",
      "[step: 1099] loss: 129.18197631835938\n",
      "[step: 1099] loss: 0.0215192548930645\n",
      "[step: 1100] loss: 129.01759338378906\n",
      "[step: 1100] loss: 0.021519232541322708\n",
      "[step: 1101] loss: 129.0022735595703\n",
      "[step: 1101] loss: 0.021519210189580917\n",
      "[step: 1102] loss: 129.05699157714844\n",
      "[step: 1102] loss: 0.021519189700484276\n",
      "[step: 1103] loss: 129.0894317626953\n",
      "[step: 1103] loss: 0.021519165486097336\n",
      "[step: 1104] loss: 128.98779296875\n",
      "[step: 1104] loss: 0.021519141271710396\n",
      "[step: 1105] loss: 128.85501098632812\n",
      "[step: 1105] loss: 0.021519122645258904\n",
      "[step: 1106] loss: 128.66326904296875\n",
      "[step: 1106] loss: 0.021519096568226814\n",
      "[step: 1107] loss: 128.54653930664062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1107] loss: 0.021519070491194725\n",
      "[step: 1108] loss: 128.45118713378906\n",
      "[step: 1108] loss: 0.021519048139452934\n",
      "[step: 1109] loss: 128.4195556640625\n",
      "[step: 1109] loss: 0.021519025787711143\n",
      "[step: 1110] loss: 128.40548706054688\n",
      "[step: 1110] loss: 0.021519001573324203\n",
      "[step: 1111] loss: 128.40396118164062\n",
      "[step: 1111] loss: 0.021518977358937263\n",
      "[step: 1112] loss: 128.41290283203125\n",
      "[step: 1112] loss: 0.021518955007195473\n",
      "[step: 1113] loss: 128.417236328125\n",
      "[step: 1113] loss: 0.021518932655453682\n",
      "[step: 1114] loss: 128.4364776611328\n",
      "[step: 1114] loss: 0.021518904715776443\n",
      "[step: 1115] loss: 128.4776153564453\n",
      "[step: 1115] loss: 0.021518882364034653\n",
      "[step: 1116] loss: 128.55584716796875\n",
      "[step: 1116] loss: 0.021518860012292862\n",
      "[step: 1117] loss: 128.73095703125\n",
      "[step: 1117] loss: 0.021518832072615623\n",
      "[step: 1118] loss: 129.04904174804688\n",
      "[step: 1118] loss: 0.021518807858228683\n",
      "[step: 1119] loss: 129.70033264160156\n",
      "[step: 1119] loss: 0.021518783643841743\n",
      "[step: 1120] loss: 130.8586883544922\n",
      "[step: 1120] loss: 0.021518763154745102\n",
      "[step: 1121] loss: 133.24862670898438\n",
      "[step: 1121] loss: 0.021518735215067863\n",
      "[step: 1122] loss: 137.70933532714844\n",
      "[step: 1122] loss: 0.021518705412745476\n",
      "[step: 1123] loss: 147.28404235839844\n",
      "[step: 1123] loss: 0.021518675610423088\n",
      "[step: 1124] loss: 164.20347595214844\n",
      "[step: 1124] loss: 0.021518649533391\n",
      "[step: 1125] loss: 200.6673583984375\n",
      "[step: 1125] loss: 0.02151862159371376\n",
      "[step: 1126] loss: 249.41856384277344\n",
      "[step: 1126] loss: 0.021518593654036522\n",
      "[step: 1127] loss: 326.814208984375\n",
      "[step: 1127] loss: 0.021518565714359283\n",
      "[step: 1128] loss: 326.9854736328125\n",
      "[step: 1128] loss: 0.021518541499972343\n",
      "[step: 1129] loss: 254.26771545410156\n",
      "[step: 1129] loss: 0.021518515422940254\n",
      "[step: 1130] loss: 147.07830810546875\n",
      "[step: 1130] loss: 0.021518485620617867\n",
      "[step: 1131] loss: 153.57737731933594\n",
      "[step: 1131] loss: 0.021518459543585777\n",
      "[step: 1132] loss: 219.15859985351562\n",
      "[step: 1132] loss: 0.02151842974126339\n",
      "[step: 1133] loss: 189.85922241210938\n",
      "[step: 1133] loss: 0.021518398076295853\n",
      "[step: 1134] loss: 134.04031372070312\n",
      "[step: 1134] loss: 0.021518368273973465\n",
      "[step: 1135] loss: 173.64707946777344\n",
      "[step: 1135] loss: 0.021518340334296227\n",
      "[step: 1136] loss: 185.54611206054688\n",
      "[step: 1136] loss: 0.021518312394618988\n",
      "[step: 1137] loss: 139.12896728515625\n",
      "[step: 1137] loss: 0.02151828072965145\n",
      "[step: 1138] loss: 155.3173828125\n",
      "[step: 1138] loss: 0.021518252789974213\n",
      "[step: 1139] loss: 174.9602508544922\n",
      "[step: 1139] loss: 0.021518219262361526\n",
      "[step: 1140] loss: 142.5795135498047\n",
      "[step: 1140] loss: 0.02151818946003914\n",
      "[step: 1141] loss: 145.65538024902344\n",
      "[step: 1141] loss: 0.021518155932426453\n",
      "[step: 1142] loss: 165.9337615966797\n",
      "[step: 1142] loss: 0.021518127992749214\n",
      "[step: 1143] loss: 139.5701904296875\n",
      "[step: 1143] loss: 0.021518096327781677\n",
      "[step: 1144] loss: 141.789306640625\n",
      "[step: 1144] loss: 0.02151806838810444\n",
      "[step: 1145] loss: 154.84866333007812\n",
      "[step: 1145] loss: 0.02151803858578205\n",
      "[step: 1146] loss: 136.50775146484375\n",
      "[step: 1146] loss: 0.021518003195524216\n",
      "[step: 1147] loss: 139.1265869140625\n",
      "[step: 1147] loss: 0.02151796966791153\n",
      "[step: 1148] loss: 147.16131591796875\n",
      "[step: 1148] loss: 0.021517938002943993\n",
      "[step: 1149] loss: 135.10826110839844\n",
      "[step: 1149] loss: 0.021517906337976456\n",
      "[step: 1150] loss: 136.8837890625\n",
      "[step: 1150] loss: 0.02151787467300892\n",
      "[step: 1151] loss: 143.24717712402344\n",
      "[step: 1151] loss: 0.02151784673333168\n",
      "[step: 1152] loss: 133.80010986328125\n",
      "[step: 1152] loss: 0.021517816931009293\n",
      "[step: 1153] loss: 135.41702270507812\n",
      "[step: 1153] loss: 0.021517787128686905\n",
      "[step: 1154] loss: 140.26258850097656\n",
      "[step: 1154] loss: 0.02151774801313877\n",
      "[step: 1155] loss: 133.254638671875\n",
      "[step: 1155] loss: 0.021517714485526085\n",
      "[step: 1156] loss: 133.61013793945312\n",
      "[step: 1156] loss: 0.0215176772326231\n",
      "[step: 1157] loss: 137.55479431152344\n",
      "[step: 1157] loss: 0.021517645567655563\n",
      "[step: 1158] loss: 133.5594482421875\n",
      "[step: 1158] loss: 0.021517612040042877\n",
      "[step: 1159] loss: 131.63442993164062\n",
      "[step: 1159] loss: 0.021517576649785042\n",
      "[step: 1160] loss: 135.57508850097656\n",
      "[step: 1160] loss: 0.021517541259527206\n",
      "[step: 1161] loss: 134.15362548828125\n",
      "[step: 1161] loss: 0.021517502143979073\n",
      "[step: 1162] loss: 130.73959350585938\n",
      "[step: 1162] loss: 0.021517464891076088\n",
      "[step: 1163] loss: 133.13356018066406\n",
      "[step: 1163] loss: 0.021517427638173103\n",
      "[step: 1164] loss: 133.96652221679688\n",
      "[step: 1164] loss: 0.02151738852262497\n",
      "[step: 1165] loss: 131.1013946533203\n",
      "[step: 1165] loss: 0.021517349407076836\n",
      "[step: 1166] loss: 130.7250518798828\n",
      "[step: 1166] loss: 0.021517310291528702\n",
      "[step: 1167] loss: 132.76158142089844\n",
      "[step: 1167] loss: 0.021517271175980568\n",
      "[step: 1168] loss: 131.85903930664062\n",
      "[step: 1168] loss: 0.021517232060432434\n",
      "[step: 1169] loss: 129.87069702148438\n",
      "[step: 1169] loss: 0.02151719480752945\n",
      "[step: 1170] loss: 130.658203125\n",
      "[step: 1170] loss: 0.021517153829336166\n",
      "[step: 1171] loss: 131.55276489257812\n",
      "[step: 1171] loss: 0.021517112851142883\n",
      "[step: 1172] loss: 130.62034606933594\n",
      "[step: 1172] loss: 0.02151707001030445\n",
      "[step: 1173] loss: 129.36117553710938\n",
      "[step: 1173] loss: 0.021517032757401466\n",
      "[step: 1174] loss: 129.846923828125\n",
      "[step: 1174] loss: 0.021516988053917885\n",
      "[step: 1175] loss: 130.633056640625\n",
      "[step: 1175] loss: 0.02151694893836975\n",
      "[step: 1176] loss: 129.9757080078125\n",
      "[step: 1176] loss: 0.02151690423488617\n",
      "[step: 1177] loss: 129.05906677246094\n",
      "[step: 1177] loss: 0.021516859531402588\n",
      "[step: 1178] loss: 128.94107055664062\n",
      "[step: 1178] loss: 0.021516811102628708\n",
      "[step: 1179] loss: 129.53993225097656\n",
      "[step: 1179] loss: 0.021516766399145126\n",
      "[step: 1180] loss: 129.6612548828125\n",
      "[step: 1180] loss: 0.021516721695661545\n",
      "[step: 1181] loss: 129.07012939453125\n",
      "[step: 1181] loss: 0.02151668071746826\n",
      "[step: 1182] loss: 128.52450561523438\n",
      "[step: 1182] loss: 0.02151663601398468\n",
      "[step: 1183] loss: 128.41262817382812\n",
      "[step: 1183] loss: 0.0215165913105011\n",
      "[step: 1184] loss: 128.7386932373047\n",
      "[step: 1184] loss: 0.021516544744372368\n",
      "[step: 1185] loss: 128.92623901367188\n",
      "[step: 1185] loss: 0.02151649259030819\n",
      "[step: 1186] loss: 128.7542266845703\n",
      "[step: 1186] loss: 0.02151644602417946\n",
      "[step: 1187] loss: 128.37664794921875\n",
      "[step: 1187] loss: 0.02151639573276043\n",
      "[step: 1188] loss: 128.0145263671875\n",
      "[step: 1188] loss: 0.0215163417160511\n",
      "[step: 1189] loss: 127.91123962402344\n",
      "[step: 1189] loss: 0.02151629514992237\n",
      "[step: 1190] loss: 127.97420501708984\n",
      "[step: 1190] loss: 0.021516241133213043\n",
      "[step: 1191] loss: 128.11431884765625\n",
      "[step: 1191] loss: 0.021516185253858566\n",
      "[step: 1192] loss: 128.20541381835938\n",
      "[step: 1192] loss: 0.021516134962439537\n",
      "[step: 1193] loss: 128.15966796875\n",
      "[step: 1193] loss: 0.02151608280837536\n",
      "[step: 1194] loss: 128.05548095703125\n",
      "[step: 1194] loss: 0.02151602692902088\n",
      "[step: 1195] loss: 127.88131713867188\n",
      "[step: 1195] loss: 0.021515967324376106\n",
      "[step: 1196] loss: 127.71907043457031\n",
      "[step: 1196] loss: 0.021515918895602226\n",
      "[step: 1197] loss: 127.56988525390625\n",
      "[step: 1197] loss: 0.02151586301624775\n",
      "[step: 1198] loss: 127.442138671875\n",
      "[step: 1198] loss: 0.021515805274248123\n",
      "[step: 1199] loss: 127.35139465332031\n",
      "[step: 1199] loss: 0.021515745669603348\n",
      "[step: 1200] loss: 127.2659683227539\n",
      "[step: 1200] loss: 0.021515682339668274\n",
      "[step: 1201] loss: 127.20130920410156\n",
      "[step: 1201] loss: 0.0215156227350235\n",
      "[step: 1202] loss: 127.14450073242188\n",
      "[step: 1202] loss: 0.021515555679798126\n",
      "[step: 1203] loss: 127.08992004394531\n",
      "[step: 1203] loss: 0.021515486761927605\n",
      "[step: 1204] loss: 127.05144500732422\n",
      "[step: 1204] loss: 0.02151542901992798\n",
      "[step: 1205] loss: 127.01747131347656\n",
      "[step: 1205] loss: 0.021515361964702606\n",
      "[step: 1206] loss: 127.0123291015625\n",
      "[step: 1206] loss: 0.021515294909477234\n",
      "[step: 1207] loss: 127.05819702148438\n",
      "[step: 1207] loss: 0.021515222266316414\n",
      "[step: 1208] loss: 127.221923828125\n",
      "[step: 1208] loss: 0.02151515521109104\n",
      "[step: 1209] loss: 127.67431640625\n",
      "[step: 1209] loss: 0.02151508256793022\n",
      "[step: 1210] loss: 128.85540771484375\n",
      "[step: 1210] loss: 0.02151501178741455\n",
      "[step: 1211] loss: 131.82327270507812\n",
      "[step: 1211] loss: 0.02151493728160858\n",
      "[step: 1212] loss: 139.8520965576172\n",
      "[step: 1212] loss: 0.021514859050512314\n",
      "[step: 1213] loss: 159.5943603515625\n",
      "[step: 1213] loss: 0.021514786407351494\n",
      "[step: 1214] loss: 214.92807006835938\n",
      "[step: 1214] loss: 0.021514704450964928\n",
      "[step: 1215] loss: 324.413818359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1215] loss: 0.02151462249457836\n",
      "[step: 1216] loss: 559.5452880859375\n",
      "[step: 1216] loss: 0.021514538675546646\n",
      "[step: 1217] loss: 600.6736450195312\n",
      "[step: 1217] loss: 0.02151445485651493\n",
      "[step: 1218] loss: 360.251220703125\n",
      "[step: 1218] loss: 0.021514372900128365\n",
      "[step: 1219] loss: 144.47021484375\n",
      "[step: 1219] loss: 0.021514281630516052\n",
      "[step: 1220] loss: 336.3755187988281\n",
      "[step: 1220] loss: 0.02151418849825859\n",
      "[step: 1221] loss: 326.1029052734375\n",
      "[step: 1221] loss: 0.02151408977806568\n",
      "[step: 1222] loss: 163.9744415283203\n",
      "[step: 1222] loss: 0.021513987332582474\n",
      "[step: 1223] loss: 307.0421142578125\n",
      "[step: 1223] loss: 0.02151387929916382\n",
      "[step: 1224] loss: 196.49490356445312\n",
      "[step: 1224] loss: 0.021513773128390312\n",
      "[step: 1225] loss: 212.5830078125\n",
      "[step: 1225] loss: 0.021513663232326508\n",
      "[step: 1226] loss: 197.56561279296875\n",
      "[step: 1226] loss: 0.021513544023036957\n",
      "[step: 1227] loss: 198.4558868408203\n",
      "[step: 1227] loss: 0.021513422951102257\n",
      "[step: 1228] loss: 181.76303100585938\n",
      "[step: 1228] loss: 0.021513286978006363\n",
      "[step: 1229] loss: 218.1776123046875\n",
      "[step: 1229] loss: 0.02151314541697502\n",
      "[step: 1230] loss: 158.67864990234375\n",
      "[step: 1230] loss: 0.02151300199329853\n",
      "[step: 1231] loss: 198.94540405273438\n",
      "[step: 1231] loss: 0.021512847393751144\n",
      "[step: 1232] loss: 160.26248168945312\n",
      "[step: 1232] loss: 0.021512676030397415\n",
      "[step: 1233] loss: 175.1977081298828\n",
      "[step: 1233] loss: 0.021512502804398537\n",
      "[step: 1234] loss: 156.39434814453125\n",
      "[step: 1234] loss: 0.021512312814593315\n",
      "[step: 1235] loss: 169.49917602539062\n",
      "[step: 1235] loss: 0.021512096747756004\n",
      "[step: 1236] loss: 149.4795379638672\n",
      "[step: 1236] loss: 0.021511860191822052\n",
      "[step: 1237] loss: 165.75169372558594\n",
      "[step: 1237] loss: 0.021511578932404518\n",
      "[step: 1238] loss: 147.92550659179688\n",
      "[step: 1238] loss: 0.021511271595954895\n",
      "[step: 1239] loss: 158.35293579101562\n",
      "[step: 1239] loss: 0.021510936319828033\n",
      "[step: 1240] loss: 150.4158935546875\n",
      "[step: 1240] loss: 0.021510537713766098\n",
      "[step: 1241] loss: 149.00408935546875\n",
      "[step: 1241] loss: 0.02151007577776909\n",
      "[step: 1242] loss: 157.05218505859375\n",
      "[step: 1242] loss: 0.021509526297450066\n",
      "[step: 1243] loss: 141.5550537109375\n",
      "[step: 1243] loss: 0.021508896723389626\n",
      "[step: 1244] loss: 151.4637451171875\n",
      "[step: 1244] loss: 0.02150813676416874\n",
      "[step: 1245] loss: 141.22837829589844\n",
      "[step: 1245] loss: 0.02150721475481987\n",
      "[step: 1246] loss: 144.37725830078125\n",
      "[step: 1246] loss: 0.021506134420633316\n",
      "[step: 1247] loss: 145.21817016601562\n",
      "[step: 1247] loss: 0.02150489203631878\n",
      "[step: 1248] loss: 141.13189697265625\n",
      "[step: 1248] loss: 0.02150346152484417\n",
      "[step: 1249] loss: 141.46121215820312\n",
      "[step: 1249] loss: 0.021501731127500534\n",
      "[step: 1250] loss: 143.1107177734375\n",
      "[step: 1250] loss: 0.021499255672097206\n",
      "[step: 1251] loss: 137.54067993164062\n",
      "[step: 1251] loss: 0.021495182067155838\n",
      "[step: 1252] loss: 140.85159301757812\n",
      "[step: 1252] loss: 0.02148832567036152\n",
      "[step: 1253] loss: 138.91943359375\n",
      "[step: 1253] loss: 0.02147662453353405\n",
      "[step: 1254] loss: 136.0437469482422\n",
      "[step: 1254] loss: 0.02145390212535858\n",
      "[step: 1255] loss: 139.9137725830078\n",
      "[step: 1255] loss: 0.02139621041715145\n",
      "[step: 1256] loss: 136.2392578125\n",
      "[step: 1256] loss: 0.021165233105421066\n",
      "[step: 1257] loss: 136.16123962402344\n",
      "[step: 1257] loss: 0.027582356706261635\n",
      "[step: 1258] loss: 136.6099395751953\n",
      "[step: 1258] loss: 0.022859932854771614\n",
      "[step: 1259] loss: 135.16818237304688\n",
      "[step: 1259] loss: 0.02380572445690632\n",
      "[step: 1260] loss: 134.05709838867188\n",
      "[step: 1260] loss: 0.022613465785980225\n",
      "[step: 1261] loss: 135.77320861816406\n",
      "[step: 1261] loss: 0.021653588861227036\n",
      "[step: 1262] loss: 133.1121826171875\n",
      "[step: 1262] loss: 0.02224324643611908\n",
      "[step: 1263] loss: 133.46897888183594\n",
      "[step: 1263] loss: 0.0228052269667387\n",
      "[step: 1264] loss: 133.70883178710938\n",
      "[step: 1264] loss: 0.022343188524246216\n",
      "[step: 1265] loss: 132.7399444580078\n",
      "[step: 1265] loss: 0.021817093715071678\n",
      "[step: 1266] loss: 131.9284210205078\n",
      "[step: 1266] loss: 0.02193097211420536\n",
      "[step: 1267] loss: 133.0770721435547\n",
      "[step: 1267] loss: 0.022178981453180313\n",
      "[step: 1268] loss: 131.97434997558594\n",
      "[step: 1268] loss: 0.022085102275013924\n",
      "[step: 1269] loss: 131.25820922851562\n",
      "[step: 1269] loss: 0.021847449243068695\n",
      "[step: 1270] loss: 131.43411254882812\n",
      "[step: 1270] loss: 0.021770210936665535\n",
      "[step: 1271] loss: 131.7191619873047\n",
      "[step: 1271] loss: 0.0218505896627903\n",
      "[step: 1272] loss: 130.61996459960938\n",
      "[step: 1272] loss: 0.021896805614233017\n",
      "[step: 1273] loss: 130.47152709960938\n",
      "[step: 1273] loss: 0.021799376234412193\n",
      "[step: 1274] loss: 130.74041748046875\n",
      "[step: 1274] loss: 0.021675005555152893\n",
      "[step: 1275] loss: 130.4140625\n",
      "[step: 1275] loss: 0.021686341613531113\n",
      "[step: 1276] loss: 129.84423828125\n",
      "[step: 1276] loss: 0.02176230400800705\n",
      "[step: 1277] loss: 129.76376342773438\n",
      "[step: 1277] loss: 0.021727388724684715\n",
      "[step: 1278] loss: 129.95846557617188\n",
      "[step: 1278] loss: 0.021623769775032997\n",
      "[step: 1279] loss: 129.62680053710938\n",
      "[step: 1279] loss: 0.021611984819173813\n",
      "[step: 1280] loss: 129.28370666503906\n",
      "[step: 1280] loss: 0.021665094420313835\n",
      "[step: 1281] loss: 129.17376708984375\n",
      "[step: 1281] loss: 0.021651899442076683\n",
      "[step: 1282] loss: 129.27520751953125\n",
      "[step: 1282] loss: 0.021592574194073677\n",
      "[step: 1283] loss: 129.09378051757812\n",
      "[step: 1283] loss: 0.021583104506134987\n",
      "[step: 1284] loss: 128.8004150390625\n",
      "[step: 1284] loss: 0.021603822708129883\n",
      "[step: 1285] loss: 128.62315368652344\n",
      "[step: 1285] loss: 0.02159108594059944\n",
      "[step: 1286] loss: 128.6323699951172\n",
      "[step: 1286] loss: 0.02156613953411579\n",
      "[step: 1287] loss: 128.65841674804688\n",
      "[step: 1287] loss: 0.021566126495599747\n",
      "[step: 1288] loss: 131.1485595703125\n",
      "[step: 1288] loss: 0.02156827412545681\n",
      "[step: 1289] loss: 148.8935546875\n",
      "[step: 1289] loss: 0.02155420370399952\n",
      "[step: 1290] loss: 173.62997436523438\n",
      "[step: 1290] loss: 0.02154572308063507\n",
      "[step: 1291] loss: 175.75999450683594\n",
      "[step: 1291] loss: 0.021548988297581673\n",
      "[step: 1292] loss: 143.79949951171875\n",
      "[step: 1292] loss: 0.02154482714831829\n",
      "[step: 1293] loss: 128.79165649414062\n",
      "[step: 1293] loss: 0.021534502506256104\n",
      "[step: 1294] loss: 141.55850219726562\n",
      "[step: 1294] loss: 0.021532690152525902\n",
      "[step: 1295] loss: 155.9585418701172\n",
      "[step: 1295] loss: 0.021534113213419914\n",
      "[step: 1296] loss: 150.20416259765625\n",
      "[step: 1296] loss: 0.02152848429977894\n",
      "[step: 1297] loss: 132.38656616210938\n",
      "[step: 1297] loss: 0.021522734314203262\n",
      "[step: 1298] loss: 129.96852111816406\n",
      "[step: 1298] loss: 0.021523112431168556\n",
      "[step: 1299] loss: 141.42010498046875\n",
      "[step: 1299] loss: 0.021523000672459602\n",
      "[step: 1300] loss: 145.16542053222656\n",
      "[step: 1300] loss: 0.021518664434552193\n",
      "[step: 1301] loss: 136.699951171875\n",
      "[step: 1301] loss: 0.021515171974897385\n",
      "[step: 1302] loss: 128.59359741210938\n",
      "[step: 1302] loss: 0.02151462994515896\n",
      "[step: 1303] loss: 131.30517578125\n",
      "[step: 1303] loss: 0.021514257416129112\n",
      "[step: 1304] loss: 138.29910278320312\n",
      "[step: 1304] loss: 0.021512839943170547\n",
      "[step: 1305] loss: 137.6947784423828\n",
      "[step: 1305] loss: 0.021510567516088486\n",
      "[step: 1306] loss: 131.7244110107422\n",
      "[step: 1306] loss: 0.0215079877525568\n",
      "[step: 1307] loss: 128.01580810546875\n",
      "[step: 1307] loss: 0.021507252007722855\n",
      "[step: 1308] loss: 130.42137145996094\n",
      "[step: 1308] loss: 0.02150820568203926\n",
      "[step: 1309] loss: 134.36473083496094\n",
      "[step: 1309] loss: 0.02150692418217659\n",
      "[step: 1310] loss: 134.0779266357422\n",
      "[step: 1310] loss: 0.02150338515639305\n",
      "[step: 1311] loss: 130.63912963867188\n",
      "[step: 1311] loss: 0.02150234580039978\n",
      "[step: 1312] loss: 127.90557098388672\n",
      "[step: 1312] loss: 0.021503867581486702\n",
      "[step: 1313] loss: 128.5883331298828\n",
      "[step: 1313] loss: 0.021503038704395294\n",
      "[step: 1314] loss: 131.02072143554688\n",
      "[step: 1314] loss: 0.0214998722076416\n",
      "[step: 1315] loss: 131.99514770507812\n",
      "[step: 1315] loss: 0.021498948335647583\n",
      "[step: 1316] loss: 130.72300720214844\n",
      "[step: 1316] loss: 0.021500006318092346\n",
      "[step: 1317] loss: 128.52639770507812\n",
      "[step: 1317] loss: 0.021499082446098328\n",
      "[step: 1318] loss: 127.4671630859375\n",
      "[step: 1318] loss: 0.021496661007404327\n",
      "[step: 1319] loss: 128.03500366210938\n",
      "[step: 1319] loss: 0.02149600349366665\n",
      "[step: 1320] loss: 129.30426025390625\n",
      "[step: 1320] loss: 0.021496376022696495\n",
      "[step: 1321] loss: 130.19190979003906\n",
      "[step: 1321] loss: 0.021495316177606583\n",
      "[step: 1322] loss: 130.01693725585938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1322] loss: 0.02149353176355362\n",
      "[step: 1323] loss: 129.1678009033203\n",
      "[step: 1323] loss: 0.02149282954633236\n",
      "[step: 1324] loss: 128.07058715820312\n",
      "[step: 1324] loss: 0.02149263024330139\n",
      "[step: 1325] loss: 127.3113784790039\n",
      "[step: 1325] loss: 0.021491581574082375\n",
      "[step: 1326] loss: 127.061767578125\n",
      "[step: 1326] loss: 0.021490223705768585\n",
      "[step: 1327] loss: 127.26451110839844\n",
      "[step: 1327] loss: 0.021489331498742104\n",
      "[step: 1328] loss: 127.73368835449219\n",
      "[step: 1328] loss: 0.021488619968295097\n",
      "[step: 1329] loss: 128.2710418701172\n",
      "[step: 1329] loss: 0.021487627178430557\n",
      "[step: 1330] loss: 128.8170928955078\n",
      "[step: 1330] loss: 0.021486470475792885\n",
      "[step: 1331] loss: 129.2587890625\n",
      "[step: 1331] loss: 0.021485313773155212\n",
      "[step: 1332] loss: 129.75604248046875\n",
      "[step: 1332] loss: 0.021484194323420525\n",
      "[step: 1333] loss: 130.2361602783203\n",
      "[step: 1333] loss: 0.021483080461621284\n",
      "[step: 1334] loss: 131.03790283203125\n",
      "[step: 1334] loss: 0.021481895819306374\n",
      "[step: 1335] loss: 132.03575134277344\n",
      "[step: 1335] loss: 0.02148052304983139\n",
      "[step: 1336] loss: 133.8939208984375\n",
      "[step: 1336] loss: 0.021479081362485886\n",
      "[step: 1337] loss: 136.48361206054688\n",
      "[step: 1337] loss: 0.021477799862623215\n",
      "[step: 1338] loss: 141.36981201171875\n",
      "[step: 1338] loss: 0.021476346999406815\n",
      "[step: 1339] loss: 147.95407104492188\n",
      "[step: 1339] loss: 0.021474646404385567\n",
      "[step: 1340] loss: 160.215576171875\n",
      "[step: 1340] loss: 0.021472929045557976\n",
      "[step: 1341] loss: 174.2685546875\n",
      "[step: 1341] loss: 0.021471261978149414\n",
      "[step: 1342] loss: 196.9549560546875\n",
      "[step: 1342] loss: 0.021469425410032272\n",
      "[step: 1343] loss: 208.15773010253906\n",
      "[step: 1343] loss: 0.02146734669804573\n",
      "[step: 1344] loss: 212.31439208984375\n",
      "[step: 1344] loss: 0.021465221419930458\n",
      "[step: 1345] loss: 183.83648681640625\n",
      "[step: 1345] loss: 0.021463053300976753\n",
      "[step: 1346] loss: 147.80587768554688\n",
      "[step: 1346] loss: 0.021460654214024544\n",
      "[step: 1347] loss: 128.70278930664062\n",
      "[step: 1347] loss: 0.021458039060235023\n",
      "[step: 1348] loss: 139.7984619140625\n",
      "[step: 1348] loss: 0.02145528793334961\n",
      "[step: 1349] loss: 159.0230712890625\n",
      "[step: 1349] loss: 0.02145235799252987\n",
      "[step: 1350] loss: 156.15896606445312\n",
      "[step: 1350] loss: 0.02144918218255043\n",
      "[step: 1351] loss: 137.3828582763672\n",
      "[step: 1351] loss: 0.02144574001431465\n",
      "[step: 1352] loss: 128.63641357421875\n",
      "[step: 1352] loss: 0.02144201286137104\n",
      "[step: 1353] loss: 138.4378662109375\n",
      "[step: 1353] loss: 0.02143806405365467\n",
      "[step: 1354] loss: 147.48448181152344\n",
      "[step: 1354] loss: 0.021433716639876366\n",
      "[step: 1355] loss: 140.289306640625\n",
      "[step: 1355] loss: 0.021428991109132767\n",
      "[step: 1356] loss: 129.6996612548828\n",
      "[step: 1356] loss: 0.02142387442290783\n",
      "[step: 1357] loss: 130.4560546875\n",
      "[step: 1357] loss: 0.02141827531158924\n",
      "[step: 1358] loss: 138.10472106933594\n",
      "[step: 1358] loss: 0.02141215279698372\n",
      "[step: 1359] loss: 139.60733032226562\n",
      "[step: 1359] loss: 0.0214054174721241\n",
      "[step: 1360] loss: 132.3781280517578\n",
      "[step: 1360] loss: 0.021398019045591354\n",
      "[step: 1361] loss: 128.05145263671875\n",
      "[step: 1361] loss: 0.021389905363321304\n",
      "[step: 1362] loss: 131.12911987304688\n",
      "[step: 1362] loss: 0.021380934864282608\n",
      "[step: 1363] loss: 135.36082458496094\n",
      "[step: 1363] loss: 0.02137099578976631\n",
      "[step: 1364] loss: 134.26239013671875\n",
      "[step: 1364] loss: 0.0213600043207407\n",
      "[step: 1365] loss: 129.67506408691406\n",
      "[step: 1365] loss: 0.02134779840707779\n",
      "[step: 1366] loss: 127.78177642822266\n",
      "[step: 1366] loss: 0.021334240213036537\n",
      "[step: 1367] loss: 129.97581481933594\n",
      "[step: 1367] loss: 0.02131921984255314\n",
      "[step: 1368] loss: 132.3426971435547\n",
      "[step: 1368] loss: 0.021302616223692894\n",
      "[step: 1369] loss: 131.924560546875\n",
      "[step: 1369] loss: 0.02128426544368267\n",
      "[step: 1370] loss: 129.28948974609375\n",
      "[step: 1370] loss: 0.02126411907374859\n",
      "[step: 1371] loss: 127.43102264404297\n",
      "[step: 1371] loss: 0.02124216966331005\n",
      "[step: 1372] loss: 127.79017639160156\n",
      "[step: 1372] loss: 0.021218571811914444\n",
      "[step: 1373] loss: 129.41268920898438\n",
      "[step: 1373] loss: 0.021193737164139748\n",
      "[step: 1374] loss: 130.6464385986328\n",
      "[step: 1374] loss: 0.021168261766433716\n",
      "[step: 1375] loss: 130.39503479003906\n",
      "[step: 1375] loss: 0.021143117919564247\n",
      "[step: 1376] loss: 129.1972198486328\n",
      "[step: 1376] loss: 0.021119585260748863\n",
      "[step: 1377] loss: 127.7901611328125\n",
      "[step: 1377] loss: 0.021098852157592773\n",
      "[step: 1378] loss: 126.97454833984375\n",
      "[step: 1378] loss: 0.021081553772091866\n",
      "[step: 1379] loss: 126.93656921386719\n",
      "[step: 1379] loss: 0.021066846325993538\n",
      "[step: 1380] loss: 127.47636413574219\n",
      "[step: 1380] loss: 0.021051736548542976\n",
      "[step: 1381] loss: 128.22427368164062\n",
      "[step: 1381] loss: 0.02103201113641262\n",
      "[step: 1382] loss: 128.8417205810547\n",
      "[step: 1382] loss: 0.021003952249884605\n",
      "[step: 1383] loss: 129.2766571044922\n",
      "[step: 1383] loss: 0.020966654643416405\n",
      "[step: 1384] loss: 129.43931579589844\n",
      "[step: 1384] loss: 0.020922360941767693\n",
      "[step: 1385] loss: 129.56829833984375\n",
      "[step: 1385] loss: 0.020874639973044395\n",
      "[step: 1386] loss: 129.5990753173828\n",
      "[step: 1386] loss: 0.02082611247897148\n",
      "[step: 1387] loss: 129.83169555664062\n",
      "[step: 1387] loss: 0.020777303725481033\n",
      "[step: 1388] loss: 130.24365234375\n",
      "[step: 1388] loss: 0.02072666957974434\n",
      "[step: 1389] loss: 131.24542236328125\n",
      "[step: 1389] loss: 0.020671676844358444\n",
      "[step: 1390] loss: 132.85147094726562\n",
      "[step: 1390] loss: 0.020609339699149132\n",
      "[step: 1391] loss: 136.1481170654297\n",
      "[step: 1391] loss: 0.020537327975034714\n",
      "[step: 1392] loss: 141.57774353027344\n",
      "[step: 1392] loss: 0.02045416831970215\n",
      "[step: 1393] loss: 152.57749938964844\n",
      "[step: 1393] loss: 0.0203599464148283\n",
      "[step: 1394] loss: 169.4640350341797\n",
      "[step: 1394] loss: 0.02025633677840233\n",
      "[step: 1395] loss: 202.39968872070312\n",
      "[step: 1395] loss: 0.02014542743563652\n",
      "[step: 1396] loss: 238.2854461669922\n",
      "[step: 1396] loss: 0.020026782527565956\n",
      "[step: 1397] loss: 282.8682861328125\n",
      "[step: 1397] loss: 0.019895602017641068\n",
      "[step: 1398] loss: 269.4530944824219\n",
      "[step: 1398] loss: 0.01974601484835148\n",
      "[step: 1399] loss: 209.22988891601562\n",
      "[step: 1399] loss: 0.019577590748667717\n",
      "[step: 1400] loss: 140.27561950683594\n",
      "[step: 1400] loss: 0.01939561776816845\n",
      "[step: 1401] loss: 140.77566528320312\n",
      "[step: 1401] loss: 0.01920667290687561\n",
      "[step: 1402] loss: 185.07861328125\n",
      "[step: 1402] loss: 0.019016854465007782\n",
      "[step: 1403] loss: 181.7197723388672\n",
      "[step: 1403] loss: 0.018831897526979446\n",
      "[step: 1404] loss: 138.69126892089844\n",
      "[step: 1404] loss: 0.018655261024832726\n",
      "[step: 1405] loss: 137.421142578125\n",
      "[step: 1405] loss: 0.01848723366856575\n",
      "[step: 1406] loss: 165.59129333496094\n",
      "[step: 1406] loss: 0.018325896933674812\n",
      "[step: 1407] loss: 155.28317260742188\n",
      "[step: 1407] loss: 0.018166271969676018\n",
      "[step: 1408] loss: 131.02383422851562\n",
      "[step: 1408] loss: 0.01799970492720604\n",
      "[step: 1409] loss: 143.1012725830078\n",
      "[step: 1409] loss: 0.01782779023051262\n",
      "[step: 1410] loss: 155.66217041015625\n",
      "[step: 1410] loss: 0.017677081748843193\n",
      "[step: 1411] loss: 137.3951416015625\n",
      "[step: 1411] loss: 0.017543504014611244\n",
      "[step: 1412] loss: 132.29885864257812\n",
      "[step: 1412] loss: 0.01739494688808918\n",
      "[step: 1413] loss: 145.98165893554688\n",
      "[step: 1413] loss: 0.01722666248679161\n",
      "[step: 1414] loss: 141.8339080810547\n",
      "[step: 1414] loss: 0.01705523021519184\n",
      "[step: 1415] loss: 130.1101837158203\n",
      "[step: 1415] loss: 0.016910405829548836\n",
      "[step: 1416] loss: 136.323486328125\n",
      "[step: 1416] loss: 0.016804587095975876\n",
      "[step: 1417] loss: 141.90725708007812\n",
      "[step: 1417] loss: 0.016711635515093803\n",
      "[step: 1418] loss: 133.1327667236328\n",
      "[step: 1418] loss: 0.01662042923271656\n",
      "[step: 1419] loss: 130.50750732421875\n",
      "[step: 1419] loss: 0.016554739326238632\n",
      "[step: 1420] loss: 137.5118408203125\n",
      "[step: 1420] loss: 0.016531556844711304\n",
      "[step: 1421] loss: 136.30027770996094\n",
      "[step: 1421] loss: 0.016516217961907387\n",
      "[step: 1422] loss: 130.05625915527344\n",
      "[step: 1422] loss: 0.01645248755812645\n",
      "[step: 1423] loss: 131.25650024414062\n",
      "[step: 1423] loss: 0.01634303107857704\n",
      "[step: 1424] loss: 135.27174377441406\n",
      "[step: 1424] loss: 0.01620469242334366\n",
      "[step: 1425] loss: 132.9149169921875\n",
      "[step: 1425] loss: 0.01602814719080925\n",
      "[step: 1426] loss: 128.74847412109375\n",
      "[step: 1426] loss: 0.01579318381845951\n",
      "[step: 1427] loss: 130.34564208984375\n",
      "[step: 1427] loss: 0.01549054216593504\n",
      "[step: 1428] loss: 133.0006103515625\n",
      "[step: 1428] loss: 0.015107352286577225\n",
      "[step: 1429] loss: 131.788818359375\n",
      "[step: 1429] loss: 0.01495437603443861\n",
      "[step: 1430] loss: 128.6848602294922\n",
      "[step: 1430] loss: 0.014803675003349781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1431] loss: 128.74575805664062\n",
      "[step: 1431] loss: 0.018322177231311798\n",
      "[step: 1432] loss: 130.71255493164062\n",
      "[step: 1432] loss: 0.014147138223052025\n",
      "[step: 1433] loss: 130.97677612304688\n",
      "[step: 1433] loss: 0.01630307361483574\n",
      "[step: 1434] loss: 129.0385284423828\n",
      "[step: 1434] loss: 0.027924364432692528\n",
      "[step: 1435] loss: 127.69547271728516\n",
      "[step: 1435] loss: 0.024781646206974983\n",
      "[step: 1436] loss: 128.45672607421875\n",
      "[step: 1436] loss: 0.017868632450699806\n",
      "[step: 1437] loss: 129.45042419433594\n",
      "[step: 1437] loss: 0.021660655736923218\n",
      "[step: 1438] loss: 129.4049072265625\n",
      "[step: 1438] loss: 0.026263507083058357\n",
      "[step: 1439] loss: 128.29388427734375\n",
      "[step: 1439] loss: 0.02230914682149887\n",
      "[step: 1440] loss: 127.26849365234375\n",
      "[step: 1440] loss: 0.018662771210074425\n",
      "[step: 1441] loss: 127.23728942871094\n",
      "[step: 1441] loss: 0.020424436777830124\n",
      "[step: 1442] loss: 127.84590148925781\n",
      "[step: 1442] loss: 0.021315347403287888\n",
      "[step: 1443] loss: 128.38400268554688\n",
      "[step: 1443] loss: 0.018624361604452133\n",
      "[step: 1444] loss: 128.4381866455078\n",
      "[step: 1444] loss: 0.01994691975414753\n",
      "[step: 1445] loss: 127.96684265136719\n",
      "[step: 1445] loss: 0.019534530118107796\n",
      "[step: 1446] loss: 127.32254791259766\n",
      "[step: 1446] loss: 0.01676734909415245\n",
      "[step: 1447] loss: 126.82318115234375\n",
      "[step: 1447] loss: 0.01787376031279564\n",
      "[step: 1448] loss: 126.51617431640625\n",
      "[step: 1448] loss: 0.018401874229311943\n",
      "[step: 1449] loss: 126.48789978027344\n",
      "[step: 1449] loss: 0.018009386956691742\n",
      "[step: 1450] loss: 126.64620971679688\n",
      "[step: 1450] loss: 0.01807592250406742\n",
      "[step: 1451] loss: 126.89530944824219\n",
      "[step: 1451] loss: 0.018378635868430138\n",
      "[step: 1452] loss: 127.25190734863281\n",
      "[step: 1452] loss: 0.017916744574904442\n",
      "[step: 1453] loss: 127.72882080078125\n",
      "[step: 1453] loss: 0.017049221321940422\n",
      "[step: 1454] loss: 128.3880615234375\n",
      "[step: 1454] loss: 0.016666609793901443\n",
      "[step: 1455] loss: 129.48394775390625\n",
      "[step: 1455] loss: 0.01644051820039749\n",
      "[step: 1456] loss: 131.1910400390625\n",
      "[step: 1456] loss: 0.015774963423609734\n",
      "[step: 1457] loss: 134.54281616210938\n",
      "[step: 1457] loss: 0.015852490440011024\n",
      "[step: 1458] loss: 140.10537719726562\n",
      "[step: 1458] loss: 0.015679145231842995\n",
      "[step: 1459] loss: 151.61769104003906\n",
      "[step: 1459] loss: 0.014578905887901783\n",
      "[step: 1460] loss: 170.43431091308594\n",
      "[step: 1460] loss: 0.015057425014674664\n",
      "[step: 1461] loss: 209.99029541015625\n",
      "[step: 1461] loss: 0.02239335887134075\n",
      "[step: 1462] loss: 254.74752807617188\n",
      "[step: 1462] loss: 0.013904403895139694\n",
      "[step: 1463] loss: 319.7759704589844\n",
      "[step: 1463] loss: 0.030928712338209152\n",
      "[step: 1464] loss: 306.0694885253906\n",
      "[step: 1464] loss: 0.03878151625394821\n",
      "[step: 1465] loss: 229.23590087890625\n",
      "[step: 1465] loss: 0.03604290261864662\n",
      "[step: 1466] loss: 140.400390625\n",
      "[step: 1466] loss: 0.023261651396751404\n",
      "[step: 1467] loss: 152.15243530273438\n",
      "[step: 1467] loss: 0.02472984418272972\n",
      "[step: 1468] loss: 208.885498046875\n",
      "[step: 1468] loss: 0.033929742872714996\n",
      "[step: 1469] loss: 181.85926818847656\n",
      "[step: 1469] loss: 0.033754557371139526\n",
      "[step: 1470] loss: 133.00814819335938\n",
      "[step: 1470] loss: 0.02581961639225483\n",
      "[step: 1471] loss: 158.93063354492188\n",
      "[step: 1471] loss: 0.022698204964399338\n",
      "[step: 1472] loss: 179.30499267578125\n",
      "[step: 1472] loss: 0.02667905203998089\n",
      "[step: 1473] loss: 141.89117431640625\n",
      "[step: 1473] loss: 0.029273495078086853\n",
      "[step: 1474] loss: 139.79501342773438\n",
      "[step: 1474] loss: 0.026529433205723763\n",
      "[step: 1475] loss: 166.99732971191406\n",
      "[step: 1475] loss: 0.023233724758028984\n",
      "[step: 1476] loss: 152.41384887695312\n",
      "[step: 1476] loss: 0.023532401770353317\n",
      "[step: 1477] loss: 132.65159606933594\n",
      "[step: 1477] loss: 0.025390615686774254\n",
      "[step: 1478] loss: 159.62088012695312\n",
      "[step: 1478] loss: 0.02536860667169094\n",
      "[step: 1479] loss: 153.58750915527344\n",
      "[step: 1479] loss: 0.023732392117381096\n",
      "[step: 1480] loss: 134.90798950195312\n",
      "[step: 1480] loss: 0.022832432761788368\n",
      "[step: 1481] loss: 150.34393310546875\n",
      "[step: 1481] loss: 0.02326463907957077\n",
      "[step: 1482] loss: 147.1116943359375\n",
      "[step: 1482] loss: 0.023736845701932907\n",
      "[step: 1483] loss: 138.26016235351562\n",
      "[step: 1483] loss: 0.02341373823583126\n",
      "[step: 1484] loss: 137.2086639404297\n",
      "[step: 1484] loss: 0.022776953876018524\n",
      "[step: 1485] loss: 144.82174682617188\n",
      "[step: 1485] loss: 0.022515369579195976\n",
      "[step: 1486] loss: 132.60623168945312\n",
      "[step: 1486] loss: 0.02263055182993412\n",
      "[step: 1487] loss: 136.6172637939453\n",
      "[step: 1487] loss: 0.022719239816069603\n",
      "[step: 1488] loss: 136.78897094726562\n",
      "[step: 1488] loss: 0.022570546716451645\n",
      "[step: 1489] loss: 135.84423828125\n",
      "[step: 1489] loss: 0.02230789326131344\n",
      "[step: 1490] loss: 133.35227966308594\n",
      "[step: 1490] loss: 0.022150671109557152\n",
      "[step: 1491] loss: 132.5264892578125\n",
      "[step: 1491] loss: 0.02216341532766819\n",
      "[step: 1492] loss: 137.21212768554688\n",
      "[step: 1492] loss: 0.022212568670511246\n",
      "[step: 1493] loss: 131.46681213378906\n",
      "[step: 1493] loss: 0.02214621566236019\n",
      "[step: 1494] loss: 131.6368408203125\n",
      "[step: 1494] loss: 0.021987732499837875\n",
      "[step: 1495] loss: 131.33843994140625\n",
      "[step: 1495] loss: 0.021888436749577522\n",
      "[step: 1496] loss: 132.62002563476562\n",
      "[step: 1496] loss: 0.021907372400164604\n",
      "[step: 1497] loss: 130.55197143554688\n",
      "[step: 1497] loss: 0.02193951979279518\n",
      "[step: 1498] loss: 129.29623413085938\n",
      "[step: 1498] loss: 0.021884774789214134\n",
      "[step: 1499] loss: 131.0753173828125\n",
      "[step: 1499] loss: 0.02178608812391758\n",
      "RMSE: 0.05401621386408806\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.1\n",
    "iterations = 1500\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL만들기\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
