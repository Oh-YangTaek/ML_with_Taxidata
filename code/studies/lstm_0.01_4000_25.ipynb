{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5a4fa71c421f>:89: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 19893.150390625\n",
      "[step: 0] loss: 0.3186686038970947\n",
      "[step: 1] loss: 13350.0615234375\n",
      "[step: 1] loss: 0.22086718678474426\n",
      "[step: 2] loss: 9445.087890625\n",
      "[step: 2] loss: 0.1540890485048294\n",
      "[step: 3] loss: 6837.685546875\n",
      "[step: 3] loss: 0.11645430326461792\n",
      "[step: 4] loss: 5063.9072265625\n",
      "[step: 4] loss: 0.09055017679929733\n",
      "[step: 5] loss: 3818.775634765625\n",
      "[step: 5] loss: 0.07259102165699005\n",
      "[step: 6] loss: 3022.6396484375\n",
      "[step: 6] loss: 0.05909343436360359\n",
      "[step: 7] loss: 2605.7705078125\n",
      "[step: 7] loss: 0.04891415312886238\n",
      "[step: 8] loss: 2438.03076171875\n",
      "[step: 8] loss: 0.041822079569101334\n",
      "[step: 9] loss: 2329.1484375\n",
      "[step: 9] loss: 0.03690825775265694\n",
      "[step: 10] loss: 2277.142333984375\n",
      "[step: 10] loss: 0.03307557478547096\n",
      "[step: 11] loss: 2217.0751953125\n",
      "[step: 11] loss: 0.029616810381412506\n",
      "[step: 12] loss: 2113.99951171875\n",
      "[step: 12] loss: 0.026760414242744446\n",
      "[step: 13] loss: 2005.900390625\n",
      "[step: 13] loss: 0.02489929459989071\n",
      "[step: 14] loss: 1916.9404296875\n",
      "[step: 14] loss: 0.023730628192424774\n",
      "[step: 15] loss: 1818.877197265625\n",
      "[step: 15] loss: 0.0228007473051548\n",
      "[step: 16] loss: 1704.4569091796875\n",
      "[step: 16] loss: 0.022131014615297318\n",
      "[step: 17] loss: 1610.5074462890625\n",
      "[step: 17] loss: 0.02174939028918743\n",
      "[step: 18] loss: 1524.145263671875\n",
      "[step: 18] loss: 0.021487219259142876\n",
      "[step: 19] loss: 1436.02783203125\n",
      "[step: 19] loss: 0.021128976717591286\n",
      "[step: 20] loss: 1352.82080078125\n",
      "[step: 20] loss: 0.020589293912053108\n",
      "[step: 21] loss: 1276.14501953125\n",
      "[step: 21] loss: 0.019955571740865707\n",
      "[step: 22] loss: 1208.6793212890625\n",
      "[step: 22] loss: 0.019370075315237045\n",
      "[step: 23] loss: 1127.85498046875\n",
      "[step: 23] loss: 0.018907519057393074\n",
      "[step: 24] loss: 1063.4737548828125\n",
      "[step: 24] loss: 0.018553031608462334\n",
      "[step: 25] loss: 1019.2478637695312\n",
      "[step: 25] loss: 0.018242839723825455\n",
      "[step: 26] loss: 974.0534057617188\n",
      "[step: 26] loss: 0.01791306957602501\n",
      "[step: 27] loss: 949.960205078125\n",
      "[step: 27] loss: 0.017541201785206795\n",
      "[step: 28] loss: 910.77490234375\n",
      "[step: 28] loss: 0.017154738306999207\n",
      "[step: 29] loss: 894.179443359375\n",
      "[step: 29] loss: 0.016795318573713303\n",
      "[step: 30] loss: 857.7942504882812\n",
      "[step: 30] loss: 0.0164861511439085\n",
      "[step: 31] loss: 836.7630004882812\n",
      "[step: 31] loss: 0.016231190413236618\n",
      "[step: 32] loss: 795.3461303710938\n",
      "[step: 32] loss: 0.016003482043743134\n",
      "[step: 33] loss: 769.819091796875\n",
      "[step: 33] loss: 0.015763813629746437\n",
      "[step: 34] loss: 734.9259033203125\n",
      "[step: 34] loss: 0.015509688295423985\n",
      "[step: 35] loss: 706.7092895507812\n",
      "[step: 35] loss: 0.015237526036798954\n",
      "[step: 36] loss: 676.4248657226562\n",
      "[step: 36] loss: 0.014966819435358047\n",
      "[step: 37] loss: 651.2777709960938\n",
      "[step: 37] loss: 0.014702011831104755\n",
      "[step: 38] loss: 627.49755859375\n",
      "[step: 38] loss: 0.01443954836577177\n",
      "[step: 39] loss: 605.6290893554688\n",
      "[step: 39] loss: 0.014177320525050163\n",
      "[step: 40] loss: 588.7785034179688\n",
      "[step: 40] loss: 0.01390831172466278\n",
      "[step: 41] loss: 567.1177978515625\n",
      "[step: 41] loss: 0.013642554171383381\n",
      "[step: 42] loss: 551.3355712890625\n",
      "[step: 42] loss: 0.013396013528108597\n",
      "[step: 43] loss: 529.98388671875\n",
      "[step: 43] loss: 0.013167253695428371\n",
      "[step: 44] loss: 512.2640991210938\n",
      "[step: 44] loss: 0.012961826287209988\n",
      "[step: 45] loss: 491.6121520996094\n",
      "[step: 45] loss: 0.012777159921824932\n",
      "[step: 46] loss: 471.4183044433594\n",
      "[step: 46] loss: 0.012603542767465115\n",
      "[step: 47] loss: 452.98968505859375\n",
      "[step: 47] loss: 0.012433669529855251\n",
      "[step: 48] loss: 434.39825439453125\n",
      "[step: 48] loss: 0.012231777422130108\n",
      "[step: 49] loss: 419.9205322265625\n",
      "[step: 49] loss: 0.012029889971017838\n",
      "[step: 50] loss: 406.322265625\n",
      "[step: 50] loss: 0.0118536576628685\n",
      "[step: 51] loss: 393.92572021484375\n",
      "[step: 51] loss: 0.011699935421347618\n",
      "[step: 52] loss: 381.8406982421875\n",
      "[step: 52] loss: 0.011551613919436932\n",
      "[step: 53] loss: 371.09100341796875\n",
      "[step: 53] loss: 0.011361795477569103\n",
      "[step: 54] loss: 361.2391662597656\n",
      "[step: 54] loss: 0.011183518916368484\n",
      "[step: 55] loss: 351.2317199707031\n",
      "[step: 55] loss: 0.011043795384466648\n",
      "[step: 56] loss: 342.7249450683594\n",
      "[step: 56] loss: 0.010911254212260246\n",
      "[step: 57] loss: 334.62640380859375\n",
      "[step: 57] loss: 0.01076070312410593\n",
      "[step: 58] loss: 326.7800598144531\n",
      "[step: 58] loss: 0.010590257123112679\n",
      "[step: 59] loss: 319.01446533203125\n",
      "[step: 59] loss: 0.01045505702495575\n",
      "[step: 60] loss: 311.0785217285156\n",
      "[step: 60] loss: 0.010345038957893848\n",
      "[step: 61] loss: 303.57257080078125\n",
      "[step: 61] loss: 0.010214692912995815\n",
      "[step: 62] loss: 296.472900390625\n",
      "[step: 62] loss: 0.010078215040266514\n",
      "[step: 63] loss: 290.0531005859375\n",
      "[step: 63] loss: 0.009947766549885273\n",
      "[step: 64] loss: 284.07147216796875\n",
      "[step: 64] loss: 0.00984097458422184\n",
      "[step: 65] loss: 279.07623291015625\n",
      "[step: 65] loss: 0.00974405650049448\n",
      "[step: 66] loss: 274.16192626953125\n",
      "[step: 66] loss: 0.009627782739698887\n",
      "[step: 67] loss: 270.8475646972656\n",
      "[step: 67] loss: 0.009510112926363945\n",
      "[step: 68] loss: 269.18914794921875\n",
      "[step: 68] loss: 0.009392580948770046\n",
      "[step: 69] loss: 271.6285705566406\n",
      "[step: 69] loss: 0.009286894463002682\n",
      "[step: 70] loss: 272.77520751953125\n",
      "[step: 70] loss: 0.009198646061122417\n",
      "[step: 71] loss: 268.463623046875\n",
      "[step: 71] loss: 0.009122402407228947\n",
      "[step: 72] loss: 251.72589111328125\n",
      "[step: 72] loss: 0.009066061116755009\n",
      "[step: 73] loss: 248.56829833984375\n",
      "[step: 73] loss: 0.009021886624395847\n",
      "[step: 74] loss: 254.57040405273438\n",
      "[step: 74] loss: 0.008985219523310661\n",
      "[step: 75] loss: 245.4598388671875\n",
      "[step: 75] loss: 0.008855167776346207\n",
      "[step: 76] loss: 236.9104461669922\n",
      "[step: 76] loss: 0.008735746145248413\n",
      "[step: 77] loss: 240.10598754882812\n",
      "[step: 77] loss: 0.008702164515852928\n",
      "[step: 78] loss: 236.78164672851562\n",
      "[step: 78] loss: 0.00867153238505125\n",
      "[step: 79] loss: 229.33609008789062\n",
      "[step: 79] loss: 0.008585873059928417\n",
      "[step: 80] loss: 229.70147705078125\n",
      "[step: 80] loss: 0.008495034649968147\n",
      "[step: 81] loss: 229.3919677734375\n",
      "[step: 81] loss: 0.00847066380083561\n",
      "[step: 82] loss: 224.11279296875\n",
      "[step: 82] loss: 0.008449136279523373\n",
      "[step: 83] loss: 221.77810668945312\n",
      "[step: 83] loss: 0.00836303923279047\n",
      "[step: 84] loss: 222.65565490722656\n",
      "[step: 84] loss: 0.008298064582049847\n",
      "[step: 85] loss: 219.84837341308594\n",
      "[step: 85] loss: 0.00828198716044426\n",
      "[step: 86] loss: 215.642822265625\n",
      "[step: 86] loss: 0.008251572959125042\n",
      "[step: 87] loss: 215.60488891601562\n",
      "[step: 87] loss: 0.008190102875232697\n",
      "[step: 88] loss: 215.36158752441406\n",
      "[step: 88] loss: 0.00814166571944952\n",
      "[step: 89] loss: 211.75045776367188\n",
      "[step: 89] loss: 0.008125090040266514\n",
      "[step: 90] loss: 208.86892700195312\n",
      "[step: 90] loss: 0.008106417022645473\n",
      "[step: 91] loss: 208.51991271972656\n",
      "[step: 91] loss: 0.008065380156040192\n",
      "[step: 92] loss: 208.65087890625\n",
      "[step: 92] loss: 0.008021023124456406\n",
      "[step: 93] loss: 208.1912841796875\n",
      "[step: 93] loss: 0.007995846681296825\n",
      "[step: 94] loss: 204.89044189453125\n",
      "[step: 94] loss: 0.007984058000147343\n",
      "[step: 95] loss: 201.60682678222656\n",
      "[step: 95] loss: 0.007970449514687061\n",
      "[step: 96] loss: 201.8446044921875\n",
      "[step: 96] loss: 0.007945156656205654\n",
      "[step: 97] loss: 200.74444580078125\n",
      "[step: 97] loss: 0.007913520559668541\n",
      "[step: 98] loss: 200.31439208984375\n",
      "[step: 98] loss: 0.007885369472205639\n",
      "[step: 99] loss: 197.4114990234375\n",
      "[step: 99] loss: 0.007867726497352123\n",
      "[step: 100] loss: 196.14450073242188\n",
      "[step: 100] loss: 0.007857320830225945\n",
      "[step: 101] loss: 194.29989624023438\n",
      "[step: 101] loss: 0.007850825786590576\n",
      "[step: 102] loss: 194.58877563476562\n",
      "[step: 102] loss: 0.007842890918254852\n",
      "[step: 103] loss: 193.88441467285156\n",
      "[step: 103] loss: 0.00783182680606842\n",
      "[step: 104] loss: 195.160400390625\n",
      "[step: 104] loss: 0.00781536940485239\n",
      "[step: 105] loss: 198.86680603027344\n",
      "[step: 105] loss: 0.007794530130922794\n",
      "[step: 106] loss: 201.79144287109375\n",
      "[step: 106] loss: 0.0077714077197015285\n",
      "[step: 107] loss: 199.65786743164062\n",
      "[step: 107] loss: 0.007751800585538149\n",
      "[step: 108] loss: 193.87347412109375\n",
      "[step: 108] loss: 0.007737749256193638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 109] loss: 187.52374267578125\n",
      "[step: 109] loss: 0.007729025557637215\n",
      "[step: 110] loss: 185.93887329101562\n",
      "[step: 110] loss: 0.007723530754446983\n",
      "[step: 111] loss: 187.21856689453125\n",
      "[step: 111] loss: 0.00771965179592371\n",
      "[step: 112] loss: 188.7935791015625\n",
      "[step: 112] loss: 0.00771602476015687\n",
      "[step: 113] loss: 187.3172149658203\n",
      "[step: 113] loss: 0.007710547186434269\n",
      "[step: 114] loss: 185.3411102294922\n",
      "[step: 114] loss: 0.007701356895267963\n",
      "[step: 115] loss: 182.25384521484375\n",
      "[step: 115] loss: 0.0076873572543263435\n",
      "[step: 116] loss: 180.2296142578125\n",
      "[step: 116] loss: 0.007671221625059843\n",
      "[step: 117] loss: 180.17929077148438\n",
      "[step: 117] loss: 0.007656156551092863\n",
      "[step: 118] loss: 180.7012176513672\n",
      "[step: 118] loss: 0.0076451716013252735\n",
      "[step: 119] loss: 181.52059936523438\n",
      "[step: 119] loss: 0.007638854905962944\n",
      "[step: 120] loss: 181.39195251464844\n",
      "[step: 120] loss: 0.00763559527695179\n",
      "[step: 121] loss: 181.34835815429688\n",
      "[step: 121] loss: 0.007633374072611332\n",
      "[step: 122] loss: 180.8731231689453\n",
      "[step: 122] loss: 0.0076298462226986885\n",
      "[step: 123] loss: 179.4691162109375\n",
      "[step: 123] loss: 0.007624114863574505\n",
      "[step: 124] loss: 176.75784301757812\n",
      "[step: 124] loss: 0.007615664042532444\n",
      "[step: 125] loss: 173.2424774169922\n",
      "[step: 125] loss: 0.0076058050617575645\n",
      "[step: 126] loss: 171.969970703125\n",
      "[step: 126] loss: 0.007595865987241268\n",
      "[step: 127] loss: 170.7402801513672\n",
      "[step: 127] loss: 0.007587507367134094\n",
      "[step: 128] loss: 170.81080627441406\n",
      "[step: 128] loss: 0.007581242825835943\n",
      "[step: 129] loss: 171.02114868164062\n",
      "[step: 129] loss: 0.007576780393719673\n",
      "[step: 130] loss: 173.850830078125\n",
      "[step: 130] loss: 0.007573366165161133\n",
      "[step: 131] loss: 176.9026336669922\n",
      "[step: 131] loss: 0.0075702848844230175\n",
      "[step: 132] loss: 188.4517059326172\n",
      "[step: 132] loss: 0.0075670755468308926\n",
      "[step: 133] loss: 196.44537353515625\n",
      "[step: 133] loss: 0.007563325576484203\n",
      "[step: 134] loss: 204.94778442382812\n",
      "[step: 134] loss: 0.007558971643447876\n",
      "[step: 135] loss: 176.83860778808594\n",
      "[step: 135] loss: 0.0075538321398198605\n",
      "[step: 136] loss: 169.5681915283203\n",
      "[step: 136] loss: 0.007548195309937\n",
      "[step: 137] loss: 187.52532958984375\n",
      "[step: 137] loss: 0.007542070932686329\n",
      "[step: 138] loss: 177.7157745361328\n",
      "[step: 138] loss: 0.007535827811807394\n",
      "[step: 139] loss: 165.7474365234375\n",
      "[step: 139] loss: 0.007529660128057003\n",
      "[step: 140] loss: 176.31063842773438\n",
      "[step: 140] loss: 0.007523795124143362\n",
      "[step: 141] loss: 174.81919860839844\n",
      "[step: 141] loss: 0.0075182863511145115\n",
      "[step: 142] loss: 166.82431030273438\n",
      "[step: 142] loss: 0.007513128686696291\n",
      "[step: 143] loss: 170.7576904296875\n",
      "[step: 143] loss: 0.007508278824388981\n",
      "[step: 144] loss: 172.60015869140625\n",
      "[step: 144] loss: 0.007503689732402563\n",
      "[step: 145] loss: 165.75831604003906\n",
      "[step: 145] loss: 0.007499268278479576\n",
      "[step: 146] loss: 164.1767578125\n",
      "[step: 146] loss: 0.007494976744055748\n",
      "[step: 147] loss: 165.88250732421875\n",
      "[step: 147] loss: 0.007490771822631359\n",
      "[step: 148] loss: 164.82733154296875\n",
      "[step: 148] loss: 0.007486636284738779\n",
      "[step: 149] loss: 163.56820678710938\n",
      "[step: 149] loss: 0.007482578977942467\n",
      "[step: 150] loss: 161.14840698242188\n",
      "[step: 150] loss: 0.007478598039597273\n",
      "[step: 151] loss: 160.9901123046875\n",
      "[step: 151] loss: 0.007474686950445175\n",
      "[step: 152] loss: 162.66973876953125\n",
      "[step: 152] loss: 0.007470914628356695\n",
      "[step: 153] loss: 158.54776000976562\n",
      "[step: 153] loss: 0.0074673425406217575\n",
      "[step: 154] loss: 157.3673095703125\n",
      "[step: 154] loss: 0.007464258931577206\n",
      "[step: 155] loss: 158.86924743652344\n",
      "[step: 155] loss: 0.00746225006878376\n",
      "[step: 156] loss: 158.44998168945312\n",
      "[step: 156] loss: 0.007462956011295319\n",
      "[step: 157] loss: 157.79327392578125\n",
      "[step: 157] loss: 0.007470143027603626\n",
      "[step: 158] loss: 156.7994384765625\n",
      "[step: 158] loss: 0.007493369281291962\n",
      "[step: 159] loss: 154.27536010742188\n",
      "[step: 159] loss: 0.0075450134463608265\n",
      "[step: 160] loss: 154.54898071289062\n",
      "[step: 160] loss: 0.007629739586263895\n",
      "[step: 161] loss: 154.68850708007812\n",
      "[step: 161] loss: 0.007652494125068188\n",
      "[step: 162] loss: 155.111328125\n",
      "[step: 162] loss: 0.007544221822172403\n",
      "[step: 163] loss: 154.90744018554688\n",
      "[step: 163] loss: 0.007433086633682251\n",
      "[step: 164] loss: 153.21725463867188\n",
      "[step: 164] loss: 0.007486302405595779\n",
      "[step: 165] loss: 151.93849182128906\n",
      "[step: 165] loss: 0.007551202550530434\n",
      "[step: 166] loss: 151.16053771972656\n",
      "[step: 166] loss: 0.007470082491636276\n",
      "[step: 167] loss: 150.4230499267578\n",
      "[step: 167] loss: 0.0074224090203642845\n",
      "[step: 168] loss: 151.08111572265625\n",
      "[step: 168] loss: 0.007485832087695599\n",
      "[step: 169] loss: 151.13958740234375\n",
      "[step: 169] loss: 0.007474105339497328\n",
      "[step: 170] loss: 152.5860137939453\n",
      "[step: 170] loss: 0.007412009872496128\n",
      "[step: 171] loss: 155.65679931640625\n",
      "[step: 171] loss: 0.007439564913511276\n",
      "[step: 172] loss: 164.82907104492188\n",
      "[step: 172] loss: 0.0074568698182702065\n",
      "[step: 173] loss: 171.93295288085938\n",
      "[step: 173] loss: 0.0074089085683226585\n",
      "[step: 174] loss: 183.79507446289062\n",
      "[step: 174] loss: 0.007411937694996595\n",
      "[step: 175] loss: 166.49156188964844\n",
      "[step: 175] loss: 0.0074353390373289585\n",
      "[step: 176] loss: 149.7967987060547\n",
      "[step: 176] loss: 0.007404688745737076\n",
      "[step: 177] loss: 151.59725952148438\n",
      "[step: 177] loss: 0.0073943049646914005\n",
      "[step: 178] loss: 160.38360595703125\n",
      "[step: 178] loss: 0.007415180094540119\n",
      "[step: 179] loss: 160.38624572753906\n",
      "[step: 179] loss: 0.007398861926048994\n",
      "[step: 180] loss: 148.48553466796875\n",
      "[step: 180] loss: 0.007382275070995092\n",
      "[step: 181] loss: 151.32598876953125\n",
      "[step: 181] loss: 0.007396867964416742\n",
      "[step: 182] loss: 158.9734344482422\n",
      "[step: 182] loss: 0.007391700521111488\n",
      "[step: 183] loss: 157.989013671875\n",
      "[step: 183] loss: 0.007373846136033535\n",
      "[step: 184] loss: 150.41317749023438\n",
      "[step: 184] loss: 0.007380402181297541\n",
      "[step: 185] loss: 146.8136749267578\n",
      "[step: 185] loss: 0.0073827882297337055\n",
      "[step: 186] loss: 153.58111572265625\n",
      "[step: 186] loss: 0.007367794867604971\n",
      "[step: 187] loss: 157.3925018310547\n",
      "[step: 187] loss: 0.007366456091403961\n",
      "[step: 188] loss: 148.62673950195312\n",
      "[step: 188] loss: 0.007371819112449884\n",
      "[step: 189] loss: 147.8565216064453\n",
      "[step: 189] loss: 0.007362745236605406\n",
      "[step: 190] loss: 150.9331817626953\n",
      "[step: 190] loss: 0.007355756592005491\n",
      "[step: 191] loss: 154.4561309814453\n",
      "[step: 191] loss: 0.007359612267464399\n",
      "[step: 192] loss: 149.01687622070312\n",
      "[step: 192] loss: 0.007356834597885609\n",
      "[step: 193] loss: 144.85838317871094\n",
      "[step: 193] loss: 0.007348405662924051\n",
      "[step: 194] loss: 151.6590576171875\n",
      "[step: 194] loss: 0.00734777981415391\n",
      "[step: 195] loss: 148.81109619140625\n",
      "[step: 195] loss: 0.007348795887082815\n",
      "[step: 196] loss: 150.82708740234375\n",
      "[step: 196] loss: 0.007342998404055834\n",
      "[step: 197] loss: 141.51800537109375\n",
      "[step: 197] loss: 0.007338296622037888\n",
      "[step: 198] loss: 146.65631103515625\n",
      "[step: 198] loss: 0.00733886007219553\n",
      "[step: 199] loss: 143.96141052246094\n",
      "[step: 199] loss: 0.007337082177400589\n",
      "[step: 200] loss: 144.6668701171875\n",
      "[step: 200] loss: 0.007331719156354666\n",
      "[step: 201] loss: 143.393798828125\n",
      "[step: 201] loss: 0.007329169195145369\n",
      "[step: 202] loss: 140.76837158203125\n",
      "[step: 202] loss: 0.007328988052904606\n",
      "[step: 203] loss: 141.55491638183594\n",
      "[step: 203] loss: 0.007326159160584211\n",
      "[step: 204] loss: 140.0576171875\n",
      "[step: 204] loss: 0.007321896031498909\n",
      "[step: 205] loss: 139.80728149414062\n",
      "[step: 205] loss: 0.007319939322769642\n",
      "[step: 206] loss: 139.33944702148438\n",
      "[step: 206] loss: 0.007319025695323944\n",
      "[step: 207] loss: 138.72071838378906\n",
      "[step: 207] loss: 0.007316234987229109\n",
      "[step: 208] loss: 138.0262451171875\n",
      "[step: 208] loss: 0.007312677334994078\n",
      "[step: 209] loss: 137.78732299804688\n",
      "[step: 209] loss: 0.007310655899345875\n",
      "[step: 210] loss: 136.8880615234375\n",
      "[step: 210] loss: 0.007309355307370424\n",
      "[step: 211] loss: 137.38217163085938\n",
      "[step: 211] loss: 0.007306917570531368\n",
      "[step: 212] loss: 136.16055297851562\n",
      "[step: 212] loss: 0.007303805090487003\n",
      "[step: 213] loss: 137.5096435546875\n",
      "[step: 213] loss: 0.0073015084490180016\n",
      "[step: 214] loss: 137.69200134277344\n",
      "[step: 214] loss: 0.007299946155399084\n",
      "[step: 215] loss: 142.8173828125\n",
      "[step: 215] loss: 0.007297929376363754\n",
      "[step: 216] loss: 151.66482543945312\n",
      "[step: 216] loss: 0.0072952089831233025\n",
      "[step: 217] loss: 160.9727783203125\n",
      "[step: 217] loss: 0.007292687892913818\n",
      "[step: 218] loss: 181.2625732421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 218] loss: 0.007290774490684271\n",
      "[step: 219] loss: 158.1293487548828\n",
      "[step: 219] loss: 0.007288978900760412\n",
      "[step: 220] loss: 137.6046142578125\n",
      "[step: 220] loss: 0.007286765147000551\n",
      "[step: 221] loss: 149.46331787109375\n",
      "[step: 221] loss: 0.007284283172339201\n",
      "[step: 222] loss: 152.0477752685547\n",
      "[step: 222] loss: 0.007281993515789509\n",
      "[step: 223] loss: 144.03697204589844\n",
      "[step: 223] loss: 0.007280061021447182\n",
      "[step: 224] loss: 138.84719848632812\n",
      "[step: 224] loss: 0.007278169970959425\n",
      "[step: 225] loss: 149.42950439453125\n",
      "[step: 225] loss: 0.007276056334376335\n",
      "[step: 226] loss: 148.7539520263672\n",
      "[step: 226] loss: 0.007273765746504068\n",
      "[step: 227] loss: 139.73953247070312\n",
      "[step: 227] loss: 0.007271537557244301\n",
      "[step: 228] loss: 136.92869567871094\n",
      "[step: 228] loss: 0.00726949330419302\n",
      "[step: 229] loss: 145.20587158203125\n",
      "[step: 229] loss: 0.007267565466463566\n",
      "[step: 230] loss: 141.76705932617188\n",
      "[step: 230] loss: 0.007265596184879541\n",
      "[step: 231] loss: 133.40744018554688\n",
      "[step: 231] loss: 0.007263502571731806\n",
      "[step: 232] loss: 135.74925231933594\n",
      "[step: 232] loss: 0.007261353544890881\n",
      "[step: 233] loss: 141.12356567382812\n",
      "[step: 233] loss: 0.007259225007146597\n",
      "[step: 234] loss: 136.0260772705078\n",
      "[step: 234] loss: 0.007257180754095316\n",
      "[step: 235] loss: 134.4398193359375\n",
      "[step: 235] loss: 0.007255209144204855\n",
      "[step: 236] loss: 133.43821716308594\n",
      "[step: 236] loss: 0.007253257557749748\n",
      "[step: 237] loss: 136.75680541992188\n",
      "[step: 237] loss: 0.007251285482198\n",
      "[step: 238] loss: 133.15843200683594\n",
      "[step: 238] loss: 0.007249269634485245\n",
      "[step: 239] loss: 130.56707763671875\n",
      "[step: 239] loss: 0.007247233297675848\n",
      "[step: 240] loss: 132.45071411132812\n",
      "[step: 240] loss: 0.007245185784995556\n",
      "[step: 241] loss: 133.13433837890625\n",
      "[step: 241] loss: 0.007243158761411905\n",
      "[step: 242] loss: 130.76844787597656\n",
      "[step: 242] loss: 0.007241156417876482\n",
      "[step: 243] loss: 130.21414184570312\n",
      "[step: 243] loss: 0.007239175494760275\n",
      "[step: 244] loss: 128.84857177734375\n",
      "[step: 244] loss: 0.007237219717353582\n",
      "[step: 245] loss: 130.73419189453125\n",
      "[step: 245] loss: 0.007235275581479073\n",
      "[step: 246] loss: 130.42825317382812\n",
      "[step: 246] loss: 0.007233343552798033\n",
      "[step: 247] loss: 128.20120239257812\n",
      "[step: 247] loss: 0.007231427356600761\n",
      "[step: 248] loss: 128.32984924316406\n",
      "[step: 248] loss: 0.007229515351355076\n",
      "[step: 249] loss: 126.81572723388672\n",
      "[step: 249] loss: 0.007227618247270584\n",
      "[step: 250] loss: 128.00381469726562\n",
      "[step: 250] loss: 0.007225739769637585\n",
      "[step: 251] loss: 127.3178482055664\n",
      "[step: 251] loss: 0.0072239157743752\n",
      "[step: 252] loss: 126.51165771484375\n",
      "[step: 252] loss: 0.007222163490951061\n",
      "[step: 253] loss: 126.41058349609375\n",
      "[step: 253] loss: 0.007220543455332518\n",
      "[step: 254] loss: 125.4656982421875\n",
      "[step: 254] loss: 0.007219177670776844\n",
      "[step: 255] loss: 125.61286163330078\n",
      "[step: 255] loss: 0.007218295708298683\n",
      "[step: 256] loss: 124.94755554199219\n",
      "[step: 256] loss: 0.007218408863991499\n",
      "[step: 257] loss: 124.82884216308594\n",
      "[step: 257] loss: 0.007220502011477947\n",
      "[step: 258] loss: 124.71894836425781\n",
      "[step: 258] loss: 0.007226784247905016\n",
      "[step: 259] loss: 124.22499084472656\n",
      "[step: 259] loss: 0.0072415666654706\n",
      "[step: 260] loss: 124.64411926269531\n",
      "[step: 260] loss: 0.007273201365023851\n",
      "[step: 261] loss: 124.51625061035156\n",
      "[step: 261] loss: 0.007331725209951401\n",
      "[step: 262] loss: 126.06442260742188\n",
      "[step: 262] loss: 0.00741778127849102\n",
      "[step: 263] loss: 129.28280639648438\n",
      "[step: 263] loss: 0.007477634586393833\n",
      "[step: 264] loss: 137.16664123535156\n",
      "[step: 264] loss: 0.007420612033456564\n",
      "[step: 265] loss: 162.1991424560547\n",
      "[step: 265] loss: 0.007264327257871628\n",
      "[step: 266] loss: 167.09579467773438\n",
      "[step: 266] loss: 0.00719901779666543\n",
      "[step: 267] loss: 170.78219604492188\n",
      "[step: 267] loss: 0.007281052879989147\n",
      "[step: 268] loss: 134.12396240234375\n",
      "[step: 268] loss: 0.007335906382650137\n",
      "[step: 269] loss: 129.4372100830078\n",
      "[step: 269] loss: 0.007258489727973938\n",
      "[step: 270] loss: 145.08641052246094\n",
      "[step: 270] loss: 0.007191661279648542\n",
      "[step: 271] loss: 133.2162322998047\n",
      "[step: 271] loss: 0.007241046987473965\n",
      "[step: 272] loss: 124.30555725097656\n",
      "[step: 272] loss: 0.007277742028236389\n",
      "[step: 273] loss: 132.21343994140625\n",
      "[step: 273] loss: 0.007220117375254631\n",
      "[step: 274] loss: 134.67376708984375\n",
      "[step: 274] loss: 0.007186269387602806\n",
      "[step: 275] loss: 125.68780517578125\n",
      "[step: 275] loss: 0.007227807305753231\n",
      "[step: 276] loss: 123.39197540283203\n",
      "[step: 276] loss: 0.00723780132830143\n",
      "[step: 277] loss: 128.66104125976562\n",
      "[step: 277] loss: 0.0071925329975783825\n",
      "[step: 278] loss: 130.45404052734375\n",
      "[step: 278] loss: 0.00718365702778101\n",
      "[step: 279] loss: 123.17644500732422\n",
      "[step: 279] loss: 0.007214116398245096\n",
      "[step: 280] loss: 123.10005950927734\n",
      "[step: 280] loss: 0.007208507042378187\n",
      "[step: 281] loss: 128.2560272216797\n",
      "[step: 281] loss: 0.007177368737757206\n",
      "[step: 282] loss: 124.08183288574219\n",
      "[step: 282] loss: 0.007179765496402979\n",
      "[step: 283] loss: 120.55064392089844\n",
      "[step: 283] loss: 0.007198631763458252\n",
      "[step: 284] loss: 122.91636657714844\n",
      "[step: 284] loss: 0.007188398856669664\n",
      "[step: 285] loss: 124.47564697265625\n",
      "[step: 285] loss: 0.007168424781411886\n",
      "[step: 286] loss: 122.88956451416016\n",
      "[step: 286] loss: 0.007173210382461548\n",
      "[step: 287] loss: 119.13549041748047\n",
      "[step: 287] loss: 0.007184124551713467\n",
      "[step: 288] loss: 121.36695098876953\n",
      "[step: 288] loss: 0.0071747335605323315\n",
      "[step: 289] loss: 123.93424987792969\n",
      "[step: 289] loss: 0.007161627523601055\n",
      "[step: 290] loss: 122.73681640625\n",
      "[step: 290] loss: 0.007165113929659128\n",
      "[step: 291] loss: 118.91551208496094\n",
      "[step: 291] loss: 0.007171705365180969\n",
      "[step: 292] loss: 118.66012573242188\n",
      "[step: 292] loss: 0.007165152579545975\n",
      "[step: 293] loss: 120.32630920410156\n",
      "[step: 293] loss: 0.0071555874310433865\n",
      "[step: 294] loss: 121.42930603027344\n",
      "[step: 294] loss: 0.007156500592827797\n",
      "[step: 295] loss: 118.76651000976562\n",
      "[step: 295] loss: 0.007160969544202089\n",
      "[step: 296] loss: 117.39988708496094\n",
      "[step: 296] loss: 0.007157552987337112\n",
      "[step: 297] loss: 117.1414794921875\n",
      "[step: 297] loss: 0.00715014711022377\n",
      "[step: 298] loss: 118.60474395751953\n",
      "[step: 298] loss: 0.007148314267396927\n",
      "[step: 299] loss: 119.32141876220703\n",
      "[step: 299] loss: 0.007150988094508648\n",
      "[step: 300] loss: 118.63557434082031\n",
      "[step: 300] loss: 0.007150445599108934\n",
      "[step: 301] loss: 117.6753158569336\n",
      "[step: 301] loss: 0.0071454099379479885\n",
      "[step: 302] loss: 115.97676086425781\n",
      "[step: 302] loss: 0.007141497451812029\n",
      "[step: 303] loss: 115.46311950683594\n",
      "[step: 303] loss: 0.007141603156924248\n",
      "[step: 304] loss: 115.43769836425781\n",
      "[step: 304] loss: 0.007142459508031607\n",
      "[step: 305] loss: 115.87820434570312\n",
      "[step: 305] loss: 0.007140519563108683\n",
      "[step: 306] loss: 116.67913818359375\n",
      "[step: 306] loss: 0.0071366699412465096\n",
      "[step: 307] loss: 116.7420654296875\n",
      "[step: 307] loss: 0.007134047336876392\n",
      "[step: 308] loss: 117.624755859375\n",
      "[step: 308] loss: 0.007133641745895147\n",
      "[step: 309] loss: 117.01490020751953\n",
      "[step: 309] loss: 0.007133612874895334\n",
      "[step: 310] loss: 116.9706802368164\n",
      "[step: 310] loss: 0.007132120430469513\n",
      "[step: 311] loss: 116.54640197753906\n",
      "[step: 311] loss: 0.007129359524697065\n",
      "[step: 312] loss: 117.364990234375\n",
      "[step: 312] loss: 0.007126821205019951\n",
      "[step: 313] loss: 116.43513488769531\n",
      "[step: 313] loss: 0.007125450298190117\n",
      "[step: 314] loss: 116.87373352050781\n",
      "[step: 314] loss: 0.0071248565800487995\n",
      "[step: 315] loss: 116.62937927246094\n",
      "[step: 315] loss: 0.0071240379475057125\n",
      "[step: 316] loss: 118.59242248535156\n",
      "[step: 316] loss: 0.007122453302145004\n",
      "[step: 317] loss: 117.68043518066406\n",
      "[step: 317] loss: 0.007120305672287941\n",
      "[step: 318] loss: 118.5544662475586\n",
      "[step: 318] loss: 0.0071182032115757465\n",
      "[step: 319] loss: 118.57765197753906\n",
      "[step: 319] loss: 0.007116576191037893\n",
      "[step: 320] loss: 121.50167846679688\n",
      "[step: 320] loss: 0.007115412037819624\n",
      "[step: 321] loss: 118.94349670410156\n",
      "[step: 321] loss: 0.0071144322864711285\n",
      "[step: 322] loss: 117.62904357910156\n",
      "[step: 322] loss: 0.0071133277378976345\n",
      "[step: 323] loss: 115.38021850585938\n",
      "[step: 323] loss: 0.007111955434083939\n",
      "[step: 324] loss: 115.24514770507812\n",
      "[step: 324] loss: 0.007110342383384705\n",
      "[step: 325] loss: 112.617431640625\n",
      "[step: 325] loss: 0.007108605466783047\n",
      "[step: 326] loss: 112.14375305175781\n",
      "[step: 326] loss: 0.0071068815886974335\n",
      "[step: 327] loss: 111.15460205078125\n",
      "[step: 327] loss: 0.0071052610874176025\n",
      "[step: 328] loss: 112.41962432861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 328] loss: 0.007103754207491875\n",
      "[step: 329] loss: 113.70221710205078\n",
      "[step: 329] loss: 0.007102352567017078\n",
      "[step: 330] loss: 116.52227783203125\n",
      "[step: 330] loss: 0.007101037073880434\n",
      "[step: 331] loss: 123.85237121582031\n",
      "[step: 331] loss: 0.007099774666130543\n",
      "[step: 332] loss: 126.11654663085938\n",
      "[step: 332] loss: 0.0070985471829771996\n",
      "[step: 333] loss: 130.03289794921875\n",
      "[step: 333] loss: 0.007097398862242699\n",
      "[step: 334] loss: 126.9283676147461\n",
      "[step: 334] loss: 0.007096346002072096\n",
      "[step: 335] loss: 125.39501190185547\n",
      "[step: 335] loss: 0.007095457520335913\n",
      "[step: 336] loss: 115.04151916503906\n",
      "[step: 336] loss: 0.007094911765307188\n",
      "[step: 337] loss: 113.41513061523438\n",
      "[step: 337] loss: 0.007094970438629389\n",
      "[step: 338] loss: 121.15536499023438\n",
      "[step: 338] loss: 0.007096193265169859\n",
      "[step: 339] loss: 124.38172912597656\n",
      "[step: 339] loss: 0.007099704816937447\n",
      "[step: 340] loss: 123.47400665283203\n",
      "[step: 340] loss: 0.007107636425644159\n",
      "[step: 341] loss: 112.25129699707031\n",
      "[step: 341] loss: 0.00712455902248621\n",
      "[step: 342] loss: 110.55648803710938\n",
      "[step: 342] loss: 0.007157810963690281\n",
      "[step: 343] loss: 112.44102478027344\n",
      "[step: 343] loss: 0.0072205099277198315\n",
      "[step: 344] loss: 116.01165771484375\n",
      "[step: 344] loss: 0.007315595634281635\n",
      "[step: 345] loss: 115.05267333984375\n",
      "[step: 345] loss: 0.007420276291668415\n",
      "[step: 346] loss: 111.45490264892578\n",
      "[step: 346] loss: 0.007422279100865126\n",
      "[step: 347] loss: 108.83943939208984\n",
      "[step: 347] loss: 0.007269175257533789\n",
      "[step: 348] loss: 109.24362182617188\n",
      "[step: 348] loss: 0.007098507601767778\n",
      "[step: 349] loss: 111.15995788574219\n",
      "[step: 349] loss: 0.007113017141819\n",
      "[step: 350] loss: 111.30070495605469\n",
      "[step: 350] loss: 0.00723282853141427\n",
      "[step: 351] loss: 109.5986557006836\n",
      "[step: 351] loss: 0.007228143513202667\n",
      "[step: 352] loss: 107.93439483642578\n",
      "[step: 352] loss: 0.00710932957008481\n",
      "[step: 353] loss: 106.9453353881836\n",
      "[step: 353] loss: 0.0070861876010894775\n",
      "[step: 354] loss: 107.09403991699219\n",
      "[step: 354] loss: 0.0071662613190710545\n",
      "[step: 355] loss: 108.14285278320312\n",
      "[step: 355] loss: 0.007167913019657135\n",
      "[step: 356] loss: 109.50116729736328\n",
      "[step: 356] loss: 0.007088072132319212\n",
      "[step: 357] loss: 108.83784484863281\n",
      "[step: 357] loss: 0.007082109805196524\n",
      "[step: 358] loss: 108.51815795898438\n",
      "[step: 358] loss: 0.007136334199458361\n",
      "[step: 359] loss: 108.77421569824219\n",
      "[step: 359] loss: 0.007125073112547398\n",
      "[step: 360] loss: 106.45215606689453\n",
      "[step: 360] loss: 0.007072604261338711\n",
      "[step: 361] loss: 106.33198547363281\n",
      "[step: 361] loss: 0.0070778862573206425\n",
      "[step: 362] loss: 104.65788269042969\n",
      "[step: 362] loss: 0.00711225438863039\n",
      "[step: 363] loss: 105.67418670654297\n",
      "[step: 363] loss: 0.007098223082721233\n",
      "[step: 364] loss: 105.49090576171875\n",
      "[step: 364] loss: 0.007064005360007286\n",
      "[step: 365] loss: 107.42964935302734\n",
      "[step: 365] loss: 0.0070711285807192326\n",
      "[step: 366] loss: 110.87925720214844\n",
      "[step: 366] loss: 0.007092975080013275\n",
      "[step: 367] loss: 113.26912689208984\n",
      "[step: 367] loss: 0.007081023883074522\n",
      "[step: 368] loss: 117.28962707519531\n",
      "[step: 368] loss: 0.00705824326723814\n",
      "[step: 369] loss: 119.15850830078125\n",
      "[step: 369] loss: 0.007062829099595547\n",
      "[step: 370] loss: 126.25845336914062\n",
      "[step: 370] loss: 0.007077091373503208\n",
      "[step: 371] loss: 115.51805114746094\n",
      "[step: 371] loss: 0.007070052903145552\n",
      "[step: 372] loss: 108.12605285644531\n",
      "[step: 372] loss: 0.0070538450963795185\n",
      "[step: 373] loss: 106.85134887695312\n",
      "[step: 373] loss: 0.007054290734231472\n",
      "[step: 374] loss: 113.53925323486328\n",
      "[step: 374] loss: 0.007064106874167919\n",
      "[step: 375] loss: 117.22447204589844\n",
      "[step: 375] loss: 0.007061957847326994\n",
      "[step: 376] loss: 113.8109130859375\n",
      "[step: 376] loss: 0.007050500251352787\n",
      "[step: 377] loss: 111.53903198242188\n",
      "[step: 377] loss: 0.007046760991215706\n",
      "[step: 378] loss: 106.18922424316406\n",
      "[step: 378] loss: 0.007052400149405003\n",
      "[step: 379] loss: 103.57064819335938\n",
      "[step: 379] loss: 0.007054501213133335\n",
      "[step: 380] loss: 105.68002319335938\n",
      "[step: 380] loss: 0.007048067636787891\n",
      "[step: 381] loss: 107.3909912109375\n",
      "[step: 381] loss: 0.00704172533005476\n",
      "[step: 382] loss: 108.40892028808594\n",
      "[step: 382] loss: 0.007042180746793747\n",
      "[step: 383] loss: 103.63212585449219\n",
      "[step: 383] loss: 0.007045391481369734\n",
      "[step: 384] loss: 103.50736999511719\n",
      "[step: 384] loss: 0.007044567260891199\n",
      "[step: 385] loss: 100.96290588378906\n",
      "[step: 385] loss: 0.007039574906229973\n",
      "[step: 386] loss: 104.12771606445312\n",
      "[step: 386] loss: 0.007035689894109964\n",
      "[step: 387] loss: 104.62925720214844\n",
      "[step: 387] loss: 0.007035691291093826\n",
      "[step: 388] loss: 105.82461547851562\n",
      "[step: 388] loss: 0.0070371911861002445\n",
      "[step: 389] loss: 104.6883544921875\n",
      "[step: 389] loss: 0.007036660797894001\n",
      "[step: 390] loss: 103.19615936279297\n",
      "[step: 390] loss: 0.007033581845462322\n",
      "[step: 391] loss: 101.31890869140625\n",
      "[step: 391] loss: 0.0070302896201610565\n",
      "[step: 392] loss: 99.8161392211914\n",
      "[step: 392] loss: 0.007028780411928892\n",
      "[step: 393] loss: 100.22746276855469\n",
      "[step: 393] loss: 0.007028908934444189\n",
      "[step: 394] loss: 100.35531616210938\n",
      "[step: 394] loss: 0.0070290761068463326\n",
      "[step: 395] loss: 101.163818359375\n",
      "[step: 395] loss: 0.007028043270111084\n",
      "[step: 396] loss: 101.76114654541016\n",
      "[step: 396] loss: 0.007025884930044413\n",
      "[step: 397] loss: 102.25042724609375\n",
      "[step: 397] loss: 0.00702348817139864\n",
      "[step: 398] loss: 103.13644409179688\n",
      "[step: 398] loss: 0.00702172564342618\n",
      "[step: 399] loss: 102.28614807128906\n",
      "[step: 399] loss: 0.007020795252174139\n",
      "[step: 400] loss: 102.3263931274414\n",
      "[step: 400] loss: 0.007020363584160805\n",
      "[step: 401] loss: 101.86654663085938\n",
      "[step: 401] loss: 0.007019908167421818\n",
      "[step: 402] loss: 101.50834655761719\n",
      "[step: 402] loss: 0.00701908441260457\n",
      "[step: 403] loss: 100.34550476074219\n",
      "[step: 403] loss: 0.007017838768661022\n",
      "[step: 404] loss: 99.86956787109375\n",
      "[step: 404] loss: 0.00701629463583231\n",
      "[step: 405] loss: 99.08375549316406\n",
      "[step: 405] loss: 0.007014655042439699\n",
      "[step: 406] loss: 98.48991394042969\n",
      "[step: 406] loss: 0.0070130666717886925\n",
      "[step: 407] loss: 97.94766998291016\n",
      "[step: 407] loss: 0.0070116035640239716\n",
      "[step: 408] loss: 97.98646545410156\n",
      "[step: 408] loss: 0.007010303903371096\n",
      "[step: 409] loss: 97.49029541015625\n",
      "[step: 409] loss: 0.007009104825556278\n",
      "[step: 410] loss: 97.66815185546875\n",
      "[step: 410] loss: 0.00700800446793437\n",
      "[step: 411] loss: 97.65354919433594\n",
      "[step: 411] loss: 0.007006977219134569\n",
      "[step: 412] loss: 98.72718811035156\n",
      "[step: 412] loss: 0.007006020750850439\n",
      "[step: 413] loss: 99.47215270996094\n",
      "[step: 413] loss: 0.007005173247307539\n",
      "[step: 414] loss: 102.50625610351562\n",
      "[step: 414] loss: 0.007004540413618088\n",
      "[step: 415] loss: 106.25680541992188\n",
      "[step: 415] loss: 0.00700429268181324\n",
      "[step: 416] loss: 115.50286102294922\n",
      "[step: 416] loss: 0.007004869636148214\n",
      "[step: 417] loss: 118.74478149414062\n",
      "[step: 417] loss: 0.007007150445133448\n",
      "[step: 418] loss: 127.39429473876953\n",
      "[step: 418] loss: 0.007013335824012756\n",
      "[step: 419] loss: 116.01325988769531\n",
      "[step: 419] loss: 0.00702778110280633\n",
      "[step: 420] loss: 103.3349609375\n",
      "[step: 420] loss: 0.007062106393277645\n",
      "[step: 421] loss: 95.15086364746094\n",
      "[step: 421] loss: 0.007134134881198406\n",
      "[step: 422] loss: 98.43135070800781\n",
      "[step: 422] loss: 0.007286312989890575\n",
      "[step: 423] loss: 106.78804016113281\n",
      "[step: 423] loss: 0.007489577401429415\n",
      "[step: 424] loss: 106.55059814453125\n",
      "[step: 424] loss: 0.007643977180123329\n",
      "[step: 425] loss: 102.36024475097656\n",
      "[step: 425] loss: 0.007411120925098658\n",
      "[step: 426] loss: 95.1607894897461\n",
      "[step: 426] loss: 0.007045956328511238\n",
      "[step: 427] loss: 94.66858673095703\n",
      "[step: 427] loss: 0.007080035284161568\n",
      "[step: 428] loss: 99.22148132324219\n",
      "[step: 428] loss: 0.00730895297601819\n",
      "[step: 429] loss: 100.35324096679688\n",
      "[step: 429] loss: 0.007193883415311575\n",
      "[step: 430] loss: 97.31687927246094\n",
      "[step: 430] loss: 0.007002962287515402\n",
      "[step: 431] loss: 93.45667266845703\n",
      "[step: 431] loss: 0.007137221284210682\n",
      "[step: 432] loss: 93.65866088867188\n",
      "[step: 432] loss: 0.007190442178398371\n",
      "[step: 433] loss: 96.71766662597656\n",
      "[step: 433] loss: 0.007021635305136442\n",
      "[step: 434] loss: 97.22931671142578\n",
      "[step: 434] loss: 0.007049693260341883\n",
      "[step: 435] loss: 95.76188659667969\n",
      "[step: 435] loss: 0.007143525406718254\n",
      "[step: 436] loss: 92.89909362792969\n",
      "[step: 436] loss: 0.007037739735096693\n",
      "[step: 437] loss: 91.85421752929688\n",
      "[step: 437] loss: 0.007008346263319254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 438] loss: 93.02013397216797\n",
      "[step: 438] loss: 0.007094082422554493\n",
      "[step: 439] loss: 94.24794006347656\n",
      "[step: 439] loss: 0.007045503705739975\n",
      "[step: 440] loss: 94.56790924072266\n",
      "[step: 440] loss: 0.006990831345319748\n",
      "[step: 441] loss: 92.88137817382812\n",
      "[step: 441] loss: 0.007050210144370794\n",
      "[step: 442] loss: 91.49797058105469\n",
      "[step: 442] loss: 0.007044696249067783\n",
      "[step: 443] loss: 90.78596496582031\n",
      "[step: 443] loss: 0.006987645290791988\n",
      "[step: 444] loss: 91.05116271972656\n",
      "[step: 444] loss: 0.007014842238277197\n",
      "[step: 445] loss: 91.75627136230469\n",
      "[step: 445] loss: 0.007035394199192524\n",
      "[step: 446] loss: 92.09913635253906\n",
      "[step: 446] loss: 0.006992433685809374\n",
      "[step: 447] loss: 92.24153137207031\n",
      "[step: 447] loss: 0.006990852765738964\n",
      "[step: 448] loss: 91.44105529785156\n",
      "[step: 448] loss: 0.007019272074103355\n",
      "[step: 449] loss: 90.57563781738281\n",
      "[step: 449] loss: 0.006998987402766943\n",
      "[step: 450] loss: 89.72496032714844\n",
      "[step: 450] loss: 0.00697923032566905\n",
      "[step: 451] loss: 89.32383728027344\n",
      "[step: 451] loss: 0.006999524775892496\n",
      "[step: 452] loss: 89.2288818359375\n",
      "[step: 452] loss: 0.007001128979027271\n",
      "[step: 453] loss: 89.340576171875\n",
      "[step: 453] loss: 0.006978609133511782\n",
      "[step: 454] loss: 89.62606048583984\n",
      "[step: 454] loss: 0.006982055492699146\n",
      "[step: 455] loss: 89.80099487304688\n",
      "[step: 455] loss: 0.006994705181568861\n",
      "[step: 456] loss: 90.08838653564453\n",
      "[step: 456] loss: 0.006982867605984211\n",
      "[step: 457] loss: 90.19380187988281\n",
      "[step: 457] loss: 0.006972689181566238\n",
      "[step: 458] loss: 90.58882141113281\n",
      "[step: 458] loss: 0.006981743965297937\n",
      "[step: 459] loss: 90.55007934570312\n",
      "[step: 459] loss: 0.006983806379139423\n",
      "[step: 460] loss: 90.97660827636719\n",
      "[step: 460] loss: 0.0069725485518574715\n",
      "[step: 461] loss: 90.82770538330078\n",
      "[step: 461] loss: 0.006970137357711792\n",
      "[step: 462] loss: 91.22888946533203\n",
      "[step: 462] loss: 0.0069768247194588184\n",
      "[step: 463] loss: 91.07525634765625\n",
      "[step: 463] loss: 0.006974922493100166\n",
      "[step: 464] loss: 91.67774963378906\n",
      "[step: 464] loss: 0.006966953165829182\n",
      "[step: 465] loss: 91.3162841796875\n",
      "[step: 465] loss: 0.006966840475797653\n",
      "[step: 466] loss: 91.75105285644531\n",
      "[step: 466] loss: 0.006970952730625868\n",
      "[step: 467] loss: 91.1703872680664\n",
      "[step: 467] loss: 0.006968550384044647\n",
      "[step: 468] loss: 91.21276092529297\n",
      "[step: 468] loss: 0.006963064428418875\n",
      "[step: 469] loss: 90.17900085449219\n",
      "[step: 469] loss: 0.006962727755308151\n",
      "[step: 470] loss: 89.71527099609375\n",
      "[step: 470] loss: 0.006965183187276125\n",
      "[step: 471] loss: 88.51940155029297\n",
      "[step: 471] loss: 0.006963834166526794\n",
      "[step: 472] loss: 87.67776489257812\n",
      "[step: 472] loss: 0.006959767546504736\n",
      "[step: 473] loss: 86.75227355957031\n",
      "[step: 473] loss: 0.006958423648029566\n",
      "[step: 474] loss: 86.09197235107422\n",
      "[step: 474] loss: 0.00695975124835968\n",
      "[step: 475] loss: 85.59596252441406\n",
      "[step: 475] loss: 0.0069595323875546455\n",
      "[step: 476] loss: 85.26824951171875\n",
      "[step: 476] loss: 0.006956868804991245\n",
      "[step: 477] loss: 85.05506896972656\n",
      "[step: 477] loss: 0.006954646669328213\n",
      "[step: 478] loss: 84.94129943847656\n",
      "[step: 478] loss: 0.006954581942409277\n",
      "[step: 479] loss: 84.90495300292969\n",
      "[step: 479] loss: 0.0069550019688904285\n",
      "[step: 480] loss: 84.95207214355469\n",
      "[step: 480] loss: 0.006953965872526169\n",
      "[step: 481] loss: 85.15798950195312\n",
      "[step: 481] loss: 0.006951865740120411\n",
      "[step: 482] loss: 85.56756591796875\n",
      "[step: 482] loss: 0.006950365845113993\n",
      "[step: 483] loss: 86.54190063476562\n",
      "[step: 483] loss: 0.006950055249035358\n",
      "[step: 484] loss: 88.23219299316406\n",
      "[step: 484] loss: 0.006950004026293755\n",
      "[step: 485] loss: 92.16950988769531\n",
      "[step: 485] loss: 0.006949226837605238\n",
      "[step: 486] loss: 97.77774047851562\n",
      "[step: 486] loss: 0.006947756744921207\n",
      "[step: 487] loss: 107.91748046875\n",
      "[step: 487] loss: 0.006946293171495199\n",
      "[step: 488] loss: 113.1995620727539\n",
      "[step: 488] loss: 0.006945403758436441\n",
      "[step: 489] loss: 113.15081787109375\n",
      "[step: 489] loss: 0.006944979541003704\n",
      "[step: 490] loss: 99.1956787109375\n",
      "[step: 490] loss: 0.0069445655681192875\n",
      "[step: 491] loss: 93.78096771240234\n",
      "[step: 491] loss: 0.006943804677575827\n",
      "[step: 492] loss: 93.76656341552734\n",
      "[step: 492] loss: 0.006942689418792725\n",
      "[step: 493] loss: 98.95988464355469\n",
      "[step: 493] loss: 0.006941480562090874\n",
      "[step: 494] loss: 87.20301818847656\n",
      "[step: 494] loss: 0.006940438877791166\n",
      "[step: 495] loss: 93.09584045410156\n",
      "[step: 495] loss: 0.006939633749425411\n",
      "[step: 496] loss: 111.75466918945312\n",
      "[step: 496] loss: 0.0069390106946229935\n",
      "[step: 497] loss: 89.28662872314453\n",
      "[step: 497] loss: 0.006938421167433262\n",
      "[step: 498] loss: 96.57575988769531\n",
      "[step: 498] loss: 0.006937769707292318\n",
      "[step: 499] loss: 102.93618774414062\n",
      "[step: 499] loss: 0.006936992984265089\n",
      "[step: 500] loss: 87.47522735595703\n",
      "[step: 500] loss: 0.0069361221976578236\n",
      "[step: 501] loss: 100.46166229248047\n",
      "[step: 501] loss: 0.006935196928679943\n",
      "[step: 502] loss: 92.9944839477539\n",
      "[step: 502] loss: 0.006934260483831167\n",
      "[step: 503] loss: 87.70669555664062\n",
      "[step: 503] loss: 0.006933353375643492\n",
      "[step: 504] loss: 96.94136047363281\n",
      "[step: 504] loss: 0.006932467687875032\n",
      "[step: 505] loss: 85.79499816894531\n",
      "[step: 505] loss: 0.006931619718670845\n",
      "[step: 506] loss: 90.24513244628906\n",
      "[step: 506] loss: 0.006930799689143896\n",
      "[step: 507] loss: 90.57414245605469\n",
      "[step: 507] loss: 0.0069299922324717045\n",
      "[step: 508] loss: 84.49288940429688\n",
      "[step: 508] loss: 0.00692920433357358\n",
      "[step: 509] loss: 89.40164947509766\n",
      "[step: 509] loss: 0.006928422022610903\n",
      "[step: 510] loss: 83.95735168457031\n",
      "[step: 510] loss: 0.0069276499561965466\n",
      "[step: 511] loss: 89.06715393066406\n",
      "[step: 511] loss: 0.006926892325282097\n",
      "[step: 512] loss: 91.6878662109375\n",
      "[step: 512] loss: 0.006926155649125576\n",
      "[step: 513] loss: 87.47457122802734\n",
      "[step: 513] loss: 0.006925477180629969\n",
      "[step: 514] loss: 89.59681701660156\n",
      "[step: 514] loss: 0.006924882531166077\n",
      "[step: 515] loss: 82.35472106933594\n",
      "[step: 515] loss: 0.006924516521394253\n",
      "[step: 516] loss: 89.91758728027344\n",
      "[step: 516] loss: 0.006924615241587162\n",
      "[step: 517] loss: 86.63865661621094\n",
      "[step: 517] loss: 0.006925899535417557\n",
      "[step: 518] loss: 87.92535400390625\n",
      "[step: 518] loss: 0.006929873488843441\n",
      "[step: 519] loss: 88.57279205322266\n",
      "[step: 519] loss: 0.006940899416804314\n",
      "[step: 520] loss: 84.02962493896484\n",
      "[step: 520] loss: 0.00696801720187068\n",
      "[step: 521] loss: 86.62155151367188\n",
      "[step: 521] loss: 0.007039225194603205\n",
      "[step: 522] loss: 82.81187438964844\n",
      "[step: 522] loss: 0.007186451926827431\n",
      "[step: 523] loss: 88.62528228759766\n",
      "[step: 523] loss: 0.007495633792132139\n",
      "[step: 524] loss: 83.44200134277344\n",
      "[step: 524] loss: 0.007713834755122662\n",
      "[step: 525] loss: 84.67701721191406\n",
      "[step: 525] loss: 0.007564549800008535\n",
      "[step: 526] loss: 82.20841979980469\n",
      "[step: 526] loss: 0.007039926014840603\n",
      "[step: 527] loss: 81.9990005493164\n",
      "[step: 527] loss: 0.007031340152025223\n",
      "[step: 528] loss: 82.22042846679688\n",
      "[step: 528] loss: 0.007358275353908539\n",
      "[step: 529] loss: 81.4734115600586\n",
      "[step: 529] loss: 0.007160141598433256\n",
      "[step: 530] loss: 82.64550018310547\n",
      "[step: 530] loss: 0.006946174893528223\n",
      "[step: 531] loss: 80.08456420898438\n",
      "[step: 531] loss: 0.007174863014370203\n",
      "[step: 532] loss: 80.98906707763672\n",
      "[step: 532] loss: 0.007140156347304583\n",
      "[step: 533] loss: 79.34335327148438\n",
      "[step: 533] loss: 0.006938131991773844\n",
      "[step: 534] loss: 79.68028259277344\n",
      "[step: 534] loss: 0.007079557050019503\n",
      "[step: 535] loss: 79.37440490722656\n",
      "[step: 535] loss: 0.007089945953339338\n",
      "[step: 536] loss: 79.20513916015625\n",
      "[step: 536] loss: 0.0069346074014902115\n",
      "[step: 537] loss: 79.60810852050781\n",
      "[step: 537] loss: 0.00702712032943964\n",
      "[step: 538] loss: 78.40878295898438\n",
      "[step: 538] loss: 0.007046911865472794\n",
      "[step: 539] loss: 78.75772094726562\n",
      "[step: 539] loss: 0.006929651368409395\n",
      "[step: 540] loss: 78.21684265136719\n",
      "[step: 540] loss: 0.006990371271967888\n",
      "[step: 541] loss: 77.67375183105469\n",
      "[step: 541] loss: 0.00701520498842001\n",
      "[step: 542] loss: 77.9202880859375\n",
      "[step: 542] loss: 0.0069270445965230465\n",
      "[step: 543] loss: 77.21159362792969\n",
      "[step: 543] loss: 0.006962901912629604\n",
      "[step: 544] loss: 77.77742004394531\n",
      "[step: 544] loss: 0.006990080699324608\n",
      "[step: 545] loss: 77.2930908203125\n",
      "[step: 545] loss: 0.006925311405211687\n",
      "[step: 546] loss: 77.40080261230469\n",
      "[step: 546] loss: 0.00694201560690999\n",
      "[step: 547] loss: 77.79203796386719\n",
      "[step: 547] loss: 0.006971305701881647\n",
      "[step: 548] loss: 77.15377044677734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 548] loss: 0.006925630383193493\n",
      "[step: 549] loss: 77.45369720458984\n",
      "[step: 549] loss: 0.0069259353913366795\n",
      "[step: 550] loss: 77.0350341796875\n",
      "[step: 550] loss: 0.006955170538276434\n",
      "[step: 551] loss: 77.00390625\n",
      "[step: 551] loss: 0.006927597336471081\n",
      "[step: 552] loss: 76.78704071044922\n",
      "[step: 552] loss: 0.006915148813277483\n",
      "[step: 553] loss: 76.67543029785156\n",
      "[step: 553] loss: 0.006939596030861139\n",
      "[step: 554] loss: 76.73204040527344\n",
      "[step: 554] loss: 0.0069285505451262\n",
      "[step: 555] loss: 76.79554748535156\n",
      "[step: 555] loss: 0.00690974947065115\n",
      "[step: 556] loss: 77.00054168701172\n",
      "[step: 556] loss: 0.006925142370164394\n",
      "[step: 557] loss: 77.91990661621094\n",
      "[step: 557] loss: 0.006926630157977343\n",
      "[step: 558] loss: 78.49974060058594\n",
      "[step: 558] loss: 0.00690845400094986\n",
      "[step: 559] loss: 80.43435668945312\n",
      "[step: 559] loss: 0.006912972312420607\n",
      "[step: 560] loss: 82.61085510253906\n",
      "[step: 560] loss: 0.006921377032995224\n",
      "[step: 561] loss: 86.972412109375\n",
      "[step: 561] loss: 0.0069093722850084305\n",
      "[step: 562] loss: 89.40066528320312\n",
      "[step: 562] loss: 0.006904639303684235\n",
      "[step: 563] loss: 94.97817993164062\n",
      "[step: 563] loss: 0.006913147866725922\n",
      "[step: 564] loss: 90.33541870117188\n",
      "[step: 564] loss: 0.006910097785294056\n",
      "[step: 565] loss: 85.12956237792969\n",
      "[step: 565] loss: 0.006901419255882502\n",
      "[step: 566] loss: 77.32331848144531\n",
      "[step: 566] loss: 0.006904311012476683\n",
      "[step: 567] loss: 74.72520446777344\n",
      "[step: 567] loss: 0.0069077215157449245\n",
      "[step: 568] loss: 77.0562973022461\n",
      "[step: 568] loss: 0.006901777349412441\n",
      "[step: 569] loss: 80.03547668457031\n",
      "[step: 569] loss: 0.0068983579985797405\n",
      "[step: 570] loss: 82.92627716064453\n",
      "[step: 570] loss: 0.006901911459863186\n",
      "[step: 571] loss: 79.16242980957031\n",
      "[step: 571] loss: 0.006901714019477367\n",
      "[step: 572] loss: 75.63032531738281\n",
      "[step: 572] loss: 0.006896724458783865\n",
      "[step: 573] loss: 74.0494155883789\n",
      "[step: 573] loss: 0.006895799655467272\n",
      "[step: 574] loss: 74.71957397460938\n",
      "[step: 574] loss: 0.006898063234984875\n",
      "[step: 575] loss: 76.80598449707031\n",
      "[step: 575] loss: 0.00689669419080019\n",
      "[step: 576] loss: 77.19990539550781\n",
      "[step: 576] loss: 0.006893165875226259\n",
      "[step: 577] loss: 75.7025146484375\n",
      "[step: 577] loss: 0.006892713252454996\n",
      "[step: 578] loss: 73.84768676757812\n",
      "[step: 578] loss: 0.006893951445817947\n",
      "[step: 579] loss: 73.08773803710938\n",
      "[step: 579] loss: 0.00689280591905117\n",
      "[step: 580] loss: 73.28123474121094\n",
      "[step: 580] loss: 0.006890190299600363\n",
      "[step: 581] loss: 74.15116882324219\n",
      "[step: 581] loss: 0.0068894061259925365\n",
      "[step: 582] loss: 74.92341613769531\n",
      "[step: 582] loss: 0.006890012416988611\n",
      "[step: 583] loss: 74.21713256835938\n",
      "[step: 583] loss: 0.006889407988637686\n",
      "[step: 584] loss: 73.3399429321289\n",
      "[step: 584] loss: 0.006887483410537243\n",
      "[step: 585] loss: 72.47972106933594\n",
      "[step: 585] loss: 0.0068861995823681355\n",
      "[step: 586] loss: 72.097412109375\n",
      "[step: 586] loss: 0.006886172108352184\n",
      "[step: 587] loss: 72.32666778564453\n",
      "[step: 587] loss: 0.006886064540594816\n",
      "[step: 588] loss: 72.73509979248047\n",
      "[step: 588] loss: 0.006884938571602106\n",
      "[step: 589] loss: 73.00094604492188\n",
      "[step: 589] loss: 0.006883475463837385\n",
      "[step: 590] loss: 72.96705627441406\n",
      "[step: 590] loss: 0.006882660556584597\n",
      "[step: 591] loss: 72.78907775878906\n",
      "[step: 591] loss: 0.006882435642182827\n",
      "[step: 592] loss: 72.18357849121094\n",
      "[step: 592] loss: 0.006882052402943373\n",
      "[step: 593] loss: 71.66853332519531\n",
      "[step: 593] loss: 0.00688110152259469\n",
      "[step: 594] loss: 71.30341339111328\n",
      "[step: 594] loss: 0.006879938300698996\n",
      "[step: 595] loss: 71.06214904785156\n",
      "[step: 595] loss: 0.006879066117107868\n",
      "[step: 596] loss: 70.98066711425781\n",
      "[step: 596] loss: 0.0068785701878368855\n",
      "[step: 597] loss: 71.05979919433594\n",
      "[step: 597] loss: 0.0068781496956944466\n",
      "[step: 598] loss: 71.19807434082031\n",
      "[step: 598] loss: 0.0068774898536503315\n",
      "[step: 599] loss: 71.3356704711914\n",
      "[step: 599] loss: 0.006876589264720678\n",
      "[step: 600] loss: 71.57418823242188\n",
      "[step: 600] loss: 0.006875637453049421\n",
      "[step: 601] loss: 71.79810333251953\n",
      "[step: 601] loss: 0.006874845363199711\n",
      "[step: 602] loss: 72.21348571777344\n",
      "[step: 602] loss: 0.006874222308397293\n",
      "[step: 603] loss: 72.60469055175781\n",
      "[step: 603] loss: 0.006873684003949165\n",
      "[step: 604] loss: 73.48234558105469\n",
      "[step: 604] loss: 0.006873091217130423\n",
      "[step: 605] loss: 74.57379150390625\n",
      "[step: 605] loss: 0.006872386205941439\n",
      "[step: 606] loss: 76.59126281738281\n",
      "[step: 606] loss: 0.0068715899251401424\n",
      "[step: 607] loss: 78.53673553466797\n",
      "[step: 607] loss: 0.006870775017887354\n",
      "[step: 608] loss: 81.53166961669922\n",
      "[step: 608] loss: 0.00686999037861824\n",
      "[step: 609] loss: 81.6473617553711\n",
      "[step: 609] loss: 0.006869256496429443\n",
      "[step: 610] loss: 81.07127380371094\n",
      "[step: 610] loss: 0.006868588272482157\n",
      "[step: 611] loss: 77.05381774902344\n",
      "[step: 611] loss: 0.006867942400276661\n",
      "[step: 612] loss: 73.79214477539062\n",
      "[step: 612] loss: 0.0068673184141516685\n",
      "[step: 613] loss: 71.41343688964844\n",
      "[step: 613] loss: 0.006866694428026676\n",
      "[step: 614] loss: 71.02346801757812\n",
      "[step: 614] loss: 0.006866065785288811\n",
      "[step: 615] loss: 71.25730895996094\n",
      "[step: 615] loss: 0.00686544319614768\n",
      "[step: 616] loss: 71.77972412109375\n",
      "[step: 616] loss: 0.006864829454571009\n",
      "[step: 617] loss: 72.76974487304688\n",
      "[step: 617] loss: 0.0068642450496554375\n",
      "[step: 618] loss: 73.15813446044922\n",
      "[step: 618] loss: 0.006863727699965239\n",
      "[step: 619] loss: 73.6524429321289\n",
      "[step: 619] loss: 0.006863306276500225\n",
      "[step: 620] loss: 71.64356994628906\n",
      "[step: 620] loss: 0.006863072048872709\n",
      "[step: 621] loss: 69.90515899658203\n",
      "[step: 621] loss: 0.00686321035027504\n",
      "[step: 622] loss: 68.76285552978516\n",
      "[step: 622] loss: 0.006864030845463276\n",
      "[step: 623] loss: 68.87271118164062\n",
      "[step: 623] loss: 0.0068662636913359165\n",
      "[step: 624] loss: 70.03778839111328\n",
      "[step: 624] loss: 0.006871147081255913\n",
      "[step: 625] loss: 70.98054504394531\n",
      "[step: 625] loss: 0.006882043555378914\n",
      "[step: 626] loss: 71.45362854003906\n",
      "[step: 626] loss: 0.006903680507093668\n",
      "[step: 627] loss: 70.46939086914062\n",
      "[step: 627] loss: 0.006950162351131439\n",
      "[step: 628] loss: 69.43378448486328\n",
      "[step: 628] loss: 0.007027602754533291\n",
      "[step: 629] loss: 68.6058120727539\n",
      "[step: 629] loss: 0.007162766996771097\n",
      "[step: 630] loss: 68.37380981445312\n",
      "[step: 630] loss: 0.007254552096128464\n",
      "[step: 631] loss: 68.35142517089844\n",
      "[step: 631] loss: 0.0072431848384439945\n",
      "[step: 632] loss: 68.29991149902344\n",
      "[step: 632] loss: 0.007025655824691057\n",
      "[step: 633] loss: 68.16719055175781\n",
      "[step: 633] loss: 0.006869819946587086\n",
      "[step: 634] loss: 67.90606689453125\n",
      "[step: 634] loss: 0.006953831762075424\n",
      "[step: 635] loss: 67.78775024414062\n",
      "[step: 635] loss: 0.0070721483789384365\n",
      "[step: 636] loss: 67.73077392578125\n",
      "[step: 636] loss: 0.00700872577726841\n",
      "[step: 637] loss: 67.98667907714844\n",
      "[step: 637] loss: 0.0068775140680372715\n",
      "[step: 638] loss: 68.5526123046875\n",
      "[step: 638] loss: 0.006900044158101082\n",
      "[step: 639] loss: 69.50526428222656\n",
      "[step: 639] loss: 0.006989304907619953\n",
      "[step: 640] loss: 70.9244613647461\n",
      "[step: 640] loss: 0.00695621594786644\n",
      "[step: 641] loss: 72.6544418334961\n",
      "[step: 641] loss: 0.006868990138173103\n",
      "[step: 642] loss: 74.36373901367188\n",
      "[step: 642] loss: 0.006878136657178402\n",
      "[step: 643] loss: 75.33721923828125\n",
      "[step: 643] loss: 0.006940599065274\n",
      "[step: 644] loss: 74.885009765625\n",
      "[step: 644] loss: 0.0069357058964669704\n",
      "[step: 645] loss: 74.86986541748047\n",
      "[step: 645] loss: 0.00687375059351325\n",
      "[step: 646] loss: 74.72409057617188\n",
      "[step: 646] loss: 0.006855815649032593\n",
      "[step: 647] loss: 76.84376525878906\n",
      "[step: 647] loss: 0.006893868092447519\n",
      "[step: 648] loss: 74.33040618896484\n",
      "[step: 648] loss: 0.0069141387939453125\n",
      "[step: 649] loss: 71.64897918701172\n",
      "[step: 649] loss: 0.006887044757604599\n",
      "[step: 650] loss: 67.55357360839844\n",
      "[step: 650] loss: 0.00685292249545455\n",
      "[step: 651] loss: 66.72771453857422\n",
      "[step: 651] loss: 0.0068573798052966595\n",
      "[step: 652] loss: 67.81697082519531\n",
      "[step: 652] loss: 0.006882952060550451\n",
      "[step: 653] loss: 68.08770751953125\n",
      "[step: 653] loss: 0.0068862466141581535\n",
      "[step: 654] loss: 67.78732299804688\n",
      "[step: 654] loss: 0.006864627357572317\n",
      "[step: 655] loss: 67.76910400390625\n",
      "[step: 655] loss: 0.006847103126347065\n",
      "[step: 656] loss: 68.70179748535156\n",
      "[step: 656] loss: 0.006853423546999693\n",
      "[step: 657] loss: 68.30885314941406\n",
      "[step: 657] loss: 0.006867989432066679\n",
      "[step: 658] loss: 67.14981079101562\n",
      "[step: 658] loss: 0.006866702809929848\n",
      "[step: 659] loss: 66.2642822265625\n",
      "[step: 659] loss: 0.006852290593087673\n",
      "[step: 660] loss: 66.06451416015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 660] loss: 0.006843120325356722\n",
      "[step: 661] loss: 65.888916015625\n",
      "[step: 661] loss: 0.006848014425486326\n",
      "[step: 662] loss: 65.39295959472656\n",
      "[step: 662] loss: 0.0068563437089324\n",
      "[step: 663] loss: 65.31947326660156\n",
      "[step: 663] loss: 0.006855363957583904\n",
      "[step: 664] loss: 65.83740234375\n",
      "[step: 664] loss: 0.006846696604043245\n",
      "[step: 665] loss: 66.35768127441406\n",
      "[step: 665] loss: 0.006839987356215715\n",
      "[step: 666] loss: 66.33792114257812\n",
      "[step: 666] loss: 0.006840953603386879\n",
      "[step: 667] loss: 66.18099975585938\n",
      "[step: 667] loss: 0.006845916621387005\n",
      "[step: 668] loss: 66.1552505493164\n",
      "[step: 668] loss: 0.006848037708550692\n",
      "[step: 669] loss: 66.75373840332031\n",
      "[step: 669] loss: 0.00684501463547349\n",
      "[step: 670] loss: 67.29072570800781\n",
      "[step: 670] loss: 0.006839392706751823\n",
      "[step: 671] loss: 68.41182708740234\n",
      "[step: 671] loss: 0.006835747975856066\n",
      "[step: 672] loss: 68.14653015136719\n",
      "[step: 672] loss: 0.006835823878645897\n",
      "[step: 673] loss: 68.64170837402344\n",
      "[step: 673] loss: 0.006838077213615179\n",
      "[step: 674] loss: 69.02659606933594\n",
      "[step: 674] loss: 0.006839779671281576\n",
      "[step: 675] loss: 70.35972595214844\n",
      "[step: 675] loss: 0.006839158479124308\n",
      "[step: 676] loss: 71.10740661621094\n",
      "[step: 676] loss: 0.0068366811610758305\n",
      "[step: 677] loss: 72.0719985961914\n",
      "[step: 677] loss: 0.006833567749708891\n",
      "[step: 678] loss: 71.60101318359375\n",
      "[step: 678] loss: 0.0068312594667077065\n",
      "[step: 679] loss: 70.99183654785156\n",
      "[step: 679] loss: 0.0068302820436656475\n",
      "[step: 680] loss: 69.24671173095703\n",
      "[step: 680] loss: 0.00683044595643878\n",
      "[step: 681] loss: 67.60585021972656\n",
      "[step: 681] loss: 0.006831147242337465\n",
      "[step: 682] loss: 65.67294311523438\n",
      "[step: 682] loss: 0.0068317558616399765\n",
      "[step: 683] loss: 64.5810317993164\n",
      "[step: 683] loss: 0.006831992883235216\n",
      "[step: 684] loss: 63.97582244873047\n",
      "[step: 684] loss: 0.006831691600382328\n",
      "[step: 685] loss: 64.02755737304688\n",
      "[step: 685] loss: 0.0068310522474348545\n",
      "[step: 686] loss: 64.54373168945312\n",
      "[step: 686] loss: 0.00683008274063468\n",
      "[step: 687] loss: 65.18486785888672\n",
      "[step: 687] loss: 0.006829095538705587\n",
      "[step: 688] loss: 65.82292175292969\n",
      "[step: 688] loss: 0.006828127894550562\n",
      "[step: 689] loss: 65.86219024658203\n",
      "[step: 689] loss: 0.006827345583587885\n",
      "[step: 690] loss: 65.87164306640625\n",
      "[step: 690] loss: 0.0068267760798335075\n",
      "[step: 691] loss: 64.79058074951172\n",
      "[step: 691] loss: 0.006826604716479778\n",
      "[step: 692] loss: 63.975791931152344\n",
      "[step: 692] loss: 0.006826890632510185\n",
      "[step: 693] loss: 63.049049377441406\n",
      "[step: 693] loss: 0.006828105542808771\n",
      "[step: 694] loss: 62.73021697998047\n",
      "[step: 694] loss: 0.006830679718405008\n",
      "[step: 695] loss: 62.929718017578125\n",
      "[step: 695] loss: 0.006836201064288616\n",
      "[step: 696] loss: 63.50167465209961\n",
      "[step: 696] loss: 0.006846331525593996\n",
      "[step: 697] loss: 64.20083618164062\n",
      "[step: 697] loss: 0.006866820156574249\n",
      "[step: 698] loss: 64.8909912109375\n",
      "[step: 698] loss: 0.0069015356712043285\n",
      "[step: 699] loss: 65.25308990478516\n",
      "[step: 699] loss: 0.00696731498464942\n",
      "[step: 700] loss: 65.469970703125\n",
      "[step: 700] loss: 0.007050839718431234\n",
      "[step: 701] loss: 65.19796752929688\n",
      "[step: 701] loss: 0.007157796528190374\n",
      "[step: 702] loss: 65.93257141113281\n",
      "[step: 702] loss: 0.007160597946494818\n",
      "[step: 703] loss: 67.09529113769531\n",
      "[step: 703] loss: 0.007045637350529432\n",
      "[step: 704] loss: 71.3008804321289\n",
      "[step: 704] loss: 0.0068710423074662685\n",
      "[step: 705] loss: 69.77999877929688\n",
      "[step: 705] loss: 0.006835616659373045\n",
      "[step: 706] loss: 69.63282775878906\n",
      "[step: 706] loss: 0.0069404058158397675\n",
      "[step: 707] loss: 70.38801574707031\n",
      "[step: 707] loss: 0.006998833734542131\n",
      "[step: 708] loss: 75.96192932128906\n",
      "[step: 708] loss: 0.006928326562047005\n",
      "[step: 709] loss: 77.91213989257812\n",
      "[step: 709] loss: 0.006831779610365629\n",
      "[step: 710] loss: 75.76949310302734\n",
      "[step: 710] loss: 0.006841831840574741\n",
      "[step: 711] loss: 68.85861206054688\n",
      "[step: 711] loss: 0.006911830510944128\n",
      "[step: 712] loss: 66.9979476928711\n",
      "[step: 712] loss: 0.0069185118190944195\n",
      "[step: 713] loss: 65.67259979248047\n",
      "[step: 713] loss: 0.00685735372826457\n",
      "[step: 714] loss: 64.03665161132812\n",
      "[step: 714] loss: 0.006815068423748016\n",
      "[step: 715] loss: 62.38147735595703\n",
      "[step: 715] loss: 0.006838583387434483\n",
      "[step: 716] loss: 65.08191680908203\n",
      "[step: 716] loss: 0.006881562061607838\n",
      "[step: 717] loss: 69.21946716308594\n",
      "[step: 717] loss: 0.006882556714117527\n",
      "[step: 718] loss: 65.32743072509766\n",
      "[step: 718] loss: 0.006846365984529257\n",
      "[step: 719] loss: 63.3447380065918\n",
      "[step: 719] loss: 0.006813757587224245\n",
      "[step: 720] loss: 63.784912109375\n",
      "[step: 720] loss: 0.006818024907261133\n",
      "[step: 721] loss: 63.1932373046875\n",
      "[step: 721] loss: 0.006844034418463707\n",
      "[step: 722] loss: 61.51554870605469\n",
      "[step: 722] loss: 0.006855328567326069\n",
      "[step: 723] loss: 61.84680938720703\n",
      "[step: 723] loss: 0.0068414765410125256\n",
      "[step: 724] loss: 63.69731140136719\n",
      "[step: 724] loss: 0.00681684073060751\n",
      "[step: 725] loss: 63.76094436645508\n",
      "[step: 725] loss: 0.006807208526879549\n",
      "[step: 726] loss: 62.438697814941406\n",
      "[step: 726] loss: 0.006816927809268236\n",
      "[step: 727] loss: 61.72175979614258\n",
      "[step: 727] loss: 0.006829830817878246\n",
      "[step: 728] loss: 62.14972686767578\n",
      "[step: 728] loss: 0.006830975413322449\n",
      "[step: 729] loss: 61.39549255371094\n",
      "[step: 729] loss: 0.006818761117756367\n",
      "[step: 730] loss: 60.296241760253906\n",
      "[step: 730] loss: 0.006806358695030212\n",
      "[step: 731] loss: 60.31231689453125\n",
      "[step: 731] loss: 0.006803916767239571\n",
      "[step: 732] loss: 61.30796432495117\n",
      "[step: 732] loss: 0.00681030610576272\n",
      "[step: 733] loss: 62.664981842041016\n",
      "[step: 733] loss: 0.00681685796007514\n",
      "[step: 734] loss: 62.2913932800293\n",
      "[step: 734] loss: 0.00681639276444912\n",
      "[step: 735] loss: 61.72947692871094\n",
      "[step: 735] loss: 0.006809977814555168\n",
      "[step: 736] loss: 61.58222961425781\n",
      "[step: 736] loss: 0.006802609656006098\n",
      "[step: 737] loss: 62.1268424987793\n",
      "[step: 737] loss: 0.006799147464334965\n",
      "[step: 738] loss: 62.32026672363281\n",
      "[step: 738] loss: 0.006800459232181311\n",
      "[step: 739] loss: 62.22395324707031\n",
      "[step: 739] loss: 0.006804017815738916\n",
      "[step: 740] loss: 61.88825607299805\n",
      "[step: 740] loss: 0.006806710734963417\n",
      "[step: 741] loss: 62.44258117675781\n",
      "[step: 741] loss: 0.006806565914303064\n",
      "[step: 742] loss: 62.897300720214844\n",
      "[step: 742] loss: 0.006804036442190409\n",
      "[step: 743] loss: 64.29873657226562\n",
      "[step: 743] loss: 0.0068001653999090195\n",
      "[step: 744] loss: 63.93751525878906\n",
      "[step: 744] loss: 0.006796589586883783\n",
      "[step: 745] loss: 64.3437271118164\n",
      "[step: 745] loss: 0.0067941658198833466\n",
      "[step: 746] loss: 64.03680419921875\n",
      "[step: 746] loss: 0.00679317070171237\n",
      "[step: 747] loss: 64.91792297363281\n",
      "[step: 747] loss: 0.006793304346501827\n",
      "[step: 748] loss: 65.73271942138672\n",
      "[step: 748] loss: 0.006794144865125418\n",
      "[step: 749] loss: 65.77880859375\n",
      "[step: 749] loss: 0.00679532578215003\n",
      "[step: 750] loss: 64.72244262695312\n",
      "[step: 750] loss: 0.006796585395932198\n",
      "[step: 751] loss: 63.26816177368164\n",
      "[step: 751] loss: 0.0067980471067130566\n",
      "[step: 752] loss: 61.81724166870117\n",
      "[step: 752] loss: 0.006799633149057627\n",
      "[step: 753] loss: 61.33876037597656\n",
      "[step: 753] loss: 0.006801934912800789\n",
      "[step: 754] loss: 60.16200256347656\n",
      "[step: 754] loss: 0.006804947275668383\n",
      "[step: 755] loss: 59.18599319458008\n",
      "[step: 755] loss: 0.006809995509684086\n",
      "[step: 756] loss: 58.47608184814453\n",
      "[step: 756] loss: 0.0068170614540576935\n",
      "[step: 757] loss: 58.51142120361328\n",
      "[step: 757] loss: 0.006829145830124617\n",
      "[step: 758] loss: 59.20780563354492\n",
      "[step: 758] loss: 0.006845612544566393\n",
      "[step: 759] loss: 59.87775421142578\n",
      "[step: 759] loss: 0.006872446276247501\n",
      "[step: 760] loss: 60.608436584472656\n",
      "[step: 760] loss: 0.006902811583131552\n",
      "[step: 761] loss: 60.36375427246094\n",
      "[step: 761] loss: 0.006942783948034048\n",
      "[step: 762] loss: 60.44952392578125\n",
      "[step: 762] loss: 0.006961796898394823\n",
      "[step: 763] loss: 60.49440002441406\n",
      "[step: 763] loss: 0.006958642043173313\n",
      "[step: 764] loss: 61.0567512512207\n",
      "[step: 764] loss: 0.006903624162077904\n",
      "[step: 765] loss: 61.83776092529297\n",
      "[step: 765] loss: 0.006833973806351423\n",
      "[step: 766] loss: 63.077423095703125\n",
      "[step: 766] loss: 0.006788608152419329\n",
      "[step: 767] loss: 64.18247985839844\n",
      "[step: 767] loss: 0.006793443579226732\n",
      "[step: 768] loss: 66.38102722167969\n",
      "[step: 768] loss: 0.006831672042608261\n",
      "[step: 769] loss: 66.75895690917969\n",
      "[step: 769] loss: 0.006867073941975832\n",
      "[step: 770] loss: 68.11582946777344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 770] loss: 0.006878495216369629\n",
      "[step: 771] loss: 65.90390014648438\n",
      "[step: 771] loss: 0.006854246836155653\n",
      "[step: 772] loss: 65.01083374023438\n",
      "[step: 772] loss: 0.00681542232632637\n",
      "[step: 773] loss: 62.13310241699219\n",
      "[step: 773] loss: 0.006784890778362751\n",
      "[step: 774] loss: 61.74845504760742\n",
      "[step: 774] loss: 0.00678042508661747\n",
      "[step: 775] loss: 61.03290557861328\n",
      "[step: 775] loss: 0.006797880865633488\n",
      "[step: 776] loss: 61.54048156738281\n",
      "[step: 776] loss: 0.006821716669946909\n",
      "[step: 777] loss: 59.54438018798828\n",
      "[step: 777] loss: 0.0068404776975512505\n",
      "[step: 778] loss: 58.22061538696289\n",
      "[step: 778] loss: 0.006841825786978006\n",
      "[step: 779] loss: 59.598167419433594\n",
      "[step: 779] loss: 0.006829575635492802\n",
      "[step: 780] loss: 60.907318115234375\n",
      "[step: 780] loss: 0.006805209908634424\n",
      "[step: 781] loss: 61.78782653808594\n",
      "[step: 781] loss: 0.006783922202885151\n",
      "[step: 782] loss: 59.09723663330078\n",
      "[step: 782] loss: 0.0067740315571427345\n",
      "[step: 783] loss: 58.051177978515625\n",
      "[step: 783] loss: 0.006777090020477772\n",
      "[step: 784] loss: 58.89696502685547\n",
      "[step: 784] loss: 0.0067883143201470375\n",
      "[step: 785] loss: 59.66850662231445\n",
      "[step: 785] loss: 0.00680099381133914\n",
      "[step: 786] loss: 59.62528610229492\n",
      "[step: 786] loss: 0.006810931954532862\n",
      "[step: 787] loss: 58.92854309082031\n",
      "[step: 787] loss: 0.006812809966504574\n",
      "[step: 788] loss: 58.08610534667969\n",
      "[step: 788] loss: 0.006808350328356028\n",
      "[step: 789] loss: 57.43690490722656\n",
      "[step: 789] loss: 0.006797242444008589\n",
      "[step: 790] loss: 57.21179962158203\n",
      "[step: 790] loss: 0.006784898694604635\n",
      "[step: 791] loss: 57.11072540283203\n",
      "[step: 791] loss: 0.006774300709366798\n",
      "[step: 792] loss: 56.98808288574219\n",
      "[step: 792] loss: 0.006768364459276199\n",
      "[step: 793] loss: 57.39508819580078\n",
      "[step: 793] loss: 0.0067672692239284515\n",
      "[step: 794] loss: 58.20588302612305\n",
      "[step: 794] loss: 0.006769818253815174\n",
      "[step: 795] loss: 59.35504913330078\n",
      "[step: 795] loss: 0.0067747184075415134\n",
      "[step: 796] loss: 58.776123046875\n",
      "[step: 796] loss: 0.006780949421226978\n",
      "[step: 797] loss: 58.31598663330078\n",
      "[step: 797] loss: 0.006788649130612612\n",
      "[step: 798] loss: 57.62350845336914\n",
      "[step: 798] loss: 0.006797140929847956\n",
      "[step: 799] loss: 58.09440612792969\n",
      "[step: 799] loss: 0.006808485370129347\n",
      "[step: 800] loss: 59.14995574951172\n",
      "[step: 800] loss: 0.006820474751293659\n",
      "[step: 801] loss: 61.1440315246582\n",
      "[step: 801] loss: 0.006836330983787775\n",
      "[step: 802] loss: 64.7794189453125\n",
      "[step: 802] loss: 0.006849261000752449\n",
      "[step: 803] loss: 70.46216583251953\n",
      "[step: 803] loss: 0.006862698122859001\n",
      "[step: 804] loss: 77.42951202392578\n",
      "[step: 804] loss: 0.006862979847937822\n",
      "[step: 805] loss: 85.95658111572266\n",
      "[step: 805] loss: 0.006853790488094091\n",
      "[step: 806] loss: 87.59113311767578\n",
      "[step: 806] loss: 0.006827203556895256\n",
      "[step: 807] loss: 87.57623291015625\n",
      "[step: 807] loss: 0.0067964885383844376\n",
      "[step: 808] loss: 68.03776550292969\n",
      "[step: 808] loss: 0.006770506966859102\n",
      "[step: 809] loss: 57.24784469604492\n",
      "[step: 809] loss: 0.00675883749499917\n",
      "[step: 810] loss: 61.86515426635742\n",
      "[step: 810] loss: 0.006761821452528238\n",
      "[step: 811] loss: 71.15277862548828\n",
      "[step: 811] loss: 0.006774734240025282\n",
      "[step: 812] loss: 75.61227416992188\n",
      "[step: 812] loss: 0.006792784668505192\n",
      "[step: 813] loss: 62.703285217285156\n",
      "[step: 813] loss: 0.006811119616031647\n",
      "[step: 814] loss: 60.526641845703125\n",
      "[step: 814] loss: 0.006829122081398964\n",
      "[step: 815] loss: 67.61834716796875\n",
      "[step: 815] loss: 0.006838869769126177\n",
      "[step: 816] loss: 62.50851058959961\n",
      "[step: 816] loss: 0.00684277294203639\n",
      "[step: 817] loss: 61.62083053588867\n",
      "[step: 817] loss: 0.006831551901996136\n",
      "[step: 818] loss: 63.75904846191406\n",
      "[step: 818] loss: 0.006813106592744589\n",
      "[step: 819] loss: 58.63315963745117\n",
      "[step: 819] loss: 0.00678772060200572\n",
      "[step: 820] loss: 56.19305419921875\n",
      "[step: 820] loss: 0.00676628015935421\n",
      "[step: 821] loss: 59.595375061035156\n",
      "[step: 821] loss: 0.0067535145208239555\n",
      "[step: 822] loss: 59.99128341674805\n",
      "[step: 822] loss: 0.006751357112079859\n",
      "[step: 823] loss: 56.839508056640625\n",
      "[step: 823] loss: 0.006757612340152264\n",
      "[step: 824] loss: 56.458396911621094\n",
      "[step: 824] loss: 0.006768956780433655\n",
      "[step: 825] loss: 57.60567855834961\n",
      "[step: 825] loss: 0.006783610209822655\n",
      "[step: 826] loss: 56.532379150390625\n",
      "[step: 826] loss: 0.006799131631851196\n",
      "[step: 827] loss: 55.79724884033203\n",
      "[step: 827] loss: 0.006816481705754995\n",
      "[step: 828] loss: 56.417076110839844\n",
      "[step: 828] loss: 0.006829161662608385\n",
      "[step: 829] loss: 56.014488220214844\n",
      "[step: 829] loss: 0.006838519126176834\n",
      "[step: 830] loss: 54.92105484008789\n",
      "[step: 830] loss: 0.00683345552533865\n",
      "[step: 831] loss: 55.24547576904297\n",
      "[step: 831] loss: 0.00681902002543211\n",
      "[step: 832] loss: 56.24692153930664\n",
      "[step: 832] loss: 0.006792842876166105\n",
      "[step: 833] loss: 55.47393035888672\n",
      "[step: 833] loss: 0.006767234764993191\n",
      "[step: 834] loss: 54.347450256347656\n",
      "[step: 834] loss: 0.0067492942325770855\n",
      "[step: 835] loss: 54.45756530761719\n",
      "[step: 835] loss: 0.0067437863908708096\n",
      "[step: 836] loss: 54.95964050292969\n",
      "[step: 836] loss: 0.006749182939529419\n",
      "[step: 837] loss: 55.25074005126953\n",
      "[step: 837] loss: 0.006761412136256695\n",
      "[step: 838] loss: 54.845367431640625\n",
      "[step: 838] loss: 0.006777535192668438\n",
      "[step: 839] loss: 54.270652770996094\n",
      "[step: 839] loss: 0.006794526241719723\n",
      "[step: 840] loss: 53.89915084838867\n",
      "[step: 840] loss: 0.00681300088763237\n",
      "[step: 841] loss: 53.87449645996094\n",
      "[step: 841] loss: 0.0068259472027421\n",
      "[step: 842] loss: 54.308143615722656\n",
      "[step: 842] loss: 0.006834532134234905\n",
      "[step: 843] loss: 54.465431213378906\n",
      "[step: 843] loss: 0.006827276665717363\n",
      "[step: 844] loss: 54.27313232421875\n",
      "[step: 844] loss: 0.006809975951910019\n",
      "[step: 845] loss: 53.94286346435547\n",
      "[step: 845] loss: 0.006781827658414841\n",
      "[step: 846] loss: 53.771278381347656\n",
      "[step: 846] loss: 0.006756098475307226\n",
      "[step: 847] loss: 53.7213134765625\n",
      "[step: 847] loss: 0.006740058772265911\n",
      "[step: 848] loss: 53.6969108581543\n",
      "[step: 848] loss: 0.006737187970429659\n",
      "[step: 849] loss: 53.814552307128906\n",
      "[step: 849] loss: 0.006744954269379377\n",
      "[step: 850] loss: 54.090240478515625\n",
      "[step: 850] loss: 0.006758899427950382\n",
      "[step: 851] loss: 54.385719299316406\n",
      "[step: 851] loss: 0.006776344496756792\n",
      "[step: 852] loss: 54.68758010864258\n",
      "[step: 852] loss: 0.006793759763240814\n",
      "[step: 853] loss: 55.16939163208008\n",
      "[step: 853] loss: 0.006811427418142557\n",
      "[step: 854] loss: 56.00442123413086\n",
      "[step: 854] loss: 0.006820997688919306\n",
      "[step: 855] loss: 57.246498107910156\n",
      "[step: 855] loss: 0.006823600269854069\n",
      "[step: 856] loss: 58.780155181884766\n",
      "[step: 856] loss: 0.006809713318943977\n",
      "[step: 857] loss: 59.099586486816406\n",
      "[step: 857] loss: 0.006787302903831005\n",
      "[step: 858] loss: 58.21977233886719\n",
      "[step: 858] loss: 0.006759801413863897\n",
      "[step: 859] loss: 55.154335021972656\n",
      "[step: 859] loss: 0.006739077158272266\n",
      "[step: 860] loss: 53.22026824951172\n",
      "[step: 860] loss: 0.006730124354362488\n",
      "[step: 861] loss: 53.46458435058594\n",
      "[step: 861] loss: 0.00673293462023139\n",
      "[step: 862] loss: 54.90631103515625\n",
      "[step: 862] loss: 0.006743873935192823\n",
      "[step: 863] loss: 56.58403396606445\n",
      "[step: 863] loss: 0.006758993957191706\n",
      "[step: 864] loss: 55.27187728881836\n",
      "[step: 864] loss: 0.006776873487979174\n",
      "[step: 865] loss: 53.83591842651367\n",
      "[step: 865] loss: 0.006793901789933443\n",
      "[step: 866] loss: 52.83869934082031\n",
      "[step: 866] loss: 0.006810630671679974\n",
      "[step: 867] loss: 53.229652404785156\n",
      "[step: 867] loss: 0.006817550864070654\n",
      "[step: 868] loss: 54.07958221435547\n",
      "[step: 868] loss: 0.0068161324597895145\n",
      "[step: 869] loss: 54.173065185546875\n",
      "[step: 869] loss: 0.006797861773520708\n",
      "[step: 870] loss: 53.82839584350586\n",
      "[step: 870] loss: 0.006772307679057121\n",
      "[step: 871] loss: 53.12064743041992\n",
      "[step: 871] loss: 0.006745219696313143\n",
      "[step: 872] loss: 52.59578323364258\n",
      "[step: 872] loss: 0.006727658677846193\n",
      "[step: 873] loss: 52.877227783203125\n",
      "[step: 873] loss: 0.006723090074956417\n",
      "[step: 874] loss: 53.89909744262695\n",
      "[step: 874] loss: 0.006729629822075367\n",
      "[step: 875] loss: 55.970218658447266\n",
      "[step: 875] loss: 0.006743147503584623\n",
      "[step: 876] loss: 58.201210021972656\n",
      "[step: 876] loss: 0.00675971619784832\n",
      "[step: 877] loss: 63.134315490722656\n",
      "[step: 877] loss: 0.006778419949114323\n",
      "[step: 878] loss: 69.81676483154297\n",
      "[step: 878] loss: 0.006794502027332783\n",
      "[step: 879] loss: 85.55524444580078\n",
      "[step: 879] loss: 0.006808178965002298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 880] loss: 97.90375518798828\n",
      "[step: 880] loss: 0.006809101905673742\n",
      "[step: 881] loss: 110.32076263427734\n",
      "[step: 881] loss: 0.0068000126630067825\n",
      "[step: 882] loss: 87.51632690429688\n",
      "[step: 882] loss: 0.006776314228773117\n",
      "[step: 883] loss: 61.45476531982422\n",
      "[step: 883] loss: 0.006749609485268593\n",
      "[step: 884] loss: 58.94506072998047\n",
      "[step: 884] loss: 0.006727329455316067\n",
      "[step: 885] loss: 72.4300308227539\n",
      "[step: 885] loss: 0.006716840900480747\n",
      "[step: 886] loss: 75.029541015625\n",
      "[step: 886] loss: 0.006718423683196306\n",
      "[step: 887] loss: 59.2951545715332\n",
      "[step: 887] loss: 0.006728558335453272\n",
      "[step: 888] loss: 59.53887176513672\n",
      "[step: 888] loss: 0.006743712816387415\n",
      "[step: 889] loss: 66.08049011230469\n",
      "[step: 889] loss: 0.006760735064744949\n",
      "[step: 890] loss: 59.19416046142578\n",
      "[step: 890] loss: 0.006779494229704142\n",
      "[step: 891] loss: 59.79447937011719\n",
      "[step: 891] loss: 0.006794107146561146\n",
      "[step: 892] loss: 59.712196350097656\n",
      "[step: 892] loss: 0.006804673466831446\n",
      "[step: 893] loss: 56.37224197387695\n",
      "[step: 893] loss: 0.006800496485084295\n",
      "[step: 894] loss: 56.16713333129883\n",
      "[step: 894] loss: 0.006785749923437834\n",
      "[step: 895] loss: 56.11000061035156\n",
      "[step: 895] loss: 0.006759027484804392\n",
      "[step: 896] loss: 55.13511657714844\n",
      "[step: 896] loss: 0.006732885260134935\n",
      "[step: 897] loss: 54.37071990966797\n",
      "[step: 897] loss: 0.006714825052767992\n",
      "[step: 898] loss: 55.45256042480469\n",
      "[step: 898] loss: 0.006709431763738394\n",
      "[step: 899] loss: 53.79670333862305\n",
      "[step: 899] loss: 0.006715045310556889\n",
      "[step: 900] loss: 52.427921295166016\n",
      "[step: 900] loss: 0.006727546453475952\n",
      "[step: 901] loss: 54.443851470947266\n",
      "[step: 901] loss: 0.006744025275111198\n",
      "[step: 902] loss: 53.32679748535156\n",
      "[step: 902] loss: 0.006761403754353523\n",
      "[step: 903] loss: 52.45861053466797\n",
      "[step: 903] loss: 0.0067796665243804455\n",
      "[step: 904] loss: 54.26769256591797\n",
      "[step: 904] loss: 0.006791500840336084\n",
      "[step: 905] loss: 54.58336639404297\n",
      "[step: 905] loss: 0.0067971330136060715\n",
      "[step: 906] loss: 53.59950256347656\n",
      "[step: 906] loss: 0.0067868586629629135\n",
      "[step: 907] loss: 55.378395080566406\n",
      "[step: 907] loss: 0.00676698237657547\n",
      "[step: 908] loss: 55.102325439453125\n",
      "[step: 908] loss: 0.006739624310284853\n",
      "[step: 909] loss: 54.128021240234375\n",
      "[step: 909] loss: 0.006716767325997353\n",
      "[step: 910] loss: 52.87408447265625\n",
      "[step: 910] loss: 0.006704346276819706\n",
      "[step: 911] loss: 52.443458557128906\n",
      "[step: 911] loss: 0.0067038643173873425\n",
      "[step: 912] loss: 51.11644744873047\n",
      "[step: 912] loss: 0.006712380796670914\n",
      "[step: 913] loss: 51.34435272216797\n",
      "[step: 913] loss: 0.006726028397679329\n",
      "[step: 914] loss: 52.59673309326172\n",
      "[step: 914] loss: 0.006742882076650858\n",
      "[step: 915] loss: 52.541542053222656\n",
      "[step: 915] loss: 0.006759804207831621\n",
      "[step: 916] loss: 53.580177307128906\n",
      "[step: 916] loss: 0.006776823662221432\n",
      "[step: 917] loss: 53.8155632019043\n",
      "[step: 917] loss: 0.006785922683775425\n",
      "[step: 918] loss: 53.31743621826172\n",
      "[step: 918] loss: 0.00678775692358613\n",
      "[step: 919] loss: 51.87286376953125\n",
      "[step: 919] loss: 0.006774033885449171\n",
      "[step: 920] loss: 51.12971496582031\n",
      "[step: 920] loss: 0.006752259563654661\n",
      "[step: 921] loss: 50.51396942138672\n",
      "[step: 921] loss: 0.0067260428331792355\n",
      "[step: 922] loss: 50.51493453979492\n",
      "[step: 922] loss: 0.006706027779728174\n",
      "[step: 923] loss: 51.269954681396484\n",
      "[step: 923] loss: 0.006696650758385658\n",
      "[step: 924] loss: 52.07555389404297\n",
      "[step: 924] loss: 0.006698019802570343\n",
      "[step: 925] loss: 53.0541877746582\n",
      "[step: 925] loss: 0.006707032211124897\n",
      "[step: 926] loss: 52.988128662109375\n",
      "[step: 926] loss: 0.006720379926264286\n",
      "[step: 927] loss: 53.120182037353516\n",
      "[step: 927] loss: 0.006736698094755411\n",
      "[step: 928] loss: 51.87592697143555\n",
      "[step: 928] loss: 0.006752952933311462\n",
      "[step: 929] loss: 50.72406768798828\n",
      "[step: 929] loss: 0.0067692287266254425\n",
      "[step: 930] loss: 50.040252685546875\n",
      "[step: 930] loss: 0.006777861155569553\n",
      "[step: 931] loss: 50.015625\n",
      "[step: 931] loss: 0.006779813207685947\n",
      "[step: 932] loss: 50.297428131103516\n",
      "[step: 932] loss: 0.0067671979777514935\n",
      "[step: 933] loss: 50.913108825683594\n",
      "[step: 933] loss: 0.006747018545866013\n",
      "[step: 934] loss: 51.975135803222656\n",
      "[step: 934] loss: 0.006721902173012495\n",
      "[step: 935] loss: 52.4289436340332\n",
      "[step: 935] loss: 0.006701761391013861\n",
      "[step: 936] loss: 53.020179748535156\n",
      "[step: 936] loss: 0.006690856534987688\n",
      "[step: 937] loss: 52.420982360839844\n",
      "[step: 937] loss: 0.006689941976219416\n",
      "[step: 938] loss: 51.96492385864258\n",
      "[step: 938] loss: 0.006696484517306089\n",
      "[step: 939] loss: 50.602783203125\n",
      "[step: 939] loss: 0.006707622669637203\n",
      "[step: 940] loss: 49.66632080078125\n",
      "[step: 940] loss: 0.00672188913449645\n",
      "[step: 941] loss: 49.27313232421875\n",
      "[step: 941] loss: 0.0067369272001087666\n",
      "[step: 942] loss: 49.39506912231445\n",
      "[step: 942] loss: 0.006752945948392153\n",
      "[step: 943] loss: 49.84893035888672\n",
      "[step: 943] loss: 0.006764211691915989\n",
      "[step: 944] loss: 50.41154479980469\n",
      "[step: 944] loss: 0.006771720480173826\n",
      "[step: 945] loss: 51.401336669921875\n",
      "[step: 945] loss: 0.006767235230654478\n",
      "[step: 946] loss: 52.12367248535156\n",
      "[step: 946] loss: 0.006755197420716286\n",
      "[step: 947] loss: 53.29795837402344\n",
      "[step: 947] loss: 0.006733621936291456\n",
      "[step: 948] loss: 53.036293029785156\n",
      "[step: 948] loss: 0.006711598951369524\n",
      "[step: 949] loss: 53.05048370361328\n",
      "[step: 949] loss: 0.006693580653518438\n",
      "[step: 950] loss: 52.05205154418945\n",
      "[step: 950] loss: 0.0066839177161455154\n",
      "[step: 951] loss: 51.206939697265625\n",
      "[step: 951] loss: 0.006682417821139097\n",
      "[step: 952] loss: 50.18357849121094\n",
      "[step: 952] loss: 0.006687154062092304\n",
      "[step: 953] loss: 49.400299072265625\n",
      "[step: 953] loss: 0.006696036551147699\n",
      "[step: 954] loss: 48.87299346923828\n",
      "[step: 954] loss: 0.00670729810371995\n",
      "[step: 955] loss: 48.63965606689453\n",
      "[step: 955] loss: 0.0067206574603915215\n",
      "[step: 956] loss: 48.66659927368164\n",
      "[step: 956] loss: 0.006733876187354326\n",
      "[step: 957] loss: 48.87885665893555\n",
      "[step: 957] loss: 0.00674812775105238\n",
      "[step: 958] loss: 49.21448516845703\n",
      "[step: 958] loss: 0.006758121307939291\n",
      "[step: 959] loss: 49.648902893066406\n",
      "[step: 959] loss: 0.00676594115793705\n",
      "[step: 960] loss: 50.35588073730469\n",
      "[step: 960] loss: 0.006763418670743704\n",
      "[step: 961] loss: 51.48032760620117\n",
      "[step: 961] loss: 0.006754560396075249\n",
      "[step: 962] loss: 54.29880142211914\n",
      "[step: 962] loss: 0.006735395174473524\n",
      "[step: 963] loss: 57.410850524902344\n",
      "[step: 963] loss: 0.0067139072343707085\n",
      "[step: 964] loss: 64.47164916992188\n",
      "[step: 964] loss: 0.006693579722195864\n",
      "[step: 965] loss: 61.59880828857422\n",
      "[step: 965] loss: 0.0066800410859286785\n",
      "[step: 966] loss: 58.81752014160156\n",
      "[step: 966] loss: 0.006674579344689846\n",
      "[step: 967] loss: 57.28668212890625\n",
      "[step: 967] loss: 0.006676243152469397\n",
      "[step: 968] loss: 59.21598815917969\n",
      "[step: 968] loss: 0.006682869046926498\n",
      "[step: 969] loss: 56.13972091674805\n",
      "[step: 969] loss: 0.006692314054816961\n",
      "[step: 970] loss: 50.265602111816406\n",
      "[step: 970] loss: 0.00670368317514658\n",
      "[step: 971] loss: 49.70637893676758\n",
      "[step: 971] loss: 0.006715407129377127\n",
      "[step: 972] loss: 52.87985610961914\n",
      "[step: 972] loss: 0.006728675216436386\n",
      "[step: 973] loss: 52.633209228515625\n",
      "[step: 973] loss: 0.006740108132362366\n",
      "[step: 974] loss: 51.653160095214844\n",
      "[step: 974] loss: 0.006751910783350468\n",
      "[step: 975] loss: 52.811973571777344\n",
      "[step: 975] loss: 0.0067569962702691555\n",
      "[step: 976] loss: 51.42516326904297\n",
      "[step: 976] loss: 0.0067579252645373344\n",
      "[step: 977] loss: 49.47456359863281\n",
      "[step: 977] loss: 0.006747135892510414\n",
      "[step: 978] loss: 49.06502914428711\n",
      "[step: 978] loss: 0.00673013785853982\n",
      "[step: 979] loss: 49.96841812133789\n",
      "[step: 979] loss: 0.006707154214382172\n",
      "[step: 980] loss: 49.981964111328125\n",
      "[step: 980] loss: 0.006686476059257984\n",
      "[step: 981] loss: 49.28323745727539\n",
      "[step: 981] loss: 0.006672235205769539\n",
      "[step: 982] loss: 49.90682601928711\n",
      "[step: 982] loss: 0.006666832137852907\n",
      "[step: 983] loss: 50.631378173828125\n",
      "[step: 983] loss: 0.006669162772595882\n",
      "[step: 984] loss: 50.0856819152832\n",
      "[step: 984] loss: 0.006676680874079466\n",
      "[step: 985] loss: 48.75246810913086\n",
      "[step: 985] loss: 0.00668717548251152\n",
      "[step: 986] loss: 48.16544723510742\n",
      "[step: 986] loss: 0.006698629353195429\n",
      "[step: 987] loss: 48.544349670410156\n",
      "[step: 987] loss: 0.006711170077323914\n",
      "[step: 988] loss: 48.88916015625\n",
      "[step: 988] loss: 0.006722182501107454\n",
      "[step: 989] loss: 48.36804962158203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 989] loss: 0.006733284797519445\n",
      "[step: 990] loss: 47.881614685058594\n",
      "[step: 990] loss: 0.006739099510014057\n",
      "[step: 991] loss: 47.940635681152344\n",
      "[step: 991] loss: 0.0067419447004795074\n",
      "[step: 992] loss: 48.32816696166992\n",
      "[step: 992] loss: 0.006735342089086771\n",
      "[step: 993] loss: 48.34551239013672\n",
      "[step: 993] loss: 0.006723620463162661\n",
      "[step: 994] loss: 48.335304260253906\n",
      "[step: 994] loss: 0.006705237552523613\n",
      "[step: 995] loss: 48.92284393310547\n",
      "[step: 995] loss: 0.006686818320304155\n",
      "[step: 996] loss: 50.97654724121094\n",
      "[step: 996] loss: 0.006671292707324028\n",
      "[step: 997] loss: 54.38384246826172\n",
      "[step: 997] loss: 0.006661956198513508\n",
      "[step: 998] loss: 62.89751434326172\n",
      "[step: 998] loss: 0.00665906211361289\n",
      "[step: 999] loss: 69.68611145019531\n",
      "[step: 999] loss: 0.006661402992904186\n",
      "[step: 1000] loss: 80.8941650390625\n",
      "[step: 1000] loss: 0.00666730897501111\n",
      "[step: 1001] loss: 78.00948333740234\n",
      "[step: 1001] loss: 0.006675353739410639\n",
      "[step: 1002] loss: 75.65562438964844\n",
      "[step: 1002] loss: 0.006685252767056227\n",
      "[step: 1003] loss: 63.11949157714844\n",
      "[step: 1003] loss: 0.006696049589663744\n",
      "[step: 1004] loss: 51.986541748046875\n",
      "[step: 1004] loss: 0.006709206383675337\n",
      "[step: 1005] loss: 53.82575225830078\n",
      "[step: 1005] loss: 0.006722267717123032\n",
      "[step: 1006] loss: 60.60984802246094\n",
      "[step: 1006] loss: 0.006737689021974802\n",
      "[step: 1007] loss: 62.13017272949219\n",
      "[step: 1007] loss: 0.00674882996827364\n",
      "[step: 1008] loss: 56.35130310058594\n",
      "[step: 1008] loss: 0.006757833529263735\n",
      "[step: 1009] loss: 51.791168212890625\n",
      "[step: 1009] loss: 0.006754005327820778\n",
      "[step: 1010] loss: 50.584083557128906\n",
      "[step: 1010] loss: 0.006741172634065151\n",
      "[step: 1011] loss: 55.77701187133789\n",
      "[step: 1011] loss: 0.00671538757160306\n",
      "[step: 1012] loss: 57.41938781738281\n",
      "[step: 1012] loss: 0.006687461864203215\n",
      "[step: 1013] loss: 49.62481689453125\n",
      "[step: 1013] loss: 0.00666428916156292\n",
      "[step: 1014] loss: 48.97541427612305\n",
      "[step: 1014] loss: 0.006652886979281902\n",
      "[step: 1015] loss: 52.81343078613281\n",
      "[step: 1015] loss: 0.006653721909970045\n",
      "[step: 1016] loss: 51.594139099121094\n",
      "[step: 1016] loss: 0.006663251668214798\n",
      "[step: 1017] loss: 49.51539611816406\n",
      "[step: 1017] loss: 0.006677024066448212\n",
      "[step: 1018] loss: 49.478004455566406\n",
      "[step: 1018] loss: 0.006690737791359425\n",
      "[step: 1019] loss: 49.427146911621094\n",
      "[step: 1019] loss: 0.006703125312924385\n",
      "[step: 1020] loss: 48.75328826904297\n",
      "[step: 1020] loss: 0.006710090674459934\n",
      "[step: 1021] loss: 48.88290023803711\n",
      "[step: 1021] loss: 0.006713191978633404\n",
      "[step: 1022] loss: 49.44314193725586\n",
      "[step: 1022] loss: 0.006708132103085518\n",
      "[step: 1023] loss: 48.908531188964844\n",
      "[step: 1023] loss: 0.006698797456920147\n",
      "[step: 1024] loss: 48.099693298339844\n",
      "[step: 1024] loss: 0.006684229709208012\n",
      "[step: 1025] loss: 48.442108154296875\n",
      "[step: 1025] loss: 0.006669633090496063\n",
      "[step: 1026] loss: 48.68922424316406\n",
      "[step: 1026] loss: 0.0066569470800459385\n",
      "[step: 1027] loss: 47.23925018310547\n",
      "[step: 1027] loss: 0.006648736540228128\n",
      "[step: 1028] loss: 46.98527526855469\n",
      "[step: 1028] loss: 0.006645208690315485\n",
      "[step: 1029] loss: 47.83025360107422\n",
      "[step: 1029] loss: 0.006645622663199902\n",
      "[step: 1030] loss: 47.32303237915039\n",
      "[step: 1030] loss: 0.00664887297898531\n",
      "[step: 1031] loss: 46.49436950683594\n",
      "[step: 1031] loss: 0.006654008291661739\n",
      "[step: 1032] loss: 46.901947021484375\n",
      "[step: 1032] loss: 0.006660704035311937\n",
      "[step: 1033] loss: 47.372398376464844\n",
      "[step: 1033] loss: 0.006668486632406712\n",
      "[step: 1034] loss: 46.97930145263672\n",
      "[step: 1034] loss: 0.006678277626633644\n",
      "[step: 1035] loss: 46.917083740234375\n",
      "[step: 1035] loss: 0.006689210422337055\n",
      "[step: 1036] loss: 47.516754150390625\n",
      "[step: 1036] loss: 0.006703522522002459\n",
      "[step: 1037] loss: 48.46135711669922\n",
      "[step: 1037] loss: 0.006718248128890991\n",
      "[step: 1038] loss: 49.459022521972656\n",
      "[step: 1038] loss: 0.006736229173839092\n",
      "[step: 1039] loss: 52.624671936035156\n",
      "[step: 1039] loss: 0.006749157328158617\n",
      "[step: 1040] loss: 54.46021270751953\n",
      "[step: 1040] loss: 0.006758770439773798\n",
      "[step: 1041] loss: 57.170257568359375\n",
      "[step: 1041] loss: 0.0067518651485443115\n",
      "[step: 1042] loss: 52.500511169433594\n",
      "[step: 1042] loss: 0.00673288619145751\n",
      "[step: 1043] loss: 49.85246658325195\n",
      "[step: 1043] loss: 0.006699483375996351\n",
      "[step: 1044] loss: 49.434810638427734\n",
      "[step: 1044] loss: 0.0066664814949035645\n",
      "[step: 1045] loss: 49.10626983642578\n",
      "[step: 1045] loss: 0.006643875502049923\n",
      "[step: 1046] loss: 48.53205871582031\n",
      "[step: 1046] loss: 0.006638106890022755\n",
      "[step: 1047] loss: 48.57195281982422\n",
      "[step: 1047] loss: 0.006646743975579739\n",
      "[step: 1048] loss: 50.44967269897461\n",
      "[step: 1048] loss: 0.006662995554506779\n",
      "[step: 1049] loss: 51.447845458984375\n",
      "[step: 1049] loss: 0.006680723745375872\n",
      "[step: 1050] loss: 50.008541107177734\n",
      "[step: 1050] loss: 0.006693679839372635\n",
      "[step: 1051] loss: 47.05888366699219\n",
      "[step: 1051] loss: 0.006701161619275808\n",
      "[step: 1052] loss: 45.9324836730957\n",
      "[step: 1052] loss: 0.006698330398648977\n",
      "[step: 1053] loss: 47.07887268066406\n",
      "[step: 1053] loss: 0.006689079571515322\n",
      "[step: 1054] loss: 48.23793029785156\n",
      "[step: 1054] loss: 0.006672797724604607\n",
      "[step: 1055] loss: 47.96498107910156\n",
      "[step: 1055] loss: 0.0066560362465679646\n",
      "[step: 1056] loss: 46.80644226074219\n",
      "[step: 1056] loss: 0.006641928106546402\n",
      "[step: 1057] loss: 46.506778717041016\n",
      "[step: 1057] loss: 0.006633820943534374\n",
      "[step: 1058] loss: 47.478065490722656\n",
      "[step: 1058] loss: 0.006632004864513874\n",
      "[step: 1059] loss: 49.3244743347168\n",
      "[step: 1059] loss: 0.006635128986090422\n",
      "[step: 1060] loss: 49.5622444152832\n",
      "[step: 1060] loss: 0.006641322746872902\n",
      "[step: 1061] loss: 49.7280158996582\n",
      "[step: 1061] loss: 0.00664883479475975\n",
      "[step: 1062] loss: 48.63679504394531\n",
      "[step: 1062] loss: 0.00665699690580368\n",
      "[step: 1063] loss: 49.0935173034668\n",
      "[step: 1063] loss: 0.006664333399385214\n",
      "[step: 1064] loss: 49.96518325805664\n",
      "[step: 1064] loss: 0.006671379320323467\n",
      "[step: 1065] loss: 49.79619598388672\n",
      "[step: 1065] loss: 0.006675914395600557\n",
      "[step: 1066] loss: 48.292686462402344\n",
      "[step: 1066] loss: 0.0066794161684811115\n",
      "[step: 1067] loss: 47.06863784790039\n",
      "[step: 1067] loss: 0.006679147947579622\n",
      "[step: 1068] loss: 46.987220764160156\n",
      "[step: 1068] loss: 0.006677357014268637\n",
      "[step: 1069] loss: 47.666202545166016\n",
      "[step: 1069] loss: 0.0066718352027237415\n",
      "[step: 1070] loss: 47.01868438720703\n",
      "[step: 1070] loss: 0.0066652302630245686\n",
      "[step: 1071] loss: 45.96925354003906\n",
      "[step: 1071] loss: 0.006656622514128685\n",
      "[step: 1072] loss: 45.237152099609375\n",
      "[step: 1072] loss: 0.006648306269198656\n",
      "[step: 1073] loss: 45.38911437988281\n",
      "[step: 1073] loss: 0.0066402992233633995\n",
      "[step: 1074] loss: 45.84544372558594\n",
      "[step: 1074] loss: 0.006633804179728031\n",
      "[step: 1075] loss: 45.977821350097656\n",
      "[step: 1075] loss: 0.006628823932260275\n",
      "[step: 1076] loss: 45.865970611572266\n",
      "[step: 1076] loss: 0.006625446956604719\n",
      "[step: 1077] loss: 45.721412658691406\n",
      "[step: 1077] loss: 0.006623373832553625\n",
      "[step: 1078] loss: 46.215187072753906\n",
      "[step: 1078] loss: 0.00662228325381875\n",
      "[step: 1079] loss: 47.37660598754883\n",
      "[step: 1079] loss: 0.0066218567080795765\n",
      "[step: 1080] loss: 49.4056396484375\n",
      "[step: 1080] loss: 0.006621955428272486\n",
      "[step: 1081] loss: 52.29142761230469\n",
      "[step: 1081] loss: 0.00662257382646203\n",
      "[step: 1082] loss: 58.24523162841797\n",
      "[step: 1082] loss: 0.006623899098485708\n",
      "[step: 1083] loss: 64.31814575195312\n",
      "[step: 1083] loss: 0.006626471411436796\n",
      "[step: 1084] loss: 75.2518539428711\n",
      "[step: 1084] loss: 0.006631159223616123\n",
      "[step: 1085] loss: 67.59832000732422\n",
      "[step: 1085] loss: 0.006640004459768534\n",
      "[step: 1086] loss: 58.54468536376953\n",
      "[step: 1086] loss: 0.00665591424331069\n",
      "[step: 1087] loss: 52.36896514892578\n",
      "[step: 1087] loss: 0.0066861435770988464\n",
      "[step: 1088] loss: 48.65797805786133\n",
      "[step: 1088] loss: 0.006737954448908567\n",
      "[step: 1089] loss: 50.88895797729492\n",
      "[step: 1089] loss: 0.0068307965993881226\n",
      "[step: 1090] loss: 56.46643829345703\n",
      "[step: 1090] loss: 0.006956025026738644\n",
      "[step: 1091] loss: 58.31250762939453\n",
      "[step: 1091] loss: 0.00710871908813715\n",
      "[step: 1092] loss: 52.7921142578125\n",
      "[step: 1092] loss: 0.0071344939060509205\n",
      "[step: 1093] loss: 47.810821533203125\n",
      "[step: 1093] loss: 0.006980281323194504\n",
      "[step: 1094] loss: 47.50844192504883\n",
      "[step: 1094] loss: 0.006714888382703066\n",
      "[step: 1095] loss: 49.251678466796875\n",
      "[step: 1095] loss: 0.006640046369284391\n",
      "[step: 1096] loss: 51.06509780883789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1096] loss: 0.0067876302637159824\n",
      "[step: 1097] loss: 50.10509490966797\n",
      "[step: 1097] loss: 0.006881434470415115\n",
      "[step: 1098] loss: 47.13730239868164\n",
      "[step: 1098] loss: 0.006780391093343496\n",
      "[step: 1099] loss: 45.479896545410156\n",
      "[step: 1099] loss: 0.006639386992901564\n",
      "[step: 1100] loss: 47.23712158203125\n",
      "[step: 1100] loss: 0.006662184838205576\n",
      "[step: 1101] loss: 48.91365051269531\n",
      "[step: 1101] loss: 0.006764195393770933\n",
      "[step: 1102] loss: 47.93822479248047\n",
      "[step: 1102] loss: 0.006756961345672607\n",
      "[step: 1103] loss: 46.95939254760742\n",
      "[step: 1103] loss: 0.006659817416220903\n",
      "[step: 1104] loss: 46.57290267944336\n",
      "[step: 1104] loss: 0.0066203526221215725\n",
      "[step: 1105] loss: 46.57741165161133\n",
      "[step: 1105] loss: 0.006677838973701\n",
      "[step: 1106] loss: 45.572669982910156\n",
      "[step: 1106] loss: 0.006721486337482929\n",
      "[step: 1107] loss: 45.03715515136719\n",
      "[step: 1107] loss: 0.006679210811853409\n",
      "[step: 1108] loss: 45.86219787597656\n",
      "[step: 1108] loss: 0.006622140761464834\n",
      "[step: 1109] loss: 46.874000549316406\n",
      "[step: 1109] loss: 0.006627436727285385\n",
      "[step: 1110] loss: 47.49646759033203\n",
      "[step: 1110] loss: 0.006669425871223211\n",
      "[step: 1111] loss: 46.61040496826172\n",
      "[step: 1111] loss: 0.006677210330963135\n",
      "[step: 1112] loss: 45.92665481567383\n",
      "[step: 1112] loss: 0.006639110390096903\n",
      "[step: 1113] loss: 45.23480987548828\n",
      "[step: 1113] loss: 0.006613267119973898\n",
      "[step: 1114] loss: 44.91059112548828\n",
      "[step: 1114] loss: 0.006629761774092913\n",
      "[step: 1115] loss: 44.806968688964844\n",
      "[step: 1115] loss: 0.0066510834731161594\n",
      "[step: 1116] loss: 44.96384811401367\n",
      "[step: 1116] loss: 0.006641811225563288\n",
      "[step: 1117] loss: 45.49644088745117\n",
      "[step: 1117] loss: 0.0066167511977255344\n",
      "[step: 1118] loss: 45.85429763793945\n",
      "[step: 1118] loss: 0.006612472236156464\n",
      "[step: 1119] loss: 46.46942901611328\n",
      "[step: 1119] loss: 0.006628227885812521\n",
      "[step: 1120] loss: 46.81552505493164\n",
      "[step: 1120] loss: 0.0066340090706944466\n",
      "[step: 1121] loss: 46.58848571777344\n",
      "[step: 1121] loss: 0.006621264386922121\n",
      "[step: 1122] loss: 45.69440841674805\n",
      "[step: 1122] loss: 0.006608493160456419\n",
      "[step: 1123] loss: 45.07666778564453\n",
      "[step: 1123] loss: 0.006611672695726156\n",
      "[step: 1124] loss: 44.61711883544922\n",
      "[step: 1124] loss: 0.006621338427066803\n",
      "[step: 1125] loss: 44.397132873535156\n",
      "[step: 1125] loss: 0.006621605716645718\n",
      "[step: 1126] loss: 44.06068420410156\n",
      "[step: 1126] loss: 0.006612339522689581\n",
      "[step: 1127] loss: 43.79940414428711\n",
      "[step: 1127] loss: 0.006605289410799742\n",
      "[step: 1128] loss: 43.640106201171875\n",
      "[step: 1128] loss: 0.006607620511204004\n",
      "[step: 1129] loss: 43.570899963378906\n",
      "[step: 1129] loss: 0.006613187026232481\n",
      "[step: 1130] loss: 43.54938888549805\n",
      "[step: 1130] loss: 0.006613624282181263\n",
      "[step: 1131] loss: 43.54854965209961\n",
      "[step: 1131] loss: 0.006608127150684595\n",
      "[step: 1132] loss: 43.602943420410156\n",
      "[step: 1132] loss: 0.0066029285080730915\n",
      "[step: 1133] loss: 43.7374153137207\n",
      "[step: 1133] loss: 0.006602536421269178\n",
      "[step: 1134] loss: 44.12244415283203\n",
      "[step: 1134] loss: 0.006605476140975952\n",
      "[step: 1135] loss: 44.86619186401367\n",
      "[step: 1135] loss: 0.006607243791222572\n",
      "[step: 1136] loss: 47.186439514160156\n",
      "[step: 1136] loss: 0.0066054160706698895\n",
      "[step: 1137] loss: 51.334495544433594\n",
      "[step: 1137] loss: 0.006601728033274412\n",
      "[step: 1138] loss: 62.135074615478516\n",
      "[step: 1138] loss: 0.006599160842597485\n",
      "[step: 1139] loss: 65.3321533203125\n",
      "[step: 1139] loss: 0.006599185056984425\n",
      "[step: 1140] loss: 68.83966064453125\n",
      "[step: 1140] loss: 0.006600557826459408\n",
      "[step: 1141] loss: 64.68745422363281\n",
      "[step: 1141] loss: 0.006601350847631693\n",
      "[step: 1142] loss: 65.4098892211914\n",
      "[step: 1142] loss: 0.006600475404411554\n",
      "[step: 1143] loss: 53.883243560791016\n",
      "[step: 1143] loss: 0.006598423235118389\n",
      "[step: 1144] loss: 46.487281799316406\n",
      "[step: 1144] loss: 0.0065964460372924805\n",
      "[step: 1145] loss: 51.7848014831543\n",
      "[step: 1145] loss: 0.006595374085009098\n",
      "[step: 1146] loss: 54.94407653808594\n",
      "[step: 1146] loss: 0.006595333106815815\n",
      "[step: 1147] loss: 53.58821105957031\n",
      "[step: 1147] loss: 0.006595727521926165\n",
      "[step: 1148] loss: 51.63993835449219\n",
      "[step: 1148] loss: 0.006595965474843979\n",
      "[step: 1149] loss: 48.35655212402344\n",
      "[step: 1149] loss: 0.00659559341147542\n",
      "[step: 1150] loss: 45.24446105957031\n",
      "[step: 1150] loss: 0.0065946681424975395\n",
      "[step: 1151] loss: 50.96072006225586\n",
      "[step: 1151] loss: 0.006593449506908655\n",
      "[step: 1152] loss: 52.74995040893555\n",
      "[step: 1152] loss: 0.006592248100787401\n",
      "[step: 1153] loss: 46.238189697265625\n",
      "[step: 1153] loss: 0.0065913149155676365\n",
      "[step: 1154] loss: 47.48667907714844\n",
      "[step: 1154] loss: 0.006590696983039379\n",
      "[step: 1155] loss: 48.537322998046875\n",
      "[step: 1155] loss: 0.006590353790670633\n",
      "[step: 1156] loss: 45.635162353515625\n",
      "[step: 1156] loss: 0.006590153090655804\n",
      "[step: 1157] loss: 47.49964904785156\n",
      "[step: 1157] loss: 0.006590024568140507\n",
      "[step: 1158] loss: 47.735923767089844\n",
      "[step: 1158] loss: 0.006589846685528755\n",
      "[step: 1159] loss: 44.93256378173828\n",
      "[step: 1159] loss: 0.006589620839804411\n",
      "[step: 1160] loss: 45.23067092895508\n",
      "[step: 1160] loss: 0.006589346099644899\n",
      "[step: 1161] loss: 46.95410919189453\n",
      "[step: 1161] loss: 0.0065890466794371605\n",
      "[step: 1162] loss: 45.18293762207031\n",
      "[step: 1162] loss: 0.00658876309171319\n",
      "[step: 1163] loss: 43.53255081176758\n",
      "[step: 1163] loss: 0.006588547956198454\n",
      "[step: 1164] loss: 45.660892486572266\n",
      "[step: 1164] loss: 0.006588466931134462\n",
      "[step: 1165] loss: 45.90898895263672\n",
      "[step: 1165] loss: 0.00658861780539155\n",
      "[step: 1166] loss: 43.87995147705078\n",
      "[step: 1166] loss: 0.006589120719581842\n",
      "[step: 1167] loss: 43.68798065185547\n",
      "[step: 1167] loss: 0.00659024715423584\n",
      "[step: 1168] loss: 44.534400939941406\n",
      "[step: 1168] loss: 0.006592337042093277\n",
      "[step: 1169] loss: 43.928794860839844\n",
      "[step: 1169] loss: 0.006596216931939125\n",
      "[step: 1170] loss: 43.38983154296875\n",
      "[step: 1170] loss: 0.006602892652153969\n",
      "[step: 1171] loss: 43.567352294921875\n",
      "[step: 1171] loss: 0.006614862009882927\n",
      "[step: 1172] loss: 43.71205139160156\n",
      "[step: 1172] loss: 0.006634936667978764\n",
      "[step: 1173] loss: 43.35438919067383\n",
      "[step: 1173] loss: 0.006670559756457806\n",
      "[step: 1174] loss: 43.301361083984375\n",
      "[step: 1174] loss: 0.006725883577018976\n",
      "[step: 1175] loss: 43.45189666748047\n",
      "[step: 1175] loss: 0.006814008112996817\n",
      "[step: 1176] loss: 42.97367477416992\n",
      "[step: 1176] loss: 0.006911527831107378\n",
      "[step: 1177] loss: 42.74987030029297\n",
      "[step: 1177] loss: 0.006998930126428604\n",
      "[step: 1178] loss: 43.069419860839844\n",
      "[step: 1178] loss: 0.006960973143577576\n",
      "[step: 1179] loss: 43.157752990722656\n",
      "[step: 1179] loss: 0.0068045370280742645\n",
      "[step: 1180] loss: 42.871097564697266\n",
      "[step: 1180] loss: 0.006626842077821493\n",
      "[step: 1181] loss: 42.81440734863281\n",
      "[step: 1181] loss: 0.006601813714951277\n",
      "[step: 1182] loss: 43.092201232910156\n",
      "[step: 1182] loss: 0.006710344459861517\n",
      "[step: 1183] loss: 43.864845275878906\n",
      "[step: 1183] loss: 0.00677707651630044\n",
      "[step: 1184] loss: 45.02960968017578\n",
      "[step: 1184] loss: 0.0067168669775128365\n",
      "[step: 1185] loss: 48.042869567871094\n",
      "[step: 1185] loss: 0.006608645897358656\n",
      "[step: 1186] loss: 52.202537536621094\n",
      "[step: 1186] loss: 0.006592367310076952\n",
      "[step: 1187] loss: 62.178714752197266\n",
      "[step: 1187] loss: 0.006660755258053541\n",
      "[step: 1188] loss: 63.27384948730469\n",
      "[step: 1188] loss: 0.006698103621602058\n",
      "[step: 1189] loss: 61.95751953125\n",
      "[step: 1189] loss: 0.006655729375779629\n",
      "[step: 1190] loss: 54.92951965332031\n",
      "[step: 1190] loss: 0.006591372191905975\n",
      "[step: 1191] loss: 51.83305358886719\n",
      "[step: 1191] loss: 0.006585294380784035\n",
      "[step: 1192] loss: 48.159507751464844\n",
      "[step: 1192] loss: 0.0066268001683056355\n",
      "[step: 1193] loss: 46.221107482910156\n",
      "[step: 1193] loss: 0.006648525595664978\n",
      "[step: 1194] loss: 49.74249267578125\n",
      "[step: 1194] loss: 0.006623626220971346\n",
      "[step: 1195] loss: 52.85328674316406\n",
      "[step: 1195] loss: 0.006584883667528629\n",
      "[step: 1196] loss: 52.455039978027344\n",
      "[step: 1196] loss: 0.0065790461376309395\n",
      "[step: 1197] loss: 46.99689865112305\n",
      "[step: 1197] loss: 0.006602519191801548\n",
      "[step: 1198] loss: 43.327667236328125\n",
      "[step: 1198] loss: 0.006615948397666216\n",
      "[step: 1199] loss: 44.687042236328125\n",
      "[step: 1199] loss: 0.0066010355949401855\n",
      "[step: 1200] loss: 47.732364654541016\n",
      "[step: 1200] loss: 0.006578376051038504\n",
      "[step: 1201] loss: 47.079925537109375\n",
      "[step: 1201] loss: 0.006575480569154024\n",
      "[step: 1202] loss: 44.335609436035156\n",
      "[step: 1202] loss: 0.006589505821466446\n",
      "[step: 1203] loss: 44.521114349365234\n",
      "[step: 1203] loss: 0.006597249303013086\n",
      "[step: 1204] loss: 44.70536422729492\n",
      "[step: 1204] loss: 0.0065883430652320385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1205] loss: 43.53460693359375\n",
      "[step: 1205] loss: 0.006574363447725773\n",
      "[step: 1206] loss: 43.01871109008789\n",
      "[step: 1206] loss: 0.006571376230567694\n",
      "[step: 1207] loss: 43.87723922729492\n",
      "[step: 1207] loss: 0.006579117383807898\n",
      "[step: 1208] loss: 44.14020538330078\n",
      "[step: 1208] loss: 0.006584806367754936\n",
      "[step: 1209] loss: 43.76630401611328\n",
      "[step: 1209] loss: 0.006581074092537165\n",
      "[step: 1210] loss: 44.78776168823242\n",
      "[step: 1210] loss: 0.006572365295141935\n",
      "[step: 1211] loss: 43.94392395019531\n",
      "[step: 1211] loss: 0.006567852571606636\n",
      "[step: 1212] loss: 42.52825927734375\n",
      "[step: 1212] loss: 0.006570248864591122\n",
      "[step: 1213] loss: 42.48967742919922\n",
      "[step: 1213] loss: 0.006574741564691067\n",
      "[step: 1214] loss: 42.94409942626953\n",
      "[step: 1214] loss: 0.006575580686330795\n",
      "[step: 1215] loss: 42.3138313293457\n",
      "[step: 1215] loss: 0.006571636069566011\n",
      "[step: 1216] loss: 41.908050537109375\n",
      "[step: 1216] loss: 0.006566740572452545\n",
      "[step: 1217] loss: 42.54071807861328\n",
      "[step: 1217] loss: 0.006564581301063299\n",
      "[step: 1218] loss: 42.98865509033203\n",
      "[step: 1218] loss: 0.00656575383618474\n",
      "[step: 1219] loss: 43.149147033691406\n",
      "[step: 1219] loss: 0.006567958276718855\n",
      "[step: 1220] loss: 43.56370544433594\n",
      "[step: 1220] loss: 0.006568585988134146\n",
      "[step: 1221] loss: 44.31148147583008\n",
      "[step: 1221] loss: 0.00656692311167717\n",
      "[step: 1222] loss: 44.617576599121094\n",
      "[step: 1222] loss: 0.006564045790582895\n",
      "[step: 1223] loss: 45.73474884033203\n",
      "[step: 1223] loss: 0.006561711430549622\n",
      "[step: 1224] loss: 45.68902587890625\n",
      "[step: 1224] loss: 0.00656086253002286\n",
      "[step: 1225] loss: 46.33738708496094\n",
      "[step: 1225] loss: 0.006561318878084421\n",
      "[step: 1226] loss: 45.149871826171875\n",
      "[step: 1226] loss: 0.006562149617820978\n",
      "[step: 1227] loss: 44.39421081542969\n",
      "[step: 1227] loss: 0.006562450435012579\n",
      "[step: 1228] loss: 43.65327453613281\n",
      "[step: 1228] loss: 0.006561874877661467\n",
      "[step: 1229] loss: 42.89905548095703\n",
      "[step: 1229] loss: 0.006560557056218386\n",
      "[step: 1230] loss: 41.999412536621094\n",
      "[step: 1230] loss: 0.006558977533131838\n",
      "[step: 1231] loss: 41.36649703979492\n",
      "[step: 1231] loss: 0.006557560060173273\n",
      "[step: 1232] loss: 41.244789123535156\n",
      "[step: 1232] loss: 0.0065565952099859715\n",
      "[step: 1233] loss: 41.60432434082031\n",
      "[step: 1233] loss: 0.006556075066328049\n",
      "[step: 1234] loss: 42.15827560424805\n",
      "[step: 1234] loss: 0.0065558855421841145\n",
      "[step: 1235] loss: 42.518890380859375\n",
      "[step: 1235] loss: 0.006555880419909954\n",
      "[step: 1236] loss: 42.71909713745117\n",
      "[step: 1236] loss: 0.006555864587426186\n",
      "[step: 1237] loss: 43.047569274902344\n",
      "[step: 1237] loss: 0.006555773783475161\n",
      "[step: 1238] loss: 44.145423889160156\n",
      "[step: 1238] loss: 0.006555577274411917\n",
      "[step: 1239] loss: 45.69852066040039\n",
      "[step: 1239] loss: 0.0065552969463169575\n",
      "[step: 1240] loss: 49.49610900878906\n",
      "[step: 1240] loss: 0.006554961670190096\n",
      "[step: 1241] loss: 50.01213455200195\n",
      "[step: 1241] loss: 0.006554639898240566\n",
      "[step: 1242] loss: 51.421695709228516\n",
      "[step: 1242] loss: 0.006554384250193834\n",
      "[step: 1243] loss: 48.602203369140625\n",
      "[step: 1243] loss: 0.006554259452968836\n",
      "[step: 1244] loss: 48.258033752441406\n",
      "[step: 1244] loss: 0.00655433489009738\n",
      "[step: 1245] loss: 47.56574249267578\n",
      "[step: 1245] loss: 0.006554782390594482\n",
      "[step: 1246] loss: 44.77809143066406\n",
      "[step: 1246] loss: 0.006555755622684956\n",
      "[step: 1247] loss: 41.94801330566406\n",
      "[step: 1247] loss: 0.006557669024914503\n",
      "[step: 1248] loss: 41.62507629394531\n",
      "[step: 1248] loss: 0.006560974754393101\n",
      "[step: 1249] loss: 43.288700103759766\n",
      "[step: 1249] loss: 0.006566827185451984\n",
      "[step: 1250] loss: 44.80207824707031\n",
      "[step: 1250] loss: 0.006576585117727518\n",
      "[step: 1251] loss: 45.133888244628906\n",
      "[step: 1251] loss: 0.006593627855181694\n",
      "[step: 1252] loss: 44.048118591308594\n",
      "[step: 1252] loss: 0.00662121269851923\n",
      "[step: 1253] loss: 42.96300506591797\n",
      "[step: 1253] loss: 0.0066679115407168865\n",
      "[step: 1254] loss: 42.393592834472656\n",
      "[step: 1254] loss: 0.00673493929207325\n",
      "[step: 1255] loss: 42.859378814697266\n",
      "[step: 1255] loss: 0.0068301307037472725\n",
      "[step: 1256] loss: 42.49828338623047\n",
      "[step: 1256] loss: 0.006911309901624918\n",
      "[step: 1257] loss: 41.61473083496094\n",
      "[step: 1257] loss: 0.00694507360458374\n",
      "[step: 1258] loss: 40.648067474365234\n",
      "[step: 1258] loss: 0.006840986665338278\n",
      "[step: 1259] loss: 40.68162536621094\n",
      "[step: 1259] loss: 0.006666536442935467\n",
      "[step: 1260] loss: 41.29098129272461\n",
      "[step: 1260] loss: 0.006555741187185049\n",
      "[step: 1261] loss: 41.500343322753906\n",
      "[step: 1261] loss: 0.006598426960408688\n",
      "[step: 1262] loss: 41.538265228271484\n",
      "[step: 1262] loss: 0.006702028680592775\n",
      "[step: 1263] loss: 41.74226379394531\n",
      "[step: 1263] loss: 0.006715067196637392\n",
      "[step: 1264] loss: 42.961631774902344\n",
      "[step: 1264] loss: 0.006629270501434803\n",
      "[step: 1265] loss: 44.39891815185547\n",
      "[step: 1265] loss: 0.006552299950271845\n",
      "[step: 1266] loss: 47.2607421875\n",
      "[step: 1266] loss: 0.006570370402187109\n",
      "[step: 1267] loss: 50.176021575927734\n",
      "[step: 1267] loss: 0.006634916644543409\n",
      "[step: 1268] loss: 55.40388870239258\n",
      "[step: 1268] loss: 0.0066485232673585415\n",
      "[step: 1269] loss: 56.90679931640625\n",
      "[step: 1269] loss: 0.006598625332117081\n",
      "[step: 1270] loss: 56.593746185302734\n",
      "[step: 1270] loss: 0.00654855091124773\n",
      "[step: 1271] loss: 51.548789978027344\n",
      "[step: 1271] loss: 0.00655311020091176\n",
      "[step: 1272] loss: 45.910980224609375\n",
      "[step: 1272] loss: 0.006590668112039566\n",
      "[step: 1273] loss: 42.29052734375\n",
      "[step: 1273] loss: 0.006604962982237339\n",
      "[step: 1274] loss: 41.881805419921875\n",
      "[step: 1274] loss: 0.006579547189176083\n",
      "[step: 1275] loss: 44.26062774658203\n",
      "[step: 1275] loss: 0.0065465085208415985\n",
      "[step: 1276] loss: 47.24231719970703\n",
      "[step: 1276] loss: 0.006543392781168222\n",
      "[step: 1277] loss: 48.79398727416992\n",
      "[step: 1277] loss: 0.006564255803823471\n",
      "[step: 1278] loss: 47.307464599609375\n",
      "[step: 1278] loss: 0.006575605366379023\n",
      "[step: 1279] loss: 43.599525451660156\n",
      "[step: 1279] loss: 0.006563305389136076\n",
      "[step: 1280] loss: 40.588775634765625\n",
      "[step: 1280] loss: 0.006543186958879232\n",
      "[step: 1281] loss: 40.83623123168945\n",
      "[step: 1281] loss: 0.006538173649460077\n",
      "[step: 1282] loss: 43.12885284423828\n",
      "[step: 1282] loss: 0.006548961624503136\n",
      "[step: 1283] loss: 44.32350540161133\n",
      "[step: 1283] loss: 0.006557830143719912\n",
      "[step: 1284] loss: 43.307525634765625\n",
      "[step: 1284] loss: 0.006553093437105417\n",
      "[step: 1285] loss: 41.71213150024414\n",
      "[step: 1285] loss: 0.006540793459862471\n",
      "[step: 1286] loss: 41.65019226074219\n",
      "[step: 1286] loss: 0.00653446651995182\n",
      "[step: 1287] loss: 42.46845245361328\n",
      "[step: 1287] loss: 0.006538184359669685\n",
      "[step: 1288] loss: 43.536865234375\n",
      "[step: 1288] loss: 0.00654472503811121\n",
      "[step: 1289] loss: 42.90992736816406\n",
      "[step: 1289] loss: 0.006545655429363251\n",
      "[step: 1290] loss: 42.015357971191406\n",
      "[step: 1290] loss: 0.006539739202708006\n",
      "[step: 1291] loss: 41.02584457397461\n",
      "[step: 1291] loss: 0.006533181294798851\n",
      "[step: 1292] loss: 41.03739929199219\n",
      "[step: 1292] loss: 0.006531218532472849\n",
      "[step: 1293] loss: 41.50569152832031\n",
      "[step: 1293] loss: 0.006533804815262556\n",
      "[step: 1294] loss: 41.65583038330078\n",
      "[step: 1294] loss: 0.00653704022988677\n",
      "[step: 1295] loss: 41.16168975830078\n",
      "[step: 1295] loss: 0.00653731869533658\n",
      "[step: 1296] loss: 40.48577880859375\n",
      "[step: 1296] loss: 0.0065343016758561134\n",
      "[step: 1297] loss: 40.08466339111328\n",
      "[step: 1297] loss: 0.006530250888317823\n",
      "[step: 1298] loss: 40.31249237060547\n",
      "[step: 1298] loss: 0.006527786608785391\n",
      "[step: 1299] loss: 40.80829620361328\n",
      "[step: 1299] loss: 0.0065277377143502235\n",
      "[step: 1300] loss: 41.36134338378906\n",
      "[step: 1300] loss: 0.0065291388891637325\n",
      "[step: 1301] loss: 41.462547302246094\n",
      "[step: 1301] loss: 0.006530361250042915\n",
      "[step: 1302] loss: 41.91735076904297\n",
      "[step: 1302] loss: 0.006530238315463066\n",
      "[step: 1303] loss: 42.40058898925781\n",
      "[step: 1303] loss: 0.006528743542730808\n",
      "[step: 1304] loss: 44.49392318725586\n",
      "[step: 1304] loss: 0.006526581011712551\n",
      "[step: 1305] loss: 46.1611328125\n",
      "[step: 1305] loss: 0.006524633150547743\n",
      "[step: 1306] loss: 49.55071258544922\n",
      "[step: 1306] loss: 0.006523432210087776\n",
      "[step: 1307] loss: 49.63507080078125\n",
      "[step: 1307] loss: 0.0065230391919612885\n",
      "[step: 1308] loss: 49.442081451416016\n",
      "[step: 1308] loss: 0.006523218937218189\n",
      "[step: 1309] loss: 46.405860900878906\n",
      "[step: 1309] loss: 0.006523552816361189\n",
      "[step: 1310] loss: 43.133750915527344\n",
      "[step: 1310] loss: 0.006523751188069582\n",
      "[step: 1311] loss: 40.8230094909668\n",
      "[step: 1311] loss: 0.006523654330521822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1312] loss: 40.22003936767578\n",
      "[step: 1312] loss: 0.006523249205201864\n",
      "[step: 1313] loss: 41.42493438720703\n",
      "[step: 1313] loss: 0.006522564683109522\n",
      "[step: 1314] loss: 43.159149169921875\n",
      "[step: 1314] loss: 0.006521742325276136\n",
      "[step: 1315] loss: 44.2981071472168\n",
      "[step: 1315] loss: 0.006520839873701334\n",
      "[step: 1316] loss: 44.08585739135742\n",
      "[step: 1316] loss: 0.006519959308207035\n",
      "[step: 1317] loss: 42.45069885253906\n",
      "[step: 1317] loss: 0.0065191262401640415\n",
      "[step: 1318] loss: 40.55738067626953\n",
      "[step: 1318] loss: 0.006518371868878603\n",
      "[step: 1319] loss: 39.497314453125\n",
      "[step: 1319] loss: 0.006517707370221615\n",
      "[step: 1320] loss: 39.608673095703125\n",
      "[step: 1320] loss: 0.0065171122550964355\n",
      "[step: 1321] loss: 40.51612091064453\n",
      "[step: 1321] loss: 0.006516598630696535\n",
      "[step: 1322] loss: 41.55552673339844\n",
      "[step: 1322] loss: 0.006516188848763704\n",
      "[step: 1323] loss: 42.392059326171875\n",
      "[step: 1323] loss: 0.006515910383313894\n",
      "[step: 1324] loss: 43.40442657470703\n",
      "[step: 1324] loss: 0.006515831686556339\n",
      "[step: 1325] loss: 44.00164794921875\n",
      "[step: 1325] loss: 0.0065160710364580154\n",
      "[step: 1326] loss: 45.774410247802734\n",
      "[step: 1326] loss: 0.006516879890114069\n",
      "[step: 1327] loss: 44.363914489746094\n",
      "[step: 1327] loss: 0.006518661975860596\n",
      "[step: 1328] loss: 42.8846321105957\n",
      "[step: 1328] loss: 0.006522274576127529\n",
      "[step: 1329] loss: 40.05868911743164\n",
      "[step: 1329] loss: 0.006529174279421568\n",
      "[step: 1330] loss: 39.55263900756836\n",
      "[step: 1330] loss: 0.006542505696415901\n",
      "[step: 1331] loss: 40.951148986816406\n",
      "[step: 1331] loss: 0.006567327305674553\n",
      "[step: 1332] loss: 41.8518180847168\n",
      "[step: 1332] loss: 0.0066145011223852634\n",
      "[step: 1333] loss: 42.04939270019531\n",
      "[step: 1333] loss: 0.00669658649712801\n",
      "[step: 1334] loss: 40.714256286621094\n",
      "[step: 1334] loss: 0.006835802923887968\n",
      "[step: 1335] loss: 40.3770751953125\n",
      "[step: 1335] loss: 0.007008469197899103\n",
      "[step: 1336] loss: 41.254512786865234\n",
      "[step: 1336] loss: 0.007160209584981203\n",
      "[step: 1337] loss: 41.65407943725586\n",
      "[step: 1337] loss: 0.007076001726090908\n",
      "[step: 1338] loss: 40.8096923828125\n",
      "[step: 1338] loss: 0.006781211122870445\n",
      "[step: 1339] loss: 39.50358200073242\n",
      "[step: 1339] loss: 0.006536799483001232\n",
      "[step: 1340] loss: 39.062278747558594\n",
      "[step: 1340] loss: 0.006623145658522844\n",
      "[step: 1341] loss: 39.66502380371094\n",
      "[step: 1341] loss: 0.006812293082475662\n",
      "[step: 1342] loss: 40.25299835205078\n",
      "[step: 1342] loss: 0.006741316523402929\n",
      "[step: 1343] loss: 40.3749885559082\n",
      "[step: 1343] loss: 0.006553411949425936\n",
      "[step: 1344] loss: 39.595619201660156\n",
      "[step: 1344] loss: 0.006555946543812752\n",
      "[step: 1345] loss: 38.918983459472656\n",
      "[step: 1345] loss: 0.00668048532679677\n",
      "[step: 1346] loss: 38.82733917236328\n",
      "[step: 1346] loss: 0.006669749040156603\n",
      "[step: 1347] loss: 39.17277145385742\n",
      "[step: 1347] loss: 0.006547069642692804\n",
      "[step: 1348] loss: 39.44010925292969\n",
      "[step: 1348] loss: 0.00652686133980751\n",
      "[step: 1349] loss: 39.3536376953125\n",
      "[step: 1349] loss: 0.006611944176256657\n",
      "[step: 1350] loss: 39.01795959472656\n",
      "[step: 1350] loss: 0.006623680703341961\n",
      "[step: 1351] loss: 38.77252960205078\n",
      "[step: 1351] loss: 0.006543787196278572\n",
      "[step: 1352] loss: 38.9573860168457\n",
      "[step: 1352] loss: 0.006514552515000105\n",
      "[step: 1353] loss: 39.54247283935547\n",
      "[step: 1353] loss: 0.006565203424543142\n",
      "[step: 1354] loss: 40.968475341796875\n",
      "[step: 1354] loss: 0.006586531642824411\n",
      "[step: 1355] loss: 42.792152404785156\n",
      "[step: 1355] loss: 0.006540361791849136\n",
      "[step: 1356] loss: 48.362709045410156\n",
      "[step: 1356] loss: 0.006509572267532349\n",
      "[step: 1357] loss: 53.60340881347656\n",
      "[step: 1357] loss: 0.006534663029015064\n",
      "[step: 1358] loss: 67.2946548461914\n",
      "[step: 1358] loss: 0.006556945852935314\n",
      "[step: 1359] loss: 67.87114715576172\n",
      "[step: 1359] loss: 0.006533415522426367\n",
      "[step: 1360] loss: 83.14024353027344\n",
      "[step: 1360] loss: 0.006506034638732672\n",
      "[step: 1361] loss: 90.42308044433594\n",
      "[step: 1361] loss: 0.006519385613501072\n",
      "[step: 1362] loss: 73.24977111816406\n",
      "[step: 1362] loss: 0.006538026034832001\n",
      "[step: 1363] loss: 49.446632385253906\n",
      "[step: 1363] loss: 0.006523986347019672\n",
      "[step: 1364] loss: 60.514183044433594\n",
      "[step: 1364] loss: 0.006504904944449663\n",
      "[step: 1365] loss: 62.173423767089844\n",
      "[step: 1365] loss: 0.006509058643132448\n",
      "[step: 1366] loss: 59.00127029418945\n",
      "[step: 1366] loss: 0.0065212855115532875\n",
      "[step: 1367] loss: 70.36925506591797\n",
      "[step: 1367] loss: 0.006518227513879538\n",
      "[step: 1368] loss: 53.338890075683594\n",
      "[step: 1368] loss: 0.00650441600009799\n",
      "[step: 1369] loss: 56.616172790527344\n",
      "[step: 1369] loss: 0.006500864867120981\n",
      "[step: 1370] loss: 62.747589111328125\n",
      "[step: 1370] loss: 0.00650984700769186\n",
      "[step: 1371] loss: 60.52857971191406\n",
      "[step: 1371] loss: 0.006512991618365049\n",
      "[step: 1372] loss: 58.163543701171875\n",
      "[step: 1372] loss: 0.006504255812615156\n",
      "[step: 1373] loss: 55.301605224609375\n",
      "[step: 1373] loss: 0.006497417576611042\n",
      "[step: 1374] loss: 50.852020263671875\n",
      "[step: 1374] loss: 0.0065004415810108185\n",
      "[step: 1375] loss: 51.30064010620117\n",
      "[step: 1375] loss: 0.00650501623749733\n",
      "[step: 1376] loss: 56.993873596191406\n",
      "[step: 1376] loss: 0.0065033105202019215\n",
      "[step: 1377] loss: 43.400020599365234\n",
      "[step: 1377] loss: 0.00649760477244854\n",
      "[step: 1378] loss: 51.50975799560547\n",
      "[step: 1378] loss: 0.006494592409580946\n",
      "[step: 1379] loss: 42.783935546875\n",
      "[step: 1379] loss: 0.0064965952187776566\n",
      "[step: 1380] loss: 52.99078369140625\n",
      "[step: 1380] loss: 0.006499232724308968\n",
      "[step: 1381] loss: 46.89288330078125\n",
      "[step: 1381] loss: 0.006497769150882959\n",
      "[step: 1382] loss: 47.202064514160156\n",
      "[step: 1382] loss: 0.006493760272860527\n",
      "[step: 1383] loss: 44.20512390136719\n",
      "[step: 1383] loss: 0.00649165827780962\n",
      "[step: 1384] loss: 43.58985137939453\n",
      "[step: 1384] loss: 0.006492640357464552\n",
      "[step: 1385] loss: 45.28754806518555\n",
      "[step: 1385] loss: 0.006494044326245785\n",
      "[step: 1386] loss: 44.24321365356445\n",
      "[step: 1386] loss: 0.006493519060313702\n",
      "[step: 1387] loss: 42.655609130859375\n",
      "[step: 1387] loss: 0.006491413805633783\n",
      "[step: 1388] loss: 42.50382995605469\n",
      "[step: 1388] loss: 0.006489375606179237\n",
      "[step: 1389] loss: 41.2177734375\n",
      "[step: 1389] loss: 0.006488639861345291\n",
      "[step: 1390] loss: 42.59025573730469\n",
      "[step: 1390] loss: 0.006489105988293886\n",
      "[step: 1391] loss: 41.645233154296875\n",
      "[step: 1391] loss: 0.006489611696451902\n",
      "[step: 1392] loss: 41.08099365234375\n",
      "[step: 1392] loss: 0.006489056162536144\n",
      "[step: 1393] loss: 41.11799621582031\n",
      "[step: 1393] loss: 0.006487573031336069\n",
      "[step: 1394] loss: 39.33453369140625\n",
      "[step: 1394] loss: 0.006486102007329464\n",
      "[step: 1395] loss: 40.85159683227539\n",
      "[step: 1395] loss: 0.006485373713076115\n",
      "[step: 1396] loss: 39.263389587402344\n",
      "[step: 1396] loss: 0.006485271733254194\n",
      "[step: 1397] loss: 40.59789276123047\n",
      "[step: 1397] loss: 0.006485321093350649\n",
      "[step: 1398] loss: 39.36585235595703\n",
      "[step: 1398] loss: 0.006485105957835913\n",
      "[step: 1399] loss: 39.53965377807617\n",
      "[step: 1399] loss: 0.006484463345259428\n",
      "[step: 1400] loss: 38.978240966796875\n",
      "[step: 1400] loss: 0.006483482196927071\n",
      "[step: 1401] loss: 39.009559631347656\n",
      "[step: 1401] loss: 0.006482487544417381\n",
      "[step: 1402] loss: 38.80011749267578\n",
      "[step: 1402] loss: 0.006481760181486607\n",
      "[step: 1403] loss: 38.990447998046875\n",
      "[step: 1403] loss: 0.0064813545905053616\n",
      "[step: 1404] loss: 38.93741989135742\n",
      "[step: 1404] loss: 0.006481113377958536\n",
      "[step: 1405] loss: 39.02027893066406\n",
      "[step: 1405] loss: 0.006480851676315069\n",
      "[step: 1406] loss: 39.225929260253906\n",
      "[step: 1406] loss: 0.006480476353317499\n",
      "[step: 1407] loss: 39.000335693359375\n",
      "[step: 1407] loss: 0.006479974370449781\n",
      "[step: 1408] loss: 39.37117004394531\n",
      "[step: 1408] loss: 0.006479352712631226\n",
      "[step: 1409] loss: 38.94237518310547\n",
      "[step: 1409] loss: 0.006478656083345413\n",
      "[step: 1410] loss: 39.25823211669922\n",
      "[step: 1410] loss: 0.006477964576333761\n",
      "[step: 1411] loss: 39.159034729003906\n",
      "[step: 1411] loss: 0.006477337796241045\n",
      "[step: 1412] loss: 39.620330810546875\n",
      "[step: 1412] loss: 0.006476782727986574\n",
      "[step: 1413] loss: 39.689659118652344\n",
      "[step: 1413] loss: 0.0064762914553284645\n",
      "[step: 1414] loss: 40.110103607177734\n",
      "[step: 1414] loss: 0.0064758299849927425\n",
      "[step: 1415] loss: 40.484130859375\n",
      "[step: 1415] loss: 0.006475403439253569\n",
      "[step: 1416] loss: 40.440338134765625\n",
      "[step: 1416] loss: 0.0064750006422400475\n",
      "[step: 1417] loss: 39.985740661621094\n",
      "[step: 1417] loss: 0.006474609021097422\n",
      "[step: 1418] loss: 39.519935607910156\n",
      "[step: 1418] loss: 0.00647422531619668\n",
      "[step: 1419] loss: 38.539833068847656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1419] loss: 0.006473846267908812\n",
      "[step: 1420] loss: 38.00737380981445\n",
      "[step: 1420] loss: 0.006473497021943331\n",
      "[step: 1421] loss: 37.61028289794922\n",
      "[step: 1421] loss: 0.0064731924794614315\n",
      "[step: 1422] loss: 37.64696502685547\n",
      "[step: 1422] loss: 0.006472969427704811\n",
      "[step: 1423] loss: 37.91613006591797\n",
      "[step: 1423] loss: 0.006472880020737648\n",
      "[step: 1424] loss: 38.342933654785156\n",
      "[step: 1424] loss: 0.006472986191511154\n",
      "[step: 1425] loss: 38.98542022705078\n",
      "[step: 1425] loss: 0.0064734467305243015\n",
      "[step: 1426] loss: 39.462493896484375\n",
      "[step: 1426] loss: 0.006474502384662628\n",
      "[step: 1427] loss: 40.322120666503906\n",
      "[step: 1427] loss: 0.006476595997810364\n",
      "[step: 1428] loss: 40.83811950683594\n",
      "[step: 1428] loss: 0.00648050382733345\n",
      "[step: 1429] loss: 41.83693313598633\n",
      "[step: 1429] loss: 0.00648761261254549\n",
      "[step: 1430] loss: 41.54783248901367\n",
      "[step: 1430] loss: 0.0065004355274140835\n",
      "[step: 1431] loss: 41.62812805175781\n",
      "[step: 1431] loss: 0.006523250136524439\n",
      "[step: 1432] loss: 40.38206481933594\n",
      "[step: 1432] loss: 0.0065632108598947525\n",
      "[step: 1433] loss: 39.27659606933594\n",
      "[step: 1433] loss: 0.006629225332289934\n",
      "[step: 1434] loss: 38.33905792236328\n",
      "[step: 1434] loss: 0.0067283171229064465\n",
      "[step: 1435] loss: 37.806617736816406\n",
      "[step: 1435] loss: 0.006847732700407505\n",
      "[step: 1436] loss: 37.493499755859375\n",
      "[step: 1436] loss: 0.006926526315510273\n",
      "[step: 1437] loss: 37.410491943359375\n",
      "[step: 1437] loss: 0.006885656155645847\n",
      "[step: 1438] loss: 37.59568786621094\n",
      "[step: 1438] loss: 0.006697425153106451\n",
      "[step: 1439] loss: 37.929325103759766\n",
      "[step: 1439] loss: 0.006537776440382004\n",
      "[step: 1440] loss: 38.45665740966797\n",
      "[step: 1440] loss: 0.00650573568418622\n",
      "[step: 1441] loss: 38.970489501953125\n",
      "[step: 1441] loss: 0.006611586548388004\n",
      "[step: 1442] loss: 39.364036560058594\n",
      "[step: 1442] loss: 0.006683395244181156\n",
      "[step: 1443] loss: 39.66845703125\n",
      "[step: 1443] loss: 0.006586813367903233\n",
      "[step: 1444] loss: 40.26996994018555\n",
      "[step: 1444] loss: 0.006481357384473085\n",
      "[step: 1445] loss: 41.116432189941406\n",
      "[step: 1445] loss: 0.006501135881990194\n",
      "[step: 1446] loss: 43.44492721557617\n",
      "[step: 1446] loss: 0.006571173667907715\n",
      "[step: 1447] loss: 44.328346252441406\n",
      "[step: 1447] loss: 0.006580430548638105\n",
      "[step: 1448] loss: 46.17034149169922\n",
      "[step: 1448] loss: 0.0065240394324064255\n",
      "[step: 1449] loss: 42.980804443359375\n",
      "[step: 1449] loss: 0.006473604589700699\n",
      "[step: 1450] loss: 40.937217712402344\n",
      "[step: 1450] loss: 0.006486258003860712\n",
      "[step: 1451] loss: 40.483646392822266\n",
      "[step: 1451] loss: 0.006531420163810253\n",
      "[step: 1452] loss: 40.19559860229492\n",
      "[step: 1452] loss: 0.006532857194542885\n",
      "[step: 1453] loss: 39.3058967590332\n",
      "[step: 1453] loss: 0.006491971667855978\n",
      "[step: 1454] loss: 37.70457458496094\n",
      "[step: 1454] loss: 0.006466295104473829\n",
      "[step: 1455] loss: 37.447914123535156\n",
      "[step: 1455] loss: 0.006479763891547918\n",
      "[step: 1456] loss: 38.71018981933594\n",
      "[step: 1456] loss: 0.006501692347228527\n",
      "[step: 1457] loss: 39.81096649169922\n",
      "[step: 1457] loss: 0.006500830873847008\n",
      "[step: 1458] loss: 39.694671630859375\n",
      "[step: 1458] loss: 0.006476984824985266\n",
      "[step: 1459] loss: 38.6531982421875\n",
      "[step: 1459] loss: 0.006461688783019781\n",
      "[step: 1460] loss: 38.194366455078125\n",
      "[step: 1460] loss: 0.006470365449786186\n",
      "[step: 1461] loss: 38.74250030517578\n",
      "[step: 1461] loss: 0.006485041696578264\n",
      "[step: 1462] loss: 39.00584030151367\n",
      "[step: 1462] loss: 0.006482671946287155\n",
      "[step: 1463] loss: 39.140220642089844\n",
      "[step: 1463] loss: 0.00646762503311038\n",
      "[step: 1464] loss: 38.142974853515625\n",
      "[step: 1464] loss: 0.006459007039666176\n",
      "[step: 1465] loss: 37.53192138671875\n",
      "[step: 1465] loss: 0.006463078781962395\n",
      "[step: 1466] loss: 37.239044189453125\n",
      "[step: 1466] loss: 0.006470566149801016\n",
      "[step: 1467] loss: 37.31589126586914\n",
      "[step: 1467] loss: 0.006471672561019659\n",
      "[step: 1468] loss: 37.48963165283203\n",
      "[step: 1468] loss: 0.006464806850999594\n",
      "[step: 1469] loss: 37.471290588378906\n",
      "[step: 1469] loss: 0.0064566610381007195\n",
      "[step: 1470] loss: 37.151283264160156\n",
      "[step: 1470] loss: 0.006455584429204464\n",
      "[step: 1471] loss: 36.67906188964844\n",
      "[step: 1471] loss: 0.006460479460656643\n",
      "[step: 1472] loss: 36.37351989746094\n",
      "[step: 1472] loss: 0.006463759578764439\n",
      "[step: 1473] loss: 36.37384033203125\n",
      "[step: 1473] loss: 0.006461646407842636\n",
      "[step: 1474] loss: 36.57722854614258\n",
      "[step: 1474] loss: 0.006456751376390457\n",
      "[step: 1475] loss: 36.762298583984375\n",
      "[step: 1475] loss: 0.006452817004173994\n",
      "[step: 1476] loss: 36.805789947509766\n",
      "[step: 1476] loss: 0.006452152971178293\n",
      "[step: 1477] loss: 36.817283630371094\n",
      "[step: 1477] loss: 0.00645404402166605\n",
      "[step: 1478] loss: 36.81814193725586\n",
      "[step: 1478] loss: 0.006456127390265465\n",
      "[step: 1479] loss: 37.16343688964844\n",
      "[step: 1479] loss: 0.006455906201153994\n",
      "[step: 1480] loss: 38.03062438964844\n",
      "[step: 1480] loss: 0.006453292910009623\n",
      "[step: 1481] loss: 41.07366943359375\n",
      "[step: 1481] loss: 0.006450383923947811\n",
      "[step: 1482] loss: 46.92738723754883\n",
      "[step: 1482] loss: 0.006448738742619753\n",
      "[step: 1483] loss: 62.83203887939453\n",
      "[step: 1483] loss: 0.006448386702686548\n",
      "[step: 1484] loss: 73.07164001464844\n",
      "[step: 1484] loss: 0.006449010223150253\n",
      "[step: 1485] loss: 82.75863647460938\n",
      "[step: 1485] loss: 0.006449779495596886\n",
      "[step: 1486] loss: 80.80110168457031\n",
      "[step: 1486] loss: 0.006449968088418245\n",
      "[step: 1487] loss: 64.20934295654297\n",
      "[step: 1487] loss: 0.0064491755329072475\n",
      "[step: 1488] loss: 41.625701904296875\n",
      "[step: 1488] loss: 0.0064476714469492435\n",
      "[step: 1489] loss: 49.14562225341797\n",
      "[step: 1489] loss: 0.006446127314120531\n",
      "[step: 1490] loss: 66.63763427734375\n",
      "[step: 1490] loss: 0.006444890052080154\n",
      "[step: 1491] loss: 56.45136260986328\n",
      "[step: 1491] loss: 0.006444055587053299\n",
      "[step: 1492] loss: 42.20606231689453\n",
      "[step: 1492] loss: 0.0064436099492013454\n",
      "[step: 1493] loss: 43.98329544067383\n",
      "[step: 1493] loss: 0.006443527992814779\n",
      "[step: 1494] loss: 50.95957565307617\n",
      "[step: 1494] loss: 0.006443605292588472\n",
      "[step: 1495] loss: 47.94406509399414\n",
      "[step: 1495] loss: 0.006443591322749853\n",
      "[step: 1496] loss: 45.15492248535156\n",
      "[step: 1496] loss: 0.00644338596612215\n",
      "[step: 1497] loss: 44.031944274902344\n",
      "[step: 1497] loss: 0.006443056743592024\n",
      "[step: 1498] loss: 41.496063232421875\n",
      "[step: 1498] loss: 0.006442634388804436\n",
      "[step: 1499] loss: 45.700042724609375\n",
      "[step: 1499] loss: 0.006442107260227203\n",
      "[step: 1500] loss: 48.622718811035156\n",
      "[step: 1500] loss: 0.006441493518650532\n",
      "[step: 1501] loss: 39.792320251464844\n",
      "[step: 1501] loss: 0.00644087977707386\n",
      "[step: 1502] loss: 41.725555419921875\n",
      "[step: 1502] loss: 0.0064403219148516655\n",
      "[step: 1503] loss: 47.016170501708984\n",
      "[step: 1503] loss: 0.006439812947064638\n",
      "[step: 1504] loss: 40.988258361816406\n",
      "[step: 1504] loss: 0.006439357064664364\n",
      "[step: 1505] loss: 40.99119186401367\n",
      "[step: 1505] loss: 0.006438993848860264\n",
      "[step: 1506] loss: 41.627357482910156\n",
      "[step: 1506] loss: 0.00643877824768424\n",
      "[step: 1507] loss: 40.29125213623047\n",
      "[step: 1507] loss: 0.006438770331442356\n",
      "[step: 1508] loss: 42.09241485595703\n",
      "[step: 1508] loss: 0.006439082324504852\n",
      "[step: 1509] loss: 38.467742919921875\n",
      "[step: 1509] loss: 0.00643987488001585\n",
      "[step: 1510] loss: 38.73272705078125\n",
      "[step: 1510] loss: 0.006441514007747173\n",
      "[step: 1511] loss: 41.10968017578125\n",
      "[step: 1511] loss: 0.006444572936743498\n",
      "[step: 1512] loss: 37.866031646728516\n",
      "[step: 1512] loss: 0.006450112909078598\n",
      "[step: 1513] loss: 38.45237731933594\n",
      "[step: 1513] loss: 0.00645996630191803\n",
      "[step: 1514] loss: 39.2071533203125\n",
      "[step: 1514] loss: 0.006477272603660822\n",
      "[step: 1515] loss: 36.962890625\n",
      "[step: 1515] loss: 0.006507229059934616\n",
      "[step: 1516] loss: 38.574913024902344\n",
      "[step: 1516] loss: 0.006557511631399393\n",
      "[step: 1517] loss: 38.35503387451172\n",
      "[step: 1517] loss: 0.006636112462729216\n",
      "[step: 1518] loss: 36.36089324951172\n",
      "[step: 1518] loss: 0.0067426832392811775\n",
      "[step: 1519] loss: 37.54020309448242\n",
      "[step: 1519] loss: 0.0068490998819470406\n",
      "[step: 1520] loss: 37.5064697265625\n",
      "[step: 1520] loss: 0.006876414176076651\n",
      "[step: 1521] loss: 36.469703674316406\n",
      "[step: 1521] loss: 0.006768330466002226\n",
      "[step: 1522] loss: 37.25135803222656\n",
      "[step: 1522] loss: 0.006566726602613926\n",
      "[step: 1523] loss: 36.6707763671875\n",
      "[step: 1523] loss: 0.006459975615143776\n",
      "[step: 1524] loss: 36.380672454833984\n",
      "[step: 1524] loss: 0.006503340322524309\n",
      "[step: 1525] loss: 37.13187789916992\n",
      "[step: 1525] loss: 0.006605197675526142\n",
      "[step: 1526] loss: 36.18595886230469\n",
      "[step: 1526] loss: 0.006620106287300587\n",
      "[step: 1527] loss: 36.041038513183594\n",
      "[step: 1527] loss: 0.006510586477816105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1528] loss: 36.46665573120117\n",
      "[step: 1528] loss: 0.0064379554241895676\n",
      "[step: 1529] loss: 35.897430419921875\n",
      "[step: 1529] loss: 0.006481117103248835\n",
      "[step: 1530] loss: 36.028873443603516\n",
      "[step: 1530] loss: 0.00654491176828742\n",
      "[step: 1531] loss: 36.1273078918457\n",
      "[step: 1531] loss: 0.006533886305987835\n",
      "[step: 1532] loss: 35.64623260498047\n",
      "[step: 1532] loss: 0.0064706746488809586\n",
      "[step: 1533] loss: 35.83763122558594\n",
      "[step: 1533] loss: 0.006438488140702248\n",
      "[step: 1534] loss: 36.004638671875\n",
      "[step: 1534] loss: 0.006458442658185959\n",
      "[step: 1535] loss: 35.820068359375\n",
      "[step: 1535] loss: 0.006492933724075556\n",
      "[step: 1536] loss: 36.05776596069336\n",
      "[step: 1536] loss: 0.006495306733995676\n",
      "[step: 1537] loss: 36.427398681640625\n",
      "[step: 1537] loss: 0.0064587341621518135\n",
      "[step: 1538] loss: 37.14983367919922\n",
      "[step: 1538] loss: 0.006429639179259539\n",
      "[step: 1539] loss: 38.48512268066406\n",
      "[step: 1539] loss: 0.006443994585424662\n",
      "[step: 1540] loss: 41.99095153808594\n",
      "[step: 1540] loss: 0.006467822473496199\n",
      "[step: 1541] loss: 43.324424743652344\n",
      "[step: 1541] loss: 0.006464812904596329\n",
      "[step: 1542] loss: 45.94835662841797\n",
      "[step: 1542] loss: 0.006446773186326027\n",
      "[step: 1543] loss: 41.93836212158203\n",
      "[step: 1543] loss: 0.00643188739195466\n",
      "[step: 1544] loss: 39.83102035522461\n",
      "[step: 1544] loss: 0.006430443841964006\n",
      "[step: 1545] loss: 38.8669319152832\n",
      "[step: 1545] loss: 0.006445420440286398\n",
      "[step: 1546] loss: 37.6381950378418\n",
      "[step: 1546] loss: 0.00645242165774107\n",
      "[step: 1547] loss: 37.4202880859375\n",
      "[step: 1547] loss: 0.006438341923058033\n",
      "[step: 1548] loss: 38.2461051940918\n",
      "[step: 1548] loss: 0.006426280830055475\n",
      "[step: 1549] loss: 39.487648010253906\n",
      "[step: 1549] loss: 0.006426636129617691\n",
      "[step: 1550] loss: 38.336387634277344\n",
      "[step: 1550] loss: 0.006430651992559433\n",
      "[step: 1551] loss: 36.04905319213867\n",
      "[step: 1551] loss: 0.0064355977810919285\n",
      "[step: 1552] loss: 35.669776916503906\n",
      "[step: 1552] loss: 0.00643604202196002\n",
      "[step: 1553] loss: 37.07263946533203\n",
      "[step: 1553] loss: 0.006427554879337549\n",
      "[step: 1554] loss: 37.32118225097656\n",
      "[step: 1554] loss: 0.006420328747481108\n",
      "[step: 1555] loss: 36.02976608276367\n",
      "[step: 1555] loss: 0.006421108730137348\n",
      "[step: 1556] loss: 35.85033416748047\n",
      "[step: 1556] loss: 0.006424614693969488\n",
      "[step: 1557] loss: 36.44242858886719\n",
      "[step: 1557] loss: 0.006426503881812096\n",
      "[step: 1558] loss: 36.38935089111328\n",
      "[step: 1558] loss: 0.006426296196877956\n",
      "[step: 1559] loss: 35.823692321777344\n",
      "[step: 1559] loss: 0.006422906182706356\n",
      "[step: 1560] loss: 35.57305908203125\n",
      "[step: 1560] loss: 0.006418232340365648\n",
      "[step: 1561] loss: 35.37397003173828\n",
      "[step: 1561] loss: 0.006416140589863062\n",
      "[step: 1562] loss: 35.445037841796875\n",
      "[step: 1562] loss: 0.006417224183678627\n",
      "[step: 1563] loss: 35.569156646728516\n",
      "[step: 1563] loss: 0.006418769247829914\n",
      "[step: 1564] loss: 35.399383544921875\n",
      "[step: 1564] loss: 0.0064193434081971645\n",
      "[step: 1565] loss: 35.026611328125\n",
      "[step: 1565] loss: 0.006419181823730469\n",
      "[step: 1566] loss: 34.83547592163086\n",
      "[step: 1566] loss: 0.0064177000895142555\n",
      "[step: 1567] loss: 35.05970001220703\n",
      "[step: 1567] loss: 0.006415023934096098\n",
      "[step: 1568] loss: 35.30461883544922\n",
      "[step: 1568] loss: 0.006413112860172987\n",
      "[step: 1569] loss: 35.15705871582031\n",
      "[step: 1569] loss: 0.006412440910935402\n",
      "[step: 1570] loss: 34.9986457824707\n",
      "[step: 1570] loss: 0.006412196438759565\n",
      "[step: 1571] loss: 35.58641052246094\n",
      "[step: 1571] loss: 0.006412332411855459\n",
      "[step: 1572] loss: 37.80621337890625\n",
      "[step: 1572] loss: 0.00641283392906189\n",
      "[step: 1573] loss: 46.14501190185547\n",
      "[step: 1573] loss: 0.006413045339286327\n",
      "[step: 1574] loss: 56.33174514770508\n",
      "[step: 1574] loss: 0.006412584334611893\n",
      "[step: 1575] loss: 82.63699340820312\n",
      "[step: 1575] loss: 0.006411760579794645\n",
      "[step: 1576] loss: 70.97628784179688\n",
      "[step: 1576] loss: 0.00641092611476779\n",
      "[step: 1577] loss: 84.90953063964844\n",
      "[step: 1577] loss: 0.0064099133014678955\n",
      "[step: 1578] loss: 57.666202545166016\n",
      "[step: 1578] loss: 0.0064087496139109135\n",
      "[step: 1579] loss: 51.19554901123047\n",
      "[step: 1579] loss: 0.006407811306416988\n",
      "[step: 1580] loss: 58.50416564941406\n",
      "[step: 1580] loss: 0.006407123990356922\n",
      "[step: 1581] loss: 54.79762268066406\n",
      "[step: 1581] loss: 0.006406478118151426\n",
      "[step: 1582] loss: 53.94902038574219\n",
      "[step: 1582] loss: 0.00640586344525218\n",
      "[step: 1583] loss: 43.191139221191406\n",
      "[step: 1583] loss: 0.006405434105545282\n",
      "[step: 1584] loss: 52.48612976074219\n",
      "[step: 1584] loss: 0.006405122112482786\n",
      "[step: 1585] loss: 45.57689666748047\n",
      "[step: 1585] loss: 0.006404779851436615\n",
      "[step: 1586] loss: 49.48731231689453\n",
      "[step: 1586] loss: 0.006404448300600052\n",
      "[step: 1587] loss: 46.64311981201172\n",
      "[step: 1587] loss: 0.006404262967407703\n",
      "[step: 1588] loss: 41.03854751586914\n",
      "[step: 1588] loss: 0.006404216401278973\n",
      "[step: 1589] loss: 45.59330368041992\n",
      "[step: 1589] loss: 0.006404298357665539\n",
      "[step: 1590] loss: 41.144256591796875\n",
      "[step: 1590] loss: 0.0064046503975987434\n",
      "[step: 1591] loss: 44.76359939575195\n",
      "[step: 1591] loss: 0.006405557971447706\n",
      "[step: 1592] loss: 39.413604736328125\n",
      "[step: 1592] loss: 0.006407368462532759\n",
      "[step: 1593] loss: 41.291175842285156\n",
      "[step: 1593] loss: 0.006410742178559303\n",
      "[step: 1594] loss: 40.650634765625\n",
      "[step: 1594] loss: 0.006416859105229378\n",
      "[step: 1595] loss: 38.88016128540039\n",
      "[step: 1595] loss: 0.006427987944334745\n",
      "[step: 1596] loss: 41.02488327026367\n",
      "[step: 1596] loss: 0.006448010448366404\n",
      "[step: 1597] loss: 36.73876190185547\n",
      "[step: 1597] loss: 0.006483344826847315\n",
      "[step: 1598] loss: 39.00965881347656\n",
      "[step: 1598] loss: 0.00654388964176178\n",
      "[step: 1599] loss: 37.85633087158203\n",
      "[step: 1599] loss: 0.006639786530286074\n",
      "[step: 1600] loss: 36.99514389038086\n",
      "[step: 1600] loss: 0.006767618004232645\n",
      "[step: 1601] loss: 38.58576965332031\n",
      "[step: 1601] loss: 0.0068897344172000885\n",
      "[step: 1602] loss: 36.022705078125\n",
      "[step: 1602] loss: 0.006896966136991978\n",
      "[step: 1603] loss: 37.08596420288086\n",
      "[step: 1603] loss: 0.006734691560268402\n",
      "[step: 1604] loss: 36.36919403076172\n",
      "[step: 1604] loss: 0.00649737985804677\n",
      "[step: 1605] loss: 35.91414260864258\n",
      "[step: 1605] loss: 0.006421276368200779\n",
      "[step: 1606] loss: 36.73873519897461\n",
      "[step: 1606] loss: 0.006529257632791996\n",
      "[step: 1607] loss: 35.3251953125\n",
      "[step: 1607] loss: 0.006627266760915518\n",
      "[step: 1608] loss: 35.87226104736328\n",
      "[step: 1608] loss: 0.006573230493813753\n",
      "[step: 1609] loss: 35.86848068237305\n",
      "[step: 1609] loss: 0.006434690207242966\n",
      "[step: 1610] loss: 34.94264602661133\n",
      "[step: 1610] loss: 0.006422510836273432\n",
      "[step: 1611] loss: 35.737701416015625\n",
      "[step: 1611] loss: 0.006517038214951754\n",
      "[step: 1612] loss: 35.06946563720703\n",
      "[step: 1612] loss: 0.006534841377288103\n",
      "[step: 1613] loss: 35.10493850708008\n",
      "[step: 1613] loss: 0.00646465178579092\n",
      "[step: 1614] loss: 35.223663330078125\n",
      "[step: 1614] loss: 0.006409216672182083\n",
      "[step: 1615] loss: 34.738800048828125\n",
      "[step: 1615] loss: 0.006424657069146633\n",
      "[step: 1616] loss: 34.897186279296875\n",
      "[step: 1616] loss: 0.006474877241998911\n",
      "[step: 1617] loss: 34.706871032714844\n",
      "[step: 1617] loss: 0.0064792209304869175\n",
      "[step: 1618] loss: 34.464263916015625\n",
      "[step: 1618] loss: 0.0064273979514837265\n",
      "[step: 1619] loss: 34.629486083984375\n",
      "[step: 1619] loss: 0.006397886201739311\n",
      "[step: 1620] loss: 34.521820068359375\n",
      "[step: 1620] loss: 0.006420238874852657\n",
      "[step: 1621] loss: 34.265838623046875\n",
      "[step: 1621] loss: 0.006445317529141903\n",
      "[step: 1622] loss: 34.48980712890625\n",
      "[step: 1622] loss: 0.006436499767005444\n",
      "[step: 1623] loss: 34.35314178466797\n",
      "[step: 1623] loss: 0.006408375222235918\n",
      "[step: 1624] loss: 34.20982360839844\n",
      "[step: 1624] loss: 0.006395568139851093\n",
      "[step: 1625] loss: 34.39341354370117\n",
      "[step: 1625] loss: 0.00640904949977994\n",
      "[step: 1626] loss: 34.3217887878418\n",
      "[step: 1626] loss: 0.006424554158002138\n",
      "[step: 1627] loss: 34.279571533203125\n",
      "[step: 1627] loss: 0.006416594609618187\n",
      "[step: 1628] loss: 34.6425666809082\n",
      "[step: 1628] loss: 0.0063974326476454735\n",
      "[step: 1629] loss: 35.09440994262695\n",
      "[step: 1629] loss: 0.006391616538167\n",
      "[step: 1630] loss: 35.83946228027344\n",
      "[step: 1630] loss: 0.00640092883259058\n",
      "[step: 1631] loss: 37.758262634277344\n",
      "[step: 1631] loss: 0.006408629007637501\n",
      "[step: 1632] loss: 40.509178161621094\n",
      "[step: 1632] loss: 0.006404630374163389\n",
      "[step: 1633] loss: 45.960548400878906\n",
      "[step: 1633] loss: 0.0063945092260837555\n",
      "[step: 1634] loss: 47.589576721191406\n",
      "[step: 1634] loss: 0.006388690322637558\n",
      "[step: 1635] loss: 47.93849182128906\n",
      "[step: 1635] loss: 0.006391320377588272\n",
      "[step: 1636] loss: 43.83035659790039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1636] loss: 0.006397451739758253\n",
      "[step: 1637] loss: 38.63092803955078\n",
      "[step: 1637] loss: 0.006398480851203203\n",
      "[step: 1638] loss: 35.027854919433594\n",
      "[step: 1638] loss: 0.006392775569111109\n",
      "[step: 1639] loss: 35.4263916015625\n",
      "[step: 1639] loss: 0.006386923138052225\n",
      "[step: 1640] loss: 39.33732986450195\n",
      "[step: 1640] loss: 0.006385548040270805\n",
      "[step: 1641] loss: 42.05778884887695\n",
      "[step: 1641] loss: 0.006387708242982626\n",
      "[step: 1642] loss: 40.64685821533203\n",
      "[step: 1642] loss: 0.006390295922756195\n",
      "[step: 1643] loss: 36.99638748168945\n",
      "[step: 1643] loss: 0.006390489172190428\n",
      "[step: 1644] loss: 34.81487274169922\n",
      "[step: 1644] loss: 0.006387561559677124\n",
      "[step: 1645] loss: 34.924537658691406\n",
      "[step: 1645] loss: 0.006383867468684912\n",
      "[step: 1646] loss: 36.102142333984375\n",
      "[step: 1646] loss: 0.006382043939083815\n",
      "[step: 1647] loss: 37.183528900146484\n",
      "[step: 1647] loss: 0.0063825310207903385\n",
      "[step: 1648] loss: 37.33253479003906\n",
      "[step: 1648] loss: 0.006383813451975584\n",
      "[step: 1649] loss: 37.104515075683594\n",
      "[step: 1649] loss: 0.006384473759680986\n",
      "[step: 1650] loss: 35.11573028564453\n",
      "[step: 1650] loss: 0.006383970845490694\n",
      "[step: 1651] loss: 33.70464324951172\n",
      "[step: 1651] loss: 0.006382311694324017\n",
      "[step: 1652] loss: 33.865936279296875\n",
      "[step: 1652] loss: 0.006380229257047176\n",
      "[step: 1653] loss: 34.83079528808594\n",
      "[step: 1653] loss: 0.0063787768594920635\n",
      "[step: 1654] loss: 35.822593688964844\n",
      "[step: 1654] loss: 0.006378327962011099\n",
      "[step: 1655] loss: 35.81074523925781\n",
      "[step: 1655] loss: 0.006378496531397104\n",
      "[step: 1656] loss: 35.7400016784668\n",
      "[step: 1656] loss: 0.006378779653459787\n",
      "[step: 1657] loss: 35.411354064941406\n",
      "[step: 1657] loss: 0.006378879304975271\n",
      "[step: 1658] loss: 34.74473571777344\n",
      "[step: 1658] loss: 0.006378571968525648\n",
      "[step: 1659] loss: 33.86329650878906\n",
      "[step: 1659] loss: 0.006377755664288998\n",
      "[step: 1660] loss: 33.3594970703125\n",
      "[step: 1660] loss: 0.006376665085554123\n",
      "[step: 1661] loss: 33.3807373046875\n",
      "[step: 1661] loss: 0.006375623866915703\n",
      "[step: 1662] loss: 33.573394775390625\n",
      "[step: 1662] loss: 0.0063747442327439785\n",
      "[step: 1663] loss: 33.62236404418945\n",
      "[step: 1663] loss: 0.006374042015522718\n",
      "[step: 1664] loss: 33.64186096191406\n",
      "[step: 1664] loss: 0.006373547948896885\n",
      "[step: 1665] loss: 33.93584442138672\n",
      "[step: 1665] loss: 0.006373260635882616\n",
      "[step: 1666] loss: 34.731101989746094\n",
      "[step: 1666] loss: 0.006373065058141947\n",
      "[step: 1667] loss: 36.745079040527344\n",
      "[step: 1667] loss: 0.006372879724949598\n",
      "[step: 1668] loss: 38.800228118896484\n",
      "[step: 1668] loss: 0.00637268740683794\n",
      "[step: 1669] loss: 44.28501892089844\n",
      "[step: 1669] loss: 0.006372499279677868\n",
      "[step: 1670] loss: 45.963714599609375\n",
      "[step: 1670] loss: 0.0063723293133080006\n",
      "[step: 1671] loss: 47.49852752685547\n",
      "[step: 1671] loss: 0.006372143980115652\n",
      "[step: 1672] loss: 46.049888610839844\n",
      "[step: 1672] loss: 0.006372014060616493\n",
      "[step: 1673] loss: 43.23477554321289\n",
      "[step: 1673] loss: 0.0063719917088747025\n",
      "[step: 1674] loss: 36.10179901123047\n",
      "[step: 1674] loss: 0.006372157949954271\n",
      "[step: 1675] loss: 33.8094367980957\n",
      "[step: 1675] loss: 0.006372591480612755\n",
      "[step: 1676] loss: 37.50468826293945\n",
      "[step: 1676] loss: 0.006373467855155468\n",
      "[step: 1677] loss: 39.623329162597656\n",
      "[step: 1677] loss: 0.006375046446919441\n",
      "[step: 1678] loss: 38.84656524658203\n",
      "[step: 1678] loss: 0.006377796642482281\n",
      "[step: 1679] loss: 37.36157989501953\n",
      "[step: 1679] loss: 0.006382414605468512\n",
      "[step: 1680] loss: 35.705467224121094\n",
      "[step: 1680] loss: 0.006390159018337727\n",
      "[step: 1681] loss: 33.957122802734375\n",
      "[step: 1681] loss: 0.006402973551303148\n",
      "[step: 1682] loss: 34.68634796142578\n",
      "[step: 1682] loss: 0.0064240810461342335\n",
      "[step: 1683] loss: 35.717838287353516\n",
      "[step: 1683] loss: 0.006457917392253876\n",
      "[step: 1684] loss: 35.6285400390625\n",
      "[step: 1684] loss: 0.006509679369628429\n",
      "[step: 1685] loss: 36.053184509277344\n",
      "[step: 1685] loss: 0.006580965593457222\n",
      "[step: 1686] loss: 35.04086685180664\n",
      "[step: 1686] loss: 0.006662908010184765\n",
      "[step: 1687] loss: 33.838985443115234\n",
      "[step: 1687] loss: 0.006717417389154434\n",
      "[step: 1688] loss: 33.416038513183594\n",
      "[step: 1688] loss: 0.006698925048112869\n",
      "[step: 1689] loss: 33.335750579833984\n",
      "[step: 1689] loss: 0.006578050088137388\n",
      "[step: 1690] loss: 33.81822204589844\n",
      "[step: 1690] loss: 0.00643507856875658\n",
      "[step: 1691] loss: 34.749351501464844\n",
      "[step: 1691] loss: 0.006374059244990349\n",
      "[step: 1692] loss: 35.32631301879883\n",
      "[step: 1692] loss: 0.006424474995583296\n",
      "[step: 1693] loss: 35.26554870605469\n",
      "[step: 1693] loss: 0.006501697935163975\n",
      "[step: 1694] loss: 35.78501892089844\n",
      "[step: 1694] loss: 0.006507880985736847\n",
      "[step: 1695] loss: 35.40636444091797\n",
      "[step: 1695] loss: 0.006441783159971237\n",
      "[step: 1696] loss: 35.357872009277344\n",
      "[step: 1696] loss: 0.006373732350766659\n",
      "[step: 1697] loss: 35.03554916381836\n",
      "[step: 1697] loss: 0.006379110272973776\n",
      "[step: 1698] loss: 34.490474700927734\n",
      "[step: 1698] loss: 0.006431184243410826\n",
      "[step: 1699] loss: 33.87921905517578\n",
      "[step: 1699] loss: 0.00645407335832715\n",
      "[step: 1700] loss: 33.451385498046875\n",
      "[step: 1700] loss: 0.006430921144783497\n",
      "[step: 1701] loss: 33.21257781982422\n",
      "[step: 1701] loss: 0.006388735491782427\n",
      "[step: 1702] loss: 33.067832946777344\n",
      "[step: 1702] loss: 0.00636527594178915\n",
      "[step: 1703] loss: 32.80560302734375\n",
      "[step: 1703] loss: 0.006376995239406824\n",
      "[step: 1704] loss: 32.6782112121582\n",
      "[step: 1704] loss: 0.006404221057891846\n",
      "[step: 1705] loss: 32.54755401611328\n",
      "[step: 1705] loss: 0.0064133452251553535\n",
      "[step: 1706] loss: 32.413150787353516\n",
      "[step: 1706] loss: 0.006394257303327322\n",
      "[step: 1707] loss: 32.4060173034668\n",
      "[step: 1707] loss: 0.006367995869368315\n",
      "[step: 1708] loss: 32.46400833129883\n",
      "[step: 1708] loss: 0.006360492669045925\n",
      "[step: 1709] loss: 32.50798034667969\n",
      "[step: 1709] loss: 0.006371725350618362\n",
      "[step: 1710] loss: 32.51639938354492\n",
      "[step: 1710] loss: 0.006384489592164755\n",
      "[step: 1711] loss: 32.6217041015625\n",
      "[step: 1711] loss: 0.006386084016412497\n",
      "[step: 1712] loss: 33.02619171142578\n",
      "[step: 1712] loss: 0.0063759866170585155\n",
      "[step: 1713] loss: 34.53400421142578\n",
      "[step: 1713] loss: 0.006362232845276594\n",
      "[step: 1714] loss: 38.187042236328125\n",
      "[step: 1714] loss: 0.006356275174766779\n",
      "[step: 1715] loss: 51.135780334472656\n",
      "[step: 1715] loss: 0.006361071486026049\n",
      "[step: 1716] loss: 60.77606964111328\n",
      "[step: 1716] loss: 0.006369306240230799\n",
      "[step: 1717] loss: 70.93612670898438\n",
      "[step: 1717] loss: 0.0063720522448420525\n",
      "[step: 1718] loss: 72.9814453125\n",
      "[step: 1718] loss: 0.006367305293679237\n",
      "[step: 1719] loss: 71.89630126953125\n",
      "[step: 1719] loss: 0.006360216997563839\n",
      "[step: 1720] loss: 43.82291030883789\n",
      "[step: 1720] loss: 0.006354943849146366\n",
      "[step: 1721] loss: 44.161376953125\n",
      "[step: 1721] loss: 0.006353390868753195\n",
      "[step: 1722] loss: 59.49858474731445\n",
      "[step: 1722] loss: 0.006355547346174717\n",
      "[step: 1723] loss: 58.59625244140625\n",
      "[step: 1723] loss: 0.0063591767102479935\n",
      "[step: 1724] loss: 46.580650329589844\n",
      "[step: 1724] loss: 0.006361101754009724\n",
      "[step: 1725] loss: 41.45396041870117\n",
      "[step: 1725] loss: 0.006359887775033712\n",
      "[step: 1726] loss: 44.79246520996094\n",
      "[step: 1726] loss: 0.006356492638587952\n",
      "[step: 1727] loss: 44.502159118652344\n",
      "[step: 1727] loss: 0.006352961994707584\n",
      "[step: 1728] loss: 43.184024810791016\n",
      "[step: 1728] loss: 0.006350446026772261\n",
      "[step: 1729] loss: 45.32026672363281\n",
      "[step: 1729] loss: 0.006349205505102873\n",
      "[step: 1730] loss: 38.156097412109375\n",
      "[step: 1730] loss: 0.0063494108617305756\n",
      "[step: 1731] loss: 44.01612854003906\n",
      "[step: 1731] loss: 0.006350639741867781\n",
      "[step: 1732] loss: 45.9385871887207\n",
      "[step: 1732] loss: 0.006351849064230919\n",
      "[step: 1733] loss: 36.37946319580078\n",
      "[step: 1733] loss: 0.006352379452437162\n",
      "[step: 1734] loss: 45.582420349121094\n",
      "[step: 1734] loss: 0.0063520935364067554\n",
      "[step: 1735] loss: 47.293609619140625\n",
      "[step: 1735] loss: 0.006351317744702101\n",
      "[step: 1736] loss: 40.58184814453125\n",
      "[step: 1736] loss: 0.006350181996822357\n",
      "[step: 1737] loss: 46.09043502807617\n",
      "[step: 1737] loss: 0.0063487207517027855\n",
      "[step: 1738] loss: 41.72453689575195\n",
      "[step: 1738] loss: 0.006347206886857748\n",
      "[step: 1739] loss: 40.72886657714844\n",
      "[step: 1739] loss: 0.006345921661704779\n",
      "[step: 1740] loss: 42.52606201171875\n",
      "[step: 1740] loss: 0.006344921421259642\n",
      "[step: 1741] loss: 37.40711212158203\n",
      "[step: 1741] loss: 0.006344099063426256\n",
      "[step: 1742] loss: 39.076026916503906\n",
      "[step: 1742] loss: 0.006343378685414791\n",
      "[step: 1743] loss: 37.788387298583984\n",
      "[step: 1743] loss: 0.006342820357531309\n",
      "[step: 1744] loss: 37.78293228149414\n",
      "[step: 1744] loss: 0.006342432927340269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1745] loss: 36.43822479248047\n",
      "[step: 1745] loss: 0.006342117674648762\n",
      "[step: 1746] loss: 35.9844970703125\n",
      "[step: 1746] loss: 0.006341824308037758\n",
      "[step: 1747] loss: 36.85075378417969\n",
      "[step: 1747] loss: 0.006341582629829645\n",
      "[step: 1748] loss: 35.565757751464844\n",
      "[step: 1748] loss: 0.00634147971868515\n",
      "[step: 1749] loss: 35.464378356933594\n",
      "[step: 1749] loss: 0.006341575179249048\n",
      "[step: 1750] loss: 34.74363708496094\n",
      "[step: 1750] loss: 0.006341925356537104\n",
      "[step: 1751] loss: 34.731082916259766\n",
      "[step: 1751] loss: 0.0063427407294511795\n",
      "[step: 1752] loss: 34.72681427001953\n",
      "[step: 1752] loss: 0.006344395689666271\n",
      "[step: 1753] loss: 33.97026443481445\n",
      "[step: 1753] loss: 0.00634759571403265\n",
      "[step: 1754] loss: 34.248748779296875\n",
      "[step: 1754] loss: 0.006353588309139013\n",
      "[step: 1755] loss: 33.515098571777344\n",
      "[step: 1755] loss: 0.006364612374454737\n",
      "[step: 1756] loss: 33.39221954345703\n",
      "[step: 1756] loss: 0.00638499716296792\n",
      "[step: 1757] loss: 33.93414306640625\n",
      "[step: 1757] loss: 0.0064218658953905106\n",
      "[step: 1758] loss: 33.280487060546875\n",
      "[step: 1758] loss: 0.006487278267741203\n",
      "[step: 1759] loss: 33.309059143066406\n",
      "[step: 1759] loss: 0.006593438796699047\n",
      "[step: 1760] loss: 33.21746063232422\n",
      "[step: 1760] loss: 0.006742955185472965\n",
      "[step: 1761] loss: 32.60038375854492\n",
      "[step: 1761] loss: 0.006881432607769966\n",
      "[step: 1762] loss: 33.114990234375\n",
      "[step: 1762] loss: 0.006903358735144138\n",
      "[step: 1763] loss: 32.83355712890625\n",
      "[step: 1763] loss: 0.006703068036586046\n",
      "[step: 1764] loss: 32.531612396240234\n",
      "[step: 1764] loss: 0.0064477138221263885\n",
      "[step: 1765] loss: 32.97978973388672\n",
      "[step: 1765] loss: 0.006373452488332987\n",
      "[step: 1766] loss: 32.256771087646484\n",
      "[step: 1766] loss: 0.006505716126412153\n",
      "[step: 1767] loss: 32.28862762451172\n",
      "[step: 1767] loss: 0.00659848190844059\n",
      "[step: 1768] loss: 32.3186149597168\n",
      "[step: 1768] loss: 0.0064934901893138885\n",
      "[step: 1769] loss: 31.94048500061035\n",
      "[step: 1769] loss: 0.006364337168633938\n",
      "[step: 1770] loss: 32.19626998901367\n",
      "[step: 1770] loss: 0.006392939016222954\n",
      "[step: 1771] loss: 32.205810546875\n",
      "[step: 1771] loss: 0.00649613980203867\n",
      "[step: 1772] loss: 31.933738708496094\n",
      "[step: 1772] loss: 0.006474665831774473\n",
      "[step: 1773] loss: 32.20122528076172\n",
      "[step: 1773] loss: 0.0063719842582941055\n",
      "[step: 1774] loss: 32.341087341308594\n",
      "[step: 1774] loss: 0.006362621206790209\n",
      "[step: 1775] loss: 32.37965393066406\n",
      "[step: 1775] loss: 0.006410970818251371\n",
      "[step: 1776] loss: 33.205482482910156\n",
      "[step: 1776] loss: 0.006433132570236921\n",
      "[step: 1777] loss: 34.17643737792969\n",
      "[step: 1777] loss: 0.006401878781616688\n",
      "[step: 1778] loss: 36.59796142578125\n",
      "[step: 1778] loss: 0.00634947931393981\n",
      "[step: 1779] loss: 40.04212188720703\n",
      "[step: 1779] loss: 0.006351751275360584\n",
      "[step: 1780] loss: 46.16854476928711\n",
      "[step: 1780] loss: 0.006395877804607153\n",
      "[step: 1781] loss: 45.477787017822266\n",
      "[step: 1781] loss: 0.006393597461283207\n",
      "[step: 1782] loss: 41.214134216308594\n",
      "[step: 1782] loss: 0.006355966906994581\n",
      "[step: 1783] loss: 34.86915969848633\n",
      "[step: 1783] loss: 0.006344673689454794\n",
      "[step: 1784] loss: 32.7540397644043\n",
      "[step: 1784] loss: 0.0063534751534461975\n",
      "[step: 1785] loss: 35.45112609863281\n",
      "[step: 1785] loss: 0.006370025221258402\n",
      "[step: 1786] loss: 38.25151824951172\n",
      "[step: 1786] loss: 0.00636752275750041\n",
      "[step: 1787] loss: 38.95536804199219\n",
      "[step: 1787] loss: 0.0063402061350643635\n",
      "[step: 1788] loss: 35.54100036621094\n",
      "[step: 1788] loss: 0.006334659643471241\n",
      "[step: 1789] loss: 32.47671127319336\n",
      "[step: 1789] loss: 0.006350414827466011\n",
      "[step: 1790] loss: 32.68891525268555\n",
      "[step: 1790] loss: 0.006353144999593496\n",
      "[step: 1791] loss: 34.81368637084961\n",
      "[step: 1791] loss: 0.006346507463604212\n",
      "[step: 1792] loss: 35.76552963256836\n",
      "[step: 1792] loss: 0.006336253136396408\n",
      "[step: 1793] loss: 33.772666931152344\n",
      "[step: 1793] loss: 0.006330081261694431\n",
      "[step: 1794] loss: 31.90140151977539\n",
      "[step: 1794] loss: 0.006338370498269796\n",
      "[step: 1795] loss: 32.27857208251953\n",
      "[step: 1795] loss: 0.006343686021864414\n",
      "[step: 1796] loss: 33.334259033203125\n",
      "[step: 1796] loss: 0.006337577477097511\n",
      "[step: 1797] loss: 33.60894775390625\n",
      "[step: 1797] loss: 0.0063309925608336926\n",
      "[step: 1798] loss: 33.14720153808594\n",
      "[step: 1798] loss: 0.006328078918159008\n",
      "[step: 1799] loss: 32.9443244934082\n",
      "[step: 1799] loss: 0.0063293008133769035\n",
      "[step: 1800] loss: 33.1921272277832\n",
      "[step: 1800] loss: 0.006333981174975634\n",
      "[step: 1801] loss: 33.917564392089844\n",
      "[step: 1801] loss: 0.006333925761282444\n",
      "[step: 1802] loss: 32.963584899902344\n",
      "[step: 1802] loss: 0.006328723393380642\n",
      "[step: 1803] loss: 31.86021614074707\n",
      "[step: 1803] loss: 0.006325524765998125\n",
      "[step: 1804] loss: 31.48027801513672\n",
      "[step: 1804] loss: 0.006324476096779108\n",
      "[step: 1805] loss: 31.849750518798828\n",
      "[step: 1805] loss: 0.006325470749288797\n",
      "[step: 1806] loss: 32.0986213684082\n",
      "[step: 1806] loss: 0.006327781826257706\n",
      "[step: 1807] loss: 31.92475700378418\n",
      "[step: 1807] loss: 0.006327369716018438\n",
      "[step: 1808] loss: 31.657461166381836\n",
      "[step: 1808] loss: 0.006324623711407185\n",
      "[step: 1809] loss: 31.573101043701172\n",
      "[step: 1809] loss: 0.0063224658370018005\n",
      "[step: 1810] loss: 32.089054107666016\n",
      "[step: 1810] loss: 0.006321233697235584\n",
      "[step: 1811] loss: 32.83392333984375\n",
      "[step: 1811] loss: 0.006321088410913944\n",
      "[step: 1812] loss: 33.43896484375\n",
      "[step: 1812] loss: 0.0063221994787454605\n",
      "[step: 1813] loss: 33.87901306152344\n",
      "[step: 1813] loss: 0.006322591565549374\n",
      "[step: 1814] loss: 34.9664192199707\n",
      "[step: 1814] loss: 0.0063216956332325935\n",
      "[step: 1815] loss: 35.98362731933594\n",
      "[step: 1815] loss: 0.006320467218756676\n",
      "[step: 1816] loss: 37.81047439575195\n",
      "[step: 1816] loss: 0.006319085136055946\n",
      "[step: 1817] loss: 36.90141296386719\n",
      "[step: 1817] loss: 0.006317890249192715\n",
      "[step: 1818] loss: 35.027503967285156\n",
      "[step: 1818] loss: 0.00631757453083992\n",
      "[step: 1819] loss: 32.800804138183594\n",
      "[step: 1819] loss: 0.006317732390016317\n",
      "[step: 1820] loss: 32.26897048950195\n",
      "[step: 1820] loss: 0.006317737977951765\n",
      "[step: 1821] loss: 32.5997200012207\n",
      "[step: 1821] loss: 0.006317672785371542\n",
      "[step: 1822] loss: 32.29163360595703\n",
      "[step: 1822] loss: 0.006317365914583206\n",
      "[step: 1823] loss: 32.26859664916992\n",
      "[step: 1823] loss: 0.006316577084362507\n",
      "[step: 1824] loss: 32.92512512207031\n",
      "[step: 1824] loss: 0.0063157170079648495\n",
      "[step: 1825] loss: 33.64347839355469\n",
      "[step: 1825] loss: 0.006314978003501892\n",
      "[step: 1826] loss: 33.24292755126953\n",
      "[step: 1826] loss: 0.006314256694167852\n",
      "[step: 1827] loss: 31.979894638061523\n",
      "[step: 1827] loss: 0.006313669960945845\n",
      "[step: 1828] loss: 31.118968963623047\n",
      "[step: 1828] loss: 0.00631334213539958\n",
      "[step: 1829] loss: 31.270816802978516\n",
      "[step: 1829] loss: 0.006313077174127102\n",
      "[step: 1830] loss: 31.632173538208008\n",
      "[step: 1830] loss: 0.006312803830951452\n",
      "[step: 1831] loss: 31.589435577392578\n",
      "[step: 1831] loss: 0.006312625948339701\n",
      "[step: 1832] loss: 31.207317352294922\n",
      "[step: 1832] loss: 0.006312437821179628\n",
      "[step: 1833] loss: 31.167232513427734\n",
      "[step: 1833] loss: 0.006312177050858736\n",
      "[step: 1834] loss: 31.721202850341797\n",
      "[step: 1834] loss: 0.0063119386322796345\n",
      "[step: 1835] loss: 32.68620300292969\n",
      "[step: 1835] loss: 0.006311746779829264\n",
      "[step: 1836] loss: 34.667015075683594\n",
      "[step: 1836] loss: 0.006311544217169285\n",
      "[step: 1837] loss: 36.119972229003906\n",
      "[step: 1837] loss: 0.006311386823654175\n",
      "[step: 1838] loss: 39.428443908691406\n",
      "[step: 1838] loss: 0.006311375182121992\n",
      "[step: 1839] loss: 41.28596496582031\n",
      "[step: 1839] loss: 0.006311513017863035\n",
      "[step: 1840] loss: 43.77891159057617\n",
      "[step: 1840] loss: 0.006311897654086351\n",
      "[step: 1841] loss: 43.349395751953125\n",
      "[step: 1841] loss: 0.006312704645097256\n",
      "[step: 1842] loss: 40.362220764160156\n",
      "[step: 1842] loss: 0.006314190104603767\n",
      "[step: 1843] loss: 33.86355972290039\n",
      "[step: 1843] loss: 0.006316746585071087\n",
      "[step: 1844] loss: 31.89548683166504\n",
      "[step: 1844] loss: 0.006321068853139877\n",
      "[step: 1845] loss: 33.14366912841797\n",
      "[step: 1845] loss: 0.0063283126801252365\n",
      "[step: 1846] loss: 34.50988006591797\n",
      "[step: 1846] loss: 0.006340458989143372\n",
      "[step: 1847] loss: 36.700374603271484\n",
      "[step: 1847] loss: 0.006360220722854137\n",
      "[step: 1848] loss: 37.09772491455078\n",
      "[step: 1848] loss: 0.006392346695065498\n",
      "[step: 1849] loss: 34.67304229736328\n",
      "[step: 1849] loss: 0.006440893746912479\n",
      "[step: 1850] loss: 33.58648681640625\n",
      "[step: 1850] loss: 0.006509100086987019\n",
      "[step: 1851] loss: 33.46160888671875\n",
      "[step: 1851] loss: 0.006584557704627514\n",
      "[step: 1852] loss: 31.596908569335938\n",
      "[step: 1852] loss: 0.006640124600380659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1853] loss: 32.17821502685547\n",
      "[step: 1853] loss: 0.006617015693336725\n",
      "[step: 1854] loss: 33.77751922607422\n",
      "[step: 1854] loss: 0.006506573874503374\n",
      "[step: 1855] loss: 33.635337829589844\n",
      "[step: 1855] loss: 0.0063676792196929455\n",
      "[step: 1856] loss: 34.239830017089844\n",
      "[step: 1856] loss: 0.006312350742518902\n",
      "[step: 1857] loss: 32.86156463623047\n",
      "[step: 1857] loss: 0.006364054046571255\n",
      "[step: 1858] loss: 31.04928207397461\n",
      "[step: 1858] loss: 0.006438642740249634\n",
      "[step: 1859] loss: 31.177453994750977\n",
      "[step: 1859] loss: 0.006446242798119783\n",
      "[step: 1860] loss: 31.432353973388672\n",
      "[step: 1860] loss: 0.006376004312187433\n",
      "[step: 1861] loss: 31.454151153564453\n",
      "[step: 1861] loss: 0.00631372258067131\n",
      "[step: 1862] loss: 31.762088775634766\n",
      "[step: 1862] loss: 0.0063190339133143425\n",
      "[step: 1863] loss: 32.003013610839844\n",
      "[step: 1863] loss: 0.0063663083128631115\n",
      "[step: 1864] loss: 31.922529220581055\n",
      "[step: 1864] loss: 0.0063944607973098755\n",
      "[step: 1865] loss: 32.417362213134766\n",
      "[step: 1865] loss: 0.006373491138219833\n",
      "[step: 1866] loss: 32.41648483276367\n",
      "[step: 1866] loss: 0.006329140160232782\n",
      "[step: 1867] loss: 31.97970199584961\n",
      "[step: 1867] loss: 0.006304305046796799\n",
      "[step: 1868] loss: 31.721309661865234\n",
      "[step: 1868] loss: 0.0063169728964567184\n",
      "[step: 1869] loss: 31.763320922851562\n",
      "[step: 1869] loss: 0.006344666704535484\n",
      "[step: 1870] loss: 31.644378662109375\n",
      "[step: 1870] loss: 0.006353823002427816\n",
      "[step: 1871] loss: 31.953441619873047\n",
      "[step: 1871] loss: 0.00633595185354352\n",
      "[step: 1872] loss: 32.02742385864258\n",
      "[step: 1872] loss: 0.006310951430350542\n",
      "[step: 1873] loss: 32.256629943847656\n",
      "[step: 1873] loss: 0.0063011134043335915\n",
      "[step: 1874] loss: 32.14359664916992\n",
      "[step: 1874] loss: 0.006309772375971079\n",
      "[step: 1875] loss: 32.65455627441406\n",
      "[step: 1875] loss: 0.006324187386780977\n",
      "[step: 1876] loss: 33.07688903808594\n",
      "[step: 1876] loss: 0.006328972987830639\n",
      "[step: 1877] loss: 33.40986633300781\n",
      "[step: 1877] loss: 0.006319485604763031\n",
      "[step: 1878] loss: 33.4078254699707\n",
      "[step: 1878] loss: 0.006305440329015255\n",
      "[step: 1879] loss: 33.19200897216797\n",
      "[step: 1879] loss: 0.006298013962805271\n",
      "[step: 1880] loss: 32.543678283691406\n",
      "[step: 1880] loss: 0.0063009182922542095\n",
      "[step: 1881] loss: 32.00750732421875\n",
      "[step: 1881] loss: 0.00630847318097949\n",
      "[step: 1882] loss: 31.539531707763672\n",
      "[step: 1882] loss: 0.00631284574046731\n",
      "[step: 1883] loss: 31.18110466003418\n",
      "[step: 1883] loss: 0.006311131175607443\n",
      "[step: 1884] loss: 30.667922973632812\n",
      "[step: 1884] loss: 0.006304721813648939\n",
      "[step: 1885] loss: 30.274702072143555\n",
      "[step: 1885] loss: 0.006298017222434282\n",
      "[step: 1886] loss: 30.04672622680664\n",
      "[step: 1886] loss: 0.006294916849583387\n",
      "[step: 1887] loss: 29.969541549682617\n",
      "[step: 1887] loss: 0.0062962304800748825\n",
      "[step: 1888] loss: 29.99428939819336\n",
      "[step: 1888] loss: 0.0062997424975037575\n",
      "[step: 1889] loss: 30.132064819335938\n",
      "[step: 1889] loss: 0.006302452180534601\n",
      "[step: 1890] loss: 30.368579864501953\n",
      "[step: 1890] loss: 0.006302719935774803\n",
      "[step: 1891] loss: 30.673812866210938\n",
      "[step: 1891] loss: 0.0063006095588207245\n",
      "[step: 1892] loss: 31.49527931213379\n",
      "[step: 1892] loss: 0.0062972079031169415\n",
      "[step: 1893] loss: 32.85403823852539\n",
      "[step: 1893] loss: 0.006293884478509426\n",
      "[step: 1894] loss: 36.162757873535156\n",
      "[step: 1894] loss: 0.006291861180216074\n",
      "[step: 1895] loss: 40.6197624206543\n",
      "[step: 1895] loss: 0.0062915305607020855\n",
      "[step: 1896] loss: 47.6070671081543\n",
      "[step: 1896] loss: 0.0062923491932451725\n",
      "[step: 1897] loss: 51.10143280029297\n",
      "[step: 1897] loss: 0.006293531507253647\n",
      "[step: 1898] loss: 50.73318862915039\n",
      "[step: 1898] loss: 0.006294424179941416\n",
      "[step: 1899] loss: 42.14906311035156\n",
      "[step: 1899] loss: 0.006294707767665386\n",
      "[step: 1900] loss: 34.299129486083984\n",
      "[step: 1900] loss: 0.006294314283877611\n",
      "[step: 1901] loss: 32.586483001708984\n",
      "[step: 1901] loss: 0.006293340120464563\n",
      "[step: 1902] loss: 34.97776794433594\n",
      "[step: 1902] loss: 0.006291999481618404\n",
      "[step: 1903] loss: 39.65184020996094\n",
      "[step: 1903] loss: 0.0062906271778047085\n",
      "[step: 1904] loss: 42.7748908996582\n",
      "[step: 1904] loss: 0.006289395038038492\n",
      "[step: 1905] loss: 39.25676727294922\n",
      "[step: 1905] loss: 0.006288326345384121\n",
      "[step: 1906] loss: 33.26661682128906\n",
      "[step: 1906] loss: 0.006287474650889635\n",
      "[step: 1907] loss: 31.758853912353516\n",
      "[step: 1907] loss: 0.006286842282861471\n",
      "[step: 1908] loss: 33.76392364501953\n",
      "[step: 1908] loss: 0.00628638593479991\n",
      "[step: 1909] loss: 34.74371337890625\n",
      "[step: 1909] loss: 0.006286065559834242\n",
      "[step: 1910] loss: 34.415279388427734\n",
      "[step: 1910] loss: 0.006285830866545439\n",
      "[step: 1911] loss: 32.49798583984375\n",
      "[step: 1911] loss: 0.006285661831498146\n",
      "[step: 1912] loss: 31.187847137451172\n",
      "[step: 1912] loss: 0.006285575218498707\n",
      "[step: 1913] loss: 32.45697021484375\n",
      "[step: 1913] loss: 0.006285626441240311\n",
      "[step: 1914] loss: 33.072181701660156\n",
      "[step: 1914] loss: 0.006285821087658405\n",
      "[step: 1915] loss: 31.621095657348633\n",
      "[step: 1915] loss: 0.006286283954977989\n",
      "[step: 1916] loss: 31.000629425048828\n",
      "[step: 1916] loss: 0.00628718500956893\n",
      "[step: 1917] loss: 31.069534301757812\n",
      "[step: 1917] loss: 0.006288827396929264\n",
      "[step: 1918] loss: 31.711647033691406\n",
      "[step: 1918] loss: 0.006291761063039303\n",
      "[step: 1919] loss: 32.58970260620117\n",
      "[step: 1919] loss: 0.0062967827543616295\n",
      "[step: 1920] loss: 31.82183837890625\n",
      "[step: 1920] loss: 0.00630564521998167\n",
      "[step: 1921] loss: 30.937618255615234\n",
      "[step: 1921] loss: 0.006320799235254526\n",
      "[step: 1922] loss: 30.77239227294922\n",
      "[step: 1922] loss: 0.006347241345793009\n",
      "[step: 1923] loss: 31.234481811523438\n",
      "[step: 1923] loss: 0.0063904509879648685\n",
      "[step: 1924] loss: 31.616470336914062\n",
      "[step: 1924] loss: 0.006460095290094614\n",
      "[step: 1925] loss: 31.569786071777344\n",
      "[step: 1925] loss: 0.006553096231073141\n",
      "[step: 1926] loss: 30.98287582397461\n",
      "[step: 1926] loss: 0.006658428348600864\n",
      "[step: 1927] loss: 30.51477813720703\n",
      "[step: 1927] loss: 0.0067021818831563\n",
      "[step: 1928] loss: 30.566776275634766\n",
      "[step: 1928] loss: 0.006635267287492752\n",
      "[step: 1929] loss: 31.032094955444336\n",
      "[step: 1929] loss: 0.006454783491790295\n",
      "[step: 1930] loss: 31.059513092041016\n",
      "[step: 1930] loss: 0.006316022016108036\n",
      "[step: 1931] loss: 30.949092864990234\n",
      "[step: 1931] loss: 0.006322602275758982\n",
      "[step: 1932] loss: 30.49237823486328\n",
      "[step: 1932] loss: 0.006419760175049305\n",
      "[step: 1933] loss: 30.476177215576172\n",
      "[step: 1933] loss: 0.006467420607805252\n",
      "[step: 1934] loss: 30.516921997070312\n",
      "[step: 1934] loss: 0.006396837066859007\n",
      "[step: 1935] loss: 30.72284698486328\n",
      "[step: 1935] loss: 0.006304287351667881\n",
      "[step: 1936] loss: 30.716758728027344\n",
      "[step: 1936] loss: 0.006301433313637972\n",
      "[step: 1937] loss: 30.459434509277344\n",
      "[step: 1937] loss: 0.00637100450694561\n",
      "[step: 1938] loss: 30.277767181396484\n",
      "[step: 1938] loss: 0.006400444079190493\n",
      "[step: 1939] loss: 30.38860321044922\n",
      "[step: 1939] loss: 0.006347892805933952\n",
      "[step: 1940] loss: 30.6065673828125\n",
      "[step: 1940] loss: 0.006298264488577843\n",
      "[step: 1941] loss: 31.275108337402344\n",
      "[step: 1941] loss: 0.006296935025602579\n",
      "[step: 1942] loss: 31.68488311767578\n",
      "[step: 1942] loss: 0.006322135217487812\n",
      "[step: 1943] loss: 32.52794647216797\n",
      "[step: 1943] loss: 0.006343862507492304\n",
      "[step: 1944] loss: 32.64521026611328\n",
      "[step: 1944] loss: 0.0063363551162183285\n",
      "[step: 1945] loss: 33.151573181152344\n",
      "[step: 1945] loss: 0.006301270332187414\n",
      "[step: 1946] loss: 33.29602813720703\n",
      "[step: 1946] loss: 0.0062791467644274235\n",
      "[step: 1947] loss: 33.587120056152344\n",
      "[step: 1947] loss: 0.006295737344771624\n",
      "[step: 1948] loss: 33.00082778930664\n",
      "[step: 1948] loss: 0.006317186634987593\n",
      "[step: 1949] loss: 31.974674224853516\n",
      "[step: 1949] loss: 0.006313669960945845\n",
      "[step: 1950] loss: 30.531932830810547\n",
      "[step: 1950] loss: 0.0062967948615550995\n",
      "[step: 1951] loss: 29.71576690673828\n",
      "[step: 1951] loss: 0.00628299918025732\n",
      "[step: 1952] loss: 29.541107177734375\n",
      "[step: 1952] loss: 0.006281518843024969\n",
      "[step: 1953] loss: 29.708683013916016\n",
      "[step: 1953] loss: 0.006290432531386614\n",
      "[step: 1954] loss: 29.882614135742188\n",
      "[step: 1954] loss: 0.006299247033894062\n",
      "[step: 1955] loss: 29.90909194946289\n",
      "[step: 1955] loss: 0.0062958369962871075\n",
      "[step: 1956] loss: 29.74630355834961\n",
      "[step: 1956] loss: 0.0062811546958982944\n",
      "[step: 1957] loss: 29.67483139038086\n",
      "[step: 1957] loss: 0.006275821942836046\n",
      "[step: 1958] loss: 30.195331573486328\n",
      "[step: 1958] loss: 0.006279440131038427\n",
      "[step: 1959] loss: 31.372337341308594\n",
      "[step: 1959] loss: 0.006283005233854055\n",
      "[step: 1960] loss: 34.01742935180664\n",
      "[step: 1960] loss: 0.006286388263106346\n",
      "[step: 1961] loss: 36.185707092285156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1961] loss: 0.006285336334258318\n",
      "[step: 1962] loss: 39.797767639160156\n",
      "[step: 1962] loss: 0.006278899032622576\n",
      "[step: 1963] loss: 39.791847229003906\n",
      "[step: 1963] loss: 0.00627277372404933\n",
      "[step: 1964] loss: 41.80657196044922\n",
      "[step: 1964] loss: 0.006272161845117807\n",
      "[step: 1965] loss: 42.104286193847656\n",
      "[step: 1965] loss: 0.006275217980146408\n",
      "[step: 1966] loss: 40.166778564453125\n",
      "[step: 1966] loss: 0.006276482716202736\n",
      "[step: 1967] loss: 34.50374984741211\n",
      "[step: 1967] loss: 0.006277268752455711\n",
      "[step: 1968] loss: 32.21731185913086\n",
      "[step: 1968] loss: 0.006276529747992754\n",
      "[step: 1969] loss: 31.732208251953125\n",
      "[step: 1969] loss: 0.0062729306519031525\n",
      "[step: 1970] loss: 32.39862060546875\n",
      "[step: 1970] loss: 0.006269820034503937\n",
      "[step: 1971] loss: 34.189491271972656\n",
      "[step: 1971] loss: 0.006268938072025776\n",
      "[step: 1972] loss: 34.014305114746094\n",
      "[step: 1972] loss: 0.006269572768360376\n",
      "[step: 1973] loss: 31.64173126220703\n",
      "[step: 1973] loss: 0.006270318757742643\n",
      "[step: 1974] loss: 31.897808074951172\n",
      "[step: 1974] loss: 0.006270844954997301\n",
      "[step: 1975] loss: 33.1290283203125\n",
      "[step: 1975] loss: 0.006271311081945896\n",
      "[step: 1976] loss: 30.91176986694336\n",
      "[step: 1976] loss: 0.0062703778967261314\n",
      "[step: 1977] loss: 30.03230857849121\n",
      "[step: 1977] loss: 0.006268579978495836\n",
      "[step: 1978] loss: 30.51953887939453\n",
      "[step: 1978] loss: 0.006267189979553223\n",
      "[step: 1979] loss: 30.474971771240234\n",
      "[step: 1979] loss: 0.006266163196414709\n",
      "[step: 1980] loss: 31.718990325927734\n",
      "[step: 1980] loss: 0.00626543490216136\n",
      "[step: 1981] loss: 32.53355407714844\n",
      "[step: 1981] loss: 0.006265060044825077\n",
      "[step: 1982] loss: 33.170291900634766\n",
      "[step: 1982] loss: 0.00626512011513114\n",
      "[step: 1983] loss: 32.97317886352539\n",
      "[step: 1983] loss: 0.00626549543812871\n",
      "[step: 1984] loss: 34.14695358276367\n",
      "[step: 1984] loss: 0.006265582051128149\n",
      "[step: 1985] loss: 33.710304260253906\n",
      "[step: 1985] loss: 0.006265424191951752\n",
      "[step: 1986] loss: 32.365699768066406\n",
      "[step: 1986] loss: 0.00626539159566164\n",
      "[step: 1987] loss: 30.750896453857422\n",
      "[step: 1987] loss: 0.006265191361308098\n",
      "[step: 1988] loss: 29.515642166137695\n",
      "[step: 1988] loss: 0.006264758761972189\n",
      "[step: 1989] loss: 29.316452026367188\n",
      "[step: 1989] loss: 0.006264291703701019\n",
      "[step: 1990] loss: 30.052783966064453\n",
      "[step: 1990] loss: 0.006263860501348972\n",
      "[step: 1991] loss: 30.33342170715332\n",
      "[step: 1991] loss: 0.00626353407278657\n",
      "[step: 1992] loss: 30.281768798828125\n",
      "[step: 1992] loss: 0.006263133604079485\n",
      "[step: 1993] loss: 30.792720794677734\n",
      "[step: 1993] loss: 0.006262768991291523\n",
      "[step: 1994] loss: 31.46273422241211\n",
      "[step: 1994] loss: 0.006262601353228092\n",
      "[step: 1995] loss: 32.39973068237305\n",
      "[step: 1995] loss: 0.0062625897116959095\n",
      "[step: 1996] loss: 32.430381774902344\n",
      "[step: 1996] loss: 0.006262714974582195\n",
      "[step: 1997] loss: 32.41609191894531\n",
      "[step: 1997] loss: 0.006263114977627993\n",
      "[step: 1998] loss: 31.293071746826172\n",
      "[step: 1998] loss: 0.006263966206461191\n",
      "[step: 1999] loss: 30.840858459472656\n",
      "[step: 1999] loss: 0.006265582051128149\n",
      "[step: 2000] loss: 30.845373153686523\n",
      "[step: 2000] loss: 0.006268244702368975\n",
      "[step: 2001] loss: 30.469635009765625\n",
      "[step: 2001] loss: 0.006272721569985151\n",
      "[step: 2002] loss: 29.544857025146484\n",
      "[step: 2002] loss: 0.006280024070292711\n",
      "[step: 2003] loss: 28.977725982666016\n",
      "[step: 2003] loss: 0.0062921964563429356\n",
      "[step: 2004] loss: 28.923297882080078\n",
      "[step: 2004] loss: 0.006311472039669752\n",
      "[step: 2005] loss: 29.148136138916016\n",
      "[step: 2005] loss: 0.006342408247292042\n",
      "[step: 2006] loss: 29.270015716552734\n",
      "[step: 2006] loss: 0.006387148983776569\n",
      "[step: 2007] loss: 29.154571533203125\n",
      "[step: 2007] loss: 0.006449970416724682\n",
      "[step: 2008] loss: 28.858745574951172\n",
      "[step: 2008] loss: 0.006515112705528736\n",
      "[step: 2009] loss: 28.85865020751953\n",
      "[step: 2009] loss: 0.0065650977194309235\n",
      "[step: 2010] loss: 29.340206146240234\n",
      "[step: 2010] loss: 0.006543559487909079\n",
      "[step: 2011] loss: 30.033138275146484\n",
      "[step: 2011] loss: 0.006450298707932234\n",
      "[step: 2012] loss: 31.448074340820312\n",
      "[step: 2012] loss: 0.006326050963252783\n",
      "[step: 2013] loss: 33.319679260253906\n",
      "[step: 2013] loss: 0.006265406962484121\n",
      "[step: 2014] loss: 38.18220520019531\n",
      "[step: 2014] loss: 0.006296215578913689\n",
      "[step: 2015] loss: 41.556121826171875\n",
      "[step: 2015] loss: 0.006362632382661104\n",
      "[step: 2016] loss: 47.29286193847656\n",
      "[step: 2016] loss: 0.006390034221112728\n",
      "[step: 2017] loss: 50.19192886352539\n",
      "[step: 2017] loss: 0.006346503738313913\n",
      "[step: 2018] loss: 46.868499755859375\n",
      "[step: 2018] loss: 0.006282450631260872\n",
      "[step: 2019] loss: 37.126380920410156\n",
      "[step: 2019] loss: 0.006258246023207903\n",
      "[step: 2020] loss: 33.50932312011719\n",
      "[step: 2020] loss: 0.006287099793553352\n",
      "[step: 2021] loss: 31.771873474121094\n",
      "[step: 2021] loss: 0.006326917093247175\n",
      "[step: 2022] loss: 34.65335464477539\n",
      "[step: 2022] loss: 0.006332411430776119\n",
      "[step: 2023] loss: 37.342018127441406\n",
      "[step: 2023] loss: 0.006304222159087658\n",
      "[step: 2024] loss: 36.615478515625\n",
      "[step: 2024] loss: 0.006269369274377823\n",
      "[step: 2025] loss: 33.78144454956055\n",
      "[step: 2025] loss: 0.00625714473426342\n",
      "[step: 2026] loss: 33.41889190673828\n",
      "[step: 2026] loss: 0.006269895937293768\n",
      "[step: 2027] loss: 33.76287078857422\n",
      "[step: 2027] loss: 0.00628912216052413\n",
      "[step: 2028] loss: 30.879358291625977\n",
      "[step: 2028] loss: 0.006297302432358265\n",
      "[step: 2029] loss: 31.587963104248047\n",
      "[step: 2029] loss: 0.006287828553467989\n",
      "[step: 2030] loss: 34.07371520996094\n",
      "[step: 2030] loss: 0.006268625147640705\n",
      "[step: 2031] loss: 33.41218566894531\n",
      "[step: 2031] loss: 0.006254249718040228\n",
      "[step: 2032] loss: 32.02061080932617\n",
      "[step: 2032] loss: 0.006254173815250397\n",
      "[step: 2033] loss: 30.243608474731445\n",
      "[step: 2033] loss: 0.006264982745051384\n",
      "[step: 2034] loss: 29.335994720458984\n",
      "[step: 2034] loss: 0.006274799350649118\n",
      "[step: 2035] loss: 31.071395874023438\n",
      "[step: 2035] loss: 0.006274738349020481\n",
      "[step: 2036] loss: 30.788375854492188\n",
      "[step: 2036] loss: 0.006265702657401562\n",
      "[step: 2037] loss: 29.743257522583008\n",
      "[step: 2037] loss: 0.006255327723920345\n",
      "[step: 2038] loss: 29.93341064453125\n",
      "[step: 2038] loss: 0.006250235717743635\n",
      "[step: 2039] loss: 30.48392677307129\n",
      "[step: 2039] loss: 0.006251744460314512\n",
      "[step: 2040] loss: 30.176532745361328\n",
      "[step: 2040] loss: 0.006256676744669676\n",
      "[step: 2041] loss: 29.413341522216797\n",
      "[step: 2041] loss: 0.006261156406253576\n",
      "[step: 2042] loss: 29.032577514648438\n",
      "[step: 2042] loss: 0.006262531504034996\n",
      "[step: 2043] loss: 28.691007614135742\n",
      "[step: 2043] loss: 0.006259674672037363\n",
      "[step: 2044] loss: 29.268138885498047\n",
      "[step: 2044] loss: 0.006254345178604126\n",
      "[step: 2045] loss: 29.494773864746094\n",
      "[step: 2045] loss: 0.006249306723475456\n",
      "[step: 2046] loss: 28.870540618896484\n",
      "[step: 2046] loss: 0.006246799137443304\n",
      "[step: 2047] loss: 28.647428512573242\n",
      "[step: 2047] loss: 0.006247023586183786\n",
      "[step: 2048] loss: 28.706851959228516\n",
      "[step: 2048] loss: 0.006248857825994492\n",
      "[step: 2049] loss: 29.290842056274414\n",
      "[step: 2049] loss: 0.006250882521271706\n",
      "[step: 2050] loss: 29.974605560302734\n",
      "[step: 2050] loss: 0.006252073682844639\n",
      "[step: 2051] loss: 30.52117156982422\n",
      "[step: 2051] loss: 0.0062521095387637615\n",
      "[step: 2052] loss: 32.9169921875\n",
      "[step: 2052] loss: 0.006250947248190641\n",
      "[step: 2053] loss: 34.830162048339844\n",
      "[step: 2053] loss: 0.006248947698622942\n",
      "[step: 2054] loss: 38.472991943359375\n",
      "[step: 2054] loss: 0.006246696691960096\n",
      "[step: 2055] loss: 40.95618438720703\n",
      "[step: 2055] loss: 0.006244815420359373\n",
      "[step: 2056] loss: 41.13573455810547\n",
      "[step: 2056] loss: 0.006243587005883455\n",
      "[step: 2057] loss: 36.014686584472656\n",
      "[step: 2057] loss: 0.0062430244870483875\n",
      "[step: 2058] loss: 31.687763214111328\n",
      "[step: 2058] loss: 0.006242936011403799\n",
      "[step: 2059] loss: 29.117778778076172\n",
      "[step: 2059] loss: 0.006243119016289711\n",
      "[step: 2060] loss: 29.64599609375\n",
      "[step: 2060] loss: 0.006243513431400061\n",
      "[step: 2061] loss: 32.345680236816406\n",
      "[step: 2061] loss: 0.006244019139558077\n",
      "[step: 2062] loss: 33.82560729980469\n",
      "[step: 2062] loss: 0.006244579795747995\n",
      "[step: 2063] loss: 33.02684783935547\n",
      "[step: 2063] loss: 0.00624513253569603\n",
      "[step: 2064] loss: 31.424972534179688\n",
      "[step: 2064] loss: 0.006245723459869623\n",
      "[step: 2065] loss: 30.034101486206055\n",
      "[step: 2065] loss: 0.006246384233236313\n",
      "[step: 2066] loss: 28.928306579589844\n",
      "[step: 2066] loss: 0.006247357465326786\n",
      "[step: 2067] loss: 29.20929718017578\n",
      "[step: 2067] loss: 0.006248702295124531\n",
      "[step: 2068] loss: 30.688753128051758\n",
      "[step: 2068] loss: 0.006250669714063406\n",
      "[step: 2069] loss: 31.44900131225586\n",
      "[step: 2069] loss: 0.0062534622848033905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2070] loss: 32.00143814086914\n",
      "[step: 2070] loss: 0.006257610395550728\n",
      "[step: 2071] loss: 31.263195037841797\n",
      "[step: 2071] loss: 0.006263524293899536\n",
      "[step: 2072] loss: 29.84617805480957\n",
      "[step: 2072] loss: 0.006272406782954931\n",
      "[step: 2073] loss: 28.53612518310547\n",
      "[step: 2073] loss: 0.006284758448600769\n",
      "[step: 2074] loss: 28.620216369628906\n",
      "[step: 2074] loss: 0.006302764639258385\n",
      "[step: 2075] loss: 29.073143005371094\n",
      "[step: 2075] loss: 0.006325575057417154\n",
      "[step: 2076] loss: 29.049163818359375\n",
      "[step: 2076] loss: 0.006355396471917629\n",
      "[step: 2077] loss: 29.42579460144043\n",
      "[step: 2077] loss: 0.006384397856891155\n",
      "[step: 2078] loss: 29.750255584716797\n",
      "[step: 2078] loss: 0.00641010794788599\n",
      "[step: 2079] loss: 30.15924072265625\n",
      "[step: 2079] loss: 0.00641181506216526\n",
      "[step: 2080] loss: 30.155532836914062\n",
      "[step: 2080] loss: 0.0063881645910441875\n",
      "[step: 2081] loss: 29.668167114257812\n",
      "[step: 2081] loss: 0.006334650795906782\n",
      "[step: 2082] loss: 29.04861831665039\n",
      "[step: 2082] loss: 0.006279307417571545\n",
      "[step: 2083] loss: 28.82549285888672\n",
      "[step: 2083] loss: 0.006245722994208336\n",
      "[step: 2084] loss: 28.747058868408203\n",
      "[step: 2084] loss: 0.006245547439903021\n",
      "[step: 2085] loss: 28.8360538482666\n",
      "[step: 2085] loss: 0.00626834062859416\n",
      "[step: 2086] loss: 28.483341217041016\n",
      "[step: 2086] loss: 0.00629360880702734\n",
      "[step: 2087] loss: 28.145893096923828\n",
      "[step: 2087] loss: 0.006306041963398457\n",
      "[step: 2088] loss: 27.9132022857666\n",
      "[step: 2088] loss: 0.006297323852777481\n",
      "[step: 2089] loss: 27.870586395263672\n",
      "[step: 2089] loss: 0.0062756347469985485\n",
      "[step: 2090] loss: 27.99597930908203\n",
      "[step: 2090] loss: 0.006251620128750801\n",
      "[step: 2091] loss: 28.087657928466797\n",
      "[step: 2091] loss: 0.00623732153326273\n",
      "[step: 2092] loss: 27.985612869262695\n",
      "[step: 2092] loss: 0.006237176712602377\n",
      "[step: 2093] loss: 27.824081420898438\n",
      "[step: 2093] loss: 0.006247594952583313\n",
      "[step: 2094] loss: 27.773548126220703\n",
      "[step: 2094] loss: 0.006260817404836416\n",
      "[step: 2095] loss: 28.005170822143555\n",
      "[step: 2095] loss: 0.006268781144171953\n",
      "[step: 2096] loss: 28.626506805419922\n",
      "[step: 2096] loss: 0.006268668454140425\n",
      "[step: 2097] loss: 30.391429901123047\n",
      "[step: 2097] loss: 0.006260212976485491\n",
      "[step: 2098] loss: 33.01774215698242\n",
      "[step: 2098] loss: 0.006249002646654844\n",
      "[step: 2099] loss: 39.86914825439453\n",
      "[step: 2099] loss: 0.006239217706024647\n",
      "[step: 2100] loss: 42.987586975097656\n",
      "[step: 2100] loss: 0.0062339515425264835\n",
      "[step: 2101] loss: 50.41419982910156\n",
      "[step: 2101] loss: 0.006233294028788805\n",
      "[step: 2102] loss: 51.91661834716797\n",
      "[step: 2102] loss: 0.006235543638467789\n",
      "[step: 2103] loss: 46.67162322998047\n",
      "[step: 2103] loss: 0.006238589063286781\n",
      "[step: 2104] loss: 38.30960464477539\n",
      "[step: 2104] loss: 0.006241194438189268\n",
      "[step: 2105] loss: 32.20029830932617\n",
      "[step: 2105] loss: 0.006243058945983648\n",
      "[step: 2106] loss: 33.25528335571289\n",
      "[step: 2106] loss: 0.006244118325412273\n",
      "[step: 2107] loss: 36.246063232421875\n",
      "[step: 2107] loss: 0.006244542542845011\n",
      "[step: 2108] loss: 36.510196685791016\n",
      "[step: 2108] loss: 0.006244008429348469\n",
      "[step: 2109] loss: 34.38494110107422\n",
      "[step: 2109] loss: 0.006242373492568731\n",
      "[step: 2110] loss: 31.89132308959961\n",
      "[step: 2110] loss: 0.006239613518118858\n",
      "[step: 2111] loss: 33.1193733215332\n",
      "[step: 2111] loss: 0.00623633386567235\n",
      "[step: 2112] loss: 32.06690979003906\n",
      "[step: 2112] loss: 0.006233393680304289\n",
      "[step: 2113] loss: 30.18060874938965\n",
      "[step: 2113] loss: 0.006231257691979408\n",
      "[step: 2114] loss: 31.406949996948242\n",
      "[step: 2114] loss: 0.006229955703020096\n",
      "[step: 2115] loss: 33.08199691772461\n",
      "[step: 2115] loss: 0.006229223217815161\n",
      "[step: 2116] loss: 31.067644119262695\n",
      "[step: 2116] loss: 0.006228625774383545\n",
      "[step: 2117] loss: 29.368513107299805\n",
      "[step: 2117] loss: 0.006227872800081968\n",
      "[step: 2118] loss: 28.804697036743164\n",
      "[step: 2118] loss: 0.006226982455700636\n",
      "[step: 2119] loss: 29.767208099365234\n",
      "[step: 2119] loss: 0.006226179655641317\n",
      "[step: 2120] loss: 29.754335403442383\n",
      "[step: 2120] loss: 0.006225635297596455\n",
      "[step: 2121] loss: 29.434545516967773\n",
      "[step: 2121] loss: 0.006225366145372391\n",
      "[step: 2122] loss: 28.777923583984375\n",
      "[step: 2122] loss: 0.006225306540727615\n",
      "[step: 2123] loss: 29.1920166015625\n",
      "[step: 2123] loss: 0.006225372664630413\n",
      "[step: 2124] loss: 29.321258544921875\n",
      "[step: 2124] loss: 0.006225480232387781\n",
      "[step: 2125] loss: 28.295644760131836\n",
      "[step: 2125] loss: 0.006225624121725559\n",
      "[step: 2126] loss: 28.050334930419922\n",
      "[step: 2126] loss: 0.006225865334272385\n",
      "[step: 2127] loss: 28.123626708984375\n",
      "[step: 2127] loss: 0.006226411089301109\n",
      "[step: 2128] loss: 28.03989028930664\n",
      "[step: 2128] loss: 0.00622763903811574\n",
      "[step: 2129] loss: 27.860116958618164\n",
      "[step: 2129] loss: 0.0062301261350512505\n",
      "[step: 2130] loss: 28.06134796142578\n",
      "[step: 2130] loss: 0.006235021166503429\n",
      "[step: 2131] loss: 27.885156631469727\n",
      "[step: 2131] loss: 0.006244131363928318\n",
      "[step: 2132] loss: 27.520658493041992\n",
      "[step: 2132] loss: 0.006261375732719898\n",
      "[step: 2133] loss: 27.70633888244629\n",
      "[step: 2133] loss: 0.006292366422712803\n",
      "[step: 2134] loss: 28.096939086914062\n",
      "[step: 2134] loss: 0.006348819937556982\n",
      "[step: 2135] loss: 28.724184036254883\n",
      "[step: 2135] loss: 0.006440044846385717\n",
      "[step: 2136] loss: 29.548845291137695\n",
      "[step: 2136] loss: 0.00657938327640295\n",
      "[step: 2137] loss: 33.199703216552734\n",
      "[step: 2137] loss: 0.006718532182276249\n",
      "[step: 2138] loss: 36.883888244628906\n",
      "[step: 2138] loss: 0.006785972975194454\n",
      "[step: 2139] loss: 41.55999755859375\n",
      "[step: 2139] loss: 0.006643995642662048\n",
      "[step: 2140] loss: 42.590904235839844\n",
      "[step: 2140] loss: 0.0064156982116401196\n",
      "[step: 2141] loss: 43.604408264160156\n",
      "[step: 2141] loss: 0.006322158500552177\n",
      "[step: 2142] loss: 36.55370330810547\n",
      "[step: 2142] loss: 0.006389597896486521\n",
      "[step: 2143] loss: 31.697681427001953\n",
      "[step: 2143] loss: 0.006428244523704052\n",
      "[step: 2144] loss: 29.77294158935547\n",
      "[step: 2144] loss: 0.006410724949091673\n",
      "[step: 2145] loss: 32.867637634277344\n",
      "[step: 2145] loss: 0.006341980770230293\n",
      "[step: 2146] loss: 36.40985107421875\n",
      "[step: 2146] loss: 0.006287644151598215\n",
      "[step: 2147] loss: 36.270774841308594\n",
      "[step: 2147] loss: 0.006337367929518223\n",
      "[step: 2148] loss: 32.83277130126953\n",
      "[step: 2148] loss: 0.006361703854054213\n",
      "[step: 2149] loss: 30.36932373046875\n",
      "[step: 2149] loss: 0.006323275156319141\n",
      "[step: 2150] loss: 30.860380172729492\n",
      "[step: 2150] loss: 0.00627446873113513\n",
      "[step: 2151] loss: 30.499568939208984\n",
      "[step: 2151] loss: 0.006246519740670919\n",
      "[step: 2152] loss: 30.901500701904297\n",
      "[step: 2152] loss: 0.006308726500719786\n",
      "[step: 2153] loss: 32.74998092651367\n",
      "[step: 2153] loss: 0.0063211205415427685\n",
      "[step: 2154] loss: 31.426959991455078\n",
      "[step: 2154] loss: 0.006254971027374268\n",
      "[step: 2155] loss: 28.599050521850586\n",
      "[step: 2155] loss: 0.006245470605790615\n",
      "[step: 2156] loss: 28.344829559326172\n",
      "[step: 2156] loss: 0.006256784312427044\n",
      "[step: 2157] loss: 28.57574462890625\n",
      "[step: 2157] loss: 0.0062621585093438625\n",
      "[step: 2158] loss: 29.858936309814453\n",
      "[step: 2158] loss: 0.006268360652029514\n",
      "[step: 2159] loss: 29.713973999023438\n",
      "[step: 2159] loss: 0.006254234816879034\n",
      "[step: 2160] loss: 29.14185333251953\n",
      "[step: 2160] loss: 0.006236195098608732\n",
      "[step: 2161] loss: 28.70559310913086\n",
      "[step: 2161] loss: 0.006229923572391272\n",
      "[step: 2162] loss: 28.456008911132812\n",
      "[step: 2162] loss: 0.006246228702366352\n",
      "[step: 2163] loss: 27.915067672729492\n",
      "[step: 2163] loss: 0.006253235042095184\n",
      "[step: 2164] loss: 27.382349014282227\n",
      "[step: 2164] loss: 0.006233756430447102\n",
      "[step: 2165] loss: 27.958019256591797\n",
      "[step: 2165] loss: 0.006228028330951929\n",
      "[step: 2166] loss: 28.467113494873047\n",
      "[step: 2166] loss: 0.0062286402098834515\n",
      "[step: 2167] loss: 28.790447235107422\n",
      "[step: 2167] loss: 0.006227996665984392\n",
      "[step: 2168] loss: 28.688724517822266\n",
      "[step: 2168] loss: 0.006235890556126833\n",
      "[step: 2169] loss: 29.31624984741211\n",
      "[step: 2169] loss: 0.006233507301658392\n",
      "[step: 2170] loss: 29.55785369873047\n",
      "[step: 2170] loss: 0.0062233759090304375\n",
      "[step: 2171] loss: 30.153820037841797\n",
      "[step: 2171] loss: 0.006219780538231134\n",
      "[step: 2172] loss: 29.847694396972656\n",
      "[step: 2172] loss: 0.006221701856702566\n",
      "[step: 2173] loss: 29.250965118408203\n",
      "[step: 2173] loss: 0.006225400138646364\n",
      "[step: 2174] loss: 28.34911346435547\n",
      "[step: 2174] loss: 0.0062246439047157764\n",
      "[step: 2175] loss: 28.063615798950195\n",
      "[step: 2175] loss: 0.006223078351467848\n",
      "[step: 2176] loss: 27.900596618652344\n",
      "[step: 2176] loss: 0.0062202042900025845\n",
      "[step: 2177] loss: 27.363489151000977\n",
      "[step: 2177] loss: 0.006215297617018223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2178] loss: 27.122798919677734\n",
      "[step: 2178] loss: 0.006216908805072308\n",
      "[step: 2179] loss: 27.384790420532227\n",
      "[step: 2179] loss: 0.006219706032425165\n",
      "[step: 2180] loss: 27.955854415893555\n",
      "[step: 2180] loss: 0.00621861731633544\n",
      "[step: 2181] loss: 28.294204711914062\n",
      "[step: 2181] loss: 0.006218128837645054\n",
      "[step: 2182] loss: 28.37559700012207\n",
      "[step: 2182] loss: 0.00621638773009181\n",
      "[step: 2183] loss: 28.537384033203125\n",
      "[step: 2183] loss: 0.0062138536013662815\n",
      "[step: 2184] loss: 29.23651123046875\n",
      "[step: 2184] loss: 0.0062131257727742195\n",
      "[step: 2185] loss: 29.88553237915039\n",
      "[step: 2185] loss: 0.006213658954948187\n",
      "[step: 2186] loss: 31.19499969482422\n",
      "[step: 2186] loss: 0.0062146116979420185\n",
      "[step: 2187] loss: 30.31322479248047\n",
      "[step: 2187] loss: 0.006214483175426722\n",
      "[step: 2188] loss: 29.71237564086914\n",
      "[step: 2188] loss: 0.006213563494384289\n",
      "[step: 2189] loss: 29.602203369140625\n",
      "[step: 2189] loss: 0.006212996784597635\n",
      "[step: 2190] loss: 29.355056762695312\n",
      "[step: 2190] loss: 0.00621163472533226\n",
      "[step: 2191] loss: 28.327945709228516\n",
      "[step: 2191] loss: 0.006210335064679384\n",
      "[step: 2192] loss: 27.273170471191406\n",
      "[step: 2192] loss: 0.0062104579992592335\n",
      "[step: 2193] loss: 26.985836029052734\n",
      "[step: 2193] loss: 0.006210561376065016\n",
      "[step: 2194] loss: 27.366657257080078\n",
      "[step: 2194] loss: 0.006210301071405411\n",
      "[step: 2195] loss: 27.659793853759766\n",
      "[step: 2195] loss: 0.006210478488355875\n",
      "[step: 2196] loss: 27.643333435058594\n",
      "[step: 2196] loss: 0.006210405845195055\n",
      "[step: 2197] loss: 27.455738067626953\n",
      "[step: 2197] loss: 0.0062098633497953415\n",
      "[step: 2198] loss: 27.565052032470703\n",
      "[step: 2198] loss: 0.006209214683622122\n",
      "[step: 2199] loss: 28.503093719482422\n",
      "[step: 2199] loss: 0.006208573468029499\n",
      "[step: 2200] loss: 29.422138214111328\n",
      "[step: 2200] loss: 0.006208078470081091\n",
      "[step: 2201] loss: 31.0081844329834\n",
      "[step: 2201] loss: 0.006207561586052179\n",
      "[step: 2202] loss: 31.701499938964844\n",
      "[step: 2202] loss: 0.006207031663507223\n",
      "[step: 2203] loss: 33.017364501953125\n",
      "[step: 2203] loss: 0.006206841673702002\n",
      "[step: 2204] loss: 34.501121520996094\n",
      "[step: 2204] loss: 0.006206723861396313\n",
      "[step: 2205] loss: 35.797019958496094\n",
      "[step: 2205] loss: 0.006206427700817585\n",
      "[step: 2206] loss: 34.26066589355469\n",
      "[step: 2206] loss: 0.006206246558576822\n",
      "[step: 2207] loss: 31.527210235595703\n",
      "[step: 2207] loss: 0.006206185556948185\n",
      "[step: 2208] loss: 28.609264373779297\n",
      "[step: 2208] loss: 0.006206025369465351\n",
      "[step: 2209] loss: 27.71457862854004\n",
      "[step: 2209] loss: 0.0062058353796601295\n",
      "[step: 2210] loss: 27.97469711303711\n",
      "[step: 2210] loss: 0.006205678451806307\n",
      "[step: 2211] loss: 28.29239273071289\n",
      "[step: 2211] loss: 0.006205564830452204\n",
      "[step: 2212] loss: 28.52931022644043\n",
      "[step: 2212] loss: 0.006205474492162466\n",
      "[step: 2213] loss: 29.032102584838867\n",
      "[step: 2213] loss: 0.006205399986356497\n",
      "[step: 2214] loss: 29.896516799926758\n",
      "[step: 2214] loss: 0.006205412559211254\n",
      "[step: 2215] loss: 29.96172523498535\n",
      "[step: 2215] loss: 0.006205601152032614\n",
      "[step: 2216] loss: 29.045757293701172\n",
      "[step: 2216] loss: 0.006205981131643057\n",
      "[step: 2217] loss: 27.562753677368164\n",
      "[step: 2217] loss: 0.006206647492945194\n",
      "[step: 2218] loss: 26.998130798339844\n",
      "[step: 2218] loss: 0.0062077846378088\n",
      "[step: 2219] loss: 27.245868682861328\n",
      "[step: 2219] loss: 0.006209769751876593\n",
      "[step: 2220] loss: 27.39927101135254\n",
      "[step: 2220] loss: 0.006212959531694651\n",
      "[step: 2221] loss: 27.292499542236328\n",
      "[step: 2221] loss: 0.006218218710273504\n",
      "[step: 2222] loss: 27.394323348999023\n",
      "[step: 2222] loss: 0.0062265521846711636\n",
      "[step: 2223] loss: 28.20296287536621\n",
      "[step: 2223] loss: 0.0062404111959040165\n",
      "[step: 2224] loss: 29.05672836303711\n",
      "[step: 2224] loss: 0.006261535454541445\n",
      "[step: 2225] loss: 29.997087478637695\n",
      "[step: 2225] loss: 0.006295440252870321\n",
      "[step: 2226] loss: 30.4790096282959\n",
      "[step: 2226] loss: 0.006341573316603899\n",
      "[step: 2227] loss: 31.201236724853516\n",
      "[step: 2227] loss: 0.006405359599739313\n",
      "[step: 2228] loss: 31.770000457763672\n",
      "[step: 2228] loss: 0.006462730932980776\n",
      "[step: 2229] loss: 32.07219696044922\n",
      "[step: 2229] loss: 0.006498798727989197\n",
      "[step: 2230] loss: 31.093420028686523\n",
      "[step: 2230] loss: 0.006457215175032616\n",
      "[step: 2231] loss: 29.5477294921875\n",
      "[step: 2231] loss: 0.006354902870953083\n",
      "[step: 2232] loss: 27.735395431518555\n",
      "[step: 2232] loss: 0.006244132295250893\n",
      "[step: 2233] loss: 26.8362979888916\n",
      "[step: 2233] loss: 0.006207616999745369\n",
      "[step: 2234] loss: 26.94623565673828\n",
      "[step: 2234] loss: 0.006252893712371588\n",
      "[step: 2235] loss: 27.36050033569336\n",
      "[step: 2235] loss: 0.006314417347311974\n",
      "[step: 2236] loss: 27.83718490600586\n",
      "[step: 2236] loss: 0.006327415816485882\n",
      "[step: 2237] loss: 28.22786521911621\n",
      "[step: 2237] loss: 0.006275981664657593\n",
      "[step: 2238] loss: 28.7164363861084\n",
      "[step: 2238] loss: 0.0062177772633731365\n",
      "[step: 2239] loss: 28.782093048095703\n",
      "[step: 2239] loss: 0.00620398810133338\n",
      "[step: 2240] loss: 28.32691192626953\n",
      "[step: 2240] loss: 0.006235048174858093\n",
      "[step: 2241] loss: 27.494251251220703\n",
      "[step: 2241] loss: 0.006270711310207844\n",
      "[step: 2242] loss: 26.906177520751953\n",
      "[step: 2242] loss: 0.006273599341511726\n",
      "[step: 2243] loss: 26.79058074951172\n",
      "[step: 2243] loss: 0.006245841737836599\n",
      "[step: 2244] loss: 27.096376419067383\n",
      "[step: 2244] loss: 0.006212006323039532\n",
      "[step: 2245] loss: 27.19194221496582\n",
      "[step: 2245] loss: 0.006200319621711969\n",
      "[step: 2246] loss: 27.316547393798828\n",
      "[step: 2246] loss: 0.006213327869772911\n",
      "[step: 2247] loss: 27.000886917114258\n",
      "[step: 2247] loss: 0.006233092863112688\n",
      "[step: 2248] loss: 26.94369125366211\n",
      "[step: 2248] loss: 0.006241882219910622\n",
      "[step: 2249] loss: 27.136999130249023\n",
      "[step: 2249] loss: 0.006232257001101971\n",
      "[step: 2250] loss: 27.813800811767578\n",
      "[step: 2250] loss: 0.006213806103914976\n",
      "[step: 2251] loss: 28.81265640258789\n",
      "[step: 2251] loss: 0.006200017873197794\n",
      "[step: 2252] loss: 30.28414535522461\n",
      "[step: 2252] loss: 0.006199150811880827\n",
      "[step: 2253] loss: 32.10865783691406\n",
      "[step: 2253] loss: 0.0062082079239189625\n",
      "[step: 2254] loss: 34.955116271972656\n",
      "[step: 2254] loss: 0.006217543967068195\n",
      "[step: 2255] loss: 36.8718376159668\n",
      "[step: 2255] loss: 0.0062196338549256325\n",
      "[step: 2256] loss: 37.601890563964844\n",
      "[step: 2256] loss: 0.00621282123029232\n",
      "[step: 2257] loss: 34.869224548339844\n",
      "[step: 2257] loss: 0.006202707998454571\n",
      "[step: 2258] loss: 30.887409210205078\n",
      "[step: 2258] loss: 0.006195937283337116\n",
      "[step: 2259] loss: 28.108844757080078\n",
      "[step: 2259] loss: 0.006195699330419302\n",
      "[step: 2260] loss: 27.463504791259766\n",
      "[step: 2260] loss: 0.006200169678777456\n",
      "[step: 2261] loss: 27.955677032470703\n",
      "[step: 2261] loss: 0.0062051150016486645\n",
      "[step: 2262] loss: 28.941272735595703\n",
      "[step: 2262] loss: 0.006207306403666735\n",
      "[step: 2263] loss: 30.871658325195312\n",
      "[step: 2263] loss: 0.006205524783581495\n",
      "[step: 2264] loss: 32.958900451660156\n",
      "[step: 2264] loss: 0.006201297044754028\n",
      "[step: 2265] loss: 31.905990600585938\n",
      "[step: 2265] loss: 0.006196716334670782\n",
      "[step: 2266] loss: 29.35211181640625\n",
      "[step: 2266] loss: 0.0061937193386256695\n",
      "[step: 2267] loss: 27.84476661682129\n",
      "[step: 2267] loss: 0.006192874163389206\n",
      "[step: 2268] loss: 27.717496871948242\n",
      "[step: 2268] loss: 0.006193823181092739\n",
      "[step: 2269] loss: 27.49626922607422\n",
      "[step: 2269] loss: 0.006195657886564732\n",
      "[step: 2270] loss: 27.130176544189453\n",
      "[step: 2270] loss: 0.006197267211973667\n",
      "[step: 2271] loss: 28.141944885253906\n",
      "[step: 2271] loss: 0.0061979880556464195\n",
      "[step: 2272] loss: 29.956634521484375\n",
      "[step: 2272] loss: 0.006197507493197918\n",
      "[step: 2273] loss: 30.631946563720703\n",
      "[step: 2273] loss: 0.006196146830916405\n",
      "[step: 2274] loss: 32.367958068847656\n",
      "[step: 2274] loss: 0.006194243207573891\n",
      "[step: 2275] loss: 31.118043899536133\n",
      "[step: 2275] loss: 0.006192388478666544\n",
      "[step: 2276] loss: 29.332881927490234\n",
      "[step: 2276] loss: 0.006190930027514696\n",
      "[step: 2277] loss: 27.896587371826172\n",
      "[step: 2277] loss: 0.0061899940483272076\n",
      "[step: 2278] loss: 27.821327209472656\n",
      "[step: 2278] loss: 0.006189567968249321\n",
      "[step: 2279] loss: 27.917213439941406\n",
      "[step: 2279] loss: 0.0061895474791526794\n",
      "[step: 2280] loss: 27.099836349487305\n",
      "[step: 2280] loss: 0.006189735606312752\n",
      "[step: 2281] loss: 27.380733489990234\n",
      "[step: 2281] loss: 0.006190064828842878\n",
      "[step: 2282] loss: 28.09825897216797\n",
      "[step: 2282] loss: 0.006190476007759571\n",
      "[step: 2283] loss: 28.085264205932617\n",
      "[step: 2283] loss: 0.006190918851643801\n",
      "[step: 2284] loss: 27.292875289916992\n",
      "[step: 2284] loss: 0.006191463675349951\n",
      "[step: 2285] loss: 26.93939971923828\n",
      "[step: 2285] loss: 0.006192111410200596\n",
      "[step: 2286] loss: 26.893590927124023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2286] loss: 0.006192968692630529\n",
      "[step: 2287] loss: 26.63844871520996\n",
      "[step: 2287] loss: 0.006194085814058781\n",
      "[step: 2288] loss: 26.155853271484375\n",
      "[step: 2288] loss: 0.006195712368935347\n",
      "[step: 2289] loss: 26.138893127441406\n",
      "[step: 2289] loss: 0.0061979396268725395\n",
      "[step: 2290] loss: 26.478649139404297\n",
      "[step: 2290] loss: 0.00620131054893136\n",
      "[step: 2291] loss: 26.69480323791504\n",
      "[step: 2291] loss: 0.006206047721207142\n",
      "[step: 2292] loss: 26.790830612182617\n",
      "[step: 2292] loss: 0.006213324144482613\n",
      "[step: 2293] loss: 26.991073608398438\n",
      "[step: 2293] loss: 0.006223409902304411\n",
      "[step: 2294] loss: 28.024246215820312\n",
      "[step: 2294] loss: 0.0062386454083025455\n",
      "[step: 2295] loss: 30.10245132446289\n",
      "[step: 2295] loss: 0.00625832611694932\n",
      "[step: 2296] loss: 35.30232620239258\n",
      "[step: 2296] loss: 0.0062860348261892796\n",
      "[step: 2297] loss: 37.14820861816406\n",
      "[step: 2297] loss: 0.0063155703246593475\n",
      "[step: 2298] loss: 36.569923400878906\n",
      "[step: 2298] loss: 0.00634874077513814\n",
      "[step: 2299] loss: 36.001380920410156\n",
      "[step: 2299] loss: 0.006364946253597736\n",
      "[step: 2300] loss: 37.164520263671875\n",
      "[step: 2300] loss: 0.006362984888255596\n",
      "[step: 2301] loss: 35.11012268066406\n",
      "[step: 2301] loss: 0.006323853973299265\n",
      "[step: 2302] loss: 32.19150924682617\n",
      "[step: 2302] loss: 0.0062688300386071205\n",
      "[step: 2303] loss: 29.37023162841797\n",
      "[step: 2303] loss: 0.006218582391738892\n",
      "[step: 2304] loss: 28.86815071105957\n",
      "[step: 2304] loss: 0.0061986506916582584\n",
      "[step: 2305] loss: 29.034984588623047\n",
      "[step: 2305] loss: 0.0062097166664898396\n",
      "[step: 2306] loss: 29.996700286865234\n",
      "[step: 2306] loss: 0.006235246546566486\n",
      "[step: 2307] loss: 30.99209976196289\n",
      "[step: 2307] loss: 0.006256368942558765\n",
      "[step: 2308] loss: 28.516799926757812\n",
      "[step: 2308] loss: 0.006257578264921904\n",
      "[step: 2309] loss: 27.323026657104492\n",
      "[step: 2309] loss: 0.0062393988482654095\n",
      "[step: 2310] loss: 29.13718605041504\n",
      "[step: 2310] loss: 0.006211616564542055\n",
      "[step: 2311] loss: 29.480545043945312\n",
      "[step: 2311] loss: 0.006191769149154425\n",
      "[step: 2312] loss: 27.48853302001953\n",
      "[step: 2312] loss: 0.006187092047184706\n",
      "[step: 2313] loss: 27.07834815979004\n",
      "[step: 2313] loss: 0.006195291876792908\n",
      "[step: 2314] loss: 27.464956283569336\n",
      "[step: 2314] loss: 0.006209602579474449\n",
      "[step: 2315] loss: 27.52635955810547\n",
      "[step: 2315] loss: 0.006220877170562744\n",
      "[step: 2316] loss: 27.195091247558594\n",
      "[step: 2316] loss: 0.0062231156043708324\n",
      "[step: 2317] loss: 27.094205856323242\n",
      "[step: 2317] loss: 0.006214093416929245\n",
      "[step: 2318] loss: 26.824783325195312\n",
      "[step: 2318] loss: 0.006200920790433884\n",
      "[step: 2319] loss: 26.58504867553711\n",
      "[step: 2319] loss: 0.006188882049173117\n",
      "[step: 2320] loss: 27.000268936157227\n",
      "[step: 2320] loss: 0.006182277109473944\n",
      "[step: 2321] loss: 28.040409088134766\n",
      "[step: 2321] loss: 0.006182343699038029\n",
      "[step: 2322] loss: 27.68471908569336\n",
      "[step: 2322] loss: 0.006187316495925188\n",
      "[step: 2323] loss: 28.35028076171875\n",
      "[step: 2323] loss: 0.006193267181515694\n",
      "[step: 2324] loss: 29.289020538330078\n",
      "[step: 2324] loss: 0.006196938455104828\n",
      "[step: 2325] loss: 30.364620208740234\n",
      "[step: 2325] loss: 0.006198009941726923\n",
      "[step: 2326] loss: 30.985023498535156\n",
      "[step: 2326] loss: 0.006196481175720692\n",
      "[step: 2327] loss: 32.24755859375\n",
      "[step: 2327] loss: 0.006193360313773155\n",
      "[step: 2328] loss: 32.7912712097168\n",
      "[step: 2328] loss: 0.006189259700477123\n",
      "[step: 2329] loss: 31.751239776611328\n",
      "[step: 2329] loss: 0.006185510661453009\n",
      "[step: 2330] loss: 30.289798736572266\n",
      "[step: 2330] loss: 0.0061825718730688095\n",
      "[step: 2331] loss: 29.150672912597656\n",
      "[step: 2331] loss: 0.006180332973599434\n",
      "[step: 2332] loss: 27.27847671508789\n",
      "[step: 2332] loss: 0.006178947631269693\n",
      "[step: 2333] loss: 26.213706970214844\n",
      "[step: 2333] loss: 0.006178608629852533\n",
      "[step: 2334] loss: 26.18103790283203\n",
      "[step: 2334] loss: 0.006179137621074915\n",
      "[step: 2335] loss: 26.913619995117188\n",
      "[step: 2335] loss: 0.006180171854794025\n",
      "[step: 2336] loss: 27.803096771240234\n",
      "[step: 2336] loss: 0.006181505974382162\n",
      "[step: 2337] loss: 28.421045303344727\n",
      "[step: 2337] loss: 0.006183044519275427\n",
      "[step: 2338] loss: 28.92869758605957\n",
      "[step: 2338] loss: 0.006184658035635948\n",
      "[step: 2339] loss: 28.177654266357422\n",
      "[step: 2339] loss: 0.006186138838529587\n",
      "[step: 2340] loss: 27.386531829833984\n",
      "[step: 2340] loss: 0.006187652237713337\n",
      "[step: 2341] loss: 26.86098861694336\n",
      "[step: 2341] loss: 0.006189295090734959\n",
      "[step: 2342] loss: 26.250314712524414\n",
      "[step: 2342] loss: 0.0061913444660604\n",
      "[step: 2343] loss: 25.641807556152344\n",
      "[step: 2343] loss: 0.006193863693624735\n",
      "[step: 2344] loss: 25.609960556030273\n",
      "[step: 2344] loss: 0.006197436712682247\n",
      "[step: 2345] loss: 25.788166046142578\n",
      "[step: 2345] loss: 0.006202028598636389\n",
      "[step: 2346] loss: 25.90935707092285\n",
      "[step: 2346] loss: 0.006208690349012613\n",
      "[step: 2347] loss: 26.27530860900879\n",
      "[step: 2347] loss: 0.006216832436621189\n",
      "[step: 2348] loss: 26.869279861450195\n",
      "[step: 2348] loss: 0.006228248588740826\n",
      "[step: 2349] loss: 28.135812759399414\n",
      "[step: 2349] loss: 0.006240994203835726\n",
      "[step: 2350] loss: 29.589847564697266\n",
      "[step: 2350] loss: 0.006257464177906513\n",
      "[step: 2351] loss: 32.211669921875\n",
      "[step: 2351] loss: 0.00627207662910223\n",
      "[step: 2352] loss: 33.33172607421875\n",
      "[step: 2352] loss: 0.006286915857344866\n",
      "[step: 2353] loss: 34.02121353149414\n",
      "[step: 2353] loss: 0.006291136145591736\n",
      "[step: 2354] loss: 33.36879348754883\n",
      "[step: 2354] loss: 0.006287805270403624\n",
      "[step: 2355] loss: 32.034976959228516\n",
      "[step: 2355] loss: 0.0062679932452738285\n",
      "[step: 2356] loss: 30.307350158691406\n",
      "[step: 2356] loss: 0.0062413085252046585\n",
      "[step: 2357] loss: 27.82117462158203\n",
      "[step: 2357] loss: 0.006211503874510527\n",
      "[step: 2358] loss: 26.090431213378906\n",
      "[step: 2358] loss: 0.006189863197505474\n",
      "[step: 2359] loss: 26.207366943359375\n",
      "[step: 2359] loss: 0.006181069649755955\n",
      "[step: 2360] loss: 27.613069534301758\n",
      "[step: 2360] loss: 0.006184993777424097\n",
      "[step: 2361] loss: 29.102855682373047\n",
      "[step: 2361] loss: 0.006197107490152121\n",
      "[step: 2362] loss: 29.064544677734375\n",
      "[step: 2362] loss: 0.006211037747561932\n",
      "[step: 2363] loss: 28.34383201599121\n",
      "[step: 2363] loss: 0.00622186902910471\n",
      "[step: 2364] loss: 28.604549407958984\n",
      "[step: 2364] loss: 0.006224629003554583\n",
      "[step: 2365] loss: 28.653247833251953\n",
      "[step: 2365] loss: 0.006220381706953049\n",
      "[step: 2366] loss: 28.52195167541504\n",
      "[step: 2366] loss: 0.006208916660398245\n",
      "[step: 2367] loss: 27.247650146484375\n",
      "[step: 2367] loss: 0.006195352412760258\n",
      "[step: 2368] loss: 25.988636016845703\n",
      "[step: 2368] loss: 0.006182542070746422\n",
      "[step: 2369] loss: 25.414291381835938\n",
      "[step: 2369] loss: 0.0061740512028336525\n",
      "[step: 2370] loss: 26.07317352294922\n",
      "[step: 2370] loss: 0.006170876324176788\n",
      "[step: 2371] loss: 26.961650848388672\n",
      "[step: 2371] loss: 0.006172279827296734\n",
      "[step: 2372] loss: 27.040721893310547\n",
      "[step: 2372] loss: 0.0061765313148498535\n",
      "[step: 2373] loss: 26.865055084228516\n",
      "[step: 2373] loss: 0.006181766744703054\n",
      "[step: 2374] loss: 26.256500244140625\n",
      "[step: 2374] loss: 0.006186856888234615\n",
      "[step: 2375] loss: 26.622623443603516\n",
      "[step: 2375] loss: 0.0061905947513878345\n",
      "[step: 2376] loss: 27.268505096435547\n",
      "[step: 2376] loss: 0.006193214096128941\n",
      "[step: 2377] loss: 27.468124389648438\n",
      "[step: 2377] loss: 0.00619408069178462\n",
      "[step: 2378] loss: 27.11847686767578\n",
      "[step: 2378] loss: 0.006194279063493013\n",
      "[step: 2379] loss: 26.51228904724121\n",
      "[step: 2379] loss: 0.0061934939585626125\n",
      "[step: 2380] loss: 26.138607025146484\n",
      "[step: 2380] loss: 0.006192726548761129\n",
      "[step: 2381] loss: 26.56894302368164\n",
      "[step: 2381] loss: 0.006191893480718136\n",
      "[step: 2382] loss: 27.27379608154297\n",
      "[step: 2382] loss: 0.00619166623800993\n",
      "[step: 2383] loss: 28.209857940673828\n",
      "[step: 2383] loss: 0.006192147731781006\n",
      "[step: 2384] loss: 27.888145446777344\n",
      "[step: 2384] loss: 0.006193242501467466\n",
      "[step: 2385] loss: 27.650981903076172\n",
      "[step: 2385] loss: 0.006194815970957279\n",
      "[step: 2386] loss: 27.699745178222656\n",
      "[step: 2386] loss: 0.0061958045698702335\n",
      "[step: 2387] loss: 28.79819679260254\n",
      "[step: 2387] loss: 0.0061956169083714485\n",
      "[step: 2388] loss: 29.627256393432617\n",
      "[step: 2388] loss: 0.006192795000970364\n",
      "[step: 2389] loss: 29.611217498779297\n",
      "[step: 2389] loss: 0.0061875940300524235\n",
      "[step: 2390] loss: 28.62042236328125\n",
      "[step: 2390] loss: 0.006180137395858765\n",
      "[step: 2391] loss: 28.00827407836914\n",
      "[step: 2391] loss: 0.006172878202050924\n",
      "[step: 2392] loss: 28.261003494262695\n",
      "[step: 2392] loss: 0.006167560815811157\n",
      "[step: 2393] loss: 29.143943786621094\n",
      "[step: 2393] loss: 0.006165606901049614\n",
      "[step: 2394] loss: 28.0836238861084\n",
      "[step: 2394] loss: 0.006166737526655197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2395] loss: 26.566858291625977\n",
      "[step: 2395] loss: 0.006169462110847235\n",
      "[step: 2396] loss: 25.849634170532227\n",
      "[step: 2396] loss: 0.006171951070427895\n",
      "[step: 2397] loss: 26.124195098876953\n",
      "[step: 2397] loss: 0.0061726998537778854\n",
      "[step: 2398] loss: 26.524585723876953\n",
      "[step: 2398] loss: 0.006171560380607843\n",
      "[step: 2399] loss: 25.932815551757812\n",
      "[step: 2399] loss: 0.006168995052576065\n",
      "[step: 2400] loss: 25.305498123168945\n",
      "[step: 2400] loss: 0.006166345439851284\n",
      "[step: 2401] loss: 25.198890686035156\n",
      "[step: 2401] loss: 0.006164658814668655\n",
      "[step: 2402] loss: 25.663814544677734\n",
      "[step: 2402] loss: 0.006164472550153732\n",
      "[step: 2403] loss: 26.12006187438965\n",
      "[step: 2403] loss: 0.00616573728621006\n",
      "[step: 2404] loss: 25.954795837402344\n",
      "[step: 2404] loss: 0.006168125197291374\n",
      "[step: 2405] loss: 25.54806900024414\n",
      "[step: 2405] loss: 0.006171566899865866\n",
      "[step: 2406] loss: 25.4837646484375\n",
      "[step: 2406] loss: 0.006176697090268135\n",
      "[step: 2407] loss: 26.08172607421875\n",
      "[step: 2407] loss: 0.006185646168887615\n",
      "[step: 2408] loss: 27.541250228881836\n",
      "[step: 2408] loss: 0.006202730815857649\n",
      "[step: 2409] loss: 29.244653701782227\n",
      "[step: 2409] loss: 0.006237650755792856\n",
      "[step: 2410] loss: 32.80828857421875\n",
      "[step: 2410] loss: 0.006303037516772747\n",
      "[step: 2411] loss: 35.538902282714844\n",
      "[step: 2411] loss: 0.0064300415106117725\n",
      "[step: 2412] loss: 41.704368591308594\n",
      "[step: 2412] loss: 0.006615540478378534\n",
      "[step: 2413] loss: 47.47428894042969\n",
      "[step: 2413] loss: 0.006863777060061693\n",
      "[step: 2414] loss: 48.44691467285156\n",
      "[step: 2414] loss: 0.006931135430932045\n",
      "[step: 2415] loss: 41.39514923095703\n",
      "[step: 2415] loss: 0.006705211475491524\n",
      "[step: 2416] loss: 36.058441162109375\n",
      "[step: 2416] loss: 0.0063141449354588985\n",
      "[step: 2417] loss: 33.513465881347656\n",
      "[step: 2417] loss: 0.006236541084945202\n",
      "[step: 2418] loss: 36.45500564575195\n",
      "[step: 2418] loss: 0.006446901708841324\n",
      "[step: 2419] loss: 30.941287994384766\n",
      "[step: 2419] loss: 0.006512579508125782\n",
      "[step: 2420] loss: 31.92799949645996\n",
      "[step: 2420] loss: 0.006323901005089283\n",
      "[step: 2421] loss: 29.82018280029297\n",
      "[step: 2421] loss: 0.006204388570040464\n",
      "[step: 2422] loss: 31.326732635498047\n",
      "[step: 2422] loss: 0.0063422322273254395\n",
      "[step: 2423] loss: 30.815629959106445\n",
      "[step: 2423] loss: 0.006405413616448641\n",
      "[step: 2424] loss: 30.248207092285156\n",
      "[step: 2424] loss: 0.006245273631066084\n",
      "[step: 2425] loss: 27.501239776611328\n",
      "[step: 2425] loss: 0.0061969212256371975\n",
      "[step: 2426] loss: 28.91070556640625\n",
      "[step: 2426] loss: 0.006299787666648626\n",
      "[step: 2427] loss: 29.320846557617188\n",
      "[step: 2427] loss: 0.006299254950135946\n",
      "[step: 2428] loss: 26.62188720703125\n",
      "[step: 2428] loss: 0.00622769957408309\n",
      "[step: 2429] loss: 27.40817642211914\n",
      "[step: 2429] loss: 0.006213363725692034\n",
      "[step: 2430] loss: 27.914405822753906\n",
      "[step: 2430] loss: 0.006230963859707117\n",
      "[step: 2431] loss: 27.059553146362305\n",
      "[step: 2431] loss: 0.006251566112041473\n",
      "[step: 2432] loss: 26.00060272216797\n",
      "[step: 2432] loss: 0.006230506580322981\n",
      "[step: 2433] loss: 27.373844146728516\n",
      "[step: 2433] loss: 0.0061961147002875805\n",
      "[step: 2434] loss: 26.431631088256836\n",
      "[step: 2434] loss: 0.0062172794714570045\n",
      "[step: 2435] loss: 26.469573974609375\n",
      "[step: 2435] loss: 0.006219451315701008\n",
      "[step: 2436] loss: 26.26132583618164\n",
      "[step: 2436] loss: 0.006197807379066944\n",
      "[step: 2437] loss: 25.957225799560547\n",
      "[step: 2437] loss: 0.006202797405421734\n",
      "[step: 2438] loss: 25.716581344604492\n",
      "[step: 2438] loss: 0.0062027196399867535\n",
      "[step: 2439] loss: 25.999507904052734\n",
      "[step: 2439] loss: 0.006193056236952543\n",
      "[step: 2440] loss: 26.122894287109375\n",
      "[step: 2440] loss: 0.006189131643623114\n",
      "[step: 2441] loss: 25.234848022460938\n",
      "[step: 2441] loss: 0.006184597499668598\n",
      "[step: 2442] loss: 25.447017669677734\n",
      "[step: 2442] loss: 0.006192831788212061\n",
      "[step: 2443] loss: 25.269981384277344\n",
      "[step: 2443] loss: 0.00617841724306345\n",
      "[step: 2444] loss: 25.25665283203125\n",
      "[step: 2444] loss: 0.006172034423798323\n",
      "[step: 2445] loss: 25.05859375\n",
      "[step: 2445] loss: 0.006184680387377739\n",
      "[step: 2446] loss: 25.460514068603516\n",
      "[step: 2446] loss: 0.006177689414471388\n",
      "[step: 2447] loss: 25.157583236694336\n",
      "[step: 2447] loss: 0.006171001121401787\n",
      "[step: 2448] loss: 24.99167823791504\n",
      "[step: 2448] loss: 0.006166807841509581\n",
      "[step: 2449] loss: 24.946319580078125\n",
      "[step: 2449] loss: 0.006172455847263336\n",
      "[step: 2450] loss: 24.918149948120117\n",
      "[step: 2450] loss: 0.006176330614835024\n",
      "[step: 2451] loss: 25.048812866210938\n",
      "[step: 2451] loss: 0.006164073012769222\n",
      "[step: 2452] loss: 25.335750579833984\n",
      "[step: 2452] loss: 0.006162834353744984\n",
      "[step: 2453] loss: 26.62459945678711\n",
      "[step: 2453] loss: 0.006167927291244268\n",
      "[step: 2454] loss: 27.769874572753906\n",
      "[step: 2454] loss: 0.006166303995996714\n",
      "[step: 2455] loss: 31.517250061035156\n",
      "[step: 2455] loss: 0.006164818536490202\n",
      "[step: 2456] loss: 34.319618225097656\n",
      "[step: 2456] loss: 0.006160025950521231\n",
      "[step: 2457] loss: 38.13399124145508\n",
      "[step: 2457] loss: 0.006161941215395927\n",
      "[step: 2458] loss: 41.86767578125\n",
      "[step: 2458] loss: 0.006163604557514191\n",
      "[step: 2459] loss: 43.289512634277344\n",
      "[step: 2459] loss: 0.006159499287605286\n",
      "[step: 2460] loss: 36.76335906982422\n",
      "[step: 2460] loss: 0.006159567274153233\n",
      "[step: 2461] loss: 29.685100555419922\n",
      "[step: 2461] loss: 0.006160466931760311\n",
      "[step: 2462] loss: 27.129438400268555\n",
      "[step: 2462] loss: 0.006159013602882624\n",
      "[step: 2463] loss: 29.17755699157715\n",
      "[step: 2463] loss: 0.006158353295177221\n",
      "[step: 2464] loss: 30.703277587890625\n",
      "[step: 2464] loss: 0.006156632676720619\n",
      "[step: 2465] loss: 30.683269500732422\n",
      "[step: 2465] loss: 0.006157564464956522\n",
      "[step: 2466] loss: 31.043357849121094\n",
      "[step: 2466] loss: 0.006158607546240091\n",
      "[step: 2467] loss: 31.149059295654297\n",
      "[step: 2467] loss: 0.00615623127669096\n",
      "[step: 2468] loss: 29.47570037841797\n",
      "[step: 2468] loss: 0.006155256647616625\n",
      "[step: 2469] loss: 25.65636444091797\n",
      "[step: 2469] loss: 0.006155406124889851\n",
      "[step: 2470] loss: 26.40547752380371\n",
      "[step: 2470] loss: 0.006155424285680056\n",
      "[step: 2471] loss: 29.691246032714844\n",
      "[step: 2471] loss: 0.00615549786016345\n",
      "[step: 2472] loss: 28.952512741088867\n",
      "[step: 2472] loss: 0.006154493894428015\n",
      "[step: 2473] loss: 27.205257415771484\n",
      "[step: 2473] loss: 0.006153762340545654\n",
      "[step: 2474] loss: 27.582355499267578\n",
      "[step: 2474] loss: 0.006154177710413933\n",
      "[step: 2475] loss: 28.739171981811523\n",
      "[step: 2475] loss: 0.006153673864901066\n",
      "[step: 2476] loss: 27.234596252441406\n",
      "[step: 2476] loss: 0.006152952555567026\n",
      "[step: 2477] loss: 25.509675979614258\n",
      "[step: 2477] loss: 0.006152777932584286\n",
      "[step: 2478] loss: 25.400074005126953\n",
      "[step: 2478] loss: 0.0061524538323283195\n",
      "[step: 2479] loss: 26.256271362304688\n",
      "[step: 2479] loss: 0.006152580492198467\n",
      "[step: 2480] loss: 26.01137351989746\n",
      "[step: 2480] loss: 0.006152417976409197\n",
      "[step: 2481] loss: 25.416648864746094\n",
      "[step: 2481] loss: 0.006151637993752956\n",
      "[step: 2482] loss: 25.370716094970703\n",
      "[step: 2482] loss: 0.006151237525045872\n",
      "[step: 2483] loss: 25.65634536743164\n",
      "[step: 2483] loss: 0.006151167210191488\n",
      "[step: 2484] loss: 25.751869201660156\n",
      "[step: 2484] loss: 0.006150947883725166\n",
      "[step: 2485] loss: 25.796358108520508\n",
      "[step: 2485] loss: 0.006150773726403713\n",
      "[step: 2486] loss: 26.310428619384766\n",
      "[step: 2486] loss: 0.006150504108518362\n",
      "[step: 2487] loss: 26.09961700439453\n",
      "[step: 2487] loss: 0.006150067783892155\n",
      "[step: 2488] loss: 25.857595443725586\n",
      "[step: 2488] loss: 0.0061499555595219135\n",
      "[step: 2489] loss: 25.79924774169922\n",
      "[step: 2489] loss: 0.006149838212877512\n",
      "[step: 2490] loss: 26.15595245361328\n",
      "[step: 2490] loss: 0.006149497348815203\n",
      "[step: 2491] loss: 26.378952026367188\n",
      "[step: 2491] loss: 0.006149187218397856\n",
      "[step: 2492] loss: 26.055099487304688\n",
      "[step: 2492] loss: 0.0061489008367061615\n",
      "[step: 2493] loss: 25.879329681396484\n",
      "[step: 2493] loss: 0.006148613058030605\n",
      "[step: 2494] loss: 26.429916381835938\n",
      "[step: 2494] loss: 0.006148466374725103\n",
      "[step: 2495] loss: 26.739498138427734\n",
      "[step: 2495] loss: 0.006148329004645348\n",
      "[step: 2496] loss: 26.901840209960938\n",
      "[step: 2496] loss: 0.006148054730147123\n",
      "[step: 2497] loss: 26.210250854492188\n",
      "[step: 2497] loss: 0.006147801876068115\n",
      "[step: 2498] loss: 26.028057098388672\n",
      "[step: 2498] loss: 0.006147583480924368\n",
      "[step: 2499] loss: 26.141742706298828\n",
      "[step: 2499] loss: 0.006147343665361404\n",
      "[step: 2500] loss: 25.960289001464844\n",
      "[step: 2500] loss: 0.006147160194814205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2501] loss: 25.50187873840332\n",
      "[step: 2501] loss: 0.0061469790525734425\n",
      "[step: 2502] loss: 25.012149810791016\n",
      "[step: 2502] loss: 0.0061467308551073074\n",
      "[step: 2503] loss: 24.68057632446289\n",
      "[step: 2503] loss: 0.006146478001028299\n",
      "[step: 2504] loss: 24.581872940063477\n",
      "[step: 2504] loss: 0.00614625308662653\n",
      "[step: 2505] loss: 24.619178771972656\n",
      "[step: 2505] loss: 0.006146014668047428\n",
      "[step: 2506] loss: 24.736427307128906\n",
      "[step: 2506] loss: 0.006145801395177841\n",
      "[step: 2507] loss: 24.679058074951172\n",
      "[step: 2507] loss: 0.006145615596324205\n",
      "[step: 2508] loss: 24.606639862060547\n",
      "[step: 2508] loss: 0.006145399063825607\n",
      "[step: 2509] loss: 24.48464584350586\n",
      "[step: 2509] loss: 0.00614516856148839\n",
      "[step: 2510] loss: 24.679786682128906\n",
      "[step: 2510] loss: 0.006144961342215538\n",
      "[step: 2511] loss: 25.25008773803711\n",
      "[step: 2511] loss: 0.0061447410844266415\n",
      "[step: 2512] loss: 26.385486602783203\n",
      "[step: 2512] loss: 0.0061445338651537895\n",
      "[step: 2513] loss: 28.486297607421875\n",
      "[step: 2513] loss: 0.006144337356090546\n",
      "[step: 2514] loss: 33.59971618652344\n",
      "[step: 2514] loss: 0.006144160404801369\n",
      "[step: 2515] loss: 38.59148406982422\n",
      "[step: 2515] loss: 0.006143996492028236\n",
      "[step: 2516] loss: 43.4068717956543\n",
      "[step: 2516] loss: 0.0061438437551259995\n",
      "[step: 2517] loss: 44.04173278808594\n",
      "[step: 2517] loss: 0.006143735721707344\n",
      "[step: 2518] loss: 40.765411376953125\n",
      "[step: 2518] loss: 0.0061436924152076244\n",
      "[step: 2519] loss: 33.857582092285156\n",
      "[step: 2519] loss: 0.006143735256046057\n",
      "[step: 2520] loss: 29.22208023071289\n",
      "[step: 2520] loss: 0.0061439815908670425\n",
      "[step: 2521] loss: 27.698461532592773\n",
      "[step: 2521] loss: 0.006144539453089237\n",
      "[step: 2522] loss: 28.756860733032227\n",
      "[step: 2522] loss: 0.006145666353404522\n",
      "[step: 2523] loss: 30.099506378173828\n",
      "[step: 2523] loss: 0.006147803273051977\n",
      "[step: 2524] loss: 31.150135040283203\n",
      "[step: 2524] loss: 0.0061518424190580845\n",
      "[step: 2525] loss: 30.45243263244629\n",
      "[step: 2525] loss: 0.0061591314151883125\n",
      "[step: 2526] loss: 31.166126251220703\n",
      "[step: 2526] loss: 0.006172810215502977\n",
      "[step: 2527] loss: 33.84205627441406\n",
      "[step: 2527] loss: 0.006196813192218542\n",
      "[step: 2528] loss: 28.834217071533203\n",
      "[step: 2528] loss: 0.006240935064852238\n",
      "[step: 2529] loss: 27.93492889404297\n",
      "[step: 2529] loss: 0.006310398690402508\n",
      "[step: 2530] loss: 30.389781951904297\n",
      "[step: 2530] loss: 0.006420804653316736\n",
      "[step: 2531] loss: 28.453336715698242\n",
      "[step: 2531] loss: 0.00653176661580801\n",
      "[step: 2532] loss: 26.16118812561035\n",
      "[step: 2532] loss: 0.006602694280445576\n",
      "[step: 2533] loss: 29.376605987548828\n",
      "[step: 2533] loss: 0.0065051838755607605\n",
      "[step: 2534] loss: 27.94172477722168\n",
      "[step: 2534] loss: 0.006300304550677538\n",
      "[step: 2535] loss: 26.112714767456055\n",
      "[step: 2535] loss: 0.006169206462800503\n",
      "[step: 2536] loss: 26.06326675415039\n",
      "[step: 2536] loss: 0.006224171258509159\n",
      "[step: 2537] loss: 26.278244018554688\n",
      "[step: 2537] loss: 0.0063342866487801075\n",
      "[step: 2538] loss: 25.99312973022461\n",
      "[step: 2538] loss: 0.006326764822006226\n",
      "[step: 2539] loss: 25.26213264465332\n",
      "[step: 2539] loss: 0.0062215905636549\n",
      "[step: 2540] loss: 26.462480545043945\n",
      "[step: 2540] loss: 0.006168203428387642\n",
      "[step: 2541] loss: 26.55330467224121\n",
      "[step: 2541] loss: 0.006219758186489344\n",
      "[step: 2542] loss: 25.810134887695312\n",
      "[step: 2542] loss: 0.0062637426890432835\n",
      "[step: 2543] loss: 25.17386245727539\n",
      "[step: 2543] loss: 0.006224426906555891\n",
      "[step: 2544] loss: 25.72616195678711\n",
      "[step: 2544] loss: 0.0061784544959664345\n",
      "[step: 2545] loss: 25.1824893951416\n",
      "[step: 2545] loss: 0.006185820791870356\n",
      "[step: 2546] loss: 24.663076400756836\n",
      "[step: 2546] loss: 0.006201570853590965\n",
      "[step: 2547] loss: 25.18791961669922\n",
      "[step: 2547] loss: 0.006190377753227949\n",
      "[step: 2548] loss: 25.111854553222656\n",
      "[step: 2548] loss: 0.006171554327011108\n",
      "[step: 2549] loss: 25.53873062133789\n",
      "[step: 2549] loss: 0.006175401154905558\n",
      "[step: 2550] loss: 25.313302993774414\n",
      "[step: 2550] loss: 0.006187474355101585\n",
      "[step: 2551] loss: 25.701696395874023\n",
      "[step: 2551] loss: 0.0061752675101161\n",
      "[step: 2552] loss: 26.428001403808594\n",
      "[step: 2552] loss: 0.0061500221490859985\n",
      "[step: 2553] loss: 27.01386260986328\n",
      "[step: 2553] loss: 0.006151399575173855\n",
      "[step: 2554] loss: 27.812484741210938\n",
      "[step: 2554] loss: 0.006172581110149622\n",
      "[step: 2555] loss: 27.301637649536133\n",
      "[step: 2555] loss: 0.006172998808324337\n",
      "[step: 2556] loss: 26.998876571655273\n",
      "[step: 2556] loss: 0.0061526428908109665\n",
      "[step: 2557] loss: 26.433536529541016\n",
      "[step: 2557] loss: 0.006143506150692701\n",
      "[step: 2558] loss: 26.10406494140625\n",
      "[step: 2558] loss: 0.0061526126228272915\n",
      "[step: 2559] loss: 24.96484375\n",
      "[step: 2559] loss: 0.0061570266261696815\n",
      "[step: 2560] loss: 24.48200035095215\n",
      "[step: 2560] loss: 0.006148035638034344\n",
      "[step: 2561] loss: 24.334976196289062\n",
      "[step: 2561] loss: 0.006144688464701176\n",
      "[step: 2562] loss: 24.67133331298828\n",
      "[step: 2562] loss: 0.0061517274007201195\n",
      "[step: 2563] loss: 24.94207763671875\n",
      "[step: 2563] loss: 0.006151438690721989\n",
      "[step: 2564] loss: 25.083194732666016\n",
      "[step: 2564] loss: 0.006141866557300091\n",
      "[step: 2565] loss: 25.281631469726562\n",
      "[step: 2565] loss: 0.006138491909950972\n",
      "[step: 2566] loss: 25.4912109375\n",
      "[step: 2566] loss: 0.006143303122371435\n",
      "[step: 2567] loss: 27.194093704223633\n",
      "[step: 2567] loss: 0.006145185325294733\n",
      "[step: 2568] loss: 27.789264678955078\n",
      "[step: 2568] loss: 0.006141752004623413\n",
      "[step: 2569] loss: 28.738637924194336\n",
      "[step: 2569] loss: 0.006139619275927544\n",
      "[step: 2570] loss: 28.67411231994629\n",
      "[step: 2570] loss: 0.006141379941254854\n",
      "[step: 2571] loss: 28.947052001953125\n",
      "[step: 2571] loss: 0.006142430938780308\n",
      "[step: 2572] loss: 28.967430114746094\n",
      "[step: 2572] loss: 0.0061391806229949\n",
      "[step: 2573] loss: 28.032421112060547\n",
      "[step: 2573] loss: 0.0061358679085969925\n",
      "[step: 2574] loss: 25.95322036743164\n",
      "[step: 2574] loss: 0.006136737298220396\n",
      "[step: 2575] loss: 24.667091369628906\n",
      "[step: 2575] loss: 0.0061384825967252254\n",
      "[step: 2576] loss: 24.69388198852539\n",
      "[step: 2576] loss: 0.006137511692941189\n",
      "[step: 2577] loss: 25.09443473815918\n",
      "[step: 2577] loss: 0.006135730538517237\n",
      "[step: 2578] loss: 24.827922821044922\n",
      "[step: 2578] loss: 0.006135980598628521\n",
      "[step: 2579] loss: 24.600534439086914\n",
      "[step: 2579] loss: 0.0061372932977974415\n",
      "[step: 2580] loss: 25.02907371520996\n",
      "[step: 2580] loss: 0.0061371601186692715\n",
      "[step: 2581] loss: 26.018526077270508\n",
      "[step: 2581] loss: 0.006135642994195223\n",
      "[step: 2582] loss: 27.52960968017578\n",
      "[step: 2582] loss: 0.006134720053523779\n",
      "[step: 2583] loss: 27.122650146484375\n",
      "[step: 2583] loss: 0.006135118659585714\n",
      "[step: 2584] loss: 26.84775161743164\n",
      "[step: 2584] loss: 0.006135355215519667\n",
      "[step: 2585] loss: 27.182111740112305\n",
      "[step: 2585] loss: 0.006134444382041693\n",
      "[step: 2586] loss: 27.44963836669922\n",
      "[step: 2586] loss: 0.0061333999037742615\n",
      "[step: 2587] loss: 26.784603118896484\n",
      "[step: 2587] loss: 0.006133146584033966\n",
      "[step: 2588] loss: 25.708206176757812\n",
      "[step: 2588] loss: 0.006133365910500288\n",
      "[step: 2589] loss: 24.92892074584961\n",
      "[step: 2589] loss: 0.006133273243904114\n",
      "[step: 2590] loss: 25.166484832763672\n",
      "[step: 2590] loss: 0.006132678594440222\n",
      "[step: 2591] loss: 25.002906799316406\n",
      "[step: 2591] loss: 0.006132112815976143\n",
      "[step: 2592] loss: 24.35312271118164\n",
      "[step: 2592] loss: 0.006132095120847225\n",
      "[step: 2593] loss: 23.850919723510742\n",
      "[step: 2593] loss: 0.006132400594651699\n",
      "[step: 2594] loss: 23.93730926513672\n",
      "[step: 2594] loss: 0.006132511887699366\n",
      "[step: 2595] loss: 24.352468490600586\n",
      "[step: 2595] loss: 0.0061324224807322025\n",
      "[step: 2596] loss: 24.4873046875\n",
      "[step: 2596] loss: 0.006132698152214289\n",
      "[step: 2597] loss: 24.525463104248047\n",
      "[step: 2597] loss: 0.006133702117949724\n",
      "[step: 2598] loss: 24.584259033203125\n",
      "[step: 2598] loss: 0.006135497707873583\n",
      "[step: 2599] loss: 25.193470001220703\n",
      "[step: 2599] loss: 0.006138431839644909\n",
      "[step: 2600] loss: 26.219144821166992\n",
      "[step: 2600] loss: 0.0061431946232914925\n",
      "[step: 2601] loss: 27.608007431030273\n",
      "[step: 2601] loss: 0.006151760462671518\n",
      "[step: 2602] loss: 29.183725357055664\n",
      "[step: 2602] loss: 0.0061663975939154625\n",
      "[step: 2603] loss: 31.961715698242188\n",
      "[step: 2603] loss: 0.0061921230517327785\n",
      "[step: 2604] loss: 33.824851989746094\n",
      "[step: 2604] loss: 0.0062331994995474815\n",
      "[step: 2605] loss: 34.56254577636719\n",
      "[step: 2605] loss: 0.006299342028796673\n",
      "[step: 2606] loss: 32.9673957824707\n",
      "[step: 2606] loss: 0.0063833799213171005\n",
      "[step: 2607] loss: 29.805843353271484\n",
      "[step: 2607] loss: 0.006482451688498259\n",
      "[step: 2608] loss: 26.69556427001953\n",
      "[step: 2608] loss: 0.00651504984125495\n",
      "[step: 2609] loss: 24.672277450561523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2609] loss: 0.0064540160819888115\n",
      "[step: 2610] loss: 24.366195678710938\n",
      "[step: 2610] loss: 0.0062819551676511765\n",
      "[step: 2611] loss: 25.558570861816406\n",
      "[step: 2611] loss: 0.006151068955659866\n",
      "[step: 2612] loss: 27.31552505493164\n",
      "[step: 2612] loss: 0.0061655608005821705\n",
      "[step: 2613] loss: 28.775718688964844\n",
      "[step: 2613] loss: 0.006263438146561384\n",
      "[step: 2614] loss: 28.82028579711914\n",
      "[step: 2614] loss: 0.006303348578512669\n",
      "[step: 2615] loss: 27.66260528564453\n",
      "[step: 2615] loss: 0.006230836734175682\n",
      "[step: 2616] loss: 26.64560317993164\n",
      "[step: 2616] loss: 0.006150698289275169\n",
      "[step: 2617] loss: 25.515445709228516\n",
      "[step: 2617] loss: 0.006153501570224762\n",
      "[step: 2618] loss: 24.620460510253906\n",
      "[step: 2618] loss: 0.006208517588675022\n",
      "[step: 2619] loss: 24.001529693603516\n",
      "[step: 2619] loss: 0.006232873070985079\n",
      "[step: 2620] loss: 24.41685676574707\n",
      "[step: 2620] loss: 0.0061986325308680534\n",
      "[step: 2621] loss: 25.53893280029297\n",
      "[step: 2621] loss: 0.006154672242701054\n",
      "[step: 2622] loss: 26.151973724365234\n",
      "[step: 2622] loss: 0.006141625344753265\n",
      "[step: 2623] loss: 26.41102409362793\n",
      "[step: 2623] loss: 0.006161408498883247\n",
      "[step: 2624] loss: 26.170562744140625\n",
      "[step: 2624] loss: 0.0061853788793087006\n",
      "[step: 2625] loss: 26.400428771972656\n",
      "[step: 2625] loss: 0.006184983532875776\n",
      "[step: 2626] loss: 26.23859405517578\n",
      "[step: 2626] loss: 0.0061604613438248634\n",
      "[step: 2627] loss: 25.463542938232422\n",
      "[step: 2627] loss: 0.006134870927780867\n",
      "[step: 2628] loss: 24.254642486572266\n",
      "[step: 2628] loss: 0.006133793853223324\n",
      "[step: 2629] loss: 23.637428283691406\n",
      "[step: 2629] loss: 0.0061530922539532185\n",
      "[step: 2630] loss: 23.820430755615234\n",
      "[step: 2630] loss: 0.006165578030049801\n",
      "[step: 2631] loss: 24.275131225585938\n",
      "[step: 2631] loss: 0.006156265735626221\n",
      "[step: 2632] loss: 24.56899642944336\n",
      "[step: 2632] loss: 0.006136615760624409\n",
      "[step: 2633] loss: 24.410789489746094\n",
      "[step: 2633] loss: 0.006128191947937012\n",
      "[step: 2634] loss: 24.212120056152344\n",
      "[step: 2634] loss: 0.006134961266070604\n",
      "[step: 2635] loss: 24.48168182373047\n",
      "[step: 2635] loss: 0.006144288461655378\n",
      "[step: 2636] loss: 26.343833923339844\n",
      "[step: 2636] loss: 0.006145840976387262\n",
      "[step: 2637] loss: 27.81188201904297\n",
      "[step: 2637] loss: 0.006139758508652449\n",
      "[step: 2638] loss: 29.855751037597656\n",
      "[step: 2638] loss: 0.0061326478607952595\n",
      "[step: 2639] loss: 27.658897399902344\n",
      "[step: 2639] loss: 0.006128600332885981\n",
      "[step: 2640] loss: 28.02109146118164\n",
      "[step: 2640] loss: 0.00612860731780529\n",
      "[step: 2641] loss: 29.979841232299805\n",
      "[step: 2641] loss: 0.006131614558398724\n",
      "[step: 2642] loss: 29.452903747558594\n",
      "[step: 2642] loss: 0.00613489979878068\n",
      "[step: 2643] loss: 26.741899490356445\n",
      "[step: 2643] loss: 0.006135358940809965\n",
      "[step: 2644] loss: 25.1881103515625\n",
      "[step: 2644] loss: 0.006131563801318407\n",
      "[step: 2645] loss: 25.814983367919922\n",
      "[step: 2645] loss: 0.006126366555690765\n",
      "[step: 2646] loss: 26.05980682373047\n",
      "[step: 2646] loss: 0.006123837549239397\n",
      "[step: 2647] loss: 24.351455688476562\n",
      "[step: 2647] loss: 0.006125239189714193\n",
      "[step: 2648] loss: 24.508115768432617\n",
      "[step: 2648] loss: 0.00612805038690567\n",
      "[step: 2649] loss: 26.14112091064453\n",
      "[step: 2649] loss: 0.006129339803010225\n",
      "[step: 2650] loss: 25.38275146484375\n",
      "[step: 2650] loss: 0.0061284517869353294\n",
      "[step: 2651] loss: 25.156719207763672\n",
      "[step: 2651] loss: 0.0061263726092875\n",
      "[step: 2652] loss: 25.7067813873291\n",
      "[step: 2652] loss: 0.006124482490122318\n",
      "[step: 2653] loss: 26.519912719726562\n",
      "[step: 2653] loss: 0.006123214960098267\n",
      "[step: 2654] loss: 26.281965255737305\n",
      "[step: 2654] loss: 0.006122603081166744\n",
      "[step: 2655] loss: 25.505992889404297\n",
      "[step: 2655] loss: 0.006122761406004429\n",
      "[step: 2656] loss: 25.397008895874023\n",
      "[step: 2656] loss: 0.006123493425548077\n",
      "[step: 2657] loss: 26.304597854614258\n",
      "[step: 2657] loss: 0.006124281324446201\n",
      "[step: 2658] loss: 26.02316665649414\n",
      "[step: 2658] loss: 0.006124525796622038\n",
      "[step: 2659] loss: 25.846176147460938\n",
      "[step: 2659] loss: 0.0061239018104970455\n",
      "[step: 2660] loss: 25.80547332763672\n",
      "[step: 2660] loss: 0.006122712977230549\n",
      "[step: 2661] loss: 26.00218963623047\n",
      "[step: 2661] loss: 0.0061215730383992195\n",
      "[step: 2662] loss: 25.710865020751953\n",
      "[step: 2662] loss: 0.006120802368968725\n",
      "[step: 2663] loss: 25.446426391601562\n",
      "[step: 2663] loss: 0.0061204275116324425\n",
      "[step: 2664] loss: 25.407865524291992\n",
      "[step: 2664] loss: 0.006120233330875635\n",
      "[step: 2665] loss: 25.03106689453125\n",
      "[step: 2665] loss: 0.006120122503489256\n",
      "[step: 2666] loss: 24.702213287353516\n",
      "[step: 2666] loss: 0.006120132282376289\n",
      "[step: 2667] loss: 25.1234130859375\n",
      "[step: 2667] loss: 0.006120246835052967\n",
      "[step: 2668] loss: 25.187870025634766\n",
      "[step: 2668] loss: 0.00612040888518095\n",
      "[step: 2669] loss: 24.993335723876953\n",
      "[step: 2669] loss: 0.00612050574272871\n",
      "[step: 2670] loss: 24.692184448242188\n",
      "[step: 2670] loss: 0.0061204456724226475\n",
      "[step: 2671] loss: 25.199817657470703\n",
      "[step: 2671] loss: 0.006120241247117519\n",
      "[step: 2672] loss: 25.95457649230957\n",
      "[step: 2672] loss: 0.006119988393038511\n",
      "[step: 2673] loss: 26.207765579223633\n",
      "[step: 2673] loss: 0.006119715049862862\n",
      "[step: 2674] loss: 26.370647430419922\n",
      "[step: 2674] loss: 0.006119512487202883\n",
      "[step: 2675] loss: 27.353374481201172\n",
      "[step: 2675] loss: 0.006119316443800926\n",
      "[step: 2676] loss: 28.263898849487305\n",
      "[step: 2676] loss: 0.006119133904576302\n",
      "[step: 2677] loss: 29.159425735473633\n",
      "[step: 2677] loss: 0.006118956953287125\n",
      "[step: 2678] loss: 29.00634002685547\n",
      "[step: 2678] loss: 0.006118808872997761\n",
      "[step: 2679] loss: 28.628543853759766\n",
      "[step: 2679] loss: 0.006118748337030411\n",
      "[step: 2680] loss: 27.710784912109375\n",
      "[step: 2680] loss: 0.00611881772056222\n",
      "[step: 2681] loss: 26.463836669921875\n",
      "[step: 2681] loss: 0.006119041703641415\n",
      "[step: 2682] loss: 25.148651123046875\n",
      "[step: 2682] loss: 0.006119513884186745\n",
      "[step: 2683] loss: 23.864013671875\n",
      "[step: 2683] loss: 0.006120274309068918\n",
      "[step: 2684] loss: 23.308452606201172\n",
      "[step: 2684] loss: 0.006121557205915451\n",
      "[step: 2685] loss: 23.521169662475586\n",
      "[step: 2685] loss: 0.006123548839241266\n",
      "[step: 2686] loss: 24.076847076416016\n",
      "[step: 2686] loss: 0.006126841530203819\n",
      "[step: 2687] loss: 24.65591049194336\n",
      "[step: 2687] loss: 0.0061318883672356606\n",
      "[step: 2688] loss: 25.044076919555664\n",
      "[step: 2688] loss: 0.006140266545116901\n",
      "[step: 2689] loss: 25.61777114868164\n",
      "[step: 2689] loss: 0.006152738351374865\n",
      "[step: 2690] loss: 27.153778076171875\n",
      "[step: 2690] loss: 0.006173139903694391\n",
      "[step: 2691] loss: 28.141796112060547\n",
      "[step: 2691] loss: 0.006201187614351511\n",
      "[step: 2692] loss: 28.97252655029297\n",
      "[step: 2692] loss: 0.006244041956961155\n",
      "[step: 2693] loss: 27.791135787963867\n",
      "[step: 2693] loss: 0.006290532182902098\n",
      "[step: 2694] loss: 28.279998779296875\n",
      "[step: 2694] loss: 0.006344123277813196\n",
      "[step: 2695] loss: 28.74854850769043\n",
      "[step: 2695] loss: 0.006360556930303574\n",
      "[step: 2696] loss: 27.573402404785156\n",
      "[step: 2696] loss: 0.006337298545986414\n",
      "[step: 2697] loss: 25.485111236572266\n",
      "[step: 2697] loss: 0.006252428516745567\n",
      "[step: 2698] loss: 24.084091186523438\n",
      "[step: 2698] loss: 0.006163426209241152\n",
      "[step: 2699] loss: 24.405052185058594\n",
      "[step: 2699] loss: 0.006121340207755566\n",
      "[step: 2700] loss: 24.92351722717285\n",
      "[step: 2700] loss: 0.0061433701775968075\n",
      "[step: 2701] loss: 24.683151245117188\n",
      "[step: 2701] loss: 0.006194164510816336\n",
      "[step: 2702] loss: 24.821117401123047\n",
      "[step: 2702] loss: 0.006221152376383543\n",
      "[step: 2703] loss: 25.296768188476562\n",
      "[step: 2703] loss: 0.006205511745065451\n",
      "[step: 2704] loss: 25.29206085205078\n",
      "[step: 2704] loss: 0.006157788448035717\n",
      "[step: 2705] loss: 26.949228286743164\n",
      "[step: 2705] loss: 0.006121921818703413\n",
      "[step: 2706] loss: 27.742706298828125\n",
      "[step: 2706] loss: 0.006121119949966669\n",
      "[step: 2707] loss: 28.529781341552734\n",
      "[step: 2707] loss: 0.006146562285721302\n",
      "[step: 2708] loss: 28.618602752685547\n",
      "[step: 2708] loss: 0.00617264024913311\n",
      "[step: 2709] loss: 27.52861785888672\n",
      "[step: 2709] loss: 0.00617638835683465\n",
      "[step: 2710] loss: 26.102340698242188\n",
      "[step: 2710] loss: 0.00615968182682991\n",
      "[step: 2711] loss: 25.661212921142578\n",
      "[step: 2711] loss: 0.006133217830210924\n",
      "[step: 2712] loss: 25.50635528564453\n",
      "[step: 2712] loss: 0.006116820964962244\n",
      "[step: 2713] loss: 24.86713409423828\n",
      "[step: 2713] loss: 0.0061178128235042095\n",
      "[step: 2714] loss: 24.25779151916504\n",
      "[step: 2714] loss: 0.006130246911197901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2715] loss: 23.626487731933594\n",
      "[step: 2715] loss: 0.006142954342067242\n",
      "[step: 2716] loss: 23.376998901367188\n",
      "[step: 2716] loss: 0.006146056577563286\n",
      "[step: 2717] loss: 23.82068634033203\n",
      "[step: 2717] loss: 0.006138905882835388\n",
      "[step: 2718] loss: 24.00995635986328\n",
      "[step: 2718] loss: 0.006125676445662975\n",
      "[step: 2719] loss: 23.975513458251953\n",
      "[step: 2719] loss: 0.006115676369518042\n",
      "[step: 2720] loss: 23.915096282958984\n",
      "[step: 2720] loss: 0.006113523151725531\n",
      "[step: 2721] loss: 24.351701736450195\n",
      "[step: 2721] loss: 0.006117918528616428\n",
      "[step: 2722] loss: 25.907447814941406\n",
      "[step: 2722] loss: 0.006124380510300398\n",
      "[step: 2723] loss: 27.454845428466797\n",
      "[step: 2723] loss: 0.006128219421952963\n",
      "[step: 2724] loss: 29.809640884399414\n",
      "[step: 2724] loss: 0.006127552594989538\n",
      "[step: 2725] loss: 31.39841079711914\n",
      "[step: 2725] loss: 0.006122888997197151\n",
      "[step: 2726] loss: 33.59840774536133\n",
      "[step: 2726] loss: 0.006117530167102814\n",
      "[step: 2727] loss: 34.70003890991211\n",
      "[step: 2727] loss: 0.0061136893928050995\n",
      "[step: 2728] loss: 34.478057861328125\n",
      "[step: 2728] loss: 0.006112538278102875\n",
      "[step: 2729] loss: 30.539386749267578\n",
      "[step: 2729] loss: 0.006113750860095024\n",
      "[step: 2730] loss: 28.52413558959961\n",
      "[step: 2730] loss: 0.0061159878969192505\n",
      "[step: 2731] loss: 26.562206268310547\n",
      "[step: 2731] loss: 0.006117811426520348\n",
      "[step: 2732] loss: 24.500886917114258\n",
      "[step: 2732] loss: 0.0061180684715509415\n",
      "[step: 2733] loss: 25.69671058654785\n",
      "[step: 2733] loss: 0.006116672419011593\n",
      "[step: 2734] loss: 26.953018188476562\n",
      "[step: 2734] loss: 0.00611424446105957\n",
      "[step: 2735] loss: 25.8271427154541\n",
      "[step: 2735] loss: 0.006111684255301952\n",
      "[step: 2736] loss: 25.024600982666016\n",
      "[step: 2736] loss: 0.006109647918492556\n",
      "[step: 2737] loss: 26.339920043945312\n",
      "[step: 2737] loss: 0.006108567584306002\n",
      "[step: 2738] loss: 27.74932098388672\n",
      "[step: 2738] loss: 0.006108475849032402\n",
      "[step: 2739] loss: 27.753002166748047\n",
      "[step: 2739] loss: 0.006109091453254223\n",
      "[step: 2740] loss: 26.113231658935547\n",
      "[step: 2740] loss: 0.006110046524554491\n",
      "[step: 2741] loss: 25.053043365478516\n",
      "[step: 2741] loss: 0.006110935006290674\n",
      "[step: 2742] loss: 23.90595245361328\n",
      "[step: 2742] loss: 0.006111494265496731\n",
      "[step: 2743] loss: 24.184593200683594\n",
      "[step: 2743] loss: 0.006111628841608763\n",
      "[step: 2744] loss: 25.136459350585938\n",
      "[step: 2744] loss: 0.006111381575465202\n",
      "[step: 2745] loss: 24.458328247070312\n",
      "[step: 2745] loss: 0.006110796704888344\n",
      "[step: 2746] loss: 23.370893478393555\n",
      "[step: 2746] loss: 0.006110095884650946\n",
      "[step: 2747] loss: 23.647052764892578\n",
      "[step: 2747] loss: 0.006109426263719797\n",
      "[step: 2748] loss: 24.609241485595703\n",
      "[step: 2748] loss: 0.006108957342803478\n",
      "[step: 2749] loss: 24.53331756591797\n",
      "[step: 2749] loss: 0.0061088488437235355\n",
      "[step: 2750] loss: 24.06814956665039\n",
      "[step: 2750] loss: 0.006109380628913641\n",
      "[step: 2751] loss: 23.757492065429688\n",
      "[step: 2751] loss: 0.006110762245953083\n",
      "[step: 2752] loss: 23.664535522460938\n",
      "[step: 2752] loss: 0.006113478913903236\n",
      "[step: 2753] loss: 24.114591598510742\n",
      "[step: 2753] loss: 0.006118308752775192\n",
      "[step: 2754] loss: 24.896297454833984\n",
      "[step: 2754] loss: 0.006126502528786659\n",
      "[step: 2755] loss: 24.646501541137695\n",
      "[step: 2755] loss: 0.006140543147921562\n",
      "[step: 2756] loss: 24.489154815673828\n",
      "[step: 2756] loss: 0.006163482088595629\n",
      "[step: 2757] loss: 24.541316986083984\n",
      "[step: 2757] loss: 0.006201431155204773\n",
      "[step: 2758] loss: 25.14997673034668\n",
      "[step: 2758] loss: 0.006256905384361744\n",
      "[step: 2759] loss: 25.772972106933594\n",
      "[step: 2759] loss: 0.006334753707051277\n",
      "[step: 2760] loss: 26.00368881225586\n",
      "[step: 2760] loss: 0.0064081838354468346\n",
      "[step: 2761] loss: 25.927072525024414\n",
      "[step: 2761] loss: 0.006452120374888182\n",
      "[step: 2762] loss: 26.176048278808594\n",
      "[step: 2762] loss: 0.006395099684596062\n",
      "[step: 2763] loss: 26.61499786376953\n",
      "[step: 2763] loss: 0.006268215365707874\n",
      "[step: 2764] loss: 26.95071792602539\n",
      "[step: 2764] loss: 0.006153413560241461\n",
      "[step: 2765] loss: 25.994108200073242\n",
      "[step: 2765] loss: 0.006142016965895891\n",
      "[step: 2766] loss: 24.927047729492188\n",
      "[step: 2766] loss: 0.006194811314344406\n",
      "[step: 2767] loss: 23.999862670898438\n",
      "[step: 2767] loss: 0.0062184701673686504\n",
      "[step: 2768] loss: 23.832876205444336\n",
      "[step: 2768] loss: 0.006191792897880077\n",
      "[step: 2769] loss: 23.71002197265625\n",
      "[step: 2769] loss: 0.006170552223920822\n",
      "[step: 2770] loss: 23.218456268310547\n",
      "[step: 2770] loss: 0.006176631432026625\n",
      "[step: 2771] loss: 22.886150360107422\n",
      "[step: 2771] loss: 0.0061632804572582245\n",
      "[step: 2772] loss: 22.835281372070312\n",
      "[step: 2772] loss: 0.006130034103989601\n",
      "[step: 2773] loss: 23.318370819091797\n",
      "[step: 2773] loss: 0.006124619860202074\n",
      "[step: 2774] loss: 23.88962745666504\n",
      "[step: 2774] loss: 0.0061554373241961\n",
      "[step: 2775] loss: 24.197599411010742\n",
      "[step: 2775] loss: 0.006170294713228941\n",
      "[step: 2776] loss: 24.435394287109375\n",
      "[step: 2776] loss: 0.006144951097667217\n",
      "[step: 2777] loss: 25.172412872314453\n",
      "[step: 2777] loss: 0.006122743245214224\n",
      "[step: 2778] loss: 26.632930755615234\n",
      "[step: 2778] loss: 0.0061277602799236774\n",
      "[step: 2779] loss: 29.640195846557617\n",
      "[step: 2779] loss: 0.0061274063773453236\n",
      "[step: 2780] loss: 30.94338607788086\n",
      "[step: 2780] loss: 0.006114686839282513\n",
      "[step: 2781] loss: 31.461881637573242\n",
      "[step: 2781] loss: 0.006114860996603966\n",
      "[step: 2782] loss: 32.11989212036133\n",
      "[step: 2782] loss: 0.006127660162746906\n",
      "[step: 2783] loss: 32.07984924316406\n",
      "[step: 2783] loss: 0.006130773574113846\n",
      "[step: 2784] loss: 29.5693359375\n",
      "[step: 2784] loss: 0.006120514124631882\n",
      "[step: 2785] loss: 26.826854705810547\n",
      "[step: 2785] loss: 0.006115070544183254\n",
      "[step: 2786] loss: 24.60885238647461\n",
      "[step: 2786] loss: 0.0061157941818237305\n",
      "[step: 2787] loss: 24.305938720703125\n",
      "[step: 2787] loss: 0.006110538262873888\n",
      "[step: 2788] loss: 25.1013240814209\n",
      "[step: 2788] loss: 0.0061050718650221825\n",
      "[step: 2789] loss: 25.238590240478516\n",
      "[step: 2789] loss: 0.00610752310603857\n",
      "[step: 2790] loss: 24.60655975341797\n",
      "[step: 2790] loss: 0.006111134774982929\n",
      "[step: 2791] loss: 24.243959426879883\n",
      "[step: 2791] loss: 0.0061104679480195045\n",
      "[step: 2792] loss: 25.285865783691406\n",
      "[step: 2792] loss: 0.006109445355832577\n",
      "[step: 2793] loss: 26.79581069946289\n",
      "[step: 2793] loss: 0.006111103110015392\n",
      "[step: 2794] loss: 28.630107879638672\n",
      "[step: 2794] loss: 0.006111541297286749\n",
      "[step: 2795] loss: 27.019954681396484\n",
      "[step: 2795] loss: 0.006107613909989595\n",
      "[step: 2796] loss: 26.571990966796875\n",
      "[step: 2796] loss: 0.006105240434408188\n",
      "[step: 2797] loss: 27.148849487304688\n",
      "[step: 2797] loss: 0.006105335894972086\n",
      "[step: 2798] loss: 26.102924346923828\n",
      "[step: 2798] loss: 0.006103535648435354\n",
      "[step: 2799] loss: 24.766569137573242\n",
      "[step: 2799] loss: 0.006100837606936693\n",
      "[step: 2800] loss: 23.884796142578125\n",
      "[step: 2800] loss: 0.006100268568843603\n",
      "[step: 2801] loss: 23.95722007751465\n",
      "[step: 2801] loss: 0.006101297214627266\n",
      "[step: 2802] loss: 24.238204956054688\n",
      "[step: 2802] loss: 0.006101103499531746\n",
      "[step: 2803] loss: 24.272422790527344\n",
      "[step: 2803] loss: 0.0060998620465397835\n",
      "[step: 2804] loss: 24.09652328491211\n",
      "[step: 2804] loss: 0.006100228056311607\n",
      "[step: 2805] loss: 24.983016967773438\n",
      "[step: 2805] loss: 0.006101705599576235\n",
      "[step: 2806] loss: 25.193035125732422\n",
      "[step: 2806] loss: 0.006101902574300766\n",
      "[step: 2807] loss: 25.921794891357422\n",
      "[step: 2807] loss: 0.006101539824157953\n",
      "[step: 2808] loss: 26.113739013671875\n",
      "[step: 2808] loss: 0.006102243438363075\n",
      "[step: 2809] loss: 25.804346084594727\n",
      "[step: 2809] loss: 0.006104063242673874\n",
      "[step: 2810] loss: 25.896682739257812\n",
      "[step: 2810] loss: 0.006105905864387751\n",
      "[step: 2811] loss: 25.659486770629883\n",
      "[step: 2811] loss: 0.0061075929552316666\n",
      "[step: 2812] loss: 24.420974731445312\n",
      "[step: 2812] loss: 0.006110891699790955\n",
      "[step: 2813] loss: 23.768905639648438\n",
      "[step: 2813] loss: 0.006116349250078201\n",
      "[step: 2814] loss: 23.796350479125977\n",
      "[step: 2814] loss: 0.006124263163655996\n",
      "[step: 2815] loss: 23.417102813720703\n",
      "[step: 2815] loss: 0.0061349025927484035\n",
      "[step: 2816] loss: 23.245288848876953\n",
      "[step: 2816] loss: 0.006150881294161081\n",
      "[step: 2817] loss: 22.77841567993164\n",
      "[step: 2817] loss: 0.006171918474137783\n",
      "[step: 2818] loss: 22.754741668701172\n",
      "[step: 2818] loss: 0.006201566196978092\n",
      "[step: 2819] loss: 23.118671417236328\n",
      "[step: 2819] loss: 0.006230570841580629\n",
      "[step: 2820] loss: 23.113018035888672\n",
      "[step: 2820] loss: 0.006261257920414209\n",
      "[step: 2821] loss: 23.286815643310547\n",
      "[step: 2821] loss: 0.006270331330597401\n",
      "[step: 2822] loss: 23.772382736206055\n",
      "[step: 2822] loss: 0.006257229950278997\n",
      "[step: 2823] loss: 24.92043685913086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2823] loss: 0.006208390463143587\n",
      "[step: 2824] loss: 27.447925567626953\n",
      "[step: 2824] loss: 0.006149467080831528\n",
      "[step: 2825] loss: 28.74458122253418\n",
      "[step: 2825] loss: 0.006106778979301453\n",
      "[step: 2826] loss: 31.203567504882812\n",
      "[step: 2826] loss: 0.006099977530539036\n",
      "[step: 2827] loss: 33.387107849121094\n",
      "[step: 2827] loss: 0.00612244475632906\n",
      "[step: 2828] loss: 34.953224182128906\n",
      "[step: 2828] loss: 0.006151983980089426\n",
      "[step: 2829] loss: 33.727142333984375\n",
      "[step: 2829] loss: 0.006169038824737072\n",
      "[step: 2830] loss: 31.710033416748047\n",
      "[step: 2830] loss: 0.006160906050354242\n",
      "[step: 2831] loss: 28.267501831054688\n",
      "[step: 2831] loss: 0.006137254182249308\n",
      "[step: 2832] loss: 29.53298568725586\n",
      "[step: 2832] loss: 0.0061107538640499115\n",
      "[step: 2833] loss: 26.070743560791016\n",
      "[step: 2833] loss: 0.0060967011377215385\n",
      "[step: 2834] loss: 26.53297233581543\n",
      "[step: 2834] loss: 0.006098599638789892\n",
      "[step: 2835] loss: 24.584659576416016\n",
      "[step: 2835] loss: 0.006111109163612127\n",
      "[step: 2836] loss: 25.584949493408203\n",
      "[step: 2836] loss: 0.006125654559582472\n",
      "[step: 2837] loss: 27.476943969726562\n",
      "[step: 2837] loss: 0.006133189890533686\n",
      "[step: 2838] loss: 27.016433715820312\n",
      "[step: 2838] loss: 0.0061318539083004\n",
      "[step: 2839] loss: 28.13515853881836\n",
      "[step: 2839] loss: 0.006121186539530754\n",
      "[step: 2840] loss: 25.7703857421875\n",
      "[step: 2840] loss: 0.0061080604791641235\n",
      "[step: 2841] loss: 27.304473876953125\n",
      "[step: 2841] loss: 0.006097514182329178\n",
      "[step: 2842] loss: 24.77113151550293\n",
      "[step: 2842] loss: 0.006093291100114584\n",
      "[step: 2843] loss: 24.52847671508789\n",
      "[step: 2843] loss: 0.00609530508518219\n",
      "[step: 2844] loss: 23.638046264648438\n",
      "[step: 2844] loss: 0.00610090559348464\n",
      "[step: 2845] loss: 23.212356567382812\n",
      "[step: 2845] loss: 0.00610692473128438\n",
      "[step: 2846] loss: 24.77027130126953\n",
      "[step: 2846] loss: 0.006110657937824726\n",
      "[step: 2847] loss: 23.586645126342773\n",
      "[step: 2847] loss: 0.0061115981079638\n",
      "[step: 2848] loss: 24.49225425720215\n",
      "[step: 2848] loss: 0.006109100766479969\n",
      "[step: 2849] loss: 23.674049377441406\n",
      "[step: 2849] loss: 0.006104855332523584\n",
      "[step: 2850] loss: 24.39171600341797\n",
      "[step: 2850] loss: 0.0060996850952506065\n",
      "[step: 2851] loss: 23.85903549194336\n",
      "[step: 2851] loss: 0.006095255725085735\n",
      "[step: 2852] loss: 23.424694061279297\n",
      "[step: 2852] loss: 0.006092272698879242\n",
      "[step: 2853] loss: 23.350032806396484\n",
      "[step: 2853] loss: 0.006091011222451925\n",
      "[step: 2854] loss: 23.21048355102539\n",
      "[step: 2854] loss: 0.006091222632676363\n",
      "[step: 2855] loss: 23.317766189575195\n",
      "[step: 2855] loss: 0.006092406809329987\n",
      "[step: 2856] loss: 22.721633911132812\n",
      "[step: 2856] loss: 0.006094058509916067\n",
      "[step: 2857] loss: 22.86520004272461\n",
      "[step: 2857] loss: 0.0060957977548241615\n",
      "[step: 2858] loss: 22.9539794921875\n",
      "[step: 2858] loss: 0.006097453646361828\n",
      "[step: 2859] loss: 22.726539611816406\n",
      "[step: 2859] loss: 0.006098774261772633\n",
      "[step: 2860] loss: 22.676958084106445\n",
      "[step: 2860] loss: 0.006099958438426256\n",
      "[step: 2861] loss: 22.533042907714844\n",
      "[step: 2861] loss: 0.006100814323872328\n",
      "[step: 2862] loss: 22.90554428100586\n",
      "[step: 2862] loss: 0.00610173586755991\n",
      "[step: 2863] loss: 22.725629806518555\n",
      "[step: 2863] loss: 0.006102415267378092\n",
      "[step: 2864] loss: 23.075204849243164\n",
      "[step: 2864] loss: 0.006103376857936382\n",
      "[step: 2865] loss: 23.575122833251953\n",
      "[step: 2865] loss: 0.006104272324591875\n",
      "[step: 2866] loss: 24.773216247558594\n",
      "[step: 2866] loss: 0.006105720531195402\n",
      "[step: 2867] loss: 25.46869659423828\n",
      "[step: 2867] loss: 0.006107232999056578\n",
      "[step: 2868] loss: 26.588045120239258\n",
      "[step: 2868] loss: 0.006109551526606083\n",
      "[step: 2869] loss: 27.32186508178711\n",
      "[step: 2869] loss: 0.006112022325396538\n",
      "[step: 2870] loss: 28.25661849975586\n",
      "[step: 2870] loss: 0.006115688942372799\n",
      "[step: 2871] loss: 29.222766876220703\n",
      "[step: 2871] loss: 0.006119525991380215\n",
      "[step: 2872] loss: 29.778146743774414\n",
      "[step: 2872] loss: 0.006124973297119141\n",
      "[step: 2873] loss: 27.854164123535156\n",
      "[step: 2873] loss: 0.006130311172455549\n",
      "[step: 2874] loss: 25.55074691772461\n",
      "[step: 2874] loss: 0.00613749586045742\n",
      "[step: 2875] loss: 24.029476165771484\n",
      "[step: 2875] loss: 0.006143480539321899\n",
      "[step: 2876] loss: 24.067365646362305\n",
      "[step: 2876] loss: 0.0061506349593400955\n",
      "[step: 2877] loss: 23.915878295898438\n",
      "[step: 2877] loss: 0.006154090631753206\n",
      "[step: 2878] loss: 23.487592697143555\n",
      "[step: 2878] loss: 0.006156604271382093\n",
      "[step: 2879] loss: 22.797210693359375\n",
      "[step: 2879] loss: 0.0061526307836174965\n",
      "[step: 2880] loss: 22.670379638671875\n",
      "[step: 2880] loss: 0.0061458805575966835\n",
      "[step: 2881] loss: 23.303184509277344\n",
      "[step: 2881] loss: 0.006133317016065121\n",
      "[step: 2882] loss: 24.073991775512695\n",
      "[step: 2882] loss: 0.006119746249169111\n",
      "[step: 2883] loss: 25.65191078186035\n",
      "[step: 2883] loss: 0.006105815060436726\n",
      "[step: 2884] loss: 25.47435760498047\n",
      "[step: 2884] loss: 0.006094996817409992\n",
      "[step: 2885] loss: 25.377485275268555\n",
      "[step: 2885] loss: 0.006088257767260075\n",
      "[step: 2886] loss: 25.634410858154297\n",
      "[step: 2886] loss: 0.006085907109081745\n",
      "[step: 2887] loss: 26.521007537841797\n",
      "[step: 2887] loss: 0.006087159272283316\n",
      "[step: 2888] loss: 26.749534606933594\n",
      "[step: 2888] loss: 0.006090859416872263\n",
      "[step: 2889] loss: 25.83755111694336\n",
      "[step: 2889] loss: 0.0060960385017097\n",
      "[step: 2890] loss: 24.31002426147461\n",
      "[step: 2890] loss: 0.0061018140986561775\n",
      "[step: 2891] loss: 23.455671310424805\n",
      "[step: 2891] loss: 0.006108305882662535\n",
      "[step: 2892] loss: 23.46273422241211\n",
      "[step: 2892] loss: 0.00611467519775033\n",
      "[step: 2893] loss: 23.80622100830078\n",
      "[step: 2893] loss: 0.006122150458395481\n",
      "[step: 2894] loss: 23.00493621826172\n",
      "[step: 2894] loss: 0.006128985434770584\n",
      "[step: 2895] loss: 22.325294494628906\n",
      "[step: 2895] loss: 0.006137131713330746\n",
      "[step: 2896] loss: 22.17104721069336\n",
      "[step: 2896] loss: 0.006143052130937576\n",
      "[step: 2897] loss: 22.707168579101562\n",
      "[step: 2897] loss: 0.00614904472604394\n",
      "[step: 2898] loss: 23.636444091796875\n",
      "[step: 2898] loss: 0.00614982470870018\n",
      "[step: 2899] loss: 23.834516525268555\n",
      "[step: 2899] loss: 0.006148207932710648\n",
      "[step: 2900] loss: 24.22270393371582\n",
      "[step: 2900] loss: 0.006139650009572506\n",
      "[step: 2901] loss: 24.43856430053711\n",
      "[step: 2901] loss: 0.006128376815468073\n",
      "[step: 2902] loss: 25.536449432373047\n",
      "[step: 2902] loss: 0.006113813724368811\n",
      "[step: 2903] loss: 26.83428382873535\n",
      "[step: 2903] loss: 0.006100560072809458\n",
      "[step: 2904] loss: 27.767974853515625\n",
      "[step: 2904] loss: 0.006090167444199324\n",
      "[step: 2905] loss: 27.792964935302734\n",
      "[step: 2905] loss: 0.006084374152123928\n",
      "[step: 2906] loss: 27.801280975341797\n",
      "[step: 2906] loss: 0.00608304888010025\n",
      "[step: 2907] loss: 27.52916145324707\n",
      "[step: 2907] loss: 0.006085223983973265\n",
      "[step: 2908] loss: 27.176021575927734\n",
      "[step: 2908] loss: 0.006089629139751196\n",
      "[step: 2909] loss: 24.80552101135254\n",
      "[step: 2909] loss: 0.0060950228944420815\n",
      "[step: 2910] loss: 22.93340301513672\n",
      "[step: 2910] loss: 0.006100904196500778\n",
      "[step: 2911] loss: 22.389347076416016\n",
      "[step: 2911] loss: 0.00610637990757823\n",
      "[step: 2912] loss: 22.761735916137695\n",
      "[step: 2912] loss: 0.006112144328653812\n",
      "[step: 2913] loss: 23.667606353759766\n",
      "[step: 2913] loss: 0.006117002107203007\n",
      "[step: 2914] loss: 23.70616340637207\n",
      "[step: 2914] loss: 0.0061224838718771935\n",
      "[step: 2915] loss: 24.092594146728516\n",
      "[step: 2915] loss: 0.006126491352915764\n",
      "[step: 2916] loss: 24.786741256713867\n",
      "[step: 2916] loss: 0.006131123751401901\n",
      "[step: 2917] loss: 25.471729278564453\n",
      "[step: 2917] loss: 0.006133556365966797\n",
      "[step: 2918] loss: 25.438016891479492\n",
      "[step: 2918] loss: 0.00613646162673831\n",
      "[step: 2919] loss: 24.769495010375977\n",
      "[step: 2919] loss: 0.006137208081781864\n",
      "[step: 2920] loss: 24.049606323242188\n",
      "[step: 2920] loss: 0.006138719618320465\n",
      "[step: 2921] loss: 24.058731079101562\n",
      "[step: 2921] loss: 0.006139750126749277\n",
      "[step: 2922] loss: 24.230497360229492\n",
      "[step: 2922] loss: 0.006141368765383959\n",
      "[step: 2923] loss: 24.97360610961914\n",
      "[step: 2923] loss: 0.00614288542419672\n",
      "[step: 2924] loss: 23.46705436706543\n",
      "[step: 2924] loss: 0.006140139419585466\n",
      "[step: 2925] loss: 22.512575149536133\n",
      "[step: 2925] loss: 0.006131740752607584\n",
      "[step: 2926] loss: 22.415555953979492\n",
      "[step: 2926] loss: 0.006115012802183628\n",
      "[step: 2927] loss: 22.834190368652344\n",
      "[step: 2927] loss: 0.006096988450735807\n",
      "[step: 2928] loss: 23.265029907226562\n",
      "[step: 2928] loss: 0.0060846079140901566\n",
      "[step: 2929] loss: 22.668169021606445\n",
      "[step: 2929] loss: 0.006083667743951082\n",
      "[step: 2930] loss: 22.379276275634766\n",
      "[step: 2930] loss: 0.006092805415391922\n",
      "[step: 2931] loss: 22.4056396484375\n",
      "[step: 2931] loss: 0.00610432168468833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2932] loss: 22.8475341796875\n",
      "[step: 2932] loss: 0.006110548507422209\n",
      "[step: 2933] loss: 23.16326904296875\n",
      "[step: 2933] loss: 0.0061085219494998455\n",
      "[step: 2934] loss: 23.015499114990234\n",
      "[step: 2934] loss: 0.006102685816586018\n",
      "[step: 2935] loss: 22.827312469482422\n",
      "[step: 2935] loss: 0.006099313031882048\n",
      "[step: 2936] loss: 22.992855072021484\n",
      "[step: 2936] loss: 0.006101703736931086\n",
      "[step: 2937] loss: 23.844907760620117\n",
      "[step: 2937] loss: 0.006109921261668205\n",
      "[step: 2938] loss: 25.556961059570312\n",
      "[step: 2938] loss: 0.006119186524301767\n",
      "[step: 2939] loss: 26.709911346435547\n",
      "[step: 2939] loss: 0.006125918123871088\n",
      "[step: 2940] loss: 27.980178833007812\n",
      "[step: 2940] loss: 0.006128646899014711\n",
      "[step: 2941] loss: 28.890594482421875\n",
      "[step: 2941] loss: 0.006130805239081383\n",
      "[step: 2942] loss: 31.382766723632812\n",
      "[step: 2942] loss: 0.006135099567472935\n",
      "[step: 2943] loss: 32.40712356567383\n",
      "[step: 2943] loss: 0.006144105922430754\n",
      "[step: 2944] loss: 31.511478424072266\n",
      "[step: 2944] loss: 0.006152182351797819\n",
      "[step: 2945] loss: 29.086977005004883\n",
      "[step: 2945] loss: 0.006158668082207441\n",
      "[step: 2946] loss: 27.64315414428711\n",
      "[step: 2946] loss: 0.006155140232294798\n",
      "[step: 2947] loss: 26.86501693725586\n",
      "[step: 2947] loss: 0.006146426312625408\n",
      "[step: 2948] loss: 25.9078369140625\n",
      "[step: 2948] loss: 0.006130519788712263\n",
      "[step: 2949] loss: 23.15612030029297\n",
      "[step: 2949] loss: 0.006114806514233351\n",
      "[step: 2950] loss: 24.189281463623047\n",
      "[step: 2950] loss: 0.006099794991314411\n",
      "[step: 2951] loss: 24.54483413696289\n",
      "[step: 2951] loss: 0.006088439840823412\n",
      "[step: 2952] loss: 25.25630760192871\n",
      "[step: 2952] loss: 0.006081038154661655\n",
      "[step: 2953] loss: 27.023849487304688\n",
      "[step: 2953] loss: 0.006078285630792379\n",
      "[step: 2954] loss: 26.887649536132812\n",
      "[step: 2954] loss: 0.006080179940909147\n",
      "[step: 2955] loss: 29.83279037475586\n",
      "[step: 2955] loss: 0.006085610017180443\n",
      "[step: 2956] loss: 29.024761199951172\n",
      "[step: 2956] loss: 0.00609284732490778\n",
      "[step: 2957] loss: 30.054439544677734\n",
      "[step: 2957] loss: 0.006099631544202566\n",
      "[step: 2958] loss: 27.298076629638672\n",
      "[step: 2958] loss: 0.006105996202677488\n",
      "[step: 2959] loss: 25.843143463134766\n",
      "[step: 2959] loss: 0.006111032795161009\n",
      "[step: 2960] loss: 25.21822738647461\n",
      "[step: 2960] loss: 0.006116206757724285\n",
      "[step: 2961] loss: 22.884750366210938\n",
      "[step: 2961] loss: 0.006120151840150356\n",
      "[step: 2962] loss: 24.505016326904297\n",
      "[step: 2962] loss: 0.006123952101916075\n",
      "[step: 2963] loss: 23.953468322753906\n",
      "[step: 2963] loss: 0.006124729756265879\n",
      "[step: 2964] loss: 24.733726501464844\n",
      "[step: 2964] loss: 0.006123834289610386\n",
      "[step: 2965] loss: 23.851276397705078\n",
      "[step: 2965] loss: 0.006118937861174345\n",
      "[step: 2966] loss: 24.410945892333984\n",
      "[step: 2966] loss: 0.0061126104556024075\n",
      "[step: 2967] loss: 24.820566177368164\n",
      "[step: 2967] loss: 0.006103971973061562\n",
      "[step: 2968] loss: 23.777339935302734\n",
      "[step: 2968] loss: 0.006095661781728268\n",
      "[step: 2969] loss: 23.41615867614746\n",
      "[step: 2969] loss: 0.0060875858180224895\n",
      "[step: 2970] loss: 23.075641632080078\n",
      "[step: 2970] loss: 0.0060811699368059635\n",
      "[step: 2971] loss: 23.23090362548828\n",
      "[step: 2971] loss: 0.006076512858271599\n",
      "[step: 2972] loss: 23.03445053100586\n",
      "[step: 2972] loss: 0.006073962897062302\n",
      "[step: 2973] loss: 22.405044555664062\n",
      "[step: 2973] loss: 0.006073306314647198\n",
      "[step: 2974] loss: 22.78780746459961\n",
      "[step: 2974] loss: 0.00607411889359355\n",
      "[step: 2975] loss: 22.36211395263672\n",
      "[step: 2975] loss: 0.006075913552194834\n",
      "[step: 2976] loss: 22.437358856201172\n",
      "[step: 2976] loss: 0.0060782707296311855\n",
      "[step: 2977] loss: 22.0279541015625\n",
      "[step: 2977] loss: 0.006081186700612307\n",
      "[step: 2978] loss: 22.16410255432129\n",
      "[step: 2978] loss: 0.006084655411541462\n",
      "[step: 2979] loss: 22.177860260009766\n",
      "[step: 2979] loss: 0.006089488044381142\n",
      "[step: 2980] loss: 22.074398040771484\n",
      "[step: 2980] loss: 0.006095785181969404\n",
      "[step: 2981] loss: 22.027864456176758\n",
      "[step: 2981] loss: 0.006105184555053711\n",
      "[step: 2982] loss: 22.189300537109375\n",
      "[step: 2982] loss: 0.006117159966379404\n",
      "[step: 2983] loss: 22.7966251373291\n",
      "[step: 2983] loss: 0.0061342353001236916\n",
      "[step: 2984] loss: 23.82253646850586\n",
      "[step: 2984] loss: 0.006153640802949667\n",
      "[step: 2985] loss: 25.277767181396484\n",
      "[step: 2985] loss: 0.00617845356464386\n",
      "[step: 2986] loss: 28.382030487060547\n",
      "[step: 2986] loss: 0.0061989836394786835\n",
      "[step: 2987] loss: 30.227169036865234\n",
      "[step: 2987] loss: 0.006217131856828928\n",
      "[step: 2988] loss: 31.3818359375\n",
      "[step: 2988] loss: 0.006214343011379242\n",
      "[step: 2989] loss: 31.1820068359375\n",
      "[step: 2989] loss: 0.0061960723251104355\n",
      "[step: 2990] loss: 31.74445152282715\n",
      "[step: 2990] loss: 0.006156240124255419\n",
      "[step: 2991] loss: 29.46361541748047\n",
      "[step: 2991] loss: 0.006115035153925419\n",
      "[step: 2992] loss: 26.286056518554688\n",
      "[step: 2992] loss: 0.006085007451474667\n",
      "[step: 2993] loss: 23.338176727294922\n",
      "[step: 2993] loss: 0.0060765803791582584\n",
      "[step: 2994] loss: 24.09233856201172\n",
      "[step: 2994] loss: 0.006086321081966162\n",
      "[step: 2995] loss: 25.032976150512695\n",
      "[step: 2995] loss: 0.006104881875216961\n",
      "[step: 2996] loss: 24.389997482299805\n",
      "[step: 2996] loss: 0.006121603772044182\n",
      "[step: 2997] loss: 24.17318344116211\n",
      "[step: 2997] loss: 0.006127119995653629\n",
      "[step: 2998] loss: 24.579524993896484\n",
      "[step: 2998] loss: 0.006122010760009289\n",
      "[step: 2999] loss: 27.87149429321289\n",
      "[step: 2999] loss: 0.0061066108755767345\n",
      "[step: 3000] loss: 27.24264144897461\n",
      "[step: 3000] loss: 0.006090220529586077\n",
      "[step: 3001] loss: 27.201828002929688\n",
      "[step: 3001] loss: 0.006077611818909645\n",
      "[step: 3002] loss: 25.839384078979492\n",
      "[step: 3002] loss: 0.006072738207876682\n",
      "[step: 3003] loss: 26.441471099853516\n",
      "[step: 3003] loss: 0.006075236480683088\n",
      "[step: 3004] loss: 25.87415885925293\n",
      "[step: 3004] loss: 0.00608207518234849\n",
      "[step: 3005] loss: 23.66360092163086\n",
      "[step: 3005] loss: 0.006089451257139444\n",
      "[step: 3006] loss: 23.242090225219727\n",
      "[step: 3006] loss: 0.006093727424740791\n",
      "[step: 3007] loss: 23.287036895751953\n",
      "[step: 3007] loss: 0.0060943663120269775\n",
      "[step: 3008] loss: 23.46904754638672\n",
      "[step: 3008] loss: 0.006091081537306309\n",
      "[step: 3009] loss: 23.5423583984375\n",
      "[step: 3009] loss: 0.006086458917707205\n",
      "[step: 3010] loss: 23.56052017211914\n",
      "[step: 3010] loss: 0.006081562489271164\n",
      "[step: 3011] loss: 23.34865379333496\n",
      "[step: 3011] loss: 0.006077837664633989\n",
      "[step: 3012] loss: 23.763166427612305\n",
      "[step: 3012] loss: 0.006075620651245117\n",
      "[step: 3013] loss: 24.110328674316406\n",
      "[step: 3013] loss: 0.006074641831219196\n",
      "[step: 3014] loss: 24.21334457397461\n",
      "[step: 3014] loss: 0.0060751489363610744\n",
      "[step: 3015] loss: 23.84231185913086\n",
      "[step: 3015] loss: 0.006076584570109844\n",
      "[step: 3016] loss: 23.424087524414062\n",
      "[step: 3016] loss: 0.006079103332012892\n",
      "[step: 3017] loss: 23.329755783081055\n",
      "[step: 3017] loss: 0.006081760860979557\n",
      "[step: 3018] loss: 23.328857421875\n",
      "[step: 3018] loss: 0.006084131542593241\n",
      "[step: 3019] loss: 22.54266929626465\n",
      "[step: 3019] loss: 0.006085268687456846\n",
      "[step: 3020] loss: 22.087398529052734\n",
      "[step: 3020] loss: 0.006084885913878679\n",
      "[step: 3021] loss: 22.02297592163086\n",
      "[step: 3021] loss: 0.0060829343274235725\n",
      "[step: 3022] loss: 22.11672019958496\n",
      "[step: 3022] loss: 0.006080273073166609\n",
      "[step: 3023] loss: 21.88298988342285\n",
      "[step: 3023] loss: 0.006077304016798735\n",
      "[step: 3024] loss: 21.596860885620117\n",
      "[step: 3024] loss: 0.006075634155422449\n",
      "[step: 3025] loss: 21.562786102294922\n",
      "[step: 3025] loss: 0.006075752433389425\n",
      "[step: 3026] loss: 21.689329147338867\n",
      "[step: 3026] loss: 0.006078455597162247\n",
      "[step: 3027] loss: 21.666030883789062\n",
      "[step: 3027] loss: 0.00608412642031908\n",
      "[step: 3028] loss: 21.54615020751953\n",
      "[step: 3028] loss: 0.006092695984989405\n",
      "[step: 3029] loss: 21.689083099365234\n",
      "[step: 3029] loss: 0.006105212960392237\n",
      "[step: 3030] loss: 22.17523193359375\n",
      "[step: 3030] loss: 0.006120841484516859\n",
      "[step: 3031] loss: 23.004661560058594\n",
      "[step: 3031] loss: 0.006142726168036461\n",
      "[step: 3032] loss: 25.516254425048828\n",
      "[step: 3032] loss: 0.006166957318782806\n",
      "[step: 3033] loss: 29.191204071044922\n",
      "[step: 3033] loss: 0.006198490969836712\n",
      "[step: 3034] loss: 34.845306396484375\n",
      "[step: 3034] loss: 0.006223401520401239\n",
      "[step: 3035] loss: 36.87841796875\n",
      "[step: 3035] loss: 0.006244363263249397\n",
      "[step: 3036] loss: 47.326026916503906\n",
      "[step: 3036] loss: 0.0062371776439249516\n",
      "[step: 3037] loss: 52.557228088378906\n",
      "[step: 3037] loss: 0.006207308266311884\n",
      "[step: 3038] loss: 59.25157165527344\n",
      "[step: 3038] loss: 0.006154132541269064\n",
      "[step: 3039] loss: 53.356937408447266\n",
      "[step: 3039] loss: 0.006102652754634619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3040] loss: 69.57542419433594\n",
      "[step: 3040] loss: 0.006075557321310043\n",
      "[step: 3041] loss: 47.06840515136719\n",
      "[step: 3041] loss: 0.006080976687371731\n",
      "[step: 3042] loss: 46.6503791809082\n",
      "[step: 3042] loss: 0.00610774289816618\n",
      "[step: 3043] loss: 42.16936492919922\n",
      "[step: 3043] loss: 0.006132747512310743\n",
      "[step: 3044] loss: 48.2463493347168\n",
      "[step: 3044] loss: 0.0061395904049277306\n",
      "[step: 3045] loss: 38.01585388183594\n",
      "[step: 3045] loss: 0.006121152080595493\n",
      "[step: 3046] loss: 49.85065460205078\n",
      "[step: 3046] loss: 0.006092574447393417\n",
      "[step: 3047] loss: 32.195838928222656\n",
      "[step: 3047] loss: 0.006070110481232405\n",
      "[step: 3048] loss: 35.43220901489258\n",
      "[step: 3048] loss: 0.0060648685321211815\n",
      "[step: 3049] loss: 36.68815994262695\n",
      "[step: 3049] loss: 0.006074624601751566\n",
      "[step: 3050] loss: 34.467918395996094\n",
      "[step: 3050] loss: 0.006089328322559595\n",
      "[step: 3051] loss: 33.073333740234375\n",
      "[step: 3051] loss: 0.006099247373640537\n",
      "[step: 3052] loss: 30.847225189208984\n",
      "[step: 3052] loss: 0.006098848767578602\n",
      "[step: 3053] loss: 31.913761138916016\n",
      "[step: 3053] loss: 0.006091852206736803\n",
      "[step: 3054] loss: 29.61626434326172\n",
      "[step: 3054] loss: 0.006082844454795122\n",
      "[step: 3055] loss: 32.51874542236328\n",
      "[step: 3055] loss: 0.006077826954424381\n",
      "[step: 3056] loss: 28.05522918701172\n",
      "[step: 3056] loss: 0.0060775792226195335\n",
      "[step: 3057] loss: 28.375995635986328\n",
      "[step: 3057] loss: 0.0060799457132816315\n",
      "[step: 3058] loss: 26.57746124267578\n",
      "[step: 3058] loss: 0.006080251652747393\n",
      "[step: 3059] loss: 27.745325088500977\n",
      "[step: 3059] loss: 0.006076917517930269\n",
      "[step: 3060] loss: 27.93285369873047\n",
      "[step: 3060] loss: 0.006070425268262625\n",
      "[step: 3061] loss: 25.257814407348633\n",
      "[step: 3061] loss: 0.006064359564334154\n",
      "[step: 3062] loss: 25.84465789794922\n",
      "[step: 3062] loss: 0.0060614896938204765\n",
      "[step: 3063] loss: 24.420654296875\n",
      "[step: 3063] loss: 0.006062597036361694\n",
      "[step: 3064] loss: 25.49736785888672\n",
      "[step: 3064] loss: 0.0060662273317575455\n",
      "[step: 3065] loss: 24.81771469116211\n",
      "[step: 3065] loss: 0.0060699712485075\n",
      "[step: 3066] loss: 24.06955909729004\n",
      "[step: 3066] loss: 0.006071834824979305\n",
      "[step: 3067] loss: 23.70366859436035\n",
      "[step: 3067] loss: 0.006071196403354406\n",
      "[step: 3068] loss: 23.501911163330078\n",
      "[step: 3068] loss: 0.006069183349609375\n",
      "[step: 3069] loss: 23.364501953125\n",
      "[step: 3069] loss: 0.006067348178476095\n",
      "[step: 3070] loss: 23.400453567504883\n",
      "[step: 3070] loss: 0.006067101843655109\n",
      "[step: 3071] loss: 23.61875343322754\n",
      "[step: 3071] loss: 0.006068793591111898\n",
      "[step: 3072] loss: 23.049320220947266\n",
      "[step: 3072] loss: 0.006072284188121557\n",
      "[step: 3073] loss: 22.844024658203125\n",
      "[step: 3073] loss: 0.006076607387512922\n",
      "[step: 3074] loss: 22.590072631835938\n",
      "[step: 3074] loss: 0.006081784144043922\n",
      "[step: 3075] loss: 22.144207000732422\n",
      "[step: 3075] loss: 0.006087048444896936\n",
      "[step: 3076] loss: 22.28549575805664\n",
      "[step: 3076] loss: 0.006094174925237894\n",
      "[step: 3077] loss: 22.41879653930664\n",
      "[step: 3077] loss: 0.006102834828197956\n",
      "[step: 3078] loss: 22.01406478881836\n",
      "[step: 3078] loss: 0.006115405820310116\n",
      "[step: 3079] loss: 22.46617889404297\n",
      "[step: 3079] loss: 0.006130437832325697\n",
      "[step: 3080] loss: 22.226234436035156\n",
      "[step: 3080] loss: 0.006148823071271181\n",
      "[step: 3081] loss: 22.598140716552734\n",
      "[step: 3081] loss: 0.0061652460135519505\n",
      "[step: 3082] loss: 22.737266540527344\n",
      "[step: 3082] loss: 0.006177082192152739\n",
      "[step: 3083] loss: 22.819969177246094\n",
      "[step: 3083] loss: 0.006174529902637005\n",
      "[step: 3084] loss: 23.179828643798828\n",
      "[step: 3084] loss: 0.006157595664262772\n",
      "[step: 3085] loss: 24.08578109741211\n",
      "[step: 3085] loss: 0.006126001011580229\n",
      "[step: 3086] loss: 24.55331802368164\n",
      "[step: 3086] loss: 0.006094280630350113\n",
      "[step: 3087] loss: 24.30834197998047\n",
      "[step: 3087] loss: 0.006072532385587692\n",
      "[step: 3088] loss: 23.2602481842041\n",
      "[step: 3088] loss: 0.006066764704883099\n",
      "[step: 3089] loss: 22.45114517211914\n",
      "[step: 3089] loss: 0.006071713287383318\n",
      "[step: 3090] loss: 21.869234085083008\n",
      "[step: 3090] loss: 0.006078989244997501\n",
      "[step: 3091] loss: 21.53402328491211\n",
      "[step: 3091] loss: 0.006082773208618164\n",
      "[step: 3092] loss: 21.886381149291992\n",
      "[step: 3092] loss: 0.006083390675485134\n",
      "[step: 3093] loss: 22.420486450195312\n",
      "[step: 3093] loss: 0.006084943190217018\n",
      "[step: 3094] loss: 22.943729400634766\n",
      "[step: 3094] loss: 0.006088697351515293\n",
      "[step: 3095] loss: 22.676355361938477\n",
      "[step: 3095] loss: 0.0060941600240767\n",
      "[step: 3096] loss: 22.449962615966797\n",
      "[step: 3096] loss: 0.006097026169300079\n",
      "[step: 3097] loss: 22.03299331665039\n",
      "[step: 3097] loss: 0.006093668285757303\n",
      "[step: 3098] loss: 21.73487091064453\n",
      "[step: 3098] loss: 0.006084916181862354\n",
      "[step: 3099] loss: 21.36050033569336\n",
      "[step: 3099] loss: 0.006074216682463884\n",
      "[step: 3100] loss: 21.253894805908203\n",
      "[step: 3100] loss: 0.006066317204385996\n",
      "[step: 3101] loss: 21.25257110595703\n",
      "[step: 3101] loss: 0.006063329521566629\n",
      "[step: 3102] loss: 21.499624252319336\n",
      "[step: 3102] loss: 0.006063312292098999\n",
      "[step: 3103] loss: 21.779747009277344\n",
      "[step: 3103] loss: 0.006063095759600401\n",
      "[step: 3104] loss: 22.182392120361328\n",
      "[step: 3104] loss: 0.006061115302145481\n",
      "[step: 3105] loss: 22.89983367919922\n",
      "[step: 3105] loss: 0.006057891994714737\n",
      "[step: 3106] loss: 23.316335678100586\n",
      "[step: 3106] loss: 0.006055583246052265\n",
      "[step: 3107] loss: 23.44315528869629\n",
      "[step: 3107] loss: 0.006055410020053387\n",
      "[step: 3108] loss: 23.209726333618164\n",
      "[step: 3108] loss: 0.006057160906493664\n",
      "[step: 3109] loss: 22.822017669677734\n",
      "[step: 3109] loss: 0.006059515755623579\n",
      "[step: 3110] loss: 22.192279815673828\n",
      "[step: 3110] loss: 0.006061424035578966\n",
      "[step: 3111] loss: 21.528018951416016\n",
      "[step: 3111] loss: 0.006062665488570929\n",
      "[step: 3112] loss: 21.161972045898438\n",
      "[step: 3112] loss: 0.0060639590956270695\n",
      "[step: 3113] loss: 21.100753784179688\n",
      "[step: 3113] loss: 0.006066429894417524\n",
      "[step: 3114] loss: 21.24102783203125\n",
      "[step: 3114] loss: 0.006071166135370731\n",
      "[step: 3115] loss: 21.3917236328125\n",
      "[step: 3115] loss: 0.006078526843339205\n",
      "[step: 3116] loss: 21.505157470703125\n",
      "[step: 3116] loss: 0.006089502479881048\n",
      "[step: 3117] loss: 21.71272850036621\n",
      "[step: 3117] loss: 0.0061040399596095085\n",
      "[step: 3118] loss: 22.242877960205078\n",
      "[step: 3118] loss: 0.006124288309365511\n",
      "[step: 3119] loss: 22.87039566040039\n",
      "[step: 3119] loss: 0.006148441694676876\n",
      "[step: 3120] loss: 24.193416595458984\n",
      "[step: 3120] loss: 0.006180040538311005\n",
      "[step: 3121] loss: 24.21169662475586\n",
      "[step: 3121] loss: 0.00620719138532877\n",
      "[step: 3122] loss: 23.872652053833008\n",
      "[step: 3122] loss: 0.006231255829334259\n",
      "[step: 3123] loss: 23.307403564453125\n",
      "[step: 3123] loss: 0.006225460208952427\n",
      "[step: 3124] loss: 23.113361358642578\n",
      "[step: 3124] loss: 0.006195886991918087\n",
      "[step: 3125] loss: 22.66187858581543\n",
      "[step: 3125] loss: 0.006139155477285385\n",
      "[step: 3126] loss: 21.8100528717041\n",
      "[step: 3126] loss: 0.006086637265980244\n",
      "[step: 3127] loss: 21.113540649414062\n",
      "[step: 3127] loss: 0.006059818435460329\n",
      "[step: 3128] loss: 21.158344268798828\n",
      "[step: 3128] loss: 0.0060658883303403854\n",
      "[step: 3129] loss: 21.75867462158203\n",
      "[step: 3129] loss: 0.006090753711760044\n",
      "[step: 3130] loss: 22.200332641601562\n",
      "[step: 3130] loss: 0.006113896612077951\n",
      "[step: 3131] loss: 22.201414108276367\n",
      "[step: 3131] loss: 0.006122342310845852\n",
      "[step: 3132] loss: 22.05661392211914\n",
      "[step: 3132] loss: 0.006110244430601597\n",
      "[step: 3133] loss: 22.529125213623047\n",
      "[step: 3133] loss: 0.006089136470109224\n",
      "[step: 3134] loss: 23.51207733154297\n",
      "[step: 3134] loss: 0.006069027353078127\n",
      "[step: 3135] loss: 25.366539001464844\n",
      "[step: 3135] loss: 0.006059229373931885\n",
      "[step: 3136] loss: 24.718164443969727\n",
      "[step: 3136] loss: 0.006059760693460703\n",
      "[step: 3137] loss: 23.66016387939453\n",
      "[step: 3137] loss: 0.006067194044589996\n",
      "[step: 3138] loss: 23.482494354248047\n",
      "[step: 3138] loss: 0.0060764942318201065\n",
      "[step: 3139] loss: 23.864635467529297\n",
      "[step: 3139] loss: 0.006082845386117697\n",
      "[step: 3140] loss: 23.346643447875977\n",
      "[step: 3140] loss: 0.006084790453314781\n",
      "[step: 3141] loss: 21.867252349853516\n",
      "[step: 3141] loss: 0.006080461200326681\n",
      "[step: 3142] loss: 21.065593719482422\n",
      "[step: 3142] loss: 0.0060726613737642765\n",
      "[step: 3143] loss: 21.611831665039062\n",
      "[step: 3143] loss: 0.006063314154744148\n",
      "[step: 3144] loss: 22.143484115600586\n",
      "[step: 3144] loss: 0.00605581421405077\n",
      "[step: 3145] loss: 22.066547393798828\n",
      "[step: 3145] loss: 0.006051704753190279\n",
      "[step: 3146] loss: 21.544527053833008\n",
      "[step: 3146] loss: 0.006051650270819664\n",
      "[step: 3147] loss: 21.497732162475586\n",
      "[step: 3147] loss: 0.006054789759218693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3148] loss: 22.364013671875\n",
      "[step: 3148] loss: 0.006059376988559961\n",
      "[step: 3149] loss: 23.177268981933594\n",
      "[step: 3149] loss: 0.006063676439225674\n",
      "[step: 3150] loss: 24.18744468688965\n",
      "[step: 3150] loss: 0.006065861787647009\n",
      "[step: 3151] loss: 23.762470245361328\n",
      "[step: 3151] loss: 0.006065879948437214\n",
      "[step: 3152] loss: 24.115314483642578\n",
      "[step: 3152] loss: 0.006063748151063919\n",
      "[step: 3153] loss: 25.07845115661621\n",
      "[step: 3153] loss: 0.006060741376131773\n",
      "[step: 3154] loss: 25.39157485961914\n",
      "[step: 3154] loss: 0.006057522725313902\n",
      "[step: 3155] loss: 24.352996826171875\n",
      "[step: 3155] loss: 0.006054730620235205\n",
      "[step: 3156] loss: 23.310680389404297\n",
      "[step: 3156] loss: 0.006052676122635603\n",
      "[step: 3157] loss: 22.881210327148438\n",
      "[step: 3157] loss: 0.006051066797226667\n",
      "[step: 3158] loss: 23.317432403564453\n",
      "[step: 3158] loss: 0.006050013471394777\n",
      "[step: 3159] loss: 22.374038696289062\n",
      "[step: 3159] loss: 0.006049058865755796\n",
      "[step: 3160] loss: 21.595596313476562\n",
      "[step: 3160] loss: 0.006048370618373156\n",
      "[step: 3161] loss: 21.30817413330078\n",
      "[step: 3161] loss: 0.006047824863344431\n",
      "[step: 3162] loss: 21.644935607910156\n",
      "[step: 3162] loss: 0.006047465372830629\n",
      "[step: 3163] loss: 21.73705291748047\n",
      "[step: 3163] loss: 0.006047239992767572\n",
      "[step: 3164] loss: 21.656757354736328\n",
      "[step: 3164] loss: 0.0060471403412520885\n",
      "[step: 3165] loss: 21.728351593017578\n",
      "[step: 3165] loss: 0.006047144997864962\n",
      "[step: 3166] loss: 22.189979553222656\n",
      "[step: 3166] loss: 0.00604713661596179\n",
      "[step: 3167] loss: 22.710731506347656\n",
      "[step: 3167] loss: 0.006047210656106472\n",
      "[step: 3168] loss: 23.05144500732422\n",
      "[step: 3168] loss: 0.0060472646728158\n",
      "[step: 3169] loss: 23.462968826293945\n",
      "[step: 3169] loss: 0.006047564558684826\n",
      "[step: 3170] loss: 23.973838806152344\n",
      "[step: 3170] loss: 0.006048141047358513\n",
      "[step: 3171] loss: 25.15106201171875\n",
      "[step: 3171] loss: 0.006049391347914934\n",
      "[step: 3172] loss: 25.819759368896484\n",
      "[step: 3172] loss: 0.006051683332771063\n",
      "[step: 3173] loss: 26.824209213256836\n",
      "[step: 3173] loss: 0.006055654026567936\n",
      "[step: 3174] loss: 25.77162742614746\n",
      "[step: 3174] loss: 0.0060625821352005005\n",
      "[step: 3175] loss: 25.652130126953125\n",
      "[step: 3175] loss: 0.00607385765761137\n",
      "[step: 3176] loss: 25.540172576904297\n",
      "[step: 3176] loss: 0.006093299947679043\n",
      "[step: 3177] loss: 24.113506317138672\n",
      "[step: 3177] loss: 0.006123421248048544\n",
      "[step: 3178] loss: 21.876953125\n",
      "[step: 3178] loss: 0.006173527333885431\n",
      "[step: 3179] loss: 21.48874282836914\n",
      "[step: 3179] loss: 0.006239726208150387\n",
      "[step: 3180] loss: 21.989208221435547\n",
      "[step: 3180] loss: 0.006329572293907404\n",
      "[step: 3181] loss: 22.881412506103516\n",
      "[step: 3181] loss: 0.006391457747668028\n",
      "[step: 3182] loss: 22.034265518188477\n",
      "[step: 3182] loss: 0.006399862002581358\n",
      "[step: 3183] loss: 22.21941375732422\n",
      "[step: 3183] loss: 0.006293200422078371\n",
      "[step: 3184] loss: 23.21087646484375\n",
      "[step: 3184] loss: 0.006144579034298658\n",
      "[step: 3185] loss: 24.20435905456543\n",
      "[step: 3185] loss: 0.006065279245376587\n",
      "[step: 3186] loss: 25.17862319946289\n",
      "[step: 3186] loss: 0.006104889791458845\n",
      "[step: 3187] loss: 24.458776473999023\n",
      "[step: 3187] loss: 0.0061796135269105434\n",
      "[step: 3188] loss: 24.47152328491211\n",
      "[step: 3188] loss: 0.006194327957928181\n",
      "[step: 3189] loss: 24.77503204345703\n",
      "[step: 3189] loss: 0.006131005007773638\n",
      "[step: 3190] loss: 24.244266510009766\n",
      "[step: 3190] loss: 0.006071602925658226\n",
      "[step: 3191] loss: 22.4848690032959\n",
      "[step: 3191] loss: 0.006081609986722469\n",
      "[step: 3192] loss: 21.599361419677734\n",
      "[step: 3192] loss: 0.006123992148786783\n",
      "[step: 3193] loss: 21.37544822692871\n",
      "[step: 3193] loss: 0.006136038340628147\n",
      "[step: 3194] loss: 21.964664459228516\n",
      "[step: 3194] loss: 0.006094761658459902\n",
      "[step: 3195] loss: 21.597660064697266\n",
      "[step: 3195] loss: 0.0060568638145923615\n",
      "[step: 3196] loss: 21.801599502563477\n",
      "[step: 3196] loss: 0.006063472013920546\n",
      "[step: 3197] loss: 22.403539657592773\n",
      "[step: 3197] loss: 0.006091143935918808\n",
      "[step: 3198] loss: 23.22806167602539\n",
      "[step: 3198] loss: 0.006098809652030468\n",
      "[step: 3199] loss: 25.49019432067871\n",
      "[step: 3199] loss: 0.006070793140679598\n",
      "[step: 3200] loss: 26.289073944091797\n",
      "[step: 3200] loss: 0.006049595773220062\n",
      "[step: 3201] loss: 27.176673889160156\n",
      "[step: 3201] loss: 0.006059323437511921\n",
      "[step: 3202] loss: 27.93659019470215\n",
      "[step: 3202] loss: 0.006074167788028717\n",
      "[step: 3203] loss: 27.62818145751953\n",
      "[step: 3203] loss: 0.006070893257856369\n",
      "[step: 3204] loss: 24.74814796447754\n",
      "[step: 3204] loss: 0.006052686832845211\n",
      "[step: 3205] loss: 23.277729034423828\n",
      "[step: 3205] loss: 0.006047418806701899\n",
      "[step: 3206] loss: 22.08684539794922\n",
      "[step: 3206] loss: 0.006057652644813061\n",
      "[step: 3207] loss: 22.587963104248047\n",
      "[step: 3207] loss: 0.0060615320689976215\n",
      "[step: 3208] loss: 21.6309814453125\n",
      "[step: 3208] loss: 0.006054343190044165\n",
      "[step: 3209] loss: 22.28463363647461\n",
      "[step: 3209] loss: 0.006044748704880476\n",
      "[step: 3210] loss: 24.191574096679688\n",
      "[step: 3210] loss: 0.00604636175557971\n",
      "[step: 3211] loss: 25.360422134399414\n",
      "[step: 3211] loss: 0.006053714547306299\n",
      "[step: 3212] loss: 25.660627365112305\n",
      "[step: 3212] loss: 0.006052759476006031\n",
      "[step: 3213] loss: 25.13970184326172\n",
      "[step: 3213] loss: 0.00604625977575779\n",
      "[step: 3214] loss: 25.155956268310547\n",
      "[step: 3214] loss: 0.006041611544787884\n",
      "[step: 3215] loss: 24.022266387939453\n",
      "[step: 3215] loss: 0.006044328678399324\n",
      "[step: 3216] loss: 23.007217407226562\n",
      "[step: 3216] loss: 0.006048534531146288\n",
      "[step: 3217] loss: 21.118459701538086\n",
      "[step: 3217] loss: 0.00604677852243185\n",
      "[step: 3218] loss: 21.455341339111328\n",
      "[step: 3218] loss: 0.006042197346687317\n",
      "[step: 3219] loss: 21.476097106933594\n",
      "[step: 3219] loss: 0.0060399603098630905\n",
      "[step: 3220] loss: 22.441495895385742\n",
      "[step: 3220] loss: 0.006042109336704016\n",
      "[step: 3221] loss: 22.560047149658203\n",
      "[step: 3221] loss: 0.0060441019013524055\n",
      "[step: 3222] loss: 23.491443634033203\n",
      "[step: 3222] loss: 0.006042380351573229\n",
      "[step: 3223] loss: 23.978893280029297\n",
      "[step: 3223] loss: 0.006039694882929325\n",
      "[step: 3224] loss: 23.906585693359375\n",
      "[step: 3224] loss: 0.006038946565240622\n",
      "[step: 3225] loss: 24.471925735473633\n",
      "[step: 3225] loss: 0.006040358450263739\n",
      "[step: 3226] loss: 23.417877197265625\n",
      "[step: 3226] loss: 0.006040811538696289\n",
      "[step: 3227] loss: 23.442649841308594\n",
      "[step: 3227] loss: 0.0060394080355763435\n",
      "[step: 3228] loss: 22.477663040161133\n",
      "[step: 3228] loss: 0.00603812001645565\n",
      "[step: 3229] loss: 22.526622772216797\n",
      "[step: 3229] loss: 0.006037828978151083\n",
      "[step: 3230] loss: 21.360090255737305\n",
      "[step: 3230] loss: 0.006038568913936615\n",
      "[step: 3231] loss: 21.951597213745117\n",
      "[step: 3231] loss: 0.0060387179255485535\n",
      "[step: 3232] loss: 21.45073890686035\n",
      "[step: 3232] loss: 0.0060380627401173115\n",
      "[step: 3233] loss: 22.019920349121094\n",
      "[step: 3233] loss: 0.006037430372089148\n",
      "[step: 3234] loss: 21.77617835998535\n",
      "[step: 3234] loss: 0.006037302315235138\n",
      "[step: 3235] loss: 22.69689178466797\n",
      "[step: 3235] loss: 0.006038039922714233\n",
      "[step: 3236] loss: 23.613842010498047\n",
      "[step: 3236] loss: 0.0060392701998353004\n",
      "[step: 3237] loss: 24.135231018066406\n",
      "[step: 3237] loss: 0.006041272077709436\n",
      "[step: 3238] loss: 24.711933135986328\n",
      "[step: 3238] loss: 0.006045024376362562\n",
      "[step: 3239] loss: 25.02753448486328\n",
      "[step: 3239] loss: 0.006052549462765455\n",
      "[step: 3240] loss: 26.162670135498047\n",
      "[step: 3240] loss: 0.006067852023988962\n",
      "[step: 3241] loss: 25.095495223999023\n",
      "[step: 3241] loss: 0.006097966339439154\n",
      "[step: 3242] loss: 24.732948303222656\n",
      "[step: 3242] loss: 0.006153739523142576\n",
      "[step: 3243] loss: 22.7020263671875\n",
      "[step: 3243] loss: 0.00625133141875267\n",
      "[step: 3244] loss: 22.51880645751953\n",
      "[step: 3244] loss: 0.006399000529199839\n",
      "[step: 3245] loss: 21.083797454833984\n",
      "[step: 3245] loss: 0.006559155415743589\n",
      "[step: 3246] loss: 21.940340042114258\n",
      "[step: 3246] loss: 0.0066057913936674595\n",
      "[step: 3247] loss: 21.6895751953125\n",
      "[step: 3247] loss: 0.006417767144739628\n",
      "[step: 3248] loss: 23.183940887451172\n",
      "[step: 3248] loss: 0.006158269476145506\n",
      "[step: 3249] loss: 23.2098388671875\n",
      "[step: 3249] loss: 0.006116252858191729\n",
      "[step: 3250] loss: 24.686641693115234\n",
      "[step: 3250] loss: 0.006243150215595961\n",
      "[step: 3251] loss: 26.57010269165039\n",
      "[step: 3251] loss: 0.00623505050316453\n",
      "[step: 3252] loss: 27.441463470458984\n",
      "[step: 3252] loss: 0.006143895909190178\n",
      "[step: 3253] loss: 28.073745727539062\n",
      "[step: 3253] loss: 0.006152169778943062\n",
      "[step: 3254] loss: 27.432029724121094\n",
      "[step: 3254] loss: 0.00616556266322732\n",
      "[step: 3255] loss: 27.789785385131836\n",
      "[step: 3255] loss: 0.006104977801442146\n",
      "[step: 3256] loss: 24.364564895629883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3256] loss: 0.0060908980667591095\n",
      "[step: 3257] loss: 23.853443145751953\n",
      "[step: 3257] loss: 0.006145576015114784\n",
      "[step: 3258] loss: 21.587003707885742\n",
      "[step: 3258] loss: 0.006133877206593752\n",
      "[step: 3259] loss: 22.801124572753906\n",
      "[step: 3259] loss: 0.006084260065108538\n",
      "[step: 3260] loss: 22.272476196289062\n",
      "[step: 3260] loss: 0.006083576940000057\n",
      "[step: 3261] loss: 24.410106658935547\n",
      "[step: 3261] loss: 0.0060790106654167175\n",
      "[step: 3262] loss: 24.098634719848633\n",
      "[step: 3262] loss: 0.006083650980144739\n",
      "[step: 3263] loss: 26.289426803588867\n",
      "[step: 3263] loss: 0.006096040830016136\n",
      "[step: 3264] loss: 26.338754653930664\n",
      "[step: 3264] loss: 0.0060681747272610664\n",
      "[step: 3265] loss: 26.52398681640625\n",
      "[step: 3265] loss: 0.0060661290772259235\n",
      "[step: 3266] loss: 24.663480758666992\n",
      "[step: 3266] loss: 0.006081469357013702\n",
      "[step: 3267] loss: 23.007293701171875\n",
      "[step: 3267] loss: 0.006049738265573978\n",
      "[step: 3268] loss: 21.434974670410156\n",
      "[step: 3268] loss: 0.006055892910808325\n",
      "[step: 3269] loss: 22.245365142822266\n",
      "[step: 3269] loss: 0.0060813105665147305\n",
      "[step: 3270] loss: 22.60535430908203\n",
      "[step: 3270] loss: 0.006056137382984161\n",
      "[step: 3271] loss: 22.911041259765625\n",
      "[step: 3271] loss: 0.0060502574779093266\n",
      "[step: 3272] loss: 21.688915252685547\n",
      "[step: 3272] loss: 0.006061011925339699\n",
      "[step: 3273] loss: 21.675155639648438\n",
      "[step: 3273] loss: 0.006047275848686695\n",
      "[step: 3274] loss: 21.5755672454834\n",
      "[step: 3274] loss: 0.006044148467481136\n",
      "[step: 3275] loss: 22.156522750854492\n",
      "[step: 3275] loss: 0.006053869146853685\n",
      "[step: 3276] loss: 21.61754608154297\n",
      "[step: 3276] loss: 0.0060514588840305805\n",
      "[step: 3277] loss: 21.24192237854004\n",
      "[step: 3277] loss: 0.006045457441359758\n",
      "[step: 3278] loss: 20.91282081604004\n",
      "[step: 3278] loss: 0.006046212278306484\n",
      "[step: 3279] loss: 21.165828704833984\n",
      "[step: 3279] loss: 0.0060436236672103405\n",
      "[step: 3280] loss: 21.201343536376953\n",
      "[step: 3280] loss: 0.006039203144609928\n",
      "[step: 3281] loss: 21.158756256103516\n",
      "[step: 3281] loss: 0.006039848551154137\n",
      "[step: 3282] loss: 21.04029083251953\n",
      "[step: 3282] loss: 0.006040350534021854\n",
      "[step: 3283] loss: 20.975231170654297\n",
      "[step: 3283] loss: 0.006039384286850691\n",
      "[step: 3284] loss: 20.781421661376953\n",
      "[step: 3284] loss: 0.006040304899215698\n",
      "[step: 3285] loss: 20.83765983581543\n",
      "[step: 3285] loss: 0.006038394290953875\n",
      "[step: 3286] loss: 21.14715576171875\n",
      "[step: 3286] loss: 0.006036247592419386\n",
      "[step: 3287] loss: 21.66412925720215\n",
      "[step: 3287] loss: 0.006037349812686443\n",
      "[step: 3288] loss: 21.95662498474121\n",
      "[step: 3288] loss: 0.0060343933291733265\n",
      "[step: 3289] loss: 22.496536254882812\n",
      "[step: 3289] loss: 0.006032068282365799\n",
      "[step: 3290] loss: 23.891212463378906\n",
      "[step: 3290] loss: 0.006035128608345985\n",
      "[step: 3291] loss: 26.851354598999023\n",
      "[step: 3291] loss: 0.0060338219627738\n",
      "[step: 3292] loss: 29.921092987060547\n",
      "[step: 3292] loss: 0.006031411234289408\n",
      "[step: 3293] loss: 32.605751037597656\n",
      "[step: 3293] loss: 0.006033933721482754\n",
      "[step: 3294] loss: 32.458675384521484\n",
      "[step: 3294] loss: 0.006033937446773052\n",
      "[step: 3295] loss: 36.461952209472656\n",
      "[step: 3295] loss: 0.006031271535903215\n",
      "[step: 3296] loss: 39.26288986206055\n",
      "[step: 3296] loss: 0.006031660828739405\n",
      "[step: 3297] loss: 33.79954528808594\n",
      "[step: 3297] loss: 0.006032162811607122\n",
      "[step: 3298] loss: 29.537612915039062\n",
      "[step: 3298] loss: 0.006030473858118057\n",
      "[step: 3299] loss: 29.909648895263672\n",
      "[step: 3299] loss: 0.006029469892382622\n",
      "[step: 3300] loss: 24.963333129882812\n",
      "[step: 3300] loss: 0.006029967218637466\n",
      "[step: 3301] loss: 26.525569915771484\n",
      "[step: 3301] loss: 0.0060295830480754375\n",
      "[step: 3302] loss: 28.79771614074707\n",
      "[step: 3302] loss: 0.006028576288372278\n",
      "[step: 3303] loss: 28.123802185058594\n",
      "[step: 3303] loss: 0.006028621923178434\n",
      "[step: 3304] loss: 24.277612686157227\n",
      "[step: 3304] loss: 0.006028733681887388\n",
      "[step: 3305] loss: 26.86433219909668\n",
      "[step: 3305] loss: 0.006028263829648495\n",
      "[step: 3306] loss: 24.166677474975586\n",
      "[step: 3306] loss: 0.006028058007359505\n",
      "[step: 3307] loss: 22.077381134033203\n",
      "[step: 3307] loss: 0.006028070580214262\n",
      "[step: 3308] loss: 23.56747055053711\n",
      "[step: 3308] loss: 0.0060279094614088535\n",
      "[step: 3309] loss: 24.79513168334961\n",
      "[step: 3309] loss: 0.006027776747941971\n",
      "[step: 3310] loss: 22.872638702392578\n",
      "[step: 3310] loss: 0.0060277823358774185\n",
      "[step: 3311] loss: 23.599220275878906\n",
      "[step: 3311] loss: 0.006027780938893557\n",
      "[step: 3312] loss: 23.559101104736328\n",
      "[step: 3312] loss: 0.00602783914655447\n",
      "[step: 3313] loss: 23.582664489746094\n",
      "[step: 3313] loss: 0.006028118077665567\n",
      "[step: 3314] loss: 21.00848960876465\n",
      "[step: 3314] loss: 0.0060286130756139755\n",
      "[step: 3315] loss: 22.43242073059082\n",
      "[step: 3315] loss: 0.00602935254573822\n",
      "[step: 3316] loss: 23.910633087158203\n",
      "[step: 3316] loss: 0.006030559539794922\n",
      "[step: 3317] loss: 23.381866455078125\n",
      "[step: 3317] loss: 0.0060326349921524525\n",
      "[step: 3318] loss: 24.32773780822754\n",
      "[step: 3318] loss: 0.006036130711436272\n",
      "[step: 3319] loss: 25.33827781677246\n",
      "[step: 3319] loss: 0.006041277199983597\n",
      "[step: 3320] loss: 23.300857543945312\n",
      "[step: 3320] loss: 0.006049818359315395\n",
      "[step: 3321] loss: 22.626150131225586\n",
      "[step: 3321] loss: 0.006062968634068966\n",
      "[step: 3322] loss: 22.72157859802246\n",
      "[step: 3322] loss: 0.006084285210818052\n",
      "[step: 3323] loss: 21.171066284179688\n",
      "[step: 3323] loss: 0.006113759707659483\n",
      "[step: 3324] loss: 21.10208511352539\n",
      "[step: 3324] loss: 0.006158673204481602\n",
      "[step: 3325] loss: 22.928255081176758\n",
      "[step: 3325] loss: 0.006208403967320919\n",
      "[step: 3326] loss: 22.652263641357422\n",
      "[step: 3326] loss: 0.006264961324632168\n",
      "[step: 3327] loss: 22.98398208618164\n",
      "[step: 3327] loss: 0.00628291629254818\n",
      "[step: 3328] loss: 23.167316436767578\n",
      "[step: 3328] loss: 0.006256697699427605\n",
      "[step: 3329] loss: 22.81099510192871\n",
      "[step: 3329] loss: 0.0061649433337152\n",
      "[step: 3330] loss: 21.648910522460938\n",
      "[step: 3330] loss: 0.006068872287869453\n",
      "[step: 3331] loss: 21.504150390625\n",
      "[step: 3331] loss: 0.006028755102306604\n",
      "[step: 3332] loss: 21.49056625366211\n",
      "[step: 3332] loss: 0.006060107611119747\n",
      "[step: 3333] loss: 20.652881622314453\n",
      "[step: 3333] loss: 0.006115755531936884\n",
      "[step: 3334] loss: 20.4337158203125\n",
      "[step: 3334] loss: 0.00613480806350708\n",
      "[step: 3335] loss: 20.963937759399414\n",
      "[step: 3335] loss: 0.006104630883783102\n",
      "[step: 3336] loss: 21.012126922607422\n",
      "[step: 3336] loss: 0.006051886361092329\n",
      "[step: 3337] loss: 21.260236740112305\n",
      "[step: 3337] loss: 0.006026677321642637\n",
      "[step: 3338] loss: 22.030323028564453\n",
      "[step: 3338] loss: 0.006042887922376394\n",
      "[step: 3339] loss: 22.2894287109375\n",
      "[step: 3339] loss: 0.006075009237974882\n",
      "[step: 3340] loss: 22.31019401550293\n",
      "[step: 3340] loss: 0.006092335097491741\n",
      "[step: 3341] loss: 22.446508407592773\n",
      "[step: 3341] loss: 0.006077904719859362\n",
      "[step: 3342] loss: 22.459312438964844\n",
      "[step: 3342] loss: 0.006048053503036499\n",
      "[step: 3343] loss: 21.78003692626953\n",
      "[step: 3343] loss: 0.0060262735933065414\n",
      "[step: 3344] loss: 21.731992721557617\n",
      "[step: 3344] loss: 0.006027331575751305\n",
      "[step: 3345] loss: 21.74664878845215\n",
      "[step: 3345] loss: 0.006043904460966587\n",
      "[step: 3346] loss: 21.471923828125\n",
      "[step: 3346] loss: 0.006058332044631243\n",
      "[step: 3347] loss: 20.857223510742188\n",
      "[step: 3347] loss: 0.006059820763766766\n",
      "[step: 3348] loss: 20.429336547851562\n",
      "[step: 3348] loss: 0.006046592723578215\n",
      "[step: 3349] loss: 20.155532836914062\n",
      "[step: 3349] loss: 0.006030735559761524\n",
      "[step: 3350] loss: 20.082687377929688\n",
      "[step: 3350] loss: 0.006022564135491848\n",
      "[step: 3351] loss: 20.3510799407959\n",
      "[step: 3351] loss: 0.00602515647187829\n",
      "[step: 3352] loss: 20.638246536254883\n",
      "[step: 3352] loss: 0.00603387551382184\n",
      "[step: 3353] loss: 20.576671600341797\n",
      "[step: 3353] loss: 0.006040994543582201\n",
      "[step: 3354] loss: 20.359600067138672\n",
      "[step: 3354] loss: 0.0060417624190449715\n",
      "[step: 3355] loss: 20.201732635498047\n",
      "[step: 3355] loss: 0.006035562139004469\n",
      "[step: 3356] loss: 20.19582176208496\n",
      "[step: 3356] loss: 0.006027494557201862\n",
      "[step: 3357] loss: 20.309885025024414\n",
      "[step: 3357] loss: 0.006021829787641764\n",
      "[step: 3358] loss: 20.910749435424805\n",
      "[step: 3358] loss: 0.006020718719810247\n",
      "[step: 3359] loss: 22.16608428955078\n",
      "[step: 3359] loss: 0.0060234228149056435\n",
      "[step: 3360] loss: 24.887929916381836\n",
      "[step: 3360] loss: 0.006027333438396454\n",
      "[step: 3361] loss: 28.081104278564453\n",
      "[step: 3361] loss: 0.006029969081282616\n",
      "[step: 3362] loss: 31.496023178100586\n",
      "[step: 3362] loss: 0.006029873620718718\n",
      "[step: 3363] loss: 32.547935485839844\n",
      "[step: 3363] loss: 0.0060275825671851635\n",
      "[step: 3364] loss: 33.80547332763672\n",
      "[step: 3364] loss: 0.006024039350450039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3365] loss: 32.648590087890625\n",
      "[step: 3365] loss: 0.0060208700597286224\n",
      "[step: 3366] loss: 28.25064468383789\n",
      "[step: 3366] loss: 0.006019079126417637\n",
      "[step: 3367] loss: 23.557266235351562\n",
      "[step: 3367] loss: 0.006018891930580139\n",
      "[step: 3368] loss: 22.811893463134766\n",
      "[step: 3368] loss: 0.006019865162670612\n",
      "[step: 3369] loss: 23.496891021728516\n",
      "[step: 3369] loss: 0.006021267268806696\n",
      "[step: 3370] loss: 24.941761016845703\n",
      "[step: 3370] loss: 0.006022503599524498\n",
      "[step: 3371] loss: 24.1511173248291\n",
      "[step: 3371] loss: 0.006023107096552849\n",
      "[step: 3372] loss: 24.196033477783203\n",
      "[step: 3372] loss: 0.006023001857101917\n",
      "[step: 3373] loss: 26.87537956237793\n",
      "[step: 3373] loss: 0.006022220943123102\n",
      "[step: 3374] loss: 29.058488845825195\n",
      "[step: 3374] loss: 0.006021083332598209\n",
      "[step: 3375] loss: 26.8409423828125\n",
      "[step: 3375] loss: 0.006019778549671173\n",
      "[step: 3376] loss: 23.03466796875\n",
      "[step: 3376] loss: 0.006018593441694975\n",
      "[step: 3377] loss: 22.274301528930664\n",
      "[step: 3377] loss: 0.0060176546685397625\n",
      "[step: 3378] loss: 23.214202880859375\n",
      "[step: 3378] loss: 0.006016983650624752\n",
      "[step: 3379] loss: 24.045635223388672\n",
      "[step: 3379] loss: 0.006016598083078861\n",
      "[step: 3380] loss: 22.980798721313477\n",
      "[step: 3380] loss: 0.006016432773321867\n",
      "[step: 3381] loss: 22.018774032592773\n",
      "[step: 3381] loss: 0.0060164132155478\n",
      "[step: 3382] loss: 22.1739444732666\n",
      "[step: 3382] loss: 0.006016518454998732\n",
      "[step: 3383] loss: 22.54128646850586\n",
      "[step: 3383] loss: 0.006016705650836229\n",
      "[step: 3384] loss: 22.045448303222656\n",
      "[step: 3384] loss: 0.00601694080978632\n",
      "[step: 3385] loss: 21.697612762451172\n",
      "[step: 3385] loss: 0.006017226260155439\n",
      "[step: 3386] loss: 21.283185958862305\n",
      "[step: 3386] loss: 0.006017566192895174\n",
      "[step: 3387] loss: 21.545867919921875\n",
      "[step: 3387] loss: 0.006018012762069702\n",
      "[step: 3388] loss: 22.14633560180664\n",
      "[step: 3388] loss: 0.006018586922436953\n",
      "[step: 3389] loss: 21.764101028442383\n",
      "[step: 3389] loss: 0.0060194106772542\n",
      "[step: 3390] loss: 21.314453125\n",
      "[step: 3390] loss: 0.006020525936037302\n",
      "[step: 3391] loss: 21.942676544189453\n",
      "[step: 3391] loss: 0.006022201851010323\n",
      "[step: 3392] loss: 22.388896942138672\n",
      "[step: 3392] loss: 0.006024525500833988\n",
      "[step: 3393] loss: 22.368732452392578\n",
      "[step: 3393] loss: 0.006028082687407732\n",
      "[step: 3394] loss: 22.040088653564453\n",
      "[step: 3394] loss: 0.0060330345295369625\n",
      "[step: 3395] loss: 22.437288284301758\n",
      "[step: 3395] loss: 0.006040698848664761\n",
      "[step: 3396] loss: 23.90835189819336\n",
      "[step: 3396] loss: 0.00605109753087163\n",
      "[step: 3397] loss: 23.803916931152344\n",
      "[step: 3397] loss: 0.006067021284252405\n",
      "[step: 3398] loss: 22.726713180541992\n",
      "[step: 3398] loss: 0.006087049841880798\n",
      "[step: 3399] loss: 22.52323341369629\n",
      "[step: 3399] loss: 0.006115707103163004\n",
      "[step: 3400] loss: 22.197021484375\n",
      "[step: 3400] loss: 0.006144597660750151\n",
      "[step: 3401] loss: 21.237682342529297\n",
      "[step: 3401] loss: 0.006176866125315428\n",
      "[step: 3402] loss: 20.662738800048828\n",
      "[step: 3402] loss: 0.006188996601849794\n",
      "[step: 3403] loss: 20.229894638061523\n",
      "[step: 3403] loss: 0.00618185568600893\n",
      "[step: 3404] loss: 20.615541458129883\n",
      "[step: 3404] loss: 0.006138073280453682\n",
      "[step: 3405] loss: 20.87942123413086\n",
      "[step: 3405] loss: 0.006080648861825466\n",
      "[step: 3406] loss: 21.110698699951172\n",
      "[step: 3406] loss: 0.006031861063092947\n",
      "[step: 3407] loss: 20.733509063720703\n",
      "[step: 3407] loss: 0.006015385966747999\n",
      "[step: 3408] loss: 21.09039306640625\n",
      "[step: 3408] loss: 0.006031178403645754\n",
      "[step: 3409] loss: 22.06220245361328\n",
      "[step: 3409] loss: 0.006060759536921978\n",
      "[step: 3410] loss: 23.385955810546875\n",
      "[step: 3410] loss: 0.006083333399146795\n",
      "[step: 3411] loss: 25.610679626464844\n",
      "[step: 3411] loss: 0.006082820240408182\n",
      "[step: 3412] loss: 25.146825790405273\n",
      "[step: 3412] loss: 0.006063823588192463\n",
      "[step: 3413] loss: 26.06150245666504\n",
      "[step: 3413] loss: 0.006036178208887577\n",
      "[step: 3414] loss: 27.843666076660156\n",
      "[step: 3414] loss: 0.006017039064317942\n",
      "[step: 3415] loss: 27.501436233520508\n",
      "[step: 3415] loss: 0.006013554520905018\n",
      "[step: 3416] loss: 24.299285888671875\n",
      "[step: 3416] loss: 0.006023131776601076\n",
      "[step: 3417] loss: 22.147968292236328\n",
      "[step: 3417] loss: 0.006037695799022913\n",
      "[step: 3418] loss: 21.737621307373047\n",
      "[step: 3418] loss: 0.006048146635293961\n",
      "[step: 3419] loss: 22.79041290283203\n",
      "[step: 3419] loss: 0.006050982978194952\n",
      "[step: 3420] loss: 21.17009735107422\n",
      "[step: 3420] loss: 0.006043963599950075\n",
      "[step: 3421] loss: 21.030534744262695\n",
      "[step: 3421] loss: 0.006032350938767195\n",
      "[step: 3422] loss: 21.165184020996094\n",
      "[step: 3422] loss: 0.00602042255923152\n",
      "[step: 3423] loss: 22.253419876098633\n",
      "[step: 3423] loss: 0.006013065110892057\n",
      "[step: 3424] loss: 22.024253845214844\n",
      "[step: 3424] loss: 0.0060115777887403965\n",
      "[step: 3425] loss: 21.803226470947266\n",
      "[step: 3425] loss: 0.006014727521687746\n",
      "[step: 3426] loss: 21.58910369873047\n",
      "[step: 3426] loss: 0.006020154803991318\n",
      "[step: 3427] loss: 22.27557373046875\n",
      "[step: 3427] loss: 0.006025318522006273\n",
      "[step: 3428] loss: 21.865873336791992\n",
      "[step: 3428] loss: 0.006028809119015932\n",
      "[step: 3429] loss: 21.215503692626953\n",
      "[step: 3429] loss: 0.006029084790498018\n",
      "[step: 3430] loss: 20.808841705322266\n",
      "[step: 3430] loss: 0.006026815623044968\n",
      "[step: 3431] loss: 20.91695785522461\n",
      "[step: 3431] loss: 0.006022510584443808\n",
      "[step: 3432] loss: 20.820560455322266\n",
      "[step: 3432] loss: 0.006017819046974182\n",
      "[step: 3433] loss: 20.430721282958984\n",
      "[step: 3433] loss: 0.006013584788888693\n",
      "[step: 3434] loss: 20.15636444091797\n",
      "[step: 3434] loss: 0.0060106017626821995\n",
      "[step: 3435] loss: 20.208385467529297\n",
      "[step: 3435] loss: 0.006009060423821211\n",
      "[step: 3436] loss: 20.303123474121094\n",
      "[step: 3436] loss: 0.0060088434256613255\n",
      "[step: 3437] loss: 20.233829498291016\n",
      "[step: 3437] loss: 0.006009576842188835\n",
      "[step: 3438] loss: 20.323230743408203\n",
      "[step: 3438] loss: 0.006010840646922588\n",
      "[step: 3439] loss: 20.095741271972656\n",
      "[step: 3439] loss: 0.006012344732880592\n",
      "[step: 3440] loss: 20.36406707763672\n",
      "[step: 3440] loss: 0.00601385859772563\n",
      "[step: 3441] loss: 20.644201278686523\n",
      "[step: 3441] loss: 0.006015366408973932\n",
      "[step: 3442] loss: 22.09988784790039\n",
      "[step: 3442] loss: 0.006016713101416826\n",
      "[step: 3443] loss: 23.116500854492188\n",
      "[step: 3443] loss: 0.006018143612891436\n",
      "[step: 3444] loss: 25.113327026367188\n",
      "[step: 3444] loss: 0.006019451655447483\n",
      "[step: 3445] loss: 26.005718231201172\n",
      "[step: 3445] loss: 0.006021030712872744\n",
      "[step: 3446] loss: 27.989013671875\n",
      "[step: 3446] loss: 0.006022564135491848\n",
      "[step: 3447] loss: 29.176054000854492\n",
      "[step: 3447] loss: 0.006024639587849379\n",
      "[step: 3448] loss: 28.06963348388672\n",
      "[step: 3448] loss: 0.006026823539286852\n",
      "[step: 3449] loss: 25.77956771850586\n",
      "[step: 3449] loss: 0.006029879674315453\n",
      "[step: 3450] loss: 24.263648986816406\n",
      "[step: 3450] loss: 0.0060331206768751144\n",
      "[step: 3451] loss: 22.04146385192871\n",
      "[step: 3451] loss: 0.0060376194305717945\n",
      "[step: 3452] loss: 21.349021911621094\n",
      "[step: 3452] loss: 0.0060421740636229515\n",
      "[step: 3453] loss: 21.500022888183594\n",
      "[step: 3453] loss: 0.00604822626337409\n",
      "[step: 3454] loss: 21.566448211669922\n",
      "[step: 3454] loss: 0.006053679157048464\n",
      "[step: 3455] loss: 21.971771240234375\n",
      "[step: 3455] loss: 0.006060385145246983\n",
      "[step: 3456] loss: 22.929584503173828\n",
      "[step: 3456] loss: 0.006064935587346554\n",
      "[step: 3457] loss: 24.19040298461914\n",
      "[step: 3457] loss: 0.006069427356123924\n",
      "[step: 3458] loss: 24.20656967163086\n",
      "[step: 3458] loss: 0.00606923270970583\n",
      "[step: 3459] loss: 23.240995407104492\n",
      "[step: 3459] loss: 0.006066890899091959\n",
      "[step: 3460] loss: 22.191612243652344\n",
      "[step: 3460] loss: 0.006058709230273962\n",
      "[step: 3461] loss: 21.081865310668945\n",
      "[step: 3461] loss: 0.006048302166163921\n",
      "[step: 3462] loss: 20.58429718017578\n",
      "[step: 3462] loss: 0.00603522639721632\n",
      "[step: 3463] loss: 20.790029525756836\n",
      "[step: 3463] loss: 0.006023241672664881\n",
      "[step: 3464] loss: 20.924808502197266\n",
      "[step: 3464] loss: 0.0060135493986308575\n",
      "[step: 3465] loss: 20.74594497680664\n",
      "[step: 3465] loss: 0.006007622927427292\n",
      "[step: 3466] loss: 20.163856506347656\n",
      "[step: 3466] loss: 0.006005462259054184\n",
      "[step: 3467] loss: 19.947860717773438\n",
      "[step: 3467] loss: 0.006006457842886448\n",
      "[step: 3468] loss: 20.27608871459961\n",
      "[step: 3468] loss: 0.006009681150317192\n",
      "[step: 3469] loss: 20.386991500854492\n",
      "[step: 3469] loss: 0.006014274898916483\n",
      "[step: 3470] loss: 20.321083068847656\n",
      "[step: 3470] loss: 0.0060197943821549416\n",
      "[step: 3471] loss: 20.39214324951172\n",
      "[step: 3471] loss: 0.006025699898600578\n",
      "[step: 3472] loss: 20.98163604736328\n",
      "[step: 3472] loss: 0.006032499950379133\n",
      "[step: 3473] loss: 21.708398818969727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3473] loss: 0.006039425265043974\n",
      "[step: 3474] loss: 23.2877197265625\n",
      "[step: 3474] loss: 0.006047660019248724\n",
      "[step: 3475] loss: 24.377267837524414\n",
      "[step: 3475] loss: 0.006055389530956745\n",
      "[step: 3476] loss: 25.78015899658203\n",
      "[step: 3476] loss: 0.006063910201191902\n",
      "[step: 3477] loss: 27.374126434326172\n",
      "[step: 3477] loss: 0.006069638300687075\n",
      "[step: 3478] loss: 28.67881202697754\n",
      "[step: 3478] loss: 0.0060734315775334835\n",
      "[step: 3479] loss: 27.074993133544922\n",
      "[step: 3479] loss: 0.006070797331631184\n",
      "[step: 3480] loss: 25.011425018310547\n",
      "[step: 3480] loss: 0.006063200533390045\n",
      "[step: 3481] loss: 23.066238403320312\n",
      "[step: 3481] loss: 0.006049238611012697\n",
      "[step: 3482] loss: 22.898582458496094\n",
      "[step: 3482] loss: 0.006033336278051138\n",
      "[step: 3483] loss: 22.368450164794922\n",
      "[step: 3483] loss: 0.006018560379743576\n",
      "[step: 3484] loss: 21.367801666259766\n",
      "[step: 3484] loss: 0.0060089752078056335\n",
      "[step: 3485] loss: 20.417387008666992\n",
      "[step: 3485] loss: 0.006005610805004835\n",
      "[step: 3486] loss: 20.842716217041016\n",
      "[step: 3486] loss: 0.006007378920912743\n",
      "[step: 3487] loss: 22.221981048583984\n",
      "[step: 3487] loss: 0.006011628080159426\n",
      "[step: 3488] loss: 22.67941665649414\n",
      "[step: 3488] loss: 0.006015873048454523\n",
      "[step: 3489] loss: 22.37419891357422\n",
      "[step: 3489] loss: 0.006017996929585934\n",
      "[step: 3490] loss: 22.338085174560547\n",
      "[step: 3490] loss: 0.006018292624503374\n",
      "[step: 3491] loss: 22.741352081298828\n",
      "[step: 3491] loss: 0.006015650928020477\n",
      "[step: 3492] loss: 22.531341552734375\n",
      "[step: 3492] loss: 0.006012731697410345\n",
      "[step: 3493] loss: 21.967880249023438\n",
      "[step: 3493] loss: 0.006008781958371401\n",
      "[step: 3494] loss: 21.15579605102539\n",
      "[step: 3494] loss: 0.006006282288581133\n",
      "[step: 3495] loss: 20.635730743408203\n",
      "[step: 3495] loss: 0.006005267146974802\n",
      "[step: 3496] loss: 20.30247688293457\n",
      "[step: 3496] loss: 0.006006162613630295\n",
      "[step: 3497] loss: 20.310176849365234\n",
      "[step: 3497] loss: 0.006008852738887072\n",
      "[step: 3498] loss: 20.28888702392578\n",
      "[step: 3498] loss: 0.0060134404338896275\n",
      "[step: 3499] loss: 20.20128059387207\n",
      "[step: 3499] loss: 0.006020881701260805\n",
      "[step: 3500] loss: 19.743656158447266\n",
      "[step: 3500] loss: 0.00602989736944437\n",
      "[step: 3501] loss: 19.5455322265625\n",
      "[step: 3501] loss: 0.006044641137123108\n",
      "[step: 3502] loss: 19.77185821533203\n",
      "[step: 3502] loss: 0.006062088999897242\n",
      "[step: 3503] loss: 20.00196647644043\n",
      "[step: 3503] loss: 0.0060925111174583435\n",
      "[step: 3504] loss: 20.008464813232422\n",
      "[step: 3504] loss: 0.006138946395367384\n",
      "[step: 3505] loss: 19.907337188720703\n",
      "[step: 3505] loss: 0.006210531573742628\n",
      "[step: 3506] loss: 20.22823715209961\n",
      "[step: 3506] loss: 0.006283323280513287\n",
      "[step: 3507] loss: 20.822803497314453\n",
      "[step: 3507] loss: 0.00634178938344121\n",
      "[step: 3508] loss: 21.726722717285156\n",
      "[step: 3508] loss: 0.006308410782366991\n",
      "[step: 3509] loss: 22.726303100585938\n",
      "[step: 3509] loss: 0.006196001078933477\n",
      "[step: 3510] loss: 24.228477478027344\n",
      "[step: 3510] loss: 0.006063898093998432\n",
      "[step: 3511] loss: 25.533111572265625\n",
      "[step: 3511] loss: 0.006013416685163975\n",
      "[step: 3512] loss: 26.5098876953125\n",
      "[step: 3512] loss: 0.00606106361374259\n",
      "[step: 3513] loss: 26.58820343017578\n",
      "[step: 3513] loss: 0.006130656227469444\n",
      "[step: 3514] loss: 25.585433959960938\n",
      "[step: 3514] loss: 0.006145051680505276\n",
      "[step: 3515] loss: 23.77163314819336\n",
      "[step: 3515] loss: 0.00609095161780715\n",
      "[step: 3516] loss: 21.721641540527344\n",
      "[step: 3516] loss: 0.0060330955311656\n",
      "[step: 3517] loss: 20.293067932128906\n",
      "[step: 3517] loss: 0.006023698020726442\n",
      "[step: 3518] loss: 19.700164794921875\n",
      "[step: 3518] loss: 0.006050411611795425\n",
      "[step: 3519] loss: 19.8243408203125\n",
      "[step: 3519] loss: 0.006074752192944288\n",
      "[step: 3520] loss: 20.312557220458984\n",
      "[step: 3520] loss: 0.006072486285120249\n",
      "[step: 3521] loss: 20.84054946899414\n",
      "[step: 3521] loss: 0.006052748300135136\n",
      "[step: 3522] loss: 21.47865867614746\n",
      "[step: 3522] loss: 0.006031370256096125\n",
      "[step: 3523] loss: 23.068809509277344\n",
      "[step: 3523] loss: 0.006018779706209898\n",
      "[step: 3524] loss: 24.57581901550293\n",
      "[step: 3524] loss: 0.0060190074145793915\n",
      "[step: 3525] loss: 26.063230514526367\n",
      "[step: 3525] loss: 0.006027828436344862\n",
      "[step: 3526] loss: 23.789119720458984\n",
      "[step: 3526] loss: 0.00603890884667635\n",
      "[step: 3527] loss: 23.141265869140625\n",
      "[step: 3527] loss: 0.006040297448635101\n",
      "[step: 3528] loss: 24.055347442626953\n",
      "[step: 3528] loss: 0.006025475915521383\n",
      "[step: 3529] loss: 23.949260711669922\n",
      "[step: 3529] loss: 0.00600788276642561\n",
      "[step: 3530] loss: 22.381473541259766\n",
      "[step: 3530] loss: 0.006000579334795475\n",
      "[step: 3531] loss: 20.41649627685547\n",
      "[step: 3531] loss: 0.00600842060521245\n",
      "[step: 3532] loss: 20.434967041015625\n",
      "[step: 3532] loss: 0.006020715460181236\n",
      "[step: 3533] loss: 21.70235252380371\n",
      "[step: 3533] loss: 0.006021739449352026\n",
      "[step: 3534] loss: 21.442617416381836\n",
      "[step: 3534] loss: 0.006013541482388973\n",
      "[step: 3535] loss: 20.577228546142578\n",
      "[step: 3535] loss: 0.006004993338137865\n",
      "[step: 3536] loss: 20.178075790405273\n",
      "[step: 3536] loss: 0.006002098321914673\n",
      "[step: 3537] loss: 20.6268310546875\n",
      "[step: 3537] loss: 0.006002330686897039\n",
      "[step: 3538] loss: 21.68523406982422\n",
      "[step: 3538] loss: 0.006001651287078857\n",
      "[step: 3539] loss: 21.670063018798828\n",
      "[step: 3539] loss: 0.006002007983624935\n",
      "[step: 3540] loss: 21.01517677307129\n",
      "[step: 3540] loss: 0.006004666443914175\n",
      "[step: 3541] loss: 21.288232803344727\n",
      "[step: 3541] loss: 0.006006751675158739\n",
      "[step: 3542] loss: 22.47332000732422\n",
      "[step: 3542] loss: 0.006005011033266783\n",
      "[step: 3543] loss: 22.362205505371094\n",
      "[step: 3543] loss: 0.006000327877700329\n",
      "[step: 3544] loss: 21.996685028076172\n",
      "[step: 3544] loss: 0.0059967623092234135\n",
      "[step: 3545] loss: 21.650388717651367\n",
      "[step: 3545] loss: 0.0059962947852909565\n",
      "[step: 3546] loss: 22.36168670654297\n",
      "[step: 3546] loss: 0.005997262429445982\n",
      "[step: 3547] loss: 23.174877166748047\n",
      "[step: 3547] loss: 0.005997769068926573\n",
      "[step: 3548] loss: 23.233213424682617\n",
      "[step: 3548] loss: 0.005997629836201668\n",
      "[step: 3549] loss: 21.90552520751953\n",
      "[step: 3549] loss: 0.005997857078909874\n",
      "[step: 3550] loss: 21.53348159790039\n",
      "[step: 3550] loss: 0.005998433567583561\n",
      "[step: 3551] loss: 22.002548217773438\n",
      "[step: 3551] loss: 0.005998371168971062\n",
      "[step: 3552] loss: 21.65987777709961\n",
      "[step: 3552] loss: 0.005997334606945515\n",
      "[step: 3553] loss: 20.36709976196289\n",
      "[step: 3553] loss: 0.005995636805891991\n",
      "[step: 3554] loss: 19.58638572692871\n",
      "[step: 3554] loss: 0.005994308739900589\n",
      "[step: 3555] loss: 19.890350341796875\n",
      "[step: 3555] loss: 0.00599374296143651\n",
      "[step: 3556] loss: 20.45113754272461\n",
      "[step: 3556] loss: 0.00599369453266263\n",
      "[step: 3557] loss: 20.128278732299805\n",
      "[step: 3557] loss: 0.005993696395307779\n",
      "[step: 3558] loss: 19.524478912353516\n",
      "[step: 3558] loss: 0.005993433762341738\n",
      "[step: 3559] loss: 19.339916229248047\n",
      "[step: 3559] loss: 0.005993074271827936\n",
      "[step: 3560] loss: 19.705110549926758\n",
      "[step: 3560] loss: 0.00599290756508708\n",
      "[step: 3561] loss: 19.940383911132812\n",
      "[step: 3561] loss: 0.005993071477860212\n",
      "[step: 3562] loss: 19.692373275756836\n",
      "[step: 3562] loss: 0.00599331222474575\n",
      "[step: 3563] loss: 19.35580062866211\n",
      "[step: 3563] loss: 0.005993397440761328\n",
      "[step: 3564] loss: 19.296037673950195\n",
      "[step: 3564] loss: 0.005993294063955545\n",
      "[step: 3565] loss: 19.531719207763672\n",
      "[step: 3565] loss: 0.005993188358843327\n",
      "[step: 3566] loss: 19.751359939575195\n",
      "[step: 3566] loss: 0.00599330011755228\n",
      "[step: 3567] loss: 19.868824005126953\n",
      "[step: 3567] loss: 0.005993606522679329\n",
      "[step: 3568] loss: 20.14825439453125\n",
      "[step: 3568] loss: 0.00599403353407979\n",
      "[step: 3569] loss: 21.519943237304688\n",
      "[step: 3569] loss: 0.005994389299303293\n",
      "[step: 3570] loss: 24.762189865112305\n",
      "[step: 3570] loss: 0.005994844250380993\n",
      "[step: 3571] loss: 31.883928298950195\n",
      "[step: 3571] loss: 0.005995471030473709\n",
      "[step: 3572] loss: 36.41779327392578\n",
      "[step: 3572] loss: 0.00599667988717556\n",
      "[step: 3573] loss: 49.35540008544922\n",
      "[step: 3573] loss: 0.005998505745083094\n",
      "[step: 3574] loss: 67.41893005371094\n",
      "[step: 3574] loss: 0.0060013821348547935\n",
      "[step: 3575] loss: 90.45361328125\n",
      "[step: 3575] loss: 0.006005390547215939\n",
      "[step: 3576] loss: 161.43487548828125\n",
      "[step: 3576] loss: 0.006011576857417822\n",
      "[step: 3577] loss: 488.64630126953125\n",
      "[step: 3577] loss: 0.0060201287269592285\n",
      "[step: 3578] loss: 807.4640502929688\n",
      "[step: 3578] loss: 0.006033328827470541\n",
      "[step: 3579] loss: 583.8953247070312\n",
      "[step: 3579] loss: 0.006050566677004099\n",
      "[step: 3580] loss: 286.4823303222656\n",
      "[step: 3580] loss: 0.006075812969356775\n",
      "[step: 3581] loss: 584.9483032226562\n",
      "[step: 3581] loss: 0.006103814579546452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3582] loss: 316.218017578125\n",
      "[step: 3582] loss: 0.006137957330793142\n",
      "[step: 3583] loss: 376.513671875\n",
      "[step: 3583] loss: 0.006159601267427206\n",
      "[step: 3584] loss: 320.49090576171875\n",
      "[step: 3584] loss: 0.006167816463857889\n",
      "[step: 3585] loss: 299.3219909667969\n",
      "[step: 3585] loss: 0.006139995064586401\n",
      "[step: 3586] loss: 266.8534851074219\n",
      "[step: 3586] loss: 0.006090280134230852\n",
      "[step: 3587] loss: 232.86595153808594\n",
      "[step: 3587] loss: 0.006033869460225105\n",
      "[step: 3588] loss: 283.3967590332031\n",
      "[step: 3588] loss: 0.006000205874443054\n",
      "[step: 3589] loss: 190.48361206054688\n",
      "[step: 3589] loss: 0.006000147201120853\n",
      "[step: 3590] loss: 245.29583740234375\n",
      "[step: 3590] loss: 0.0060234712436795235\n",
      "[step: 3591] loss: 163.70266723632812\n",
      "[step: 3591] loss: 0.00604935921728611\n",
      "[step: 3592] loss: 196.069580078125\n",
      "[step: 3592] loss: 0.006059733685106039\n",
      "[step: 3593] loss: 175.913330078125\n",
      "[step: 3593] loss: 0.0060506900772452354\n",
      "[step: 3594] loss: 173.31439208984375\n",
      "[step: 3594] loss: 0.006026891525834799\n",
      "[step: 3595] loss: 146.2634735107422\n",
      "[step: 3595] loss: 0.0060038515366613865\n",
      "[step: 3596] loss: 153.4190673828125\n",
      "[step: 3596] loss: 0.005992673337459564\n",
      "[step: 3597] loss: 143.08255004882812\n",
      "[step: 3597] loss: 0.005995810031890869\n",
      "[step: 3598] loss: 149.60348510742188\n",
      "[step: 3598] loss: 0.006007967051118612\n",
      "[step: 3599] loss: 129.03839111328125\n",
      "[step: 3599] loss: 0.006019612308591604\n",
      "[step: 3600] loss: 126.21118927001953\n",
      "[step: 3600] loss: 0.006024181377142668\n",
      "[step: 3601] loss: 129.71908569335938\n",
      "[step: 3601] loss: 0.006018815096467733\n",
      "[step: 3602] loss: 118.06159973144531\n",
      "[step: 3602] loss: 0.006007401738315821\n",
      "[step: 3603] loss: 121.11304473876953\n",
      "[step: 3603] loss: 0.005996210966259241\n",
      "[step: 3604] loss: 108.99777221679688\n",
      "[step: 3604] loss: 0.005990997888147831\n",
      "[step: 3605] loss: 110.97795867919922\n",
      "[step: 3605] loss: 0.005992912221699953\n",
      "[step: 3606] loss: 106.95277404785156\n",
      "[step: 3606] loss: 0.005998262204229832\n",
      "[step: 3607] loss: 104.80210876464844\n",
      "[step: 3607] loss: 0.006003090646117926\n",
      "[step: 3608] loss: 101.70890808105469\n",
      "[step: 3608] loss: 0.006003820337355137\n",
      "[step: 3609] loss: 95.75192260742188\n",
      "[step: 3609] loss: 0.006000806577503681\n",
      "[step: 3610] loss: 98.53387451171875\n",
      "[step: 3610] loss: 0.005995741114020348\n",
      "[step: 3611] loss: 93.86468505859375\n",
      "[step: 3611] loss: 0.005991559475660324\n",
      "[step: 3612] loss: 91.59013366699219\n",
      "[step: 3612] loss: 0.005989647004753351\n",
      "[step: 3613] loss: 90.67768859863281\n",
      "[step: 3613] loss: 0.00598943280056119\n",
      "[step: 3614] loss: 84.70133972167969\n",
      "[step: 3614] loss: 0.005990229547023773\n",
      "[step: 3615] loss: 87.71109008789062\n",
      "[step: 3615] loss: 0.0059909578412771225\n",
      "[step: 3616] loss: 84.93997955322266\n",
      "[step: 3616] loss: 0.005991787184029818\n",
      "[step: 3617] loss: 81.67023468017578\n",
      "[step: 3617] loss: 0.005992460530251265\n",
      "[step: 3618] loss: 81.77328491210938\n",
      "[step: 3618] loss: 0.005993159953504801\n",
      "[step: 3619] loss: 79.70571899414062\n",
      "[step: 3619] loss: 0.00599323958158493\n",
      "[step: 3620] loss: 78.0645751953125\n",
      "[step: 3620] loss: 0.005993125960230827\n",
      "[step: 3621] loss: 78.67241668701172\n",
      "[step: 3621] loss: 0.005993606522679329\n",
      "[step: 3622] loss: 76.63238525390625\n",
      "[step: 3622] loss: 0.0059957788325846195\n",
      "[step: 3623] loss: 74.86937713623047\n",
      "[step: 3623] loss: 0.006001058965921402\n",
      "[step: 3624] loss: 74.73158264160156\n",
      "[step: 3624] loss: 0.006010203622281551\n",
      "[step: 3625] loss: 73.58919525146484\n",
      "[step: 3625] loss: 0.006025427952408791\n",
      "[step: 3626] loss: 72.33557891845703\n",
      "[step: 3626] loss: 0.006043293513357639\n",
      "[step: 3627] loss: 71.29986572265625\n",
      "[step: 3627] loss: 0.00606852350756526\n",
      "[step: 3628] loss: 70.93924713134766\n",
      "[step: 3628] loss: 0.006094351876527071\n",
      "[step: 3629] loss: 69.4667739868164\n",
      "[step: 3629] loss: 0.006123462226241827\n",
      "[step: 3630] loss: 68.70642852783203\n",
      "[step: 3630] loss: 0.006150636821985245\n",
      "[step: 3631] loss: 68.40811157226562\n",
      "[step: 3631] loss: 0.00616254610940814\n",
      "[step: 3632] loss: 67.38531494140625\n",
      "[step: 3632] loss: 0.006154599599540234\n",
      "[step: 3633] loss: 66.17156982421875\n",
      "[step: 3633] loss: 0.006116420496255159\n",
      "[step: 3634] loss: 65.94788360595703\n",
      "[step: 3634] loss: 0.006067045498639345\n",
      "[step: 3635] loss: 65.05963134765625\n",
      "[step: 3635] loss: 0.0060239448212087154\n",
      "[step: 3636] loss: 64.41678619384766\n",
      "[step: 3636] loss: 0.006004926282912493\n",
      "[step: 3637] loss: 63.694549560546875\n",
      "[step: 3637] loss: 0.006008637137711048\n",
      "[step: 3638] loss: 63.233253479003906\n",
      "[step: 3638] loss: 0.0060238526202738285\n",
      "[step: 3639] loss: 62.33992004394531\n",
      "[step: 3639] loss: 0.006037210114300251\n",
      "[step: 3640] loss: 61.774635314941406\n",
      "[step: 3640] loss: 0.0060396320186555386\n",
      "[step: 3641] loss: 61.41923141479492\n",
      "[step: 3641] loss: 0.006032699253410101\n",
      "[step: 3642] loss: 60.75359344482422\n",
      "[step: 3642] loss: 0.0060201468877494335\n",
      "[step: 3643] loss: 60.02595138549805\n",
      "[step: 3643] loss: 0.006009087897837162\n",
      "[step: 3644] loss: 59.578285217285156\n",
      "[step: 3644] loss: 0.006005931179970503\n",
      "[step: 3645] loss: 59.14935302734375\n",
      "[step: 3645] loss: 0.00600546645000577\n",
      "[step: 3646] loss: 58.696815490722656\n",
      "[step: 3646] loss: 0.006004498340189457\n",
      "[step: 3647] loss: 58.00419616699219\n",
      "[step: 3647] loss: 0.005994442384690046\n",
      "[step: 3648] loss: 57.602657318115234\n",
      "[step: 3648] loss: 0.005986201111227274\n",
      "[step: 3649] loss: 57.18631362915039\n",
      "[step: 3649] loss: 0.005987153388559818\n",
      "[step: 3650] loss: 56.83238983154297\n",
      "[step: 3650] loss: 0.005995980929583311\n",
      "[step: 3651] loss: 56.275814056396484\n",
      "[step: 3651] loss: 0.00600431440398097\n",
      "[step: 3652] loss: 55.825740814208984\n",
      "[step: 3652] loss: 0.006000361870974302\n",
      "[step: 3653] loss: 55.412086486816406\n",
      "[step: 3653] loss: 0.005993574392050505\n",
      "[step: 3654] loss: 55.05876159667969\n",
      "[step: 3654] loss: 0.005990801844745874\n",
      "[step: 3655] loss: 54.65380096435547\n",
      "[step: 3655] loss: 0.005994321778416634\n",
      "[step: 3656] loss: 54.193450927734375\n",
      "[step: 3656] loss: 0.005998069886118174\n",
      "[step: 3657] loss: 53.771873474121094\n",
      "[step: 3657] loss: 0.005996474996209145\n",
      "[step: 3658] loss: 53.37531280517578\n",
      "[step: 3658] loss: 0.005992396268993616\n",
      "[step: 3659] loss: 53.034976959228516\n",
      "[step: 3659] loss: 0.005989850033074617\n",
      "[step: 3660] loss: 52.647708892822266\n",
      "[step: 3660] loss: 0.005989735014736652\n",
      "[step: 3661] loss: 52.28441619873047\n",
      "[step: 3661] loss: 0.005990126635879278\n",
      "[step: 3662] loss: 51.869720458984375\n",
      "[step: 3662] loss: 0.005988680757582188\n",
      "[step: 3663] loss: 51.499542236328125\n",
      "[step: 3663] loss: 0.005987432785332203\n",
      "[step: 3664] loss: 51.126869201660156\n",
      "[step: 3664] loss: 0.0059869177639484406\n",
      "[step: 3665] loss: 50.794227600097656\n",
      "[step: 3665] loss: 0.005987055599689484\n",
      "[step: 3666] loss: 50.4532356262207\n",
      "[step: 3666] loss: 0.005986748728901148\n",
      "[step: 3667] loss: 50.124855041503906\n",
      "[step: 3667] loss: 0.005986187607049942\n",
      "[step: 3668] loss: 49.804386138916016\n",
      "[step: 3668] loss: 0.005986309144645929\n",
      "[step: 3669] loss: 49.47816848754883\n",
      "[step: 3669] loss: 0.005987527314573526\n",
      "[step: 3670] loss: 49.159873962402344\n",
      "[step: 3670] loss: 0.005989943630993366\n",
      "[step: 3671] loss: 48.83484649658203\n",
      "[step: 3671] loss: 0.005992952734231949\n",
      "[step: 3672] loss: 48.526039123535156\n",
      "[step: 3672] loss: 0.00599656580016017\n",
      "[step: 3673] loss: 48.21914291381836\n",
      "[step: 3673] loss: 0.006001246627420187\n",
      "[step: 3674] loss: 47.921600341796875\n",
      "[step: 3674] loss: 0.006008618976920843\n",
      "[step: 3675] loss: 47.62986755371094\n",
      "[step: 3675] loss: 0.006019315682351589\n",
      "[step: 3676] loss: 47.33922576904297\n",
      "[step: 3676] loss: 0.006034958176314831\n",
      "[step: 3677] loss: 47.058292388916016\n",
      "[step: 3677] loss: 0.00605356739833951\n",
      "[step: 3678] loss: 46.77850341796875\n",
      "[step: 3678] loss: 0.006076548248529434\n",
      "[step: 3679] loss: 46.50434494018555\n",
      "[step: 3679] loss: 0.0060976529493927956\n",
      "[step: 3680] loss: 46.23472595214844\n",
      "[step: 3680] loss: 0.006116414442658424\n",
      "[step: 3681] loss: 45.96653747558594\n",
      "[step: 3681] loss: 0.006118969526141882\n",
      "[step: 3682] loss: 45.70478820800781\n",
      "[step: 3682] loss: 0.0061048343777656555\n",
      "[step: 3683] loss: 45.44477081298828\n",
      "[step: 3683] loss: 0.006068041082471609\n",
      "[step: 3684] loss: 45.19404602050781\n",
      "[step: 3684] loss: 0.006026016548275948\n",
      "[step: 3685] loss: 44.952354431152344\n",
      "[step: 3685] loss: 0.00599263608455658\n",
      "[step: 3686] loss: 44.726234436035156\n",
      "[step: 3686] loss: 0.0059800781309604645\n",
      "[step: 3687] loss: 44.533935546875\n",
      "[step: 3687] loss: 0.005987222772091627\n",
      "[step: 3688] loss: 44.412139892578125\n",
      "[step: 3688] loss: 0.006005278788506985\n",
      "[step: 3689] loss: 44.42485809326172\n",
      "[step: 3689] loss: 0.006023347843438387\n",
      "[step: 3690] loss: 44.790096282958984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3690] loss: 0.006031686905771494\n",
      "[step: 3691] loss: 45.75798416137695\n",
      "[step: 3691] loss: 0.0060292514972388744\n",
      "[step: 3692] loss: 48.56324005126953\n",
      "[step: 3692] loss: 0.0060158371925354\n",
      "[step: 3693] loss: 53.041778564453125\n",
      "[step: 3693] loss: 0.005999201908707619\n",
      "[step: 3694] loss: 63.78356170654297\n",
      "[step: 3694] loss: 0.00598460016772151\n",
      "[step: 3695] loss: 67.06359100341797\n",
      "[step: 3695] loss: 0.005976792890578508\n",
      "[step: 3696] loss: 65.98654174804688\n",
      "[step: 3696] loss: 0.005976658780127764\n",
      "[step: 3697] loss: 48.60273742675781\n",
      "[step: 3697] loss: 0.00598220806568861\n",
      "[step: 3698] loss: 44.50693130493164\n",
      "[step: 3698] loss: 0.005989991594105959\n",
      "[step: 3699] loss: 54.633384704589844\n",
      "[step: 3699] loss: 0.005996360443532467\n",
      "[step: 3700] loss: 51.50383377075195\n",
      "[step: 3700] loss: 0.005999854765832424\n",
      "[step: 3701] loss: 42.86558532714844\n",
      "[step: 3701] loss: 0.0059990789741277695\n",
      "[step: 3702] loss: 46.94940948486328\n",
      "[step: 3702] loss: 0.0059959255158901215\n",
      "[step: 3703] loss: 49.92936706542969\n",
      "[step: 3703] loss: 0.005990845616906881\n",
      "[step: 3704] loss: 43.337425231933594\n",
      "[step: 3704] loss: 0.005985939409583807\n",
      "[step: 3705] loss: 42.64356231689453\n",
      "[step: 3705] loss: 0.005981826689094305\n",
      "[step: 3706] loss: 46.869598388671875\n",
      "[step: 3706] loss: 0.005979022476822138\n",
      "[step: 3707] loss: 43.713783264160156\n",
      "[step: 3707] loss: 0.005977708846330643\n",
      "[step: 3708] loss: 40.876548767089844\n",
      "[step: 3708] loss: 0.005977024789899588\n",
      "[step: 3709] loss: 43.7747802734375\n",
      "[step: 3709] loss: 0.005977326538413763\n",
      "[step: 3710] loss: 43.60174560546875\n",
      "[step: 3710] loss: 0.005976981483399868\n",
      "[step: 3711] loss: 40.4925651550293\n",
      "[step: 3711] loss: 0.005976856220513582\n",
      "[step: 3712] loss: 41.06161880493164\n",
      "[step: 3712] loss: 0.0059763602912425995\n",
      "[step: 3713] loss: 42.46388626098633\n",
      "[step: 3713] loss: 0.005976243410259485\n",
      "[step: 3714] loss: 40.61720657348633\n",
      "[step: 3714] loss: 0.005976538639515638\n",
      "[step: 3715] loss: 39.369720458984375\n",
      "[step: 3715] loss: 0.005977219436317682\n",
      "[step: 3716] loss: 40.5526008605957\n",
      "[step: 3716] loss: 0.005978129804134369\n",
      "[step: 3717] loss: 40.449462890625\n",
      "[step: 3717] loss: 0.005979146808385849\n",
      "[step: 3718] loss: 38.87773895263672\n",
      "[step: 3718] loss: 0.005980306304991245\n",
      "[step: 3719] loss: 38.89634704589844\n",
      "[step: 3719] loss: 0.005981932394206524\n",
      "[step: 3720] loss: 39.66087341308594\n",
      "[step: 3720] loss: 0.005984344054013491\n",
      "[step: 3721] loss: 38.94598388671875\n",
      "[step: 3721] loss: 0.005988288205116987\n",
      "[step: 3722] loss: 37.9835205078125\n",
      "[step: 3722] loss: 0.005993939004838467\n",
      "[step: 3723] loss: 38.274925231933594\n",
      "[step: 3723] loss: 0.006002604495733976\n",
      "[step: 3724] loss: 38.64118576049805\n",
      "[step: 3724] loss: 0.006014012731611729\n",
      "[step: 3725] loss: 38.02936553955078\n",
      "[step: 3725] loss: 0.006030549295246601\n",
      "[step: 3726] loss: 37.35120391845703\n",
      "[step: 3726] loss: 0.006050159689038992\n",
      "[step: 3727] loss: 37.41197967529297\n",
      "[step: 3727] loss: 0.0060757542960345745\n",
      "[step: 3728] loss: 37.652748107910156\n",
      "[step: 3728] loss: 0.006098932586610317\n",
      "[step: 3729] loss: 37.315670013427734\n",
      "[step: 3729] loss: 0.006119526922702789\n",
      "[step: 3730] loss: 36.73663330078125\n",
      "[step: 3730] loss: 0.006120461504906416\n",
      "[step: 3731] loss: 36.58710479736328\n",
      "[step: 3731] loss: 0.0061017354018986225\n",
      "[step: 3732] loss: 36.7573127746582\n",
      "[step: 3732] loss: 0.006060014944523573\n",
      "[step: 3733] loss: 36.660430908203125\n",
      "[step: 3733] loss: 0.006014813669025898\n",
      "[step: 3734] loss: 36.236244201660156\n",
      "[step: 3734] loss: 0.005984298884868622\n",
      "[step: 3735] loss: 35.91570281982422\n",
      "[step: 3735] loss: 0.0059793870896101\n",
      "[step: 3736] loss: 35.878963470458984\n",
      "[step: 3736] loss: 0.005994137842208147\n",
      "[step: 3737] loss: 35.915199279785156\n",
      "[step: 3737] loss: 0.00601412495598197\n",
      "[step: 3738] loss: 35.76702880859375\n",
      "[step: 3738] loss: 0.006022351328283548\n",
      "[step: 3739] loss: 35.46235275268555\n",
      "[step: 3739] loss: 0.006015196908265352\n",
      "[step: 3740] loss: 35.194908142089844\n",
      "[step: 3740] loss: 0.005997614469379187\n",
      "[step: 3741] loss: 35.093650817871094\n",
      "[step: 3741] loss: 0.005981822032481432\n",
      "[step: 3742] loss: 35.07270050048828\n",
      "[step: 3742] loss: 0.005977090448141098\n",
      "[step: 3743] loss: 34.98114013671875\n",
      "[step: 3743] loss: 0.005983136128634214\n",
      "[step: 3744] loss: 34.78167724609375\n",
      "[step: 3744] loss: 0.005994164850562811\n",
      "[step: 3745] loss: 34.543582916259766\n",
      "[step: 3745] loss: 0.005998642183840275\n",
      "[step: 3746] loss: 34.360774993896484\n",
      "[step: 3746] loss: 0.005995560437440872\n",
      "[step: 3747] loss: 34.24609375\n",
      "[step: 3747] loss: 0.00598525907844305\n",
      "[step: 3748] loss: 34.170860290527344\n",
      "[step: 3748] loss: 0.00597718171775341\n",
      "[step: 3749] loss: 34.08558654785156\n",
      "[step: 3749] loss: 0.005976994056254625\n",
      "[step: 3750] loss: 33.96472930908203\n",
      "[step: 3750] loss: 0.005982942413538694\n",
      "[step: 3751] loss: 33.804954528808594\n",
      "[step: 3751] loss: 0.005990169011056423\n",
      "[step: 3752] loss: 33.627437591552734\n",
      "[step: 3752] loss: 0.005992378108203411\n",
      "[step: 3753] loss: 33.45685577392578\n",
      "[step: 3753] loss: 0.00599086657166481\n",
      "[step: 3754] loss: 33.30583953857422\n",
      "[step: 3754] loss: 0.005990846082568169\n",
      "[step: 3755] loss: 33.1769905090332\n",
      "[step: 3755] loss: 0.00599663844332099\n",
      "[step: 3756] loss: 33.06393814086914\n",
      "[step: 3756] loss: 0.0060105393640697\n",
      "[step: 3757] loss: 32.96137619018555\n",
      "[step: 3757] loss: 0.006028305739164352\n",
      "[step: 3758] loss: 32.865478515625\n",
      "[step: 3758] loss: 0.006049023475497961\n",
      "[step: 3759] loss: 32.77452087402344\n",
      "[step: 3759] loss: 0.006066968664526939\n",
      "[step: 3760] loss: 32.68844985961914\n",
      "[step: 3760] loss: 0.006086703389883041\n",
      "[step: 3761] loss: 32.61265563964844\n",
      "[step: 3761] loss: 0.006105710752308369\n",
      "[step: 3762] loss: 32.55040740966797\n",
      "[step: 3762] loss: 0.0061187222599983215\n",
      "[step: 3763] loss: 32.51866149902344\n",
      "[step: 3763] loss: 0.006108800880610943\n",
      "[step: 3764] loss: 32.52140808105469\n",
      "[step: 3764] loss: 0.006072748452425003\n",
      "[step: 3765] loss: 32.604835510253906\n",
      "[step: 3765] loss: 0.0060226572677493095\n",
      "[step: 3766] loss: 32.77195739746094\n",
      "[step: 3766] loss: 0.0059830546379089355\n",
      "[step: 3767] loss: 33.1490478515625\n",
      "[step: 3767] loss: 0.0059718964621424675\n",
      "[step: 3768] loss: 33.69634246826172\n",
      "[step: 3768] loss: 0.005985237192362547\n",
      "[step: 3769] loss: 34.69648742675781\n",
      "[step: 3769] loss: 0.006006927229464054\n",
      "[step: 3770] loss: 35.7447509765625\n",
      "[step: 3770] loss: 0.006022002547979355\n",
      "[step: 3771] loss: 37.207847595214844\n",
      "[step: 3771] loss: 0.006022115703672171\n",
      "[step: 3772] loss: 37.4892578125\n",
      "[step: 3772] loss: 0.0060106427408754826\n",
      "[step: 3773] loss: 36.90728759765625\n",
      "[step: 3773] loss: 0.0059973327443003654\n",
      "[step: 3774] loss: 34.40745544433594\n",
      "[step: 3774] loss: 0.005986884236335754\n",
      "[step: 3775] loss: 31.974393844604492\n",
      "[step: 3775] loss: 0.005982726812362671\n",
      "[step: 3776] loss: 31.065235137939453\n",
      "[step: 3776] loss: 0.005980338901281357\n",
      "[step: 3777] loss: 31.83289337158203\n",
      "[step: 3777] loss: 0.005979133769869804\n",
      "[step: 3778] loss: 33.069496154785156\n",
      "[step: 3778] loss: 0.005976221989840269\n",
      "[step: 3779] loss: 33.26312255859375\n",
      "[step: 3779] loss: 0.0059756385162472725\n",
      "[step: 3780] loss: 32.593353271484375\n",
      "[step: 3780] loss: 0.00597873330116272\n",
      "[step: 3781] loss: 31.621944427490234\n",
      "[step: 3781] loss: 0.005984141491353512\n",
      "[step: 3782] loss: 31.247774124145508\n",
      "[step: 3782] loss: 0.0059882416389882565\n",
      "[step: 3783] loss: 31.687301635742188\n",
      "[step: 3783] loss: 0.005985652096569538\n",
      "[step: 3784] loss: 31.85681915283203\n",
      "[step: 3784] loss: 0.005978379864245653\n",
      "[step: 3785] loss: 31.546262741088867\n",
      "[step: 3785] loss: 0.005970506463199854\n",
      "[step: 3786] loss: 30.65607261657715\n",
      "[step: 3786] loss: 0.005967270117253065\n",
      "[step: 3787] loss: 30.166990280151367\n",
      "[step: 3787] loss: 0.005968454759567976\n",
      "[step: 3788] loss: 30.311691284179688\n",
      "[step: 3788] loss: 0.005969780497252941\n",
      "[step: 3789] loss: 30.84223175048828\n",
      "[step: 3789] loss: 0.005969008430838585\n",
      "[step: 3790] loss: 31.470947265625\n",
      "[step: 3790] loss: 0.00596572132781148\n",
      "[step: 3791] loss: 32.70210266113281\n",
      "[step: 3791] loss: 0.0059631881304085255\n",
      "[step: 3792] loss: 35.537269592285156\n",
      "[step: 3792] loss: 0.0059631019830703735\n",
      "[step: 3793] loss: 43.1345100402832\n",
      "[step: 3793] loss: 0.005964877083897591\n",
      "[step: 3794] loss: 41.05554962158203\n",
      "[step: 3794] loss: 0.005966751836240292\n",
      "[step: 3795] loss: 33.973838806152344\n",
      "[step: 3795] loss: 0.005967364180833101\n",
      "[step: 3796] loss: 30.823177337646484\n",
      "[step: 3796] loss: 0.005967323202639818\n",
      "[step: 3797] loss: 36.623802185058594\n",
      "[step: 3797] loss: 0.0059675294905900955\n",
      "[step: 3798] loss: 35.027767181396484\n",
      "[step: 3798] loss: 0.005968713201582432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3799] loss: 30.399864196777344\n",
      "[step: 3799] loss: 0.005970959551632404\n",
      "[step: 3800] loss: 35.66645431518555\n",
      "[step: 3800] loss: 0.005973817780613899\n",
      "[step: 3801] loss: 33.638160705566406\n",
      "[step: 3801] loss: 0.005977464374154806\n",
      "[step: 3802] loss: 30.229406356811523\n",
      "[step: 3802] loss: 0.0059822238981723785\n",
      "[step: 3803] loss: 34.489532470703125\n",
      "[step: 3803] loss: 0.005989376921206713\n",
      "[step: 3804] loss: 31.264808654785156\n",
      "[step: 3804] loss: 0.0059989988803863525\n",
      "[step: 3805] loss: 30.363914489746094\n",
      "[step: 3805] loss: 0.006013268139213324\n",
      "[step: 3806] loss: 33.1983528137207\n",
      "[step: 3806] loss: 0.006029970478266478\n",
      "[step: 3807] loss: 29.930526733398438\n",
      "[step: 3807] loss: 0.0060529643669724464\n",
      "[step: 3808] loss: 30.388858795166016\n",
      "[step: 3808] loss: 0.006074736826121807\n",
      "[step: 3809] loss: 31.66248321533203\n",
      "[step: 3809] loss: 0.006098330952227116\n",
      "[step: 3810] loss: 29.160602569580078\n",
      "[step: 3810] loss: 0.006106930319219828\n",
      "[step: 3811] loss: 29.574745178222656\n",
      "[step: 3811] loss: 0.006101153325289488\n",
      "[step: 3812] loss: 30.610370635986328\n",
      "[step: 3812] loss: 0.006069905590265989\n",
      "[step: 3813] loss: 28.890159606933594\n",
      "[step: 3813] loss: 0.006026669405400753\n",
      "[step: 3814] loss: 28.729290008544922\n",
      "[step: 3814] loss: 0.005985383875668049\n",
      "[step: 3815] loss: 29.875089645385742\n",
      "[step: 3815] loss: 0.005963440518826246\n",
      "[step: 3816] loss: 29.02716636657715\n",
      "[step: 3816] loss: 0.005965427961200476\n",
      "[step: 3817] loss: 28.03641700744629\n",
      "[step: 3817] loss: 0.005983581766486168\n",
      "[step: 3818] loss: 28.984416961669922\n",
      "[step: 3818] loss: 0.006004842463880777\n",
      "[step: 3819] loss: 29.29102325439453\n",
      "[step: 3819] loss: 0.0060167331248521805\n",
      "[step: 3820] loss: 28.05743408203125\n",
      "[step: 3820] loss: 0.006016411352902651\n",
      "[step: 3821] loss: 27.883529663085938\n",
      "[step: 3821] loss: 0.006002915557473898\n",
      "[step: 3822] loss: 28.646648406982422\n",
      "[step: 3822] loss: 0.00598524883389473\n",
      "[step: 3823] loss: 28.45450210571289\n",
      "[step: 3823] loss: 0.0059695253148674965\n",
      "[step: 3824] loss: 27.653932571411133\n",
      "[step: 3824] loss: 0.0059610470198094845\n",
      "[step: 3825] loss: 27.393524169921875\n",
      "[step: 3825] loss: 0.005960626993328333\n",
      "[step: 3826] loss: 27.826862335205078\n",
      "[step: 3826] loss: 0.005965882912278175\n",
      "[step: 3827] loss: 28.15309715270996\n",
      "[step: 3827] loss: 0.00597355654463172\n",
      "[step: 3828] loss: 27.751876831054688\n",
      "[step: 3828] loss: 0.005980270449072123\n",
      "[step: 3829] loss: 27.309017181396484\n",
      "[step: 3829] loss: 0.005984770134091377\n",
      "[step: 3830] loss: 27.27242660522461\n",
      "[step: 3830] loss: 0.005985370837152004\n",
      "[step: 3831] loss: 27.517662048339844\n",
      "[step: 3831] loss: 0.005983245093375444\n",
      "[step: 3832] loss: 27.658727645874023\n",
      "[step: 3832] loss: 0.005978685803711414\n",
      "[step: 3833] loss: 27.432296752929688\n",
      "[step: 3833] loss: 0.005972609855234623\n",
      "[step: 3834] loss: 27.244352340698242\n",
      "[step: 3834] loss: 0.005966659169644117\n",
      "[step: 3835] loss: 27.568496704101562\n",
      "[step: 3835] loss: 0.005961236078292131\n",
      "[step: 3836] loss: 28.844533920288086\n",
      "[step: 3836] loss: 0.005957735236734152\n",
      "[step: 3837] loss: 32.881961822509766\n",
      "[step: 3837] loss: 0.00595636572688818\n",
      "[step: 3838] loss: 36.57099151611328\n",
      "[step: 3838] loss: 0.005956977605819702\n",
      "[step: 3839] loss: 40.697452545166016\n",
      "[step: 3839] loss: 0.005958938971161842\n",
      "[step: 3840] loss: 34.217430114746094\n",
      "[step: 3840] loss: 0.005961465649306774\n",
      "[step: 3841] loss: 35.147705078125\n",
      "[step: 3841] loss: 0.00596409197896719\n",
      "[step: 3842] loss: 32.577110290527344\n",
      "[step: 3842] loss: 0.00596647197380662\n",
      "[step: 3843] loss: 30.902400970458984\n",
      "[step: 3843] loss: 0.005968812387436628\n",
      "[step: 3844] loss: 36.17650604248047\n",
      "[step: 3844] loss: 0.005971377715468407\n",
      "[step: 3845] loss: 33.90802001953125\n",
      "[step: 3845] loss: 0.005974422674626112\n",
      "[step: 3846] loss: 30.23430633544922\n",
      "[step: 3846] loss: 0.0059785544872283936\n",
      "[step: 3847] loss: 30.503217697143555\n",
      "[step: 3847] loss: 0.005983831360936165\n",
      "[step: 3848] loss: 30.319499969482422\n",
      "[step: 3848] loss: 0.005991073790937662\n",
      "[step: 3849] loss: 32.38227844238281\n",
      "[step: 3849] loss: 0.006000286899507046\n",
      "[step: 3850] loss: 30.87592315673828\n",
      "[step: 3850] loss: 0.006012072786688805\n",
      "[step: 3851] loss: 28.149864196777344\n",
      "[step: 3851] loss: 0.0060255443677306175\n",
      "[step: 3852] loss: 28.068954467773438\n",
      "[step: 3852] loss: 0.0060387179255485535\n",
      "[step: 3853] loss: 30.53162384033203\n",
      "[step: 3853] loss: 0.00604594312608242\n",
      "[step: 3854] loss: 29.301694869995117\n",
      "[step: 3854] loss: 0.00604198919609189\n",
      "[step: 3855] loss: 28.3981876373291\n",
      "[step: 3855] loss: 0.006022495683282614\n",
      "[step: 3856] loss: 27.228050231933594\n",
      "[step: 3856] loss: 0.005995429120957851\n",
      "[step: 3857] loss: 27.391616821289062\n",
      "[step: 3857] loss: 0.005973587743937969\n",
      "[step: 3858] loss: 28.935733795166016\n",
      "[step: 3858] loss: 0.005967327393591404\n",
      "[step: 3859] loss: 28.07693099975586\n",
      "[step: 3859] loss: 0.005975284613668919\n",
      "[step: 3860] loss: 26.54590606689453\n",
      "[step: 3860] loss: 0.005984688177704811\n",
      "[step: 3861] loss: 26.49775505065918\n",
      "[step: 3861] loss: 0.005987048614770174\n",
      "[step: 3862] loss: 27.00320816040039\n",
      "[step: 3862] loss: 0.005975693929940462\n",
      "[step: 3863] loss: 27.143604278564453\n",
      "[step: 3863] loss: 0.005962792318314314\n",
      "[step: 3864] loss: 26.338886260986328\n",
      "[step: 3864] loss: 0.005958017893135548\n",
      "[step: 3865] loss: 26.806175231933594\n",
      "[step: 3865] loss: 0.0059633152559399605\n",
      "[step: 3866] loss: 28.170082092285156\n",
      "[step: 3866] loss: 0.005971331615000963\n",
      "[step: 3867] loss: 27.713058471679688\n",
      "[step: 3867] loss: 0.005973190534859896\n",
      "[step: 3868] loss: 27.277820587158203\n",
      "[step: 3868] loss: 0.00596907502040267\n",
      "[step: 3869] loss: 26.976642608642578\n",
      "[step: 3869] loss: 0.005963101051747799\n",
      "[step: 3870] loss: 26.673873901367188\n",
      "[step: 3870] loss: 0.005962626542896032\n",
      "[step: 3871] loss: 25.943416595458984\n",
      "[step: 3871] loss: 0.00596862705424428\n",
      "[step: 3872] loss: 25.419639587402344\n",
      "[step: 3872] loss: 0.005977774504572153\n",
      "[step: 3873] loss: 25.801586151123047\n",
      "[step: 3873] loss: 0.005987356882542372\n",
      "[step: 3874] loss: 26.11099624633789\n",
      "[step: 3874] loss: 0.005996581632643938\n",
      "[step: 3875] loss: 26.034482955932617\n",
      "[step: 3875] loss: 0.0060114203952252865\n",
      "[step: 3876] loss: 26.178251266479492\n",
      "[step: 3876] loss: 0.0060371835716068745\n",
      "[step: 3877] loss: 26.758705139160156\n",
      "[step: 3877] loss: 0.006072026211768389\n",
      "[step: 3878] loss: 26.816814422607422\n",
      "[step: 3878] loss: 0.006118243560194969\n",
      "[step: 3879] loss: 26.834341049194336\n",
      "[step: 3879] loss: 0.006151573266834021\n",
      "[step: 3880] loss: 26.137956619262695\n",
      "[step: 3880] loss: 0.006170925684273243\n",
      "[step: 3881] loss: 25.717039108276367\n",
      "[step: 3881] loss: 0.006142765283584595\n",
      "[step: 3882] loss: 25.285594940185547\n",
      "[step: 3882] loss: 0.006082701031118631\n",
      "[step: 3883] loss: 25.085981369018555\n",
      "[step: 3883] loss: 0.006010064389556646\n",
      "[step: 3884] loss: 25.370695114135742\n",
      "[step: 3884] loss: 0.005965576972812414\n",
      "[step: 3885] loss: 25.765396118164062\n",
      "[step: 3885] loss: 0.00596792995929718\n",
      "[step: 3886] loss: 25.928524017333984\n",
      "[step: 3886] loss: 0.006003117188811302\n",
      "[step: 3887] loss: 25.5293025970459\n",
      "[step: 3887] loss: 0.006037075072526932\n",
      "[step: 3888] loss: 25.26036834716797\n",
      "[step: 3888] loss: 0.006040346808731556\n",
      "[step: 3889] loss: 25.05058479309082\n",
      "[step: 3889] loss: 0.006012454628944397\n",
      "[step: 3890] loss: 24.859149932861328\n",
      "[step: 3890] loss: 0.005974379368126392\n",
      "[step: 3891] loss: 24.68622589111328\n",
      "[step: 3891] loss: 0.0059545948170125484\n",
      "[step: 3892] loss: 24.700754165649414\n",
      "[step: 3892] loss: 0.005960937589406967\n",
      "[step: 3893] loss: 24.91014862060547\n",
      "[step: 3893] loss: 0.005981469061225653\n",
      "[step: 3894] loss: 25.12788963317871\n",
      "[step: 3894] loss: 0.00599767966195941\n",
      "[step: 3895] loss: 25.519935607910156\n",
      "[step: 3895] loss: 0.005998981650918722\n",
      "[step: 3896] loss: 26.001741409301758\n",
      "[step: 3896] loss: 0.005988054443150759\n",
      "[step: 3897] loss: 27.222835540771484\n",
      "[step: 3897] loss: 0.005972027312964201\n",
      "[step: 3898] loss: 27.769981384277344\n",
      "[step: 3898] loss: 0.005960889160633087\n",
      "[step: 3899] loss: 27.909076690673828\n",
      "[step: 3899] loss: 0.005957083310931921\n",
      "[step: 3900] loss: 25.855003356933594\n",
      "[step: 3900] loss: 0.005959156900644302\n",
      "[step: 3901] loss: 24.62447166442871\n",
      "[step: 3901] loss: 0.00596266845241189\n",
      "[step: 3902] loss: 25.22998046875\n",
      "[step: 3902] loss: 0.005966789089143276\n",
      "[step: 3903] loss: 25.98859977722168\n",
      "[step: 3903] loss: 0.005970174912363291\n",
      "[step: 3904] loss: 25.722063064575195\n",
      "[step: 3904] loss: 0.005971002858132124\n",
      "[step: 3905] loss: 24.907678604125977\n",
      "[step: 3905] loss: 0.005968721583485603\n",
      "[step: 3906] loss: 24.781768798828125\n",
      "[step: 3906] loss: 0.005962921306490898\n",
      "[step: 3907] loss: 24.994247436523438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3907] loss: 0.005955965723842382\n",
      "[step: 3908] loss: 25.055988311767578\n",
      "[step: 3908] loss: 0.005950275342911482\n",
      "[step: 3909] loss: 25.014320373535156\n",
      "[step: 3909] loss: 0.005947896745055914\n",
      "[step: 3910] loss: 24.73632049560547\n",
      "[step: 3910] loss: 0.005949018523097038\n",
      "[step: 3911] loss: 24.35363006591797\n",
      "[step: 3911] loss: 0.005952092818915844\n",
      "[step: 3912] loss: 24.346973419189453\n",
      "[step: 3912] loss: 0.005955090746283531\n",
      "[step: 3913] loss: 24.70697784423828\n",
      "[step: 3913] loss: 0.005956642795354128\n",
      "[step: 3914] loss: 24.852218627929688\n",
      "[step: 3914] loss: 0.005957053042948246\n",
      "[step: 3915] loss: 24.607446670532227\n",
      "[step: 3915] loss: 0.005956733133643866\n",
      "[step: 3916] loss: 24.263532638549805\n",
      "[step: 3916] loss: 0.005956451408565044\n",
      "[step: 3917] loss: 24.296993255615234\n",
      "[step: 3917] loss: 0.005956139415502548\n",
      "[step: 3918] loss: 24.52581214904785\n",
      "[step: 3918] loss: 0.005955767352133989\n",
      "[step: 3919] loss: 24.479875564575195\n",
      "[step: 3919] loss: 0.005954854190349579\n",
      "[step: 3920] loss: 24.432937622070312\n",
      "[step: 3920] loss: 0.005953478626906872\n",
      "[step: 3921] loss: 24.55453109741211\n",
      "[step: 3921] loss: 0.005951884668320417\n",
      "[step: 3922] loss: 25.17890167236328\n",
      "[step: 3922] loss: 0.005950435996055603\n",
      "[step: 3923] loss: 25.83817481994629\n",
      "[step: 3923] loss: 0.005949371960014105\n",
      "[step: 3924] loss: 26.94628143310547\n",
      "[step: 3924] loss: 0.005948740988969803\n",
      "[step: 3925] loss: 27.11693572998047\n",
      "[step: 3925] loss: 0.00594849931076169\n",
      "[step: 3926] loss: 26.852401733398438\n",
      "[step: 3926] loss: 0.005948460195213556\n",
      "[step: 3927] loss: 24.55565643310547\n",
      "[step: 3927] loss: 0.005948568694293499\n",
      "[step: 3928] loss: 23.982891082763672\n",
      "[step: 3928] loss: 0.005948734935373068\n",
      "[step: 3929] loss: 25.274227142333984\n",
      "[step: 3929] loss: 0.005949009209871292\n",
      "[step: 3930] loss: 25.425485610961914\n",
      "[step: 3930] loss: 0.005949399434030056\n",
      "[step: 3931] loss: 24.37445068359375\n",
      "[step: 3931] loss: 0.005950060207396746\n",
      "[step: 3932] loss: 24.08000373840332\n",
      "[step: 3932] loss: 0.005950955208390951\n",
      "[step: 3933] loss: 24.65610694885254\n",
      "[step: 3933] loss: 0.0059525431133806705\n",
      "[step: 3934] loss: 24.599903106689453\n",
      "[step: 3934] loss: 0.00595463952049613\n",
      "[step: 3935] loss: 24.031757354736328\n",
      "[step: 3935] loss: 0.005958189722150564\n",
      "[step: 3936] loss: 24.16989517211914\n",
      "[step: 3936] loss: 0.005962931551039219\n",
      "[step: 3937] loss: 24.453187942504883\n",
      "[step: 3937] loss: 0.005970497149974108\n",
      "[step: 3938] loss: 23.984943389892578\n",
      "[step: 3938] loss: 0.005981538910418749\n",
      "[step: 3939] loss: 23.688135147094727\n",
      "[step: 3939] loss: 0.005998419132083654\n",
      "[step: 3940] loss: 23.924589157104492\n",
      "[step: 3940] loss: 0.006024348549544811\n",
      "[step: 3941] loss: 24.127038955688477\n",
      "[step: 3941] loss: 0.00606056023389101\n",
      "[step: 3942] loss: 23.841541290283203\n",
      "[step: 3942] loss: 0.006112676113843918\n",
      "[step: 3943] loss: 23.441089630126953\n",
      "[step: 3943] loss: 0.006169580388814211\n",
      "[step: 3944] loss: 23.530025482177734\n",
      "[step: 3944] loss: 0.006227137055248022\n",
      "[step: 3945] loss: 23.818164825439453\n",
      "[step: 3945] loss: 0.0062408894300460815\n",
      "[step: 3946] loss: 23.794879913330078\n",
      "[step: 3946] loss: 0.006198798771947622\n",
      "[step: 3947] loss: 23.448036193847656\n",
      "[step: 3947] loss: 0.006091962102800608\n",
      "[step: 3948] loss: 23.293031692504883\n",
      "[step: 3948] loss: 0.005992088932543993\n",
      "[step: 3949] loss: 23.450735092163086\n",
      "[step: 3949] loss: 0.005966819357126951\n",
      "[step: 3950] loss: 23.78574562072754\n",
      "[step: 3950] loss: 0.0060200258158147335\n",
      "[step: 3951] loss: 24.00527572631836\n",
      "[step: 3951] loss: 0.00607638992369175\n",
      "[step: 3952] loss: 24.937381744384766\n",
      "[step: 3952] loss: 0.006067005917429924\n",
      "[step: 3953] loss: 26.48979949951172\n",
      "[step: 3953] loss: 0.0060050697065889835\n",
      "[step: 3954] loss: 30.38491439819336\n",
      "[step: 3954] loss: 0.005962989293038845\n",
      "[step: 3955] loss: 30.175556182861328\n",
      "[step: 3955] loss: 0.005974554922431707\n",
      "[step: 3956] loss: 27.116849899291992\n",
      "[step: 3956] loss: 0.006005116272717714\n",
      "[step: 3957] loss: 24.65078353881836\n",
      "[step: 3957] loss: 0.006005460396409035\n",
      "[step: 3958] loss: 24.953752517700195\n",
      "[step: 3958] loss: 0.005980846472084522\n",
      "[step: 3959] loss: 25.364776611328125\n",
      "[step: 3959] loss: 0.0059693423099815845\n",
      "[step: 3960] loss: 25.27130889892578\n",
      "[step: 3960] loss: 0.005981663707643747\n",
      "[step: 3961] loss: 26.093894958496094\n",
      "[step: 3961] loss: 0.005994166247546673\n",
      "[step: 3962] loss: 25.660491943359375\n",
      "[step: 3962] loss: 0.005975255277007818\n",
      "[step: 3963] loss: 23.6502742767334\n",
      "[step: 3963] loss: 0.005950305610895157\n",
      "[step: 3964] loss: 23.753660202026367\n",
      "[step: 3964] loss: 0.005948010366410017\n",
      "[step: 3965] loss: 25.060070037841797\n",
      "[step: 3965] loss: 0.0059636724181473255\n",
      "[step: 3966] loss: 24.357913970947266\n",
      "[step: 3966] loss: 0.005972364451736212\n",
      "[step: 3967] loss: 23.544340133666992\n",
      "[step: 3967] loss: 0.005960296839475632\n",
      "[step: 3968] loss: 24.028982162475586\n",
      "[step: 3968] loss: 0.0059514353051781654\n",
      "[step: 3969] loss: 24.001256942749023\n",
      "[step: 3969] loss: 0.005955964792519808\n",
      "[step: 3970] loss: 23.33249855041504\n",
      "[step: 3970] loss: 0.005960629787296057\n",
      "[step: 3971] loss: 23.053585052490234\n",
      "[step: 3971] loss: 0.005954580381512642\n",
      "[step: 3972] loss: 23.453861236572266\n",
      "[step: 3972] loss: 0.005943962372839451\n",
      "[step: 3973] loss: 24.14162826538086\n",
      "[step: 3973] loss: 0.005944091826677322\n",
      "[step: 3974] loss: 23.881072998046875\n",
      "[step: 3974] loss: 0.005951155908405781\n",
      "[step: 3975] loss: 23.352590560913086\n",
      "[step: 3975] loss: 0.005951543338596821\n",
      "[step: 3976] loss: 23.091585159301758\n",
      "[step: 3976] loss: 0.005945124663412571\n",
      "[step: 3977] loss: 23.17200469970703\n",
      "[step: 3977] loss: 0.005942197982221842\n",
      "[step: 3978] loss: 23.042165756225586\n",
      "[step: 3978] loss: 0.005946212448179722\n",
      "[step: 3979] loss: 22.904987335205078\n",
      "[step: 3979] loss: 0.0059500206261873245\n",
      "[step: 3980] loss: 23.08150863647461\n",
      "[step: 3980] loss: 0.005946741905063391\n",
      "[step: 3981] loss: 23.318897247314453\n",
      "[step: 3981] loss: 0.00594212906435132\n",
      "[step: 3982] loss: 23.40846061706543\n",
      "[step: 3982] loss: 0.00594261335209012\n",
      "[step: 3983] loss: 23.21512222290039\n",
      "[step: 3983] loss: 0.005945643875747919\n",
      "[step: 3984] loss: 23.12268829345703\n",
      "[step: 3984] loss: 0.00594636145979166\n",
      "[step: 3985] loss: 23.293529510498047\n",
      "[step: 3985] loss: 0.005942802410572767\n",
      "[step: 3986] loss: 23.635000228881836\n",
      "[step: 3986] loss: 0.005940241273492575\n",
      "[step: 3987] loss: 23.490867614746094\n",
      "[step: 3987] loss: 0.005941362585872412\n",
      "[step: 3988] loss: 23.484346389770508\n",
      "[step: 3988] loss: 0.005943212658166885\n",
      "[step: 3989] loss: 23.23621368408203\n",
      "[step: 3989] loss: 0.005943188443779945\n",
      "[step: 3990] loss: 23.147804260253906\n",
      "[step: 3990] loss: 0.00594077305868268\n",
      "[step: 3991] loss: 22.967060089111328\n",
      "[step: 3991] loss: 0.005939688999205828\n",
      "[step: 3992] loss: 22.62741470336914\n",
      "[step: 3992] loss: 0.005941018927842379\n",
      "[step: 3993] loss: 22.35211181640625\n",
      "[step: 3993] loss: 0.005943036172538996\n",
      "[step: 3994] loss: 22.337478637695312\n",
      "[step: 3994] loss: 0.0059442780911922455\n",
      "[step: 3995] loss: 22.46385955810547\n",
      "[step: 3995] loss: 0.0059446613304317\n",
      "[step: 3996] loss: 22.565900802612305\n",
      "[step: 3996] loss: 0.005946608260273933\n",
      "[step: 3997] loss: 22.636821746826172\n",
      "[step: 3997] loss: 0.005951253697276115\n",
      "[step: 3998] loss: 22.776195526123047\n",
      "[step: 3998] loss: 0.0059589180164039135\n",
      "[step: 3999] loss: 23.311260223388672\n",
      "[step: 3999] loss: 0.005969881545752287\n",
      "RMSE: 0.02666093409061432\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 4000\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL만들기\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.296037673950195"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(loss_for_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
