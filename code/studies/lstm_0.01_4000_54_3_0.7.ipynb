{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-a1a1ae85a16d>:90: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 12119.41015625\n",
      "[step: 1] loss: 4281.20751953125\n",
      "[step: 2] loss: 9757.484375\n",
      "[step: 3] loss: 3695.22119140625\n",
      "[step: 4] loss: 3493.362060546875\n",
      "[step: 5] loss: 3113.639892578125\n",
      "[step: 6] loss: 3000.79248046875\n",
      "[step: 7] loss: 2719.220703125\n",
      "[step: 8] loss: 2279.71484375\n",
      "[step: 9] loss: 1930.68115234375\n",
      "[step: 10] loss: 1701.8218994140625\n",
      "[step: 11] loss: 1603.5650634765625\n",
      "[step: 12] loss: 1596.1556396484375\n",
      "[step: 13] loss: 1596.169189453125\n",
      "[step: 14] loss: 1578.91552734375\n",
      "[step: 15] loss: 1582.922607421875\n",
      "[step: 16] loss: 1538.53466796875\n",
      "[step: 17] loss: 1492.2784423828125\n",
      "[step: 18] loss: 1468.975341796875\n",
      "[step: 19] loss: 1409.046142578125\n",
      "[step: 20] loss: 1386.452392578125\n",
      "[step: 21] loss: 1347.2236328125\n",
      "[step: 22] loss: 1291.49853515625\n",
      "[step: 23] loss: 1272.7197265625\n",
      "[step: 24] loss: 1259.2432861328125\n",
      "[step: 25] loss: 1234.366455078125\n",
      "[step: 26] loss: 1199.95654296875\n",
      "[step: 27] loss: 1178.559814453125\n",
      "[step: 28] loss: 1160.21435546875\n",
      "[step: 29] loss: 1106.834228515625\n",
      "[step: 30] loss: 1088.017333984375\n",
      "[step: 31] loss: 1046.4404296875\n",
      "[step: 32] loss: 1012.847412109375\n",
      "[step: 33] loss: 975.3404541015625\n",
      "[step: 34] loss: 906.7930908203125\n",
      "[step: 35] loss: 877.098388671875\n",
      "[step: 36] loss: 850.1893920898438\n",
      "[step: 37] loss: 797.9783325195312\n",
      "[step: 38] loss: 775.8143310546875\n",
      "[step: 39] loss: 765.0631103515625\n",
      "[step: 40] loss: 694.4180297851562\n",
      "[step: 41] loss: 691.8179321289062\n",
      "[step: 42] loss: 667.7635498046875\n",
      "[step: 43] loss: 611.1251220703125\n",
      "[step: 44] loss: 629.382080078125\n",
      "[step: 45] loss: 568.1712036132812\n",
      "[step: 46] loss: 553.8167114257812\n",
      "[step: 47] loss: 573.5869140625\n",
      "[step: 48] loss: 530.1176147460938\n",
      "[step: 49] loss: 525.545166015625\n",
      "[step: 50] loss: 515.4567260742188\n",
      "[step: 51] loss: 487.72430419921875\n",
      "[step: 52] loss: 489.2817077636719\n",
      "[step: 53] loss: 467.17041015625\n",
      "[step: 54] loss: 468.84674072265625\n",
      "[step: 55] loss: 440.7613220214844\n",
      "[step: 56] loss: 444.28070068359375\n",
      "[step: 57] loss: 447.5862121582031\n",
      "[step: 58] loss: 419.991455078125\n",
      "[step: 59] loss: 427.6894836425781\n",
      "[step: 60] loss: 421.6634521484375\n",
      "[step: 61] loss: 414.94049072265625\n",
      "[step: 62] loss: 403.79150390625\n",
      "[step: 63] loss: 399.4227294921875\n",
      "[step: 64] loss: 398.1947021484375\n",
      "[step: 65] loss: 379.3565979003906\n",
      "[step: 66] loss: 394.85699462890625\n",
      "[step: 67] loss: 368.0257263183594\n",
      "[step: 68] loss: 377.07940673828125\n",
      "[step: 69] loss: 376.2132568359375\n",
      "[step: 70] loss: 372.60601806640625\n",
      "[step: 71] loss: 371.4233093261719\n",
      "[step: 72] loss: 370.3672180175781\n",
      "[step: 73] loss: 358.61102294921875\n",
      "[step: 74] loss: 361.1111755371094\n",
      "[step: 75] loss: 352.76226806640625\n",
      "[step: 76] loss: 348.28936767578125\n",
      "[step: 77] loss: 356.42486572265625\n",
      "[step: 78] loss: 346.69244384765625\n",
      "[step: 79] loss: 358.71099853515625\n",
      "[step: 80] loss: 338.23095703125\n",
      "[step: 81] loss: 347.4317626953125\n",
      "[step: 82] loss: 342.3982238769531\n",
      "[step: 83] loss: 337.3194580078125\n",
      "[step: 84] loss: 329.343505859375\n",
      "[step: 85] loss: 338.45306396484375\n",
      "[step: 86] loss: 321.7610778808594\n",
      "[step: 87] loss: 328.81182861328125\n",
      "[step: 88] loss: 323.8751220703125\n",
      "[step: 89] loss: 317.5153503417969\n",
      "[step: 90] loss: 321.2920227050781\n",
      "[step: 91] loss: 318.8229675292969\n",
      "[step: 92] loss: 311.50299072265625\n",
      "[step: 93] loss: 328.5376892089844\n",
      "[step: 94] loss: 318.11688232421875\n",
      "[step: 95] loss: 308.4964599609375\n",
      "[step: 96] loss: 320.2735290527344\n",
      "[step: 97] loss: 310.24188232421875\n",
      "[step: 98] loss: 305.22161865234375\n",
      "[step: 99] loss: 311.3536071777344\n",
      "[step: 100] loss: 313.9960632324219\n",
      "[step: 101] loss: 305.230224609375\n",
      "[step: 102] loss: 299.8233642578125\n",
      "[step: 103] loss: 293.58343505859375\n",
      "[step: 104] loss: 295.56951904296875\n",
      "[step: 105] loss: 294.07098388671875\n",
      "[step: 106] loss: 288.93408203125\n",
      "[step: 107] loss: 297.8941650390625\n",
      "[step: 108] loss: 289.17578125\n",
      "[step: 109] loss: 289.8294982910156\n",
      "[step: 110] loss: 287.73590087890625\n",
      "[step: 111] loss: 286.38812255859375\n",
      "[step: 112] loss: 287.11187744140625\n",
      "[step: 113] loss: 282.175048828125\n",
      "[step: 114] loss: 288.02471923828125\n",
      "[step: 115] loss: 280.3680419921875\n",
      "[step: 116] loss: 281.6500244140625\n",
      "[step: 117] loss: 288.1060485839844\n",
      "[step: 118] loss: 277.00628662109375\n",
      "[step: 119] loss: 288.60296630859375\n",
      "[step: 120] loss: 276.2275390625\n",
      "[step: 121] loss: 276.08392333984375\n",
      "[step: 122] loss: 282.059326171875\n",
      "[step: 123] loss: 274.4512634277344\n",
      "[step: 124] loss: 275.848876953125\n",
      "[step: 125] loss: 264.0596618652344\n",
      "[step: 126] loss: 276.3233642578125\n",
      "[step: 127] loss: 277.9010009765625\n",
      "[step: 128] loss: 271.67376708984375\n",
      "[step: 129] loss: 271.19866943359375\n",
      "[step: 130] loss: 262.82940673828125\n",
      "[step: 131] loss: 269.5126953125\n",
      "[step: 132] loss: 270.2289733886719\n",
      "[step: 133] loss: 267.26361083984375\n",
      "[step: 134] loss: 256.9328308105469\n",
      "[step: 135] loss: 256.8817138671875\n",
      "[step: 136] loss: 260.31097412109375\n",
      "[step: 137] loss: 261.6937561035156\n",
      "[step: 138] loss: 262.000732421875\n",
      "[step: 139] loss: 260.2538757324219\n",
      "[step: 140] loss: 252.98435974121094\n",
      "[step: 141] loss: 262.1620788574219\n",
      "[step: 142] loss: 261.7785949707031\n",
      "[step: 143] loss: 260.5943603515625\n",
      "[step: 144] loss: 263.27227783203125\n",
      "[step: 145] loss: 251.51876831054688\n",
      "[step: 146] loss: 250.704345703125\n",
      "[step: 147] loss: 260.37457275390625\n",
      "[step: 148] loss: 251.84335327148438\n",
      "[step: 149] loss: 251.919677734375\n",
      "[step: 150] loss: 256.3529052734375\n",
      "[step: 151] loss: 256.9238586425781\n",
      "[step: 152] loss: 250.51119995117188\n",
      "[step: 153] loss: 262.81646728515625\n",
      "[step: 154] loss: 252.21365356445312\n",
      "[step: 155] loss: 255.68405151367188\n",
      "[step: 156] loss: 247.53150939941406\n",
      "[step: 157] loss: 246.48899841308594\n",
      "[step: 158] loss: 243.90940856933594\n",
      "[step: 159] loss: 247.1576385498047\n",
      "[step: 160] loss: 247.62283325195312\n",
      "[step: 161] loss: 243.19183349609375\n",
      "[step: 162] loss: 241.06858825683594\n",
      "[step: 163] loss: 242.22430419921875\n",
      "[step: 164] loss: 253.46121215820312\n",
      "[step: 165] loss: 245.35443115234375\n",
      "[step: 166] loss: 240.27345275878906\n",
      "[step: 167] loss: 251.60475158691406\n",
      "[step: 168] loss: 258.3436279296875\n",
      "[step: 169] loss: 242.79071044921875\n",
      "[step: 170] loss: 231.40997314453125\n",
      "[step: 171] loss: 257.2162780761719\n",
      "[step: 172] loss: 243.541748046875\n",
      "[step: 173] loss: 235.59967041015625\n",
      "[step: 174] loss: 248.568603515625\n",
      "[step: 175] loss: 224.43426513671875\n",
      "[step: 176] loss: 234.7443084716797\n",
      "[step: 177] loss: 246.74786376953125\n",
      "[step: 178] loss: 238.81227111816406\n",
      "[step: 179] loss: 234.55731201171875\n",
      "[step: 180] loss: 245.27975463867188\n",
      "[step: 181] loss: 236.44827270507812\n",
      "[step: 182] loss: 236.07272338867188\n",
      "[step: 183] loss: 233.72332763671875\n",
      "[step: 184] loss: 234.87060546875\n",
      "[step: 185] loss: 230.40733337402344\n",
      "[step: 186] loss: 227.74822998046875\n",
      "[step: 187] loss: 232.9595947265625\n",
      "[step: 188] loss: 225.0152587890625\n",
      "[step: 189] loss: 223.70941162109375\n",
      "[step: 190] loss: 223.0868682861328\n",
      "[step: 191] loss: 233.75567626953125\n",
      "[step: 192] loss: 229.87290954589844\n",
      "[step: 193] loss: 231.48025512695312\n",
      "[step: 194] loss: 228.36172485351562\n",
      "[step: 195] loss: 222.4979705810547\n",
      "[step: 196] loss: 230.06695556640625\n",
      "[step: 197] loss: 221.1619873046875\n",
      "[step: 198] loss: 229.15985107421875\n",
      "[step: 199] loss: 231.28591918945312\n",
      "[step: 200] loss: 227.28497314453125\n",
      "[step: 201] loss: 233.800048828125\n",
      "[step: 202] loss: 224.8084716796875\n",
      "[step: 203] loss: 222.14573669433594\n",
      "[step: 204] loss: 236.52017211914062\n",
      "[step: 205] loss: 239.65493774414062\n",
      "[step: 206] loss: 228.28575134277344\n",
      "[step: 207] loss: 221.84710693359375\n",
      "[step: 208] loss: 220.7696533203125\n",
      "[step: 209] loss: 222.86685180664062\n",
      "[step: 210] loss: 215.5830535888672\n",
      "[step: 211] loss: 223.80447387695312\n",
      "[step: 212] loss: 226.82852172851562\n",
      "[step: 213] loss: 222.97239685058594\n",
      "[step: 214] loss: 217.6547393798828\n",
      "[step: 215] loss: 231.8431396484375\n",
      "[step: 216] loss: 223.21725463867188\n",
      "[step: 217] loss: 225.81756591796875\n",
      "[step: 218] loss: 228.43637084960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 219] loss: 218.46163940429688\n",
      "[step: 220] loss: 218.74009704589844\n",
      "[step: 221] loss: 213.45516967773438\n",
      "[step: 222] loss: 217.34677124023438\n",
      "[step: 223] loss: 217.61929321289062\n",
      "[step: 224] loss: 217.01962280273438\n",
      "[step: 225] loss: 221.06228637695312\n",
      "[step: 226] loss: 218.4774627685547\n",
      "[step: 227] loss: 214.9628448486328\n",
      "[step: 228] loss: 205.9717559814453\n",
      "[step: 229] loss: 211.05075073242188\n",
      "[step: 230] loss: 212.60362243652344\n",
      "[step: 231] loss: 214.89906311035156\n",
      "[step: 232] loss: 212.88214111328125\n",
      "[step: 233] loss: 215.60427856445312\n",
      "[step: 234] loss: 209.1430206298828\n",
      "[step: 235] loss: 211.0358123779297\n",
      "[step: 236] loss: 225.60626220703125\n",
      "[step: 237] loss: 212.98721313476562\n",
      "[step: 238] loss: 222.890625\n",
      "[step: 239] loss: 220.46649169921875\n",
      "[step: 240] loss: 220.194580078125\n",
      "[step: 241] loss: 210.15771484375\n",
      "[step: 242] loss: 224.07955932617188\n",
      "[step: 243] loss: 230.87112426757812\n",
      "[step: 244] loss: 207.69674682617188\n",
      "[step: 245] loss: 218.11302185058594\n",
      "[step: 246] loss: 228.99514770507812\n",
      "[step: 247] loss: 210.42523193359375\n",
      "[step: 248] loss: 218.99496459960938\n",
      "[step: 249] loss: 219.61221313476562\n",
      "[step: 250] loss: 204.62351989746094\n",
      "[step: 251] loss: 214.15103149414062\n",
      "[step: 252] loss: 203.99465942382812\n",
      "[step: 253] loss: 215.01846313476562\n",
      "[step: 254] loss: 218.64730834960938\n",
      "[step: 255] loss: 206.23974609375\n",
      "[step: 256] loss: 204.55062866210938\n",
      "[step: 257] loss: 218.46957397460938\n",
      "[step: 258] loss: 204.85189819335938\n",
      "[step: 259] loss: 214.51419067382812\n",
      "[step: 260] loss: 205.750732421875\n",
      "[step: 261] loss: 200.77249145507812\n",
      "[step: 262] loss: 210.99853515625\n",
      "[step: 263] loss: 206.5396728515625\n",
      "[step: 264] loss: 207.46543884277344\n",
      "[step: 265] loss: 211.30699157714844\n",
      "[step: 266] loss: 197.95697021484375\n",
      "[step: 267] loss: 204.56309509277344\n",
      "[step: 268] loss: 204.6323699951172\n",
      "[step: 269] loss: 198.3385009765625\n",
      "[step: 270] loss: 192.3415069580078\n",
      "[step: 271] loss: 208.28256225585938\n",
      "[step: 272] loss: 200.23448181152344\n",
      "[step: 273] loss: 202.06979370117188\n",
      "[step: 274] loss: 207.3323211669922\n",
      "[step: 275] loss: 206.5801239013672\n",
      "[step: 276] loss: 194.62057495117188\n",
      "[step: 277] loss: 206.75332641601562\n",
      "[step: 278] loss: 201.61502075195312\n",
      "[step: 279] loss: 197.088134765625\n",
      "[step: 280] loss: 207.3632354736328\n",
      "[step: 281] loss: 194.42921447753906\n",
      "[step: 282] loss: 205.0013427734375\n",
      "[step: 283] loss: 198.7627716064453\n",
      "[step: 284] loss: 200.80392456054688\n",
      "[step: 285] loss: 198.51641845703125\n",
      "[step: 286] loss: 203.66610717773438\n",
      "[step: 287] loss: 193.67185974121094\n",
      "[step: 288] loss: 194.11822509765625\n",
      "[step: 289] loss: 197.0880126953125\n",
      "[step: 290] loss: 201.734375\n",
      "[step: 291] loss: 193.94400024414062\n",
      "[step: 292] loss: 204.32440185546875\n",
      "[step: 293] loss: 198.70965576171875\n",
      "[step: 294] loss: 195.79550170898438\n",
      "[step: 295] loss: 201.583984375\n",
      "[step: 296] loss: 194.82781982421875\n",
      "[step: 297] loss: 197.02999877929688\n",
      "[step: 298] loss: 196.85696411132812\n",
      "[step: 299] loss: 197.51382446289062\n",
      "[step: 300] loss: 194.5301513671875\n",
      "[step: 301] loss: 191.8685302734375\n",
      "[step: 302] loss: 200.81088256835938\n",
      "[step: 303] loss: 190.332275390625\n",
      "[step: 304] loss: 195.4431915283203\n",
      "[step: 305] loss: 190.4438018798828\n",
      "[step: 306] loss: 187.32138061523438\n",
      "[step: 307] loss: 192.02635192871094\n",
      "[step: 308] loss: 193.8343505859375\n",
      "[step: 309] loss: 190.41357421875\n",
      "[step: 310] loss: 188.79632568359375\n",
      "[step: 311] loss: 188.163330078125\n",
      "[step: 312] loss: 189.0758514404297\n",
      "[step: 313] loss: 202.91171264648438\n",
      "[step: 314] loss: 193.1755828857422\n",
      "[step: 315] loss: 189.35496520996094\n",
      "[step: 316] loss: 192.4806365966797\n",
      "[step: 317] loss: 184.99752807617188\n",
      "[step: 318] loss: 192.5665740966797\n",
      "[step: 319] loss: 182.28323364257812\n",
      "[step: 320] loss: 189.4050750732422\n",
      "[step: 321] loss: 184.39991760253906\n",
      "[step: 322] loss: 187.20974731445312\n",
      "[step: 323] loss: 181.58978271484375\n",
      "[step: 324] loss: 184.24069213867188\n",
      "[step: 325] loss: 184.40155029296875\n",
      "[step: 326] loss: 187.7931365966797\n",
      "[step: 327] loss: 187.08154296875\n",
      "[step: 328] loss: 181.70135498046875\n",
      "[step: 329] loss: 191.46920776367188\n",
      "[step: 330] loss: 184.81491088867188\n",
      "[step: 331] loss: 183.97390747070312\n",
      "[step: 332] loss: 187.18597412109375\n",
      "[step: 333] loss: 184.37490844726562\n",
      "[step: 334] loss: 183.16525268554688\n",
      "[step: 335] loss: 182.1824951171875\n",
      "[step: 336] loss: 181.84844970703125\n",
      "[step: 337] loss: 184.890869140625\n",
      "[step: 338] loss: 194.05029296875\n",
      "[step: 339] loss: 180.3660888671875\n",
      "[step: 340] loss: 181.43756103515625\n",
      "[step: 341] loss: 184.5096893310547\n",
      "[step: 342] loss: 184.7087860107422\n",
      "[step: 343] loss: 185.8693389892578\n",
      "[step: 344] loss: 180.0264892578125\n",
      "[step: 345] loss: 183.18861389160156\n",
      "[step: 346] loss: 192.7013702392578\n",
      "[step: 347] loss: 188.40673828125\n",
      "[step: 348] loss: 188.5496826171875\n",
      "[step: 349] loss: 179.1917724609375\n",
      "[step: 350] loss: 192.1757354736328\n",
      "[step: 351] loss: 180.66339111328125\n",
      "[step: 352] loss: 189.3695068359375\n",
      "[step: 353] loss: 186.5116424560547\n",
      "[step: 354] loss: 195.8209228515625\n",
      "[step: 355] loss: 198.38560485839844\n",
      "[step: 356] loss: 182.51524353027344\n",
      "[step: 357] loss: 187.0001678466797\n",
      "[step: 358] loss: 188.09881591796875\n",
      "[step: 359] loss: 179.97535705566406\n",
      "[step: 360] loss: 181.56671142578125\n",
      "[step: 361] loss: 195.7852783203125\n",
      "[step: 362] loss: 181.6168212890625\n",
      "[step: 363] loss: 184.94039916992188\n",
      "[step: 364] loss: 191.19461059570312\n",
      "[step: 365] loss: 188.88604736328125\n",
      "[step: 366] loss: 191.5257568359375\n",
      "[step: 367] loss: 183.82766723632812\n",
      "[step: 368] loss: 203.49749755859375\n",
      "[step: 369] loss: 196.95660400390625\n",
      "[step: 370] loss: 193.15025329589844\n",
      "[step: 371] loss: 195.65853881835938\n",
      "[step: 372] loss: 187.20614624023438\n",
      "[step: 373] loss: 198.56842041015625\n",
      "[step: 374] loss: 183.33709716796875\n",
      "[step: 375] loss: 190.4645538330078\n",
      "[step: 376] loss: 185.30194091796875\n",
      "[step: 377] loss: 193.50054931640625\n",
      "[step: 378] loss: 183.478515625\n",
      "[step: 379] loss: 184.08529663085938\n",
      "[step: 380] loss: 177.91802978515625\n",
      "[step: 381] loss: 184.80734252929688\n",
      "[step: 382] loss: 176.32687377929688\n",
      "[step: 383] loss: 191.91592407226562\n",
      "[step: 384] loss: 181.9447479248047\n",
      "[step: 385] loss: 181.12225341796875\n",
      "[step: 386] loss: 179.18679809570312\n",
      "[step: 387] loss: 171.51785278320312\n",
      "[step: 388] loss: 185.55078125\n",
      "[step: 389] loss: 175.04351806640625\n",
      "[step: 390] loss: 177.7095947265625\n",
      "[step: 391] loss: 183.16773986816406\n",
      "[step: 392] loss: 183.92355346679688\n",
      "[step: 393] loss: 181.78839111328125\n",
      "[step: 394] loss: 176.8359375\n",
      "[step: 395] loss: 182.3455352783203\n",
      "[step: 396] loss: 176.5331268310547\n",
      "[step: 397] loss: 171.85348510742188\n",
      "[step: 398] loss: 175.89755249023438\n",
      "[step: 399] loss: 180.60140991210938\n",
      "[step: 400] loss: 172.11395263671875\n",
      "[step: 401] loss: 185.70529174804688\n",
      "[step: 402] loss: 177.48684692382812\n",
      "[step: 403] loss: 175.17437744140625\n",
      "[step: 404] loss: 178.06951904296875\n",
      "[step: 405] loss: 169.8158416748047\n",
      "[step: 406] loss: 171.32627868652344\n",
      "[step: 407] loss: 170.0106964111328\n",
      "[step: 408] loss: 179.05299377441406\n",
      "[step: 409] loss: 171.45423889160156\n",
      "[step: 410] loss: 172.64837646484375\n",
      "[step: 411] loss: 174.5225830078125\n",
      "[step: 412] loss: 171.10177612304688\n",
      "[step: 413] loss: 173.26333618164062\n",
      "[step: 414] loss: 172.7214813232422\n",
      "[step: 415] loss: 179.5380096435547\n",
      "[step: 416] loss: 178.61734008789062\n",
      "[step: 417] loss: 168.585205078125\n",
      "[step: 418] loss: 176.1378631591797\n",
      "[step: 419] loss: 169.4683837890625\n",
      "[step: 420] loss: 170.49000549316406\n",
      "[step: 421] loss: 168.7722625732422\n",
      "[step: 422] loss: 167.75881958007812\n",
      "[step: 423] loss: 165.36087036132812\n",
      "[step: 424] loss: 171.17630004882812\n",
      "[step: 425] loss: 167.28793334960938\n",
      "[step: 426] loss: 167.55772399902344\n",
      "[step: 427] loss: 172.58212280273438\n",
      "[step: 428] loss: 167.60458374023438\n",
      "[step: 429] loss: 165.6656494140625\n",
      "[step: 430] loss: 167.03497314453125\n",
      "[step: 431] loss: 169.350341796875\n",
      "[step: 432] loss: 166.858642578125\n",
      "[step: 433] loss: 166.9080810546875\n",
      "[step: 434] loss: 170.32630920410156\n",
      "[step: 435] loss: 170.531494140625\n",
      "[step: 436] loss: 160.70352172851562\n",
      "[step: 437] loss: 163.00204467773438\n",
      "[step: 438] loss: 174.5836639404297\n",
      "[step: 439] loss: 170.30018615722656\n",
      "[step: 440] loss: 166.84564208984375\n",
      "[step: 441] loss: 168.24102783203125\n",
      "[step: 442] loss: 174.7207489013672\n",
      "[step: 443] loss: 166.62171936035156\n",
      "[step: 444] loss: 167.38204956054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 445] loss: 186.00729370117188\n",
      "[step: 446] loss: 171.88204956054688\n",
      "[step: 447] loss: 164.71072387695312\n",
      "[step: 448] loss: 174.73890686035156\n",
      "[step: 449] loss: 178.45523071289062\n",
      "[step: 450] loss: 176.645263671875\n",
      "[step: 451] loss: 172.35113525390625\n",
      "[step: 452] loss: 179.71878051757812\n",
      "[step: 453] loss: 174.23170471191406\n",
      "[step: 454] loss: 174.3016357421875\n",
      "[step: 455] loss: 178.15966796875\n",
      "[step: 456] loss: 176.58731079101562\n",
      "[step: 457] loss: 165.7836151123047\n",
      "[step: 458] loss: 181.44570922851562\n",
      "[step: 459] loss: 162.12637329101562\n",
      "[step: 460] loss: 181.15750122070312\n",
      "[step: 461] loss: 185.24517822265625\n",
      "[step: 462] loss: 162.032470703125\n",
      "[step: 463] loss: 178.06138610839844\n",
      "[step: 464] loss: 177.85592651367188\n",
      "[step: 465] loss: 169.42904663085938\n",
      "[step: 466] loss: 181.15428161621094\n",
      "[step: 467] loss: 162.47488403320312\n",
      "[step: 468] loss: 175.10516357421875\n",
      "[step: 469] loss: 166.7296600341797\n",
      "[step: 470] loss: 167.18450927734375\n",
      "[step: 471] loss: 160.91331481933594\n",
      "[step: 472] loss: 165.55905151367188\n",
      "[step: 473] loss: 167.52029418945312\n",
      "[step: 474] loss: 161.8414764404297\n",
      "[step: 475] loss: 167.1978759765625\n",
      "[step: 476] loss: 167.17819213867188\n",
      "[step: 477] loss: 168.80422973632812\n",
      "[step: 478] loss: 161.1909942626953\n",
      "[step: 479] loss: 166.8559112548828\n",
      "[step: 480] loss: 162.36688232421875\n",
      "[step: 481] loss: 159.47488403320312\n",
      "[step: 482] loss: 166.5052490234375\n",
      "[step: 483] loss: 156.69418334960938\n",
      "[step: 484] loss: 167.40023803710938\n",
      "[step: 485] loss: 161.42703247070312\n",
      "[step: 486] loss: 163.81796264648438\n",
      "[step: 487] loss: 164.81027221679688\n",
      "[step: 488] loss: 161.33419799804688\n",
      "[step: 489] loss: 172.54534912109375\n",
      "[step: 490] loss: 162.63693237304688\n",
      "[step: 491] loss: 163.1905517578125\n",
      "[step: 492] loss: 160.46238708496094\n",
      "[step: 493] loss: 172.70779418945312\n",
      "[step: 494] loss: 163.18765258789062\n",
      "[step: 495] loss: 164.128662109375\n",
      "[step: 496] loss: 168.21978759765625\n",
      "[step: 497] loss: 160.21282958984375\n",
      "[step: 498] loss: 168.5944366455078\n",
      "[step: 499] loss: 166.4359130859375\n",
      "[step: 500] loss: 168.03379821777344\n",
      "[step: 501] loss: 150.96217346191406\n",
      "[step: 502] loss: 165.7745361328125\n",
      "[step: 503] loss: 155.70370483398438\n",
      "[step: 504] loss: 159.0756072998047\n",
      "[step: 505] loss: 158.5934600830078\n",
      "[step: 506] loss: 160.34365844726562\n",
      "[step: 507] loss: 159.85693359375\n",
      "[step: 508] loss: 161.55609130859375\n",
      "[step: 509] loss: 162.60116577148438\n",
      "[step: 510] loss: 151.67742919921875\n",
      "[step: 511] loss: 160.6063232421875\n",
      "[step: 512] loss: 161.47186279296875\n",
      "[step: 513] loss: 158.058349609375\n",
      "[step: 514] loss: 157.99420166015625\n",
      "[step: 515] loss: 158.79049682617188\n",
      "[step: 516] loss: 154.8692626953125\n",
      "[step: 517] loss: 156.44131469726562\n",
      "[step: 518] loss: 152.4138641357422\n",
      "[step: 519] loss: 162.44845581054688\n",
      "[step: 520] loss: 153.3450927734375\n",
      "[step: 521] loss: 156.14459228515625\n",
      "[step: 522] loss: 151.9718017578125\n",
      "[step: 523] loss: 154.55966186523438\n",
      "[step: 524] loss: 153.69345092773438\n",
      "[step: 525] loss: 156.85345458984375\n",
      "[step: 526] loss: 156.456298828125\n",
      "[step: 527] loss: 156.24925231933594\n",
      "[step: 528] loss: 148.59254455566406\n",
      "[step: 529] loss: 158.38804626464844\n",
      "[step: 530] loss: 157.067138671875\n",
      "[step: 531] loss: 159.53997802734375\n",
      "[step: 532] loss: 150.01373291015625\n",
      "[step: 533] loss: 157.58944702148438\n",
      "[step: 534] loss: 162.96865844726562\n",
      "[step: 535] loss: 162.21133422851562\n",
      "[step: 536] loss: 152.5723114013672\n",
      "[step: 537] loss: 153.05899047851562\n",
      "[step: 538] loss: 149.1776123046875\n",
      "[step: 539] loss: 152.73590087890625\n",
      "[step: 540] loss: 152.5752716064453\n",
      "[step: 541] loss: 153.8037109375\n",
      "[step: 542] loss: 148.03179931640625\n",
      "[step: 543] loss: 150.13174438476562\n",
      "[step: 544] loss: 149.58206176757812\n",
      "[step: 545] loss: 150.34083557128906\n",
      "[step: 546] loss: 152.66079711914062\n",
      "[step: 547] loss: 154.46710205078125\n",
      "[step: 548] loss: 150.51763916015625\n",
      "[step: 549] loss: 148.06362915039062\n",
      "[step: 550] loss: 148.55584716796875\n",
      "[step: 551] loss: 146.665283203125\n",
      "[step: 552] loss: 148.6629638671875\n",
      "[step: 553] loss: 148.67877197265625\n",
      "[step: 554] loss: 147.65957641601562\n",
      "[step: 555] loss: 147.191162109375\n",
      "[step: 556] loss: 150.44955444335938\n",
      "[step: 557] loss: 147.70669555664062\n",
      "[step: 558] loss: 153.56112670898438\n",
      "[step: 559] loss: 149.3950958251953\n",
      "[step: 560] loss: 151.2138671875\n",
      "[step: 561] loss: 149.66587829589844\n",
      "[step: 562] loss: 148.63067626953125\n",
      "[step: 563] loss: 152.52655029296875\n",
      "[step: 564] loss: 154.1217498779297\n",
      "[step: 565] loss: 148.32064819335938\n",
      "[step: 566] loss: 148.78253173828125\n",
      "[step: 567] loss: 150.7660675048828\n",
      "[step: 568] loss: 154.95730590820312\n",
      "[step: 569] loss: 149.40036010742188\n",
      "[step: 570] loss: 150.5257568359375\n",
      "[step: 571] loss: 153.58474731445312\n",
      "[step: 572] loss: 150.734375\n",
      "[step: 573] loss: 155.394287109375\n",
      "[step: 574] loss: 175.00489807128906\n",
      "[step: 575] loss: 169.93145751953125\n",
      "[step: 576] loss: 156.2852020263672\n",
      "[step: 577] loss: 145.62464904785156\n",
      "[step: 578] loss: 150.7621612548828\n",
      "[step: 579] loss: 162.48696899414062\n",
      "[step: 580] loss: 145.46066284179688\n",
      "[step: 581] loss: 149.0067901611328\n",
      "[step: 582] loss: 155.3670654296875\n",
      "[step: 583] loss: 146.59095764160156\n",
      "[step: 584] loss: 140.5712890625\n",
      "[step: 585] loss: 154.53407287597656\n",
      "[step: 586] loss: 152.1612548828125\n",
      "[step: 587] loss: 143.97166442871094\n",
      "[step: 588] loss: 150.24501037597656\n",
      "[step: 589] loss: 155.0518798828125\n",
      "[step: 590] loss: 141.70901489257812\n",
      "[step: 591] loss: 146.52053833007812\n",
      "[step: 592] loss: 151.74685668945312\n",
      "[step: 593] loss: 141.3988037109375\n",
      "[step: 594] loss: 144.30563354492188\n",
      "[step: 595] loss: 154.48342895507812\n",
      "[step: 596] loss: 146.72537231445312\n",
      "[step: 597] loss: 145.90975952148438\n",
      "[step: 598] loss: 153.2269287109375\n",
      "[step: 599] loss: 151.24490356445312\n",
      "[step: 600] loss: 141.15646362304688\n",
      "[step: 601] loss: 149.65101623535156\n",
      "[step: 602] loss: 153.17263793945312\n",
      "[step: 603] loss: 139.95309448242188\n",
      "[step: 604] loss: 145.77774047851562\n",
      "[step: 605] loss: 144.03623962402344\n",
      "[step: 606] loss: 143.15380859375\n",
      "[step: 607] loss: 148.76649475097656\n",
      "[step: 608] loss: 144.7261199951172\n",
      "[step: 609] loss: 146.69378662109375\n",
      "[step: 610] loss: 142.45675659179688\n",
      "[step: 611] loss: 139.8451690673828\n",
      "[step: 612] loss: 145.10621643066406\n",
      "[step: 613] loss: 143.75848388671875\n",
      "[step: 614] loss: 146.05111694335938\n",
      "[step: 615] loss: 141.37716674804688\n",
      "[step: 616] loss: 142.943603515625\n",
      "[step: 617] loss: 141.569580078125\n",
      "[step: 618] loss: 147.5240020751953\n",
      "[step: 619] loss: 149.67495727539062\n",
      "[step: 620] loss: 159.11859130859375\n",
      "[step: 621] loss: 156.20245361328125\n",
      "[step: 622] loss: 144.57179260253906\n",
      "[step: 623] loss: 151.29129028320312\n",
      "[step: 624] loss: 147.40853881835938\n",
      "[step: 625] loss: 141.58837890625\n",
      "[step: 626] loss: 145.9296417236328\n",
      "[step: 627] loss: 146.17396545410156\n",
      "[step: 628] loss: 145.25454711914062\n",
      "[step: 629] loss: 145.73890686035156\n",
      "[step: 630] loss: 146.63609313964844\n",
      "[step: 631] loss: 151.59722900390625\n",
      "[step: 632] loss: 148.81829833984375\n",
      "[step: 633] loss: 136.6177978515625\n",
      "[step: 634] loss: 137.74737548828125\n",
      "[step: 635] loss: 148.01272583007812\n",
      "[step: 636] loss: 140.37387084960938\n",
      "[step: 637] loss: 141.0393829345703\n",
      "[step: 638] loss: 143.70291137695312\n",
      "[step: 639] loss: 141.57272338867188\n",
      "[step: 640] loss: 146.96678161621094\n",
      "[step: 641] loss: 143.95510864257812\n",
      "[step: 642] loss: 141.69532775878906\n",
      "[step: 643] loss: 137.29940795898438\n",
      "[step: 644] loss: 138.76174926757812\n",
      "[step: 645] loss: 134.91925048828125\n",
      "[step: 646] loss: 139.76528930664062\n",
      "[step: 647] loss: 137.43243408203125\n",
      "[step: 648] loss: 140.31893920898438\n",
      "[step: 649] loss: 139.14002990722656\n",
      "[step: 650] loss: 137.21316528320312\n",
      "[step: 651] loss: 138.93463134765625\n",
      "[step: 652] loss: 138.19761657714844\n",
      "[step: 653] loss: 144.13009643554688\n",
      "[step: 654] loss: 153.55447387695312\n",
      "[step: 655] loss: 174.96487426757812\n",
      "[step: 656] loss: 184.29196166992188\n",
      "[step: 657] loss: 145.84646606445312\n",
      "[step: 658] loss: 143.655029296875\n",
      "[step: 659] loss: 167.27821350097656\n",
      "[step: 660] loss: 145.6411590576172\n",
      "[step: 661] loss: 144.16644287109375\n",
      "[step: 662] loss: 167.49415588378906\n",
      "[step: 663] loss: 146.38870239257812\n",
      "[step: 664] loss: 152.42401123046875\n",
      "[step: 665] loss: 154.92742919921875\n",
      "[step: 666] loss: 145.38824462890625\n",
      "[step: 667] loss: 154.84815979003906\n",
      "[step: 668] loss: 153.88165283203125\n",
      "[step: 669] loss: 139.3644256591797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 670] loss: 148.16114807128906\n",
      "[step: 671] loss: 139.08627319335938\n",
      "[step: 672] loss: 136.7813720703125\n",
      "[step: 673] loss: 148.2603759765625\n",
      "[step: 674] loss: 149.35894775390625\n",
      "[step: 675] loss: 135.30740356445312\n",
      "[step: 676] loss: 151.90089416503906\n",
      "[step: 677] loss: 158.52259826660156\n",
      "[step: 678] loss: 137.10235595703125\n",
      "[step: 679] loss: 152.9486083984375\n",
      "[step: 680] loss: 147.47203063964844\n",
      "[step: 681] loss: 137.55914306640625\n",
      "[step: 682] loss: 149.1881561279297\n",
      "[step: 683] loss: 139.78451538085938\n",
      "[step: 684] loss: 142.33807373046875\n",
      "[step: 685] loss: 140.86782836914062\n",
      "[step: 686] loss: 137.12515258789062\n",
      "[step: 687] loss: 148.1187286376953\n",
      "[step: 688] loss: 133.84140014648438\n",
      "[step: 689] loss: 142.4013671875\n",
      "[step: 690] loss: 139.99072265625\n",
      "[step: 691] loss: 150.12615966796875\n",
      "[step: 692] loss: 144.2705078125\n",
      "[step: 693] loss: 134.6423797607422\n",
      "[step: 694] loss: 150.47341918945312\n",
      "[step: 695] loss: 134.00515747070312\n",
      "[step: 696] loss: 143.53076171875\n",
      "[step: 697] loss: 144.11273193359375\n",
      "[step: 698] loss: 132.9803466796875\n",
      "[step: 699] loss: 142.849609375\n",
      "[step: 700] loss: 133.86947631835938\n",
      "[step: 701] loss: 138.04818725585938\n",
      "[step: 702] loss: 137.79510498046875\n",
      "[step: 703] loss: 131.94363403320312\n",
      "[step: 704] loss: 139.35061645507812\n",
      "[step: 705] loss: 139.44808959960938\n",
      "[step: 706] loss: 135.93719482421875\n",
      "[step: 707] loss: 135.0862579345703\n",
      "[step: 708] loss: 136.8793487548828\n",
      "[step: 709] loss: 138.11053466796875\n",
      "[step: 710] loss: 132.8130645751953\n",
      "[step: 711] loss: 148.22238159179688\n",
      "[step: 712] loss: 133.3446044921875\n",
      "[step: 713] loss: 132.72927856445312\n",
      "[step: 714] loss: 143.2364501953125\n",
      "[step: 715] loss: 130.82186889648438\n",
      "[step: 716] loss: 145.39186096191406\n",
      "[step: 717] loss: 144.0792236328125\n",
      "[step: 718] loss: 134.52415466308594\n",
      "[step: 719] loss: 146.62661743164062\n",
      "[step: 720] loss: 131.29483032226562\n",
      "[step: 721] loss: 139.2637481689453\n",
      "[step: 722] loss: 133.1837921142578\n",
      "[step: 723] loss: 138.2259521484375\n",
      "[step: 724] loss: 138.1153106689453\n",
      "[step: 725] loss: 138.04478454589844\n",
      "[step: 726] loss: 138.1029052734375\n",
      "[step: 727] loss: 133.01333618164062\n",
      "[step: 728] loss: 137.15296936035156\n",
      "[step: 729] loss: 132.46853637695312\n",
      "[step: 730] loss: 133.3804168701172\n",
      "[step: 731] loss: 138.64532470703125\n",
      "[step: 732] loss: 134.8974151611328\n",
      "[step: 733] loss: 139.23387145996094\n",
      "[step: 734] loss: 131.5494384765625\n",
      "[step: 735] loss: 137.21670532226562\n",
      "[step: 736] loss: 143.28839111328125\n",
      "[step: 737] loss: 139.96469116210938\n",
      "[step: 738] loss: 139.48158264160156\n",
      "[step: 739] loss: 126.81120300292969\n",
      "[step: 740] loss: 142.43875122070312\n",
      "[step: 741] loss: 138.17115783691406\n",
      "[step: 742] loss: 145.70767211914062\n",
      "[step: 743] loss: 131.6990966796875\n",
      "[step: 744] loss: 150.8531494140625\n",
      "[step: 745] loss: 163.06280517578125\n",
      "[step: 746] loss: 139.18511962890625\n",
      "[step: 747] loss: 140.24267578125\n",
      "[step: 748] loss: 145.44332885742188\n",
      "[step: 749] loss: 139.82266235351562\n",
      "[step: 750] loss: 130.83502197265625\n",
      "[step: 751] loss: 140.48208618164062\n",
      "[step: 752] loss: 128.31442260742188\n",
      "[step: 753] loss: 137.83111572265625\n",
      "[step: 754] loss: 139.55531311035156\n",
      "[step: 755] loss: 135.70321655273438\n",
      "[step: 756] loss: 139.3069610595703\n",
      "[step: 757] loss: 134.07235717773438\n",
      "[step: 758] loss: 130.94869995117188\n",
      "[step: 759] loss: 136.2382354736328\n",
      "[step: 760] loss: 127.92225646972656\n",
      "[step: 761] loss: 128.9102783203125\n",
      "[step: 762] loss: 129.49600219726562\n",
      "[step: 763] loss: 127.77241516113281\n",
      "[step: 764] loss: 128.61204528808594\n",
      "[step: 765] loss: 125.204833984375\n",
      "[step: 766] loss: 131.18634033203125\n",
      "[step: 767] loss: 129.89170837402344\n",
      "[step: 768] loss: 130.95571899414062\n",
      "[step: 769] loss: 130.27639770507812\n",
      "[step: 770] loss: 132.76487731933594\n",
      "[step: 771] loss: 126.17277526855469\n",
      "[step: 772] loss: 126.09469604492188\n",
      "[step: 773] loss: 123.28572082519531\n",
      "[step: 774] loss: 127.65431213378906\n",
      "[step: 775] loss: 127.85000610351562\n",
      "[step: 776] loss: 125.07476806640625\n",
      "[step: 777] loss: 130.86358642578125\n",
      "[step: 778] loss: 138.46463012695312\n",
      "[step: 779] loss: 137.22463989257812\n",
      "[step: 780] loss: 126.95356750488281\n",
      "[step: 781] loss: 128.14369201660156\n",
      "[step: 782] loss: 122.73898315429688\n",
      "[step: 783] loss: 130.43362426757812\n",
      "[step: 784] loss: 133.71435546875\n",
      "[step: 785] loss: 127.9822998046875\n",
      "[step: 786] loss: 127.31839752197266\n",
      "[step: 787] loss: 122.77819061279297\n",
      "[step: 788] loss: 122.89796447753906\n",
      "[step: 789] loss: 133.21438598632812\n",
      "[step: 790] loss: 130.8565673828125\n",
      "[step: 791] loss: 128.47885131835938\n",
      "[step: 792] loss: 126.75638580322266\n",
      "[step: 793] loss: 129.646728515625\n",
      "[step: 794] loss: 128.45843505859375\n",
      "[step: 795] loss: 126.24588012695312\n",
      "[step: 796] loss: 125.3202133178711\n",
      "[step: 797] loss: 121.09637451171875\n",
      "[step: 798] loss: 128.2570037841797\n",
      "[step: 799] loss: 122.80711364746094\n",
      "[step: 800] loss: 124.26443481445312\n",
      "[step: 801] loss: 124.08743286132812\n",
      "[step: 802] loss: 121.3358154296875\n",
      "[step: 803] loss: 123.31566619873047\n",
      "[step: 804] loss: 130.013671875\n",
      "[step: 805] loss: 128.22418212890625\n",
      "[step: 806] loss: 130.31289672851562\n",
      "[step: 807] loss: 127.94771575927734\n",
      "[step: 808] loss: 124.36331176757812\n",
      "[step: 809] loss: 122.05888366699219\n",
      "[step: 810] loss: 118.44127655029297\n",
      "[step: 811] loss: 126.04547119140625\n",
      "[step: 812] loss: 127.70367431640625\n",
      "[step: 813] loss: 136.15419006347656\n",
      "[step: 814] loss: 146.59844970703125\n",
      "[step: 815] loss: 172.7521514892578\n",
      "[step: 816] loss: 134.94036865234375\n",
      "[step: 817] loss: 141.37713623046875\n",
      "[step: 818] loss: 139.38485717773438\n",
      "[step: 819] loss: 139.52410888671875\n",
      "[step: 820] loss: 134.63214111328125\n",
      "[step: 821] loss: 136.67393493652344\n",
      "[step: 822] loss: 146.49920654296875\n",
      "[step: 823] loss: 127.73904418945312\n",
      "[step: 824] loss: 139.08404541015625\n",
      "[step: 825] loss: 128.01806640625\n",
      "[step: 826] loss: 127.63412475585938\n",
      "[step: 827] loss: 134.20343017578125\n",
      "[step: 828] loss: 131.05618286132812\n",
      "[step: 829] loss: 126.26522064208984\n",
      "[step: 830] loss: 127.8095703125\n",
      "[step: 831] loss: 126.47525787353516\n",
      "[step: 832] loss: 123.86751556396484\n",
      "[step: 833] loss: 127.156005859375\n",
      "[step: 834] loss: 125.54969787597656\n",
      "[step: 835] loss: 126.9915771484375\n",
      "[step: 836] loss: 129.06838989257812\n",
      "[step: 837] loss: 128.30108642578125\n",
      "[step: 838] loss: 131.47927856445312\n",
      "[step: 839] loss: 129.8978729248047\n",
      "[step: 840] loss: 127.67268371582031\n",
      "[step: 841] loss: 125.72589111328125\n",
      "[step: 842] loss: 132.85443115234375\n",
      "[step: 843] loss: 134.46673583984375\n",
      "[step: 844] loss: 131.87344360351562\n",
      "[step: 845] loss: 122.61361694335938\n",
      "[step: 846] loss: 137.82891845703125\n",
      "[step: 847] loss: 135.28564453125\n",
      "[step: 848] loss: 122.7325668334961\n",
      "[step: 849] loss: 128.04403686523438\n",
      "[step: 850] loss: 134.1995849609375\n",
      "[step: 851] loss: 126.52388000488281\n",
      "[step: 852] loss: 126.98405456542969\n",
      "[step: 853] loss: 120.31315612792969\n",
      "[step: 854] loss: 122.87101745605469\n",
      "[step: 855] loss: 126.1675796508789\n",
      "[step: 856] loss: 123.87400817871094\n",
      "[step: 857] loss: 122.9642333984375\n",
      "[step: 858] loss: 125.28447723388672\n",
      "[step: 859] loss: 119.01890563964844\n",
      "[step: 860] loss: 118.46522521972656\n",
      "[step: 861] loss: 123.36837768554688\n",
      "[step: 862] loss: 129.43467712402344\n",
      "[step: 863] loss: 126.3436508178711\n",
      "[step: 864] loss: 120.65369415283203\n",
      "[step: 865] loss: 122.84498596191406\n",
      "[step: 866] loss: 122.67694091796875\n",
      "[step: 867] loss: 128.02767944335938\n",
      "[step: 868] loss: 123.15638732910156\n",
      "[step: 869] loss: 121.51089477539062\n",
      "[step: 870] loss: 118.44682312011719\n",
      "[step: 871] loss: 118.93995666503906\n",
      "[step: 872] loss: 119.41680908203125\n",
      "[step: 873] loss: 120.70133972167969\n",
      "[step: 874] loss: 120.95063781738281\n",
      "[step: 875] loss: 113.46035766601562\n",
      "[step: 876] loss: 122.23770141601562\n",
      "[step: 877] loss: 117.79669952392578\n",
      "[step: 878] loss: 120.69526672363281\n",
      "[step: 879] loss: 115.20069885253906\n",
      "[step: 880] loss: 116.99055480957031\n",
      "[step: 881] loss: 117.86679077148438\n",
      "[step: 882] loss: 118.61405944824219\n",
      "[step: 883] loss: 119.30894470214844\n",
      "[step: 884] loss: 121.93890380859375\n",
      "[step: 885] loss: 122.12162017822266\n",
      "[step: 886] loss: 120.01732635498047\n",
      "[step: 887] loss: 123.07655334472656\n",
      "[step: 888] loss: 126.48628234863281\n",
      "[step: 889] loss: 136.01394653320312\n",
      "[step: 890] loss: 149.24139404296875\n",
      "[step: 891] loss: 121.6232681274414\n",
      "[step: 892] loss: 130.11764526367188\n",
      "[step: 893] loss: 147.9066162109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 894] loss: 145.68641662597656\n",
      "[step: 895] loss: 123.63702392578125\n",
      "[step: 896] loss: 139.5668182373047\n",
      "[step: 897] loss: 143.4307098388672\n",
      "[step: 898] loss: 121.40093231201172\n",
      "[step: 899] loss: 135.45913696289062\n",
      "[step: 900] loss: 135.7253875732422\n",
      "[step: 901] loss: 123.05416107177734\n",
      "[step: 902] loss: 129.58822631835938\n",
      "[step: 903] loss: 132.87440490722656\n",
      "[step: 904] loss: 129.93704223632812\n",
      "[step: 905] loss: 118.98963928222656\n",
      "[step: 906] loss: 127.74329376220703\n",
      "[step: 907] loss: 130.0518798828125\n",
      "[step: 908] loss: 120.9385757446289\n",
      "[step: 909] loss: 123.87305450439453\n",
      "[step: 910] loss: 123.45474243164062\n",
      "[step: 911] loss: 120.26295471191406\n",
      "[step: 912] loss: 131.3386993408203\n",
      "[step: 913] loss: 118.79006958007812\n",
      "[step: 914] loss: 124.21763610839844\n",
      "[step: 915] loss: 123.25433349609375\n",
      "[step: 916] loss: 120.8037109375\n",
      "[step: 917] loss: 122.35821533203125\n",
      "[step: 918] loss: 123.36709594726562\n",
      "[step: 919] loss: 115.87001037597656\n",
      "[step: 920] loss: 116.69007873535156\n",
      "[step: 921] loss: 120.25318145751953\n",
      "[step: 922] loss: 122.20370483398438\n",
      "[step: 923] loss: 118.54376983642578\n",
      "[step: 924] loss: 115.22946166992188\n",
      "[step: 925] loss: 120.47590637207031\n",
      "[step: 926] loss: 123.36600494384766\n",
      "[step: 927] loss: 114.36659240722656\n",
      "[step: 928] loss: 120.39493560791016\n",
      "[step: 929] loss: 115.66778564453125\n",
      "[step: 930] loss: 117.37841033935547\n",
      "[step: 931] loss: 118.32992553710938\n",
      "[step: 932] loss: 120.08023071289062\n",
      "[step: 933] loss: 121.35601806640625\n",
      "[step: 934] loss: 116.86450958251953\n",
      "[step: 935] loss: 120.1836166381836\n",
      "[step: 936] loss: 117.7641372680664\n",
      "[step: 937] loss: 118.99395751953125\n",
      "[step: 938] loss: 122.98599243164062\n",
      "[step: 939] loss: 119.08063507080078\n",
      "[step: 940] loss: 114.92340850830078\n",
      "[step: 941] loss: 118.85017395019531\n",
      "[step: 942] loss: 118.20042419433594\n",
      "[step: 943] loss: 115.94648742675781\n",
      "[step: 944] loss: 119.04585266113281\n",
      "[step: 945] loss: 115.3140640258789\n",
      "[step: 946] loss: 115.58387756347656\n",
      "[step: 947] loss: 119.84341430664062\n",
      "[step: 948] loss: 114.34812927246094\n",
      "[step: 949] loss: 119.23521423339844\n",
      "[step: 950] loss: 112.63648223876953\n",
      "[step: 951] loss: 117.67320251464844\n",
      "[step: 952] loss: 122.27587890625\n",
      "[step: 953] loss: 128.737060546875\n",
      "[step: 954] loss: 135.9425811767578\n",
      "[step: 955] loss: 120.6561279296875\n",
      "[step: 956] loss: 115.64555358886719\n",
      "[step: 957] loss: 133.41773986816406\n",
      "[step: 958] loss: 121.88255310058594\n",
      "[step: 959] loss: 117.64852905273438\n",
      "[step: 960] loss: 130.8328857421875\n",
      "[step: 961] loss: 131.07994079589844\n",
      "[step: 962] loss: 123.06243896484375\n",
      "[step: 963] loss: 112.390380859375\n",
      "[step: 964] loss: 122.41032409667969\n",
      "[step: 965] loss: 118.51277160644531\n",
      "[step: 966] loss: 115.63334655761719\n",
      "[step: 967] loss: 114.03804016113281\n",
      "[step: 968] loss: 114.6898193359375\n",
      "[step: 969] loss: 115.09490966796875\n",
      "[step: 970] loss: 117.57633972167969\n",
      "[step: 971] loss: 115.81649780273438\n",
      "[step: 972] loss: 119.43675231933594\n",
      "[step: 973] loss: 122.69496154785156\n",
      "[step: 974] loss: 115.16275024414062\n",
      "[step: 975] loss: 113.12779235839844\n",
      "[step: 976] loss: 117.64370727539062\n",
      "[step: 977] loss: 120.30068969726562\n",
      "[step: 978] loss: 126.59101104736328\n",
      "[step: 979] loss: 145.54702758789062\n",
      "[step: 980] loss: 130.84982299804688\n",
      "[step: 981] loss: 121.0394287109375\n",
      "[step: 982] loss: 124.10986328125\n",
      "[step: 983] loss: 120.48814392089844\n",
      "[step: 984] loss: 125.44253540039062\n",
      "[step: 985] loss: 122.97782897949219\n",
      "[step: 986] loss: 120.94615173339844\n",
      "[step: 987] loss: 119.6572494506836\n",
      "[step: 988] loss: 116.97328186035156\n",
      "[step: 989] loss: 123.97281646728516\n",
      "[step: 990] loss: 118.38031768798828\n",
      "[step: 991] loss: 114.67095184326172\n",
      "[step: 992] loss: 118.33352661132812\n",
      "[step: 993] loss: 117.2449951171875\n",
      "[step: 994] loss: 112.84799194335938\n",
      "[step: 995] loss: 116.0283432006836\n",
      "[step: 996] loss: 114.17353820800781\n",
      "[step: 997] loss: 118.23572540283203\n",
      "[step: 998] loss: 114.00238037109375\n",
      "[step: 999] loss: 113.08311462402344\n",
      "[step: 1000] loss: 115.63351440429688\n",
      "[step: 1001] loss: 120.85337829589844\n",
      "[step: 1002] loss: 115.33137512207031\n",
      "[step: 1003] loss: 115.89225006103516\n",
      "[step: 1004] loss: 110.97779846191406\n",
      "[step: 1005] loss: 113.00767517089844\n",
      "[step: 1006] loss: 125.49116516113281\n",
      "[step: 1007] loss: 135.20677185058594\n",
      "[step: 1008] loss: 133.67575073242188\n",
      "[step: 1009] loss: 120.76065063476562\n",
      "[step: 1010] loss: 114.80338287353516\n",
      "[step: 1011] loss: 113.78041076660156\n",
      "[step: 1012] loss: 111.61466979980469\n",
      "[step: 1013] loss: 115.59188079833984\n",
      "[step: 1014] loss: 113.8519287109375\n",
      "[step: 1015] loss: 107.77555847167969\n",
      "[step: 1016] loss: 113.99946594238281\n",
      "[step: 1017] loss: 117.31280517578125\n",
      "[step: 1018] loss: 111.13368225097656\n",
      "[step: 1019] loss: 108.52151489257812\n",
      "[step: 1020] loss: 110.31819152832031\n",
      "[step: 1021] loss: 108.94268035888672\n",
      "[step: 1022] loss: 112.09625244140625\n",
      "[step: 1023] loss: 120.29110717773438\n",
      "[step: 1024] loss: 113.87486267089844\n",
      "[step: 1025] loss: 110.98332214355469\n",
      "[step: 1026] loss: 111.07083892822266\n",
      "[step: 1027] loss: 111.73341369628906\n",
      "[step: 1028] loss: 114.5576171875\n",
      "[step: 1029] loss: 120.03697204589844\n",
      "[step: 1030] loss: 117.0832290649414\n",
      "[step: 1031] loss: 107.99606323242188\n",
      "[step: 1032] loss: 114.75641632080078\n",
      "[step: 1033] loss: 115.87164306640625\n",
      "[step: 1034] loss: 107.61274719238281\n",
      "[step: 1035] loss: 114.17286682128906\n",
      "[step: 1036] loss: 128.0335693359375\n",
      "[step: 1037] loss: 115.54849243164062\n",
      "[step: 1038] loss: 113.09040832519531\n",
      "[step: 1039] loss: 110.99505615234375\n",
      "[step: 1040] loss: 112.271240234375\n",
      "[step: 1041] loss: 114.36408233642578\n",
      "[step: 1042] loss: 117.97747802734375\n",
      "[step: 1043] loss: 112.02310943603516\n",
      "[step: 1044] loss: 119.58700561523438\n",
      "[step: 1045] loss: 115.16938781738281\n",
      "[step: 1046] loss: 107.526611328125\n",
      "[step: 1047] loss: 107.13884735107422\n",
      "[step: 1048] loss: 111.78573608398438\n",
      "[step: 1049] loss: 112.65402221679688\n",
      "[step: 1050] loss: 107.38914489746094\n",
      "[step: 1051] loss: 108.3400650024414\n",
      "[step: 1052] loss: 113.48712158203125\n",
      "[step: 1053] loss: 113.87479400634766\n",
      "[step: 1054] loss: 109.17071533203125\n",
      "[step: 1055] loss: 107.95167541503906\n",
      "[step: 1056] loss: 106.45034790039062\n",
      "[step: 1057] loss: 114.4648666381836\n",
      "[step: 1058] loss: 107.53976440429688\n",
      "[step: 1059] loss: 112.26326751708984\n",
      "[step: 1060] loss: 112.85176086425781\n",
      "[step: 1061] loss: 110.09657287597656\n",
      "[step: 1062] loss: 113.45818328857422\n",
      "[step: 1063] loss: 106.22988891601562\n",
      "[step: 1064] loss: 110.60128784179688\n",
      "[step: 1065] loss: 112.54679870605469\n",
      "[step: 1066] loss: 112.64208984375\n",
      "[step: 1067] loss: 119.10107421875\n",
      "[step: 1068] loss: 117.385009765625\n",
      "[step: 1069] loss: 120.14270782470703\n",
      "[step: 1070] loss: 114.30874633789062\n",
      "[step: 1071] loss: 110.71530151367188\n",
      "[step: 1072] loss: 106.69229125976562\n",
      "[step: 1073] loss: 114.90045166015625\n",
      "[step: 1074] loss: 124.01577758789062\n",
      "[step: 1075] loss: 111.28500366210938\n",
      "[step: 1076] loss: 107.6782455444336\n",
      "[step: 1077] loss: 109.97757720947266\n",
      "[step: 1078] loss: 109.25696563720703\n",
      "[step: 1079] loss: 113.16133117675781\n",
      "[step: 1080] loss: 107.35520935058594\n",
      "[step: 1081] loss: 114.46096801757812\n",
      "[step: 1082] loss: 121.08682250976562\n",
      "[step: 1083] loss: 112.17546081542969\n",
      "[step: 1084] loss: 100.07086944580078\n",
      "[step: 1085] loss: 114.48148345947266\n",
      "[step: 1086] loss: 110.33549499511719\n",
      "[step: 1087] loss: 108.56982421875\n",
      "[step: 1088] loss: 107.5260009765625\n",
      "[step: 1089] loss: 109.05056762695312\n",
      "[step: 1090] loss: 105.47396087646484\n",
      "[step: 1091] loss: 110.04827880859375\n",
      "[step: 1092] loss: 107.71790313720703\n",
      "[step: 1093] loss: 103.8770980834961\n",
      "[step: 1094] loss: 103.52032470703125\n",
      "[step: 1095] loss: 109.15635681152344\n",
      "[step: 1096] loss: 106.86479187011719\n",
      "[step: 1097] loss: 105.30145263671875\n",
      "[step: 1098] loss: 108.8778305053711\n",
      "[step: 1099] loss: 106.95189666748047\n",
      "[step: 1100] loss: 115.04444885253906\n",
      "[step: 1101] loss: 110.37730407714844\n",
      "[step: 1102] loss: 104.27275085449219\n",
      "[step: 1103] loss: 110.79766845703125\n",
      "[step: 1104] loss: 105.11770629882812\n",
      "[step: 1105] loss: 106.39848327636719\n",
      "[step: 1106] loss: 113.27845764160156\n",
      "[step: 1107] loss: 118.85345458984375\n",
      "[step: 1108] loss: 117.49037170410156\n",
      "[step: 1109] loss: 113.12873840332031\n",
      "[step: 1110] loss: 108.28669738769531\n",
      "[step: 1111] loss: 110.51007080078125\n",
      "[step: 1112] loss: 108.76579284667969\n",
      "[step: 1113] loss: 104.58551025390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1114] loss: 111.56886291503906\n",
      "[step: 1115] loss: 122.31741333007812\n",
      "[step: 1116] loss: 106.91387939453125\n",
      "[step: 1117] loss: 113.91281127929688\n",
      "[step: 1118] loss: 120.50717163085938\n",
      "[step: 1119] loss: 109.85850524902344\n",
      "[step: 1120] loss: 108.85107421875\n",
      "[step: 1121] loss: 118.2962875366211\n",
      "[step: 1122] loss: 112.29977416992188\n",
      "[step: 1123] loss: 117.02976989746094\n",
      "[step: 1124] loss: 114.09637451171875\n",
      "[step: 1125] loss: 104.5523681640625\n",
      "[step: 1126] loss: 107.47266387939453\n",
      "[step: 1127] loss: 105.89686584472656\n",
      "[step: 1128] loss: 105.07137298583984\n",
      "[step: 1129] loss: 107.17166137695312\n",
      "[step: 1130] loss: 107.92510986328125\n",
      "[step: 1131] loss: 106.35118103027344\n",
      "[step: 1132] loss: 114.32009887695312\n",
      "[step: 1133] loss: 110.20050048828125\n",
      "[step: 1134] loss: 109.86537170410156\n",
      "[step: 1135] loss: 109.21932983398438\n",
      "[step: 1136] loss: 111.25009155273438\n",
      "[step: 1137] loss: 103.70538330078125\n",
      "[step: 1138] loss: 109.98103332519531\n",
      "[step: 1139] loss: 120.41925048828125\n",
      "[step: 1140] loss: 133.47830200195312\n",
      "[step: 1141] loss: 113.45010375976562\n",
      "[step: 1142] loss: 108.42465209960938\n",
      "[step: 1143] loss: 119.14594268798828\n",
      "[step: 1144] loss: 114.61184692382812\n",
      "[step: 1145] loss: 105.67718505859375\n",
      "[step: 1146] loss: 120.025634765625\n",
      "[step: 1147] loss: 114.91765594482422\n",
      "[step: 1148] loss: 109.02308654785156\n",
      "[step: 1149] loss: 107.79313659667969\n",
      "[step: 1150] loss: 119.04031372070312\n",
      "[step: 1151] loss: 109.47482299804688\n",
      "[step: 1152] loss: 102.79299926757812\n",
      "[step: 1153] loss: 120.20628356933594\n",
      "[step: 1154] loss: 111.71781921386719\n",
      "[step: 1155] loss: 106.742919921875\n",
      "[step: 1156] loss: 114.57585906982422\n",
      "[step: 1157] loss: 112.74996948242188\n",
      "[step: 1158] loss: 104.61741638183594\n",
      "[step: 1159] loss: 114.99250793457031\n",
      "[step: 1160] loss: 124.0862808227539\n",
      "[step: 1161] loss: 106.97954559326172\n",
      "[step: 1162] loss: 114.53700256347656\n",
      "[step: 1163] loss: 124.06742858886719\n",
      "[step: 1164] loss: 106.30194091796875\n",
      "[step: 1165] loss: 134.84593200683594\n",
      "[step: 1166] loss: 119.80413818359375\n",
      "[step: 1167] loss: 116.83969116210938\n",
      "[step: 1168] loss: 116.95359802246094\n",
      "[step: 1169] loss: 115.85420227050781\n",
      "[step: 1170] loss: 113.72162628173828\n",
      "[step: 1171] loss: 109.87081909179688\n",
      "[step: 1172] loss: 118.06901550292969\n",
      "[step: 1173] loss: 103.05785369873047\n",
      "[step: 1174] loss: 113.58015441894531\n",
      "[step: 1175] loss: 110.61321258544922\n",
      "[step: 1176] loss: 106.80706787109375\n",
      "[step: 1177] loss: 107.35206604003906\n",
      "[step: 1178] loss: 107.70936584472656\n",
      "[step: 1179] loss: 111.48442077636719\n",
      "[step: 1180] loss: 109.11769104003906\n",
      "[step: 1181] loss: 106.6097412109375\n",
      "[step: 1182] loss: 108.26848602294922\n",
      "[step: 1183] loss: 107.49159240722656\n",
      "[step: 1184] loss: 106.47396850585938\n",
      "[step: 1185] loss: 105.33025360107422\n",
      "[step: 1186] loss: 108.78717041015625\n",
      "[step: 1187] loss: 108.59770202636719\n",
      "[step: 1188] loss: 102.10028076171875\n",
      "[step: 1189] loss: 111.95944213867188\n",
      "[step: 1190] loss: 100.61607360839844\n",
      "[step: 1191] loss: 106.56719970703125\n",
      "[step: 1192] loss: 107.884765625\n",
      "[step: 1193] loss: 108.44052124023438\n",
      "[step: 1194] loss: 112.56643676757812\n",
      "[step: 1195] loss: 104.2362060546875\n",
      "[step: 1196] loss: 102.91600036621094\n",
      "[step: 1197] loss: 103.15933227539062\n",
      "[step: 1198] loss: 107.67015075683594\n",
      "[step: 1199] loss: 101.93574523925781\n",
      "[step: 1200] loss: 107.25011444091797\n",
      "[step: 1201] loss: 103.31221008300781\n",
      "[step: 1202] loss: 107.99253845214844\n",
      "[step: 1203] loss: 112.66539001464844\n",
      "[step: 1204] loss: 109.27095794677734\n",
      "[step: 1205] loss: 103.09583282470703\n",
      "[step: 1206] loss: 104.72950744628906\n",
      "[step: 1207] loss: 99.70083618164062\n",
      "[step: 1208] loss: 99.81228637695312\n",
      "[step: 1209] loss: 101.5963134765625\n",
      "[step: 1210] loss: 102.45602416992188\n",
      "[step: 1211] loss: 105.30122375488281\n",
      "[step: 1212] loss: 101.9706039428711\n",
      "[step: 1213] loss: 101.56654357910156\n",
      "[step: 1214] loss: 104.84953308105469\n",
      "[step: 1215] loss: 105.1995849609375\n",
      "[step: 1216] loss: 101.8515625\n",
      "[step: 1217] loss: 99.23082733154297\n",
      "[step: 1218] loss: 94.64885711669922\n",
      "[step: 1219] loss: 105.80387878417969\n",
      "[step: 1220] loss: 114.30157470703125\n",
      "[step: 1221] loss: 125.16535949707031\n",
      "[step: 1222] loss: 116.48542785644531\n",
      "[step: 1223] loss: 108.56571197509766\n",
      "[step: 1224] loss: 108.67328643798828\n",
      "[step: 1225] loss: 112.28626251220703\n",
      "[step: 1226] loss: 112.41572570800781\n",
      "[step: 1227] loss: 100.94660949707031\n",
      "[step: 1228] loss: 111.36516571044922\n",
      "[step: 1229] loss: 103.11178588867188\n",
      "[step: 1230] loss: 104.43208312988281\n",
      "[step: 1231] loss: 113.49927520751953\n",
      "[step: 1232] loss: 105.16102600097656\n",
      "[step: 1233] loss: 103.99290466308594\n",
      "[step: 1234] loss: 109.57939147949219\n",
      "[step: 1235] loss: 113.64580535888672\n",
      "[step: 1236] loss: 119.5196533203125\n",
      "[step: 1237] loss: 113.88526916503906\n",
      "[step: 1238] loss: 102.57890319824219\n",
      "[step: 1239] loss: 112.4228286743164\n",
      "[step: 1240] loss: 109.87017059326172\n",
      "[step: 1241] loss: 103.06227111816406\n",
      "[step: 1242] loss: 103.14263916015625\n",
      "[step: 1243] loss: 103.74557495117188\n",
      "[step: 1244] loss: 103.13751220703125\n",
      "[step: 1245] loss: 102.58309936523438\n",
      "[step: 1246] loss: 108.44532775878906\n",
      "[step: 1247] loss: 103.69425201416016\n",
      "[step: 1248] loss: 97.57594299316406\n",
      "[step: 1249] loss: 101.97953796386719\n",
      "[step: 1250] loss: 100.3246078491211\n",
      "[step: 1251] loss: 104.11675262451172\n",
      "[step: 1252] loss: 100.0964126586914\n",
      "[step: 1253] loss: 99.50582122802734\n",
      "[step: 1254] loss: 104.17352294921875\n",
      "[step: 1255] loss: 104.90441131591797\n",
      "[step: 1256] loss: 105.47679138183594\n",
      "[step: 1257] loss: 100.54054260253906\n",
      "[step: 1258] loss: 99.28529357910156\n",
      "[step: 1259] loss: 105.37706756591797\n",
      "[step: 1260] loss: 120.60858154296875\n",
      "[step: 1261] loss: 127.37068176269531\n",
      "[step: 1262] loss: 100.62530517578125\n",
      "[step: 1263] loss: 113.0155029296875\n",
      "[step: 1264] loss: 132.3540802001953\n",
      "[step: 1265] loss: 101.95817565917969\n",
      "[step: 1266] loss: 120.67074584960938\n",
      "[step: 1267] loss: 111.91798400878906\n",
      "[step: 1268] loss: 118.23281860351562\n",
      "[step: 1269] loss: 109.13037109375\n",
      "[step: 1270] loss: 108.09251403808594\n",
      "[step: 1271] loss: 102.73384094238281\n",
      "[step: 1272] loss: 107.22248077392578\n",
      "[step: 1273] loss: 108.12277221679688\n",
      "[step: 1274] loss: 99.74516296386719\n",
      "[step: 1275] loss: 108.49894714355469\n",
      "[step: 1276] loss: 100.55947875976562\n",
      "[step: 1277] loss: 98.54229736328125\n",
      "[step: 1278] loss: 102.259033203125\n",
      "[step: 1279] loss: 97.81021118164062\n",
      "[step: 1280] loss: 98.8865966796875\n",
      "[step: 1281] loss: 98.3367919921875\n",
      "[step: 1282] loss: 104.2309799194336\n",
      "[step: 1283] loss: 100.47470092773438\n",
      "[step: 1284] loss: 101.61357116699219\n",
      "[step: 1285] loss: 96.94754028320312\n",
      "[step: 1286] loss: 102.75990295410156\n",
      "[step: 1287] loss: 99.27024841308594\n",
      "[step: 1288] loss: 93.78924560546875\n",
      "[step: 1289] loss: 98.99850463867188\n",
      "[step: 1290] loss: 98.14573669433594\n",
      "[step: 1291] loss: 99.2818603515625\n",
      "[step: 1292] loss: 98.54472351074219\n",
      "[step: 1293] loss: 102.92935180664062\n",
      "[step: 1294] loss: 102.11647033691406\n",
      "[step: 1295] loss: 107.53610229492188\n",
      "[step: 1296] loss: 111.76469421386719\n",
      "[step: 1297] loss: 105.16881561279297\n",
      "[step: 1298] loss: 96.29145812988281\n",
      "[step: 1299] loss: 99.59654235839844\n",
      "[step: 1300] loss: 100.0947265625\n",
      "[step: 1301] loss: 97.64848327636719\n",
      "[step: 1302] loss: 97.92462158203125\n",
      "[step: 1303] loss: 99.02609252929688\n",
      "[step: 1304] loss: 105.20649719238281\n",
      "[step: 1305] loss: 94.48507690429688\n",
      "[step: 1306] loss: 95.49786376953125\n",
      "[step: 1307] loss: 95.30531311035156\n",
      "[step: 1308] loss: 98.49763488769531\n",
      "[step: 1309] loss: 98.0082778930664\n",
      "[step: 1310] loss: 102.80491638183594\n",
      "[step: 1311] loss: 100.98330688476562\n",
      "[step: 1312] loss: 97.90926361083984\n",
      "[step: 1313] loss: 95.81180572509766\n",
      "[step: 1314] loss: 97.79016876220703\n",
      "[step: 1315] loss: 104.23036193847656\n",
      "[step: 1316] loss: 104.26363372802734\n",
      "[step: 1317] loss: 108.83580017089844\n",
      "[step: 1318] loss: 99.01126098632812\n",
      "[step: 1319] loss: 98.06242370605469\n",
      "[step: 1320] loss: 101.89382934570312\n",
      "[step: 1321] loss: 100.88069915771484\n",
      "[step: 1322] loss: 106.48300170898438\n",
      "[step: 1323] loss: 99.74763488769531\n",
      "[step: 1324] loss: 95.73451232910156\n",
      "[step: 1325] loss: 110.25086975097656\n",
      "[step: 1326] loss: 105.26634216308594\n",
      "[step: 1327] loss: 98.657958984375\n",
      "[step: 1328] loss: 96.53778076171875\n",
      "[step: 1329] loss: 103.29530334472656\n",
      "[step: 1330] loss: 101.28609466552734\n",
      "[step: 1331] loss: 99.87661743164062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1332] loss: 97.37863159179688\n",
      "[step: 1333] loss: 104.88121032714844\n",
      "[step: 1334] loss: 103.29136657714844\n",
      "[step: 1335] loss: 100.279541015625\n",
      "[step: 1336] loss: 96.4137954711914\n",
      "[step: 1337] loss: 97.316162109375\n",
      "[step: 1338] loss: 100.37373352050781\n",
      "[step: 1339] loss: 96.82919311523438\n",
      "[step: 1340] loss: 107.32823944091797\n",
      "[step: 1341] loss: 106.86143493652344\n",
      "[step: 1342] loss: 101.07098388671875\n",
      "[step: 1343] loss: 97.77316284179688\n",
      "[step: 1344] loss: 104.18507385253906\n",
      "[step: 1345] loss: 102.36477661132812\n",
      "[step: 1346] loss: 98.24524688720703\n",
      "[step: 1347] loss: 103.88943481445312\n",
      "[step: 1348] loss: 106.38284301757812\n",
      "[step: 1349] loss: 100.48741149902344\n",
      "[step: 1350] loss: 95.0244369506836\n",
      "[step: 1351] loss: 109.61952209472656\n",
      "[step: 1352] loss: 116.83680725097656\n",
      "[step: 1353] loss: 96.63871765136719\n",
      "[step: 1354] loss: 108.21940612792969\n",
      "[step: 1355] loss: 114.98524475097656\n",
      "[step: 1356] loss: 98.64241027832031\n",
      "[step: 1357] loss: 113.19648742675781\n",
      "[step: 1358] loss: 114.56669616699219\n",
      "[step: 1359] loss: 96.9506607055664\n",
      "[step: 1360] loss: 108.48341369628906\n",
      "[step: 1361] loss: 100.30839538574219\n",
      "[step: 1362] loss: 96.9478759765625\n",
      "[step: 1363] loss: 105.14747619628906\n",
      "[step: 1364] loss: 105.12470245361328\n",
      "[step: 1365] loss: 95.04148864746094\n",
      "[step: 1366] loss: 107.66969299316406\n",
      "[step: 1367] loss: 101.89022827148438\n",
      "[step: 1368] loss: 100.150634765625\n",
      "[step: 1369] loss: 98.29336547851562\n",
      "[step: 1370] loss: 100.83087158203125\n",
      "[step: 1371] loss: 99.60223388671875\n",
      "[step: 1372] loss: 93.69898986816406\n",
      "[step: 1373] loss: 101.61492156982422\n",
      "[step: 1374] loss: 107.37976837158203\n",
      "[step: 1375] loss: 104.20832824707031\n",
      "[step: 1376] loss: 101.70805358886719\n",
      "[step: 1377] loss: 115.57566833496094\n",
      "[step: 1378] loss: 106.00028991699219\n",
      "[step: 1379] loss: 109.16117095947266\n",
      "[step: 1380] loss: 106.39358520507812\n",
      "[step: 1381] loss: 107.72956848144531\n",
      "[step: 1382] loss: 105.23789978027344\n",
      "[step: 1383] loss: 94.29792785644531\n",
      "[step: 1384] loss: 105.69682312011719\n",
      "[step: 1385] loss: 100.2137451171875\n",
      "[step: 1386] loss: 99.22999572753906\n",
      "[step: 1387] loss: 103.12239074707031\n",
      "[step: 1388] loss: 95.46631622314453\n",
      "[step: 1389] loss: 94.71981811523438\n",
      "[step: 1390] loss: 102.6222915649414\n",
      "[step: 1391] loss: 96.05020904541016\n",
      "[step: 1392] loss: 102.1004409790039\n",
      "[step: 1393] loss: 93.25537872314453\n",
      "[step: 1394] loss: 96.0792236328125\n",
      "[step: 1395] loss: 100.62348937988281\n",
      "[step: 1396] loss: 99.71186828613281\n",
      "[step: 1397] loss: 98.92842864990234\n",
      "[step: 1398] loss: 96.80469512939453\n",
      "[step: 1399] loss: 95.66568756103516\n",
      "[step: 1400] loss: 93.20257568359375\n",
      "[step: 1401] loss: 91.85674285888672\n",
      "[step: 1402] loss: 92.76921844482422\n",
      "[step: 1403] loss: 95.4070816040039\n",
      "[step: 1404] loss: 98.18852996826172\n",
      "[step: 1405] loss: 95.54612731933594\n",
      "[step: 1406] loss: 93.0371322631836\n",
      "[step: 1407] loss: 93.38507080078125\n",
      "[step: 1408] loss: 94.50758361816406\n",
      "[step: 1409] loss: 93.0936279296875\n",
      "[step: 1410] loss: 96.33416748046875\n",
      "[step: 1411] loss: 95.26663208007812\n",
      "[step: 1412] loss: 95.3343505859375\n",
      "[step: 1413] loss: 93.2256088256836\n",
      "[step: 1414] loss: 98.70271301269531\n",
      "[step: 1415] loss: 104.47042846679688\n",
      "[step: 1416] loss: 100.13398742675781\n",
      "[step: 1417] loss: 103.68540954589844\n",
      "[step: 1418] loss: 94.8893051147461\n",
      "[step: 1419] loss: 93.23218536376953\n",
      "[step: 1420] loss: 96.17236328125\n",
      "[step: 1421] loss: 97.13406372070312\n",
      "[step: 1422] loss: 94.76866912841797\n",
      "[step: 1423] loss: 91.9031982421875\n",
      "[step: 1424] loss: 93.117919921875\n",
      "[step: 1425] loss: 102.50628662109375\n",
      "[step: 1426] loss: 99.56219482421875\n",
      "[step: 1427] loss: 96.27565002441406\n",
      "[step: 1428] loss: 93.88323974609375\n",
      "[step: 1429] loss: 96.94099426269531\n",
      "[step: 1430] loss: 105.33963775634766\n",
      "[step: 1431] loss: 99.9085693359375\n",
      "[step: 1432] loss: 92.59892272949219\n",
      "[step: 1433] loss: 96.36325073242188\n",
      "[step: 1434] loss: 102.81708526611328\n",
      "[step: 1435] loss: 104.81321716308594\n",
      "[step: 1436] loss: 98.72450256347656\n",
      "[step: 1437] loss: 92.47000885009766\n",
      "[step: 1438] loss: 95.27511596679688\n",
      "[step: 1439] loss: 99.74746704101562\n",
      "[step: 1440] loss: 95.86430358886719\n",
      "[step: 1441] loss: 93.10092163085938\n",
      "[step: 1442] loss: 103.57965087890625\n",
      "[step: 1443] loss: 101.20767211914062\n",
      "[step: 1444] loss: 94.60682678222656\n",
      "[step: 1445] loss: 95.21754455566406\n",
      "[step: 1446] loss: 99.33538055419922\n",
      "[step: 1447] loss: 100.9630355834961\n",
      "[step: 1448] loss: 103.89773559570312\n",
      "[step: 1449] loss: 100.07737731933594\n",
      "[step: 1450] loss: 101.54095458984375\n",
      "[step: 1451] loss: 93.18040466308594\n",
      "[step: 1452] loss: 90.93002319335938\n",
      "[step: 1453] loss: 95.72169494628906\n",
      "[step: 1454] loss: 92.33683776855469\n",
      "[step: 1455] loss: 94.68072509765625\n",
      "[step: 1456] loss: 95.09913635253906\n",
      "[step: 1457] loss: 93.22447204589844\n",
      "[step: 1458] loss: 95.39085388183594\n",
      "[step: 1459] loss: 91.48814392089844\n",
      "[step: 1460] loss: 90.24046325683594\n",
      "[step: 1461] loss: 93.3411865234375\n",
      "[step: 1462] loss: 93.47857666015625\n",
      "[step: 1463] loss: 94.23175811767578\n",
      "[step: 1464] loss: 106.33123779296875\n",
      "[step: 1465] loss: 113.09140014648438\n",
      "[step: 1466] loss: 117.679443359375\n",
      "[step: 1467] loss: 95.63795471191406\n",
      "[step: 1468] loss: 95.24009704589844\n",
      "[step: 1469] loss: 99.42984008789062\n",
      "[step: 1470] loss: 97.05023193359375\n",
      "[step: 1471] loss: 94.78721618652344\n",
      "[step: 1472] loss: 93.62410736083984\n",
      "[step: 1473] loss: 97.7425308227539\n",
      "[step: 1474] loss: 93.44105529785156\n",
      "[step: 1475] loss: 91.45549774169922\n",
      "[step: 1476] loss: 93.050537109375\n",
      "[step: 1477] loss: 92.72952270507812\n",
      "[step: 1478] loss: 94.79351806640625\n",
      "[step: 1479] loss: 98.54414367675781\n",
      "[step: 1480] loss: 98.57947540283203\n",
      "[step: 1481] loss: 92.434814453125\n",
      "[step: 1482] loss: 96.78889465332031\n",
      "[step: 1483] loss: 107.93635559082031\n",
      "[step: 1484] loss: 92.95014190673828\n",
      "[step: 1485] loss: 91.2882308959961\n",
      "[step: 1486] loss: 105.47273254394531\n",
      "[step: 1487] loss: 98.1468505859375\n",
      "[step: 1488] loss: 100.99312591552734\n",
      "[step: 1489] loss: 96.12371063232422\n",
      "[step: 1490] loss: 97.28471374511719\n",
      "[step: 1491] loss: 102.3072280883789\n",
      "[step: 1492] loss: 94.96590423583984\n",
      "[step: 1493] loss: 94.57379913330078\n",
      "[step: 1494] loss: 95.7909927368164\n",
      "[step: 1495] loss: 94.83566284179688\n",
      "[step: 1496] loss: 97.88420104980469\n",
      "[step: 1497] loss: 101.48457336425781\n",
      "[step: 1498] loss: 95.01480102539062\n",
      "[step: 1499] loss: 95.81971740722656\n",
      "[step: 1500] loss: 92.68287658691406\n",
      "[step: 1501] loss: 94.20620727539062\n",
      "[step: 1502] loss: 96.42570495605469\n",
      "[step: 1503] loss: 92.45945739746094\n",
      "[step: 1504] loss: 89.25173950195312\n",
      "[step: 1505] loss: 92.76183319091797\n",
      "[step: 1506] loss: 92.66852569580078\n",
      "[step: 1507] loss: 88.39695739746094\n",
      "[step: 1508] loss: 89.69627380371094\n",
      "[step: 1509] loss: 92.9044418334961\n",
      "[step: 1510] loss: 93.84552764892578\n",
      "[step: 1511] loss: 95.68585205078125\n",
      "[step: 1512] loss: 90.28953552246094\n",
      "[step: 1513] loss: 90.45545959472656\n",
      "[step: 1514] loss: 90.51551818847656\n",
      "[step: 1515] loss: 90.05425262451172\n",
      "[step: 1516] loss: 90.23709106445312\n",
      "[step: 1517] loss: 92.08793640136719\n",
      "[step: 1518] loss: 92.62240600585938\n",
      "[step: 1519] loss: 95.46996307373047\n",
      "[step: 1520] loss: 97.5270767211914\n",
      "[step: 1521] loss: 94.15948486328125\n",
      "[step: 1522] loss: 91.26042175292969\n",
      "[step: 1523] loss: 96.59192657470703\n",
      "[step: 1524] loss: 100.9073257446289\n",
      "[step: 1525] loss: 102.79622650146484\n",
      "[step: 1526] loss: 98.40594482421875\n",
      "[step: 1527] loss: 93.81242370605469\n",
      "[step: 1528] loss: 90.70252227783203\n",
      "[step: 1529] loss: 99.31596374511719\n",
      "[step: 1530] loss: 93.10892486572266\n",
      "[step: 1531] loss: 91.85608673095703\n",
      "[step: 1532] loss: 90.19132995605469\n",
      "[step: 1533] loss: 91.53070068359375\n",
      "[step: 1534] loss: 92.54142761230469\n",
      "[step: 1535] loss: 93.95240783691406\n",
      "[step: 1536] loss: 94.65435028076172\n",
      "[step: 1537] loss: 90.53065490722656\n",
      "[step: 1538] loss: 90.4500732421875\n",
      "[step: 1539] loss: 94.53631591796875\n",
      "[step: 1540] loss: 98.57259368896484\n",
      "[step: 1541] loss: 90.57963562011719\n",
      "[step: 1542] loss: 93.51934051513672\n",
      "[step: 1543] loss: 109.74734497070312\n",
      "[step: 1544] loss: 101.5140151977539\n",
      "[step: 1545] loss: 92.08625030517578\n",
      "[step: 1546] loss: 95.38683319091797\n",
      "[step: 1547] loss: 94.49859619140625\n",
      "[step: 1548] loss: 89.81748962402344\n",
      "[step: 1549] loss: 91.92192077636719\n",
      "[step: 1550] loss: 91.21658325195312\n",
      "[step: 1551] loss: 91.49337005615234\n",
      "[step: 1552] loss: 95.70347595214844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1553] loss: 94.99085235595703\n",
      "[step: 1554] loss: 94.343505859375\n",
      "[step: 1555] loss: 89.12101745605469\n",
      "[step: 1556] loss: 92.63026428222656\n",
      "[step: 1557] loss: 95.41983795166016\n",
      "[step: 1558] loss: 92.08905029296875\n",
      "[step: 1559] loss: 94.85778045654297\n",
      "[step: 1560] loss: 102.00172424316406\n",
      "[step: 1561] loss: 92.91397094726562\n",
      "[step: 1562] loss: 93.95311737060547\n",
      "[step: 1563] loss: 102.32334899902344\n",
      "[step: 1564] loss: 94.58783721923828\n",
      "[step: 1565] loss: 94.77115631103516\n",
      "[step: 1566] loss: 92.99717712402344\n",
      "[step: 1567] loss: 93.0980224609375\n",
      "[step: 1568] loss: 95.09121704101562\n",
      "[step: 1569] loss: 90.31277465820312\n",
      "[step: 1570] loss: 90.91714477539062\n",
      "[step: 1571] loss: 93.17314147949219\n",
      "[step: 1572] loss: 87.83959197998047\n",
      "[step: 1573] loss: 89.13719940185547\n",
      "[step: 1574] loss: 91.86761474609375\n",
      "[step: 1575] loss: 87.77705383300781\n",
      "[step: 1576] loss: 94.39966583251953\n",
      "[step: 1577] loss: 91.09956359863281\n",
      "[step: 1578] loss: 90.71656799316406\n",
      "[step: 1579] loss: 91.63961791992188\n",
      "[step: 1580] loss: 87.38390350341797\n",
      "[step: 1581] loss: 86.72859191894531\n",
      "[step: 1582] loss: 91.2529296875\n",
      "[step: 1583] loss: 91.44965362548828\n",
      "[step: 1584] loss: 91.58024597167969\n",
      "[step: 1585] loss: 89.90945434570312\n",
      "[step: 1586] loss: 91.9792709350586\n",
      "[step: 1587] loss: 92.93633270263672\n",
      "[step: 1588] loss: 93.33806610107422\n",
      "[step: 1589] loss: 90.72674560546875\n",
      "[step: 1590] loss: 90.08340454101562\n",
      "[step: 1591] loss: 90.55987548828125\n",
      "[step: 1592] loss: 90.1773681640625\n",
      "[step: 1593] loss: 89.97908020019531\n",
      "[step: 1594] loss: 91.36064147949219\n",
      "[step: 1595] loss: 85.16220092773438\n",
      "[step: 1596] loss: 85.30547332763672\n",
      "[step: 1597] loss: 86.26017761230469\n",
      "[step: 1598] loss: 93.32518005371094\n",
      "[step: 1599] loss: 92.09199523925781\n",
      "[step: 1600] loss: 91.1947021484375\n",
      "[step: 1601] loss: 92.83981323242188\n",
      "[step: 1602] loss: 97.43537139892578\n",
      "[step: 1603] loss: 97.5107421875\n",
      "[step: 1604] loss: 90.01481628417969\n",
      "[step: 1605] loss: 96.43157196044922\n",
      "[step: 1606] loss: 94.14041137695312\n",
      "[step: 1607] loss: 90.40973663330078\n",
      "[step: 1608] loss: 92.19012451171875\n",
      "[step: 1609] loss: 93.52667236328125\n",
      "[step: 1610] loss: 92.14764404296875\n",
      "[step: 1611] loss: 88.90095520019531\n",
      "[step: 1612] loss: 97.4183349609375\n",
      "[step: 1613] loss: 98.67268371582031\n",
      "[step: 1614] loss: 95.99343872070312\n",
      "[step: 1615] loss: 92.43115234375\n",
      "[step: 1616] loss: 94.88362121582031\n",
      "[step: 1617] loss: 98.49575805664062\n",
      "[step: 1618] loss: 90.28022766113281\n",
      "[step: 1619] loss: 102.51573181152344\n",
      "[step: 1620] loss: 98.8912353515625\n",
      "[step: 1621] loss: 91.92875671386719\n",
      "[step: 1622] loss: 101.65512084960938\n",
      "[step: 1623] loss: 100.36642456054688\n",
      "[step: 1624] loss: 95.94303894042969\n",
      "[step: 1625] loss: 94.59449768066406\n",
      "[step: 1626] loss: 97.93519592285156\n",
      "[step: 1627] loss: 91.33343505859375\n",
      "[step: 1628] loss: 89.53323364257812\n",
      "[step: 1629] loss: 94.80029296875\n",
      "[step: 1630] loss: 96.29327392578125\n",
      "[step: 1631] loss: 90.49258422851562\n",
      "[step: 1632] loss: 95.88706970214844\n",
      "[step: 1633] loss: 94.61508178710938\n",
      "[step: 1634] loss: 91.25923156738281\n",
      "[step: 1635] loss: 94.6375732421875\n",
      "[step: 1636] loss: 87.22793579101562\n",
      "[step: 1637] loss: 93.18921661376953\n",
      "[step: 1638] loss: 98.5357666015625\n",
      "[step: 1639] loss: 97.4958267211914\n",
      "[step: 1640] loss: 93.16272735595703\n",
      "[step: 1641] loss: 90.00489807128906\n",
      "[step: 1642] loss: 92.68212890625\n",
      "[step: 1643] loss: 92.6634292602539\n",
      "[step: 1644] loss: 88.00825500488281\n",
      "[step: 1645] loss: 93.78706359863281\n",
      "[step: 1646] loss: 88.85377502441406\n",
      "[step: 1647] loss: 87.62541198730469\n",
      "[step: 1648] loss: 91.51160430908203\n",
      "[step: 1649] loss: 93.4080581665039\n",
      "[step: 1650] loss: 88.39411926269531\n",
      "[step: 1651] loss: 94.01274871826172\n",
      "[step: 1652] loss: 90.95075988769531\n",
      "[step: 1653] loss: 86.5296630859375\n",
      "[step: 1654] loss: 88.28340911865234\n",
      "[step: 1655] loss: 86.99578094482422\n",
      "[step: 1656] loss: 87.64117431640625\n",
      "[step: 1657] loss: 93.53166198730469\n",
      "[step: 1658] loss: 97.03715515136719\n",
      "[step: 1659] loss: 115.70205688476562\n",
      "[step: 1660] loss: 104.13624572753906\n",
      "[step: 1661] loss: 88.09577941894531\n",
      "[step: 1662] loss: 102.64625549316406\n",
      "[step: 1663] loss: 91.18368530273438\n",
      "[step: 1664] loss: 92.18685913085938\n",
      "[step: 1665] loss: 98.99873352050781\n",
      "[step: 1666] loss: 85.29428100585938\n",
      "[step: 1667] loss: 97.26419830322266\n",
      "[step: 1668] loss: 93.49398803710938\n",
      "[step: 1669] loss: 83.90316009521484\n",
      "[step: 1670] loss: 93.3561019897461\n",
      "[step: 1671] loss: 89.33940124511719\n",
      "[step: 1672] loss: 86.67152404785156\n",
      "[step: 1673] loss: 90.04278564453125\n",
      "[step: 1674] loss: 99.33969116210938\n",
      "[step: 1675] loss: 90.65637969970703\n",
      "[step: 1676] loss: 87.51054382324219\n",
      "[step: 1677] loss: 102.1734619140625\n",
      "[step: 1678] loss: 104.64665222167969\n",
      "[step: 1679] loss: 91.71808624267578\n",
      "[step: 1680] loss: 86.97309875488281\n",
      "[step: 1681] loss: 97.02859497070312\n",
      "[step: 1682] loss: 89.9385986328125\n",
      "[step: 1683] loss: 90.41277313232422\n",
      "[step: 1684] loss: 100.08500671386719\n",
      "[step: 1685] loss: 99.78485107421875\n",
      "[step: 1686] loss: 87.2160415649414\n",
      "[step: 1687] loss: 98.47059631347656\n",
      "[step: 1688] loss: 95.69979858398438\n",
      "[step: 1689] loss: 89.48703002929688\n",
      "[step: 1690] loss: 94.03852844238281\n",
      "[step: 1691] loss: 93.05699920654297\n",
      "[step: 1692] loss: 95.33914947509766\n",
      "[step: 1693] loss: 89.69166564941406\n",
      "[step: 1694] loss: 92.3563232421875\n",
      "[step: 1695] loss: 94.79607391357422\n",
      "[step: 1696] loss: 87.04527282714844\n",
      "[step: 1697] loss: 90.18534851074219\n",
      "[step: 1698] loss: 89.5546875\n",
      "[step: 1699] loss: 90.49758911132812\n",
      "[step: 1700] loss: 94.40729522705078\n",
      "[step: 1701] loss: 84.30722045898438\n",
      "[step: 1702] loss: 93.67594909667969\n",
      "[step: 1703] loss: 93.91490936279297\n",
      "[step: 1704] loss: 84.4211196899414\n",
      "[step: 1705] loss: 95.1985855102539\n",
      "[step: 1706] loss: 102.62208557128906\n",
      "[step: 1707] loss: 94.7251968383789\n",
      "[step: 1708] loss: 87.52007293701172\n",
      "[step: 1709] loss: 94.28627014160156\n",
      "[step: 1710] loss: 87.85568237304688\n",
      "[step: 1711] loss: 87.65927124023438\n",
      "[step: 1712] loss: 89.02838134765625\n",
      "[step: 1713] loss: 82.54613494873047\n",
      "[step: 1714] loss: 88.43498992919922\n",
      "[step: 1715] loss: 86.62641906738281\n",
      "[step: 1716] loss: 89.02595520019531\n",
      "[step: 1717] loss: 89.1641616821289\n",
      "[step: 1718] loss: 88.37454223632812\n",
      "[step: 1719] loss: 90.59493255615234\n",
      "[step: 1720] loss: 89.32342529296875\n",
      "[step: 1721] loss: 87.1044692993164\n",
      "[step: 1722] loss: 86.60285949707031\n",
      "[step: 1723] loss: 86.5784912109375\n",
      "[step: 1724] loss: 85.7934799194336\n",
      "[step: 1725] loss: 84.31106567382812\n",
      "[step: 1726] loss: 83.71530151367188\n",
      "[step: 1727] loss: 90.60598754882812\n",
      "[step: 1728] loss: 93.57682037353516\n",
      "[step: 1729] loss: 85.40153503417969\n",
      "[step: 1730] loss: 86.47380065917969\n",
      "[step: 1731] loss: 80.55876922607422\n",
      "[step: 1732] loss: 84.6201171875\n",
      "[step: 1733] loss: 88.2281494140625\n",
      "[step: 1734] loss: 89.2708511352539\n",
      "[step: 1735] loss: 85.39718627929688\n",
      "[step: 1736] loss: 90.75359344482422\n",
      "[step: 1737] loss: 87.73605346679688\n",
      "[step: 1738] loss: 87.4592056274414\n",
      "[step: 1739] loss: 87.581787109375\n",
      "[step: 1740] loss: 88.58702087402344\n",
      "[step: 1741] loss: 90.32521057128906\n",
      "[step: 1742] loss: 100.09127044677734\n",
      "[step: 1743] loss: 100.0012435913086\n",
      "[step: 1744] loss: 86.2218017578125\n",
      "[step: 1745] loss: 90.10047912597656\n",
      "[step: 1746] loss: 91.66629028320312\n",
      "[step: 1747] loss: 85.48889923095703\n",
      "[step: 1748] loss: 87.917236328125\n",
      "[step: 1749] loss: 90.7242431640625\n",
      "[step: 1750] loss: 88.87164306640625\n",
      "[step: 1751] loss: 85.81471252441406\n",
      "[step: 1752] loss: 84.70936584472656\n",
      "[step: 1753] loss: 83.0919189453125\n",
      "[step: 1754] loss: 84.38763427734375\n",
      "[step: 1755] loss: 91.67122650146484\n",
      "[step: 1756] loss: 113.82325744628906\n",
      "[step: 1757] loss: 97.47782897949219\n",
      "[step: 1758] loss: 85.88818359375\n",
      "[step: 1759] loss: 86.40376281738281\n",
      "[step: 1760] loss: 87.0516357421875\n",
      "[step: 1761] loss: 85.20793151855469\n",
      "[step: 1762] loss: 87.9790267944336\n",
      "[step: 1763] loss: 84.22810363769531\n",
      "[step: 1764] loss: 88.01272583007812\n",
      "[step: 1765] loss: 91.41477966308594\n",
      "[step: 1766] loss: 86.1951904296875\n",
      "[step: 1767] loss: 83.63075256347656\n",
      "[step: 1768] loss: 86.47511291503906\n",
      "[step: 1769] loss: 87.51054382324219\n",
      "[step: 1770] loss: 92.27500915527344\n",
      "[step: 1771] loss: 88.14833068847656\n",
      "[step: 1772] loss: 85.55720520019531\n",
      "[step: 1773] loss: 85.35865020751953\n",
      "[step: 1774] loss: 89.63275909423828\n",
      "[step: 1775] loss: 85.24180603027344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1776] loss: 84.72239685058594\n",
      "[step: 1777] loss: 83.07492065429688\n",
      "[step: 1778] loss: 85.73052978515625\n",
      "[step: 1779] loss: 89.91387939453125\n",
      "[step: 1780] loss: 85.427490234375\n",
      "[step: 1781] loss: 84.92839050292969\n",
      "[step: 1782] loss: 86.37869262695312\n",
      "[step: 1783] loss: 91.51094818115234\n",
      "[step: 1784] loss: 86.25694274902344\n",
      "[step: 1785] loss: 85.21211242675781\n",
      "[step: 1786] loss: 82.49238586425781\n",
      "[step: 1787] loss: 80.96406555175781\n",
      "[step: 1788] loss: 86.15765380859375\n",
      "[step: 1789] loss: 82.9681396484375\n",
      "[step: 1790] loss: 81.09356689453125\n",
      "[step: 1791] loss: 86.68368530273438\n",
      "[step: 1792] loss: 84.84080505371094\n",
      "[step: 1793] loss: 84.97093200683594\n",
      "[step: 1794] loss: 83.83740234375\n",
      "[step: 1795] loss: 83.43183898925781\n",
      "[step: 1796] loss: 83.51582336425781\n",
      "[step: 1797] loss: 83.15866088867188\n",
      "[step: 1798] loss: 90.25822448730469\n",
      "[step: 1799] loss: 104.81493377685547\n",
      "[step: 1800] loss: 132.92852783203125\n",
      "[step: 1801] loss: 86.68798065185547\n",
      "[step: 1802] loss: 109.67037963867188\n",
      "[step: 1803] loss: 115.7421646118164\n",
      "[step: 1804] loss: 98.76426696777344\n",
      "[step: 1805] loss: 118.71939086914062\n",
      "[step: 1806] loss: 102.2147216796875\n",
      "[step: 1807] loss: 115.44580078125\n",
      "[step: 1808] loss: 91.59318542480469\n",
      "[step: 1809] loss: 105.72068786621094\n",
      "[step: 1810] loss: 86.7847671508789\n",
      "[step: 1811] loss: 101.39202880859375\n",
      "[step: 1812] loss: 96.34638214111328\n",
      "[step: 1813] loss: 93.83676147460938\n",
      "[step: 1814] loss: 99.77757263183594\n",
      "[step: 1815] loss: 86.07467651367188\n",
      "[step: 1816] loss: 99.33746337890625\n",
      "[step: 1817] loss: 86.19869995117188\n",
      "[step: 1818] loss: 97.98004150390625\n",
      "[step: 1819] loss: 87.50288391113281\n",
      "[step: 1820] loss: 93.23077392578125\n",
      "[step: 1821] loss: 96.75686645507812\n",
      "[step: 1822] loss: 84.84117126464844\n",
      "[step: 1823] loss: 92.19053649902344\n",
      "[step: 1824] loss: 83.06852722167969\n",
      "[step: 1825] loss: 92.46083068847656\n",
      "[step: 1826] loss: 87.61360168457031\n",
      "[step: 1827] loss: 84.33283233642578\n",
      "[step: 1828] loss: 86.73593139648438\n",
      "[step: 1829] loss: 82.99394226074219\n",
      "[step: 1830] loss: 92.19004821777344\n",
      "[step: 1831] loss: 83.86894989013672\n",
      "[step: 1832] loss: 89.72127532958984\n",
      "[step: 1833] loss: 81.83782958984375\n",
      "[step: 1834] loss: 84.37709045410156\n",
      "[step: 1835] loss: 86.57632446289062\n",
      "[step: 1836] loss: 84.40695190429688\n",
      "[step: 1837] loss: 82.55071258544922\n",
      "[step: 1838] loss: 82.5538330078125\n",
      "[step: 1839] loss: 82.9281005859375\n",
      "[step: 1840] loss: 79.89384460449219\n",
      "[step: 1841] loss: 83.52777099609375\n",
      "[step: 1842] loss: 81.78731536865234\n",
      "[step: 1843] loss: 84.96551513671875\n",
      "[step: 1844] loss: 85.1797103881836\n",
      "[step: 1845] loss: 82.36815643310547\n",
      "[step: 1846] loss: 83.28935241699219\n",
      "[step: 1847] loss: 82.88780212402344\n",
      "[step: 1848] loss: 86.95889282226562\n",
      "[step: 1849] loss: 91.8980484008789\n",
      "[step: 1850] loss: 88.80852508544922\n",
      "[step: 1851] loss: 84.58291625976562\n",
      "[step: 1852] loss: 86.01567077636719\n",
      "[step: 1853] loss: 89.939453125\n",
      "[step: 1854] loss: 95.08819580078125\n",
      "[step: 1855] loss: 84.67074584960938\n",
      "[step: 1856] loss: 88.52857971191406\n",
      "[step: 1857] loss: 90.54473114013672\n",
      "[step: 1858] loss: 94.01078033447266\n",
      "[step: 1859] loss: 88.46696472167969\n",
      "[step: 1860] loss: 83.83100891113281\n",
      "[step: 1861] loss: 89.59933471679688\n",
      "[step: 1862] loss: 89.7853775024414\n",
      "[step: 1863] loss: 84.1164779663086\n",
      "[step: 1864] loss: 85.7383804321289\n",
      "[step: 1865] loss: 93.65304565429688\n",
      "[step: 1866] loss: 90.04069519042969\n",
      "[step: 1867] loss: 85.24107360839844\n",
      "[step: 1868] loss: 83.2505874633789\n",
      "[step: 1869] loss: 81.73895263671875\n",
      "[step: 1870] loss: 90.13933563232422\n",
      "[step: 1871] loss: 82.3609619140625\n",
      "[step: 1872] loss: 89.39015197753906\n",
      "[step: 1873] loss: 87.27809143066406\n",
      "[step: 1874] loss: 83.81975555419922\n",
      "[step: 1875] loss: 88.510009765625\n",
      "[step: 1876] loss: 83.87307739257812\n",
      "[step: 1877] loss: 100.19854736328125\n",
      "[step: 1878] loss: 113.59285736083984\n",
      "[step: 1879] loss: 95.7184066772461\n",
      "[step: 1880] loss: 91.1894760131836\n",
      "[step: 1881] loss: 81.74360656738281\n",
      "[step: 1882] loss: 88.23503875732422\n",
      "[step: 1883] loss: 84.45785522460938\n",
      "[step: 1884] loss: 80.01435852050781\n",
      "[step: 1885] loss: 84.94285583496094\n",
      "[step: 1886] loss: 88.15652465820312\n",
      "[step: 1887] loss: 84.2257080078125\n",
      "[step: 1888] loss: 81.51030731201172\n",
      "[step: 1889] loss: 83.18720245361328\n",
      "[step: 1890] loss: 81.45304107666016\n",
      "[step: 1891] loss: 84.7459945678711\n",
      "[step: 1892] loss: 83.66668701171875\n",
      "[step: 1893] loss: 82.74382019042969\n",
      "[step: 1894] loss: 79.17520141601562\n",
      "[step: 1895] loss: 85.69070434570312\n",
      "[step: 1896] loss: 87.0357666015625\n",
      "[step: 1897] loss: 85.50820922851562\n",
      "[step: 1898] loss: 82.63941192626953\n",
      "[step: 1899] loss: 82.25750732421875\n",
      "[step: 1900] loss: 81.37652587890625\n",
      "[step: 1901] loss: 79.51407623291016\n",
      "[step: 1902] loss: 79.08557891845703\n",
      "[step: 1903] loss: 80.2991714477539\n",
      "[step: 1904] loss: 79.90925598144531\n",
      "[step: 1905] loss: 82.96133422851562\n",
      "[step: 1906] loss: 85.89959716796875\n",
      "[step: 1907] loss: 85.46186828613281\n",
      "[step: 1908] loss: 83.19549560546875\n",
      "[step: 1909] loss: 77.13481140136719\n",
      "[step: 1910] loss: 77.5335464477539\n",
      "[step: 1911] loss: 81.17915344238281\n",
      "[step: 1912] loss: 77.92611694335938\n",
      "[step: 1913] loss: 78.09629821777344\n",
      "[step: 1914] loss: 81.05368041992188\n",
      "[step: 1915] loss: 82.07723999023438\n",
      "[step: 1916] loss: 80.14015197753906\n",
      "[step: 1917] loss: 84.76606750488281\n",
      "[step: 1918] loss: 93.55705261230469\n",
      "[step: 1919] loss: 108.92716979980469\n",
      "[step: 1920] loss: 95.14569854736328\n",
      "[step: 1921] loss: 84.29072570800781\n",
      "[step: 1922] loss: 84.57070922851562\n",
      "[step: 1923] loss: 88.00323486328125\n",
      "[step: 1924] loss: 83.5280990600586\n",
      "[step: 1925] loss: 79.74475860595703\n",
      "[step: 1926] loss: 87.31101989746094\n",
      "[step: 1927] loss: 93.63555145263672\n",
      "[step: 1928] loss: 85.21053314208984\n",
      "[step: 1929] loss: 81.52385711669922\n",
      "[step: 1930] loss: 89.5606689453125\n",
      "[step: 1931] loss: 84.28399658203125\n",
      "[step: 1932] loss: 81.46473693847656\n",
      "[step: 1933] loss: 84.3460693359375\n",
      "[step: 1934] loss: 82.4321517944336\n",
      "[step: 1935] loss: 82.47555541992188\n",
      "[step: 1936] loss: 81.77375793457031\n",
      "[step: 1937] loss: 86.53560638427734\n",
      "[step: 1938] loss: 98.5924301147461\n",
      "[step: 1939] loss: 95.12487030029297\n",
      "[step: 1940] loss: 90.34538269042969\n",
      "[step: 1941] loss: 87.1448745727539\n",
      "[step: 1942] loss: 83.56282043457031\n",
      "[step: 1943] loss: 86.69793701171875\n",
      "[step: 1944] loss: 82.16276550292969\n",
      "[step: 1945] loss: 84.0745849609375\n",
      "[step: 1946] loss: 93.14904022216797\n",
      "[step: 1947] loss: 81.29986572265625\n",
      "[step: 1948] loss: 80.50554656982422\n",
      "[step: 1949] loss: 84.2741470336914\n",
      "[step: 1950] loss: 83.4537582397461\n",
      "[step: 1951] loss: 84.5483627319336\n",
      "[step: 1952] loss: 83.38019561767578\n",
      "[step: 1953] loss: 81.79060363769531\n",
      "[step: 1954] loss: 81.80673217773438\n",
      "[step: 1955] loss: 79.24356842041016\n",
      "[step: 1956] loss: 81.70442199707031\n",
      "[step: 1957] loss: 89.32453155517578\n",
      "[step: 1958] loss: 88.45564270019531\n",
      "[step: 1959] loss: 81.04210662841797\n",
      "[step: 1960] loss: 84.35018157958984\n",
      "[step: 1961] loss: 81.19984436035156\n",
      "[step: 1962] loss: 88.38422393798828\n",
      "[step: 1963] loss: 90.63575744628906\n",
      "[step: 1964] loss: 94.25835418701172\n",
      "[step: 1965] loss: 83.26666259765625\n",
      "[step: 1966] loss: 84.5965347290039\n",
      "[step: 1967] loss: 94.57758331298828\n",
      "[step: 1968] loss: 85.43965911865234\n",
      "[step: 1969] loss: 81.261962890625\n",
      "[step: 1970] loss: 84.6196060180664\n",
      "[step: 1971] loss: 90.51875305175781\n",
      "[step: 1972] loss: 80.63288879394531\n",
      "[step: 1973] loss: 84.81356811523438\n",
      "[step: 1974] loss: 91.3254165649414\n",
      "[step: 1975] loss: 85.02796936035156\n",
      "[step: 1976] loss: 85.61884307861328\n",
      "[step: 1977] loss: 87.81187438964844\n",
      "[step: 1978] loss: 85.53002166748047\n",
      "[step: 1979] loss: 80.17945861816406\n",
      "[step: 1980] loss: 83.73391723632812\n",
      "[step: 1981] loss: 84.37989807128906\n",
      "[step: 1982] loss: 85.87837219238281\n",
      "[step: 1983] loss: 81.5971908569336\n",
      "[step: 1984] loss: 81.9411849975586\n",
      "[step: 1985] loss: 80.74168395996094\n",
      "[step: 1986] loss: 81.37639617919922\n",
      "[step: 1987] loss: 81.023193359375\n",
      "[step: 1988] loss: 84.4539566040039\n",
      "[step: 1989] loss: 86.68336486816406\n",
      "[step: 1990] loss: 80.2371826171875\n",
      "[step: 1991] loss: 78.91356658935547\n",
      "[step: 1992] loss: 78.68778228759766\n",
      "[step: 1993] loss: 79.11898040771484\n",
      "[step: 1994] loss: 81.57508850097656\n",
      "[step: 1995] loss: 82.2646484375\n",
      "[step: 1996] loss: 80.27081298828125\n",
      "[step: 1997] loss: 80.22746276855469\n",
      "[step: 1998] loss: 86.15667724609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1999] loss: 91.36719512939453\n",
      "[step: 2000] loss: 89.36900329589844\n",
      "[step: 2001] loss: 85.60441589355469\n",
      "[step: 2002] loss: 80.04937744140625\n",
      "[step: 2003] loss: 85.59327697753906\n",
      "[step: 2004] loss: 87.1903076171875\n",
      "[step: 2005] loss: 80.45393371582031\n",
      "[step: 2006] loss: 88.24627685546875\n",
      "[step: 2007] loss: 85.52664184570312\n",
      "[step: 2008] loss: 83.74604797363281\n",
      "[step: 2009] loss: 82.79263305664062\n",
      "[step: 2010] loss: 75.31428527832031\n",
      "[step: 2011] loss: 84.7972412109375\n",
      "[step: 2012] loss: 82.38853454589844\n",
      "[step: 2013] loss: 82.18695831298828\n",
      "[step: 2014] loss: 81.37676239013672\n",
      "[step: 2015] loss: 76.53801727294922\n",
      "[step: 2016] loss: 82.37263488769531\n",
      "[step: 2017] loss: 80.03872680664062\n",
      "[step: 2018] loss: 82.0578384399414\n",
      "[step: 2019] loss: 79.95176696777344\n",
      "[step: 2020] loss: 81.89118957519531\n",
      "[step: 2021] loss: 85.43876647949219\n",
      "[step: 2022] loss: 77.08509826660156\n",
      "[step: 2023] loss: 79.70660400390625\n",
      "[step: 2024] loss: 81.63894653320312\n",
      "[step: 2025] loss: 80.84956359863281\n",
      "[step: 2026] loss: 79.91771697998047\n",
      "[step: 2027] loss: 80.50126647949219\n",
      "[step: 2028] loss: 76.65310668945312\n",
      "[step: 2029] loss: 81.01851654052734\n",
      "[step: 2030] loss: 85.19318389892578\n",
      "[step: 2031] loss: 96.40625\n",
      "[step: 2032] loss: 121.1773910522461\n",
      "[step: 2033] loss: 87.22333526611328\n",
      "[step: 2034] loss: 94.61083984375\n",
      "[step: 2035] loss: 106.98234558105469\n",
      "[step: 2036] loss: 87.26203918457031\n",
      "[step: 2037] loss: 95.80850219726562\n",
      "[step: 2038] loss: 90.32414245605469\n",
      "[step: 2039] loss: 88.4935531616211\n",
      "[step: 2040] loss: 89.77876281738281\n",
      "[step: 2041] loss: 83.82469177246094\n",
      "[step: 2042] loss: 85.11784362792969\n",
      "[step: 2043] loss: 85.51895141601562\n",
      "[step: 2044] loss: 86.76411437988281\n",
      "[step: 2045] loss: 84.1843032836914\n",
      "[step: 2046] loss: 78.14918518066406\n",
      "[step: 2047] loss: 83.35883331298828\n",
      "[step: 2048] loss: 82.07194519042969\n",
      "[step: 2049] loss: 85.00714874267578\n",
      "[step: 2050] loss: 83.87217712402344\n",
      "[step: 2051] loss: 82.6435317993164\n",
      "[step: 2052] loss: 78.13655090332031\n",
      "[step: 2053] loss: 80.75846862792969\n",
      "[step: 2054] loss: 80.82859802246094\n",
      "[step: 2055] loss: 82.40335083007812\n",
      "[step: 2056] loss: 77.72364044189453\n",
      "[step: 2057] loss: 78.29719543457031\n",
      "[step: 2058] loss: 83.35302734375\n",
      "[step: 2059] loss: 85.33657836914062\n",
      "[step: 2060] loss: 81.63731384277344\n",
      "[step: 2061] loss: 82.34821319580078\n",
      "[step: 2062] loss: 84.96121978759766\n",
      "[step: 2063] loss: 83.15055847167969\n",
      "[step: 2064] loss: 76.24061584472656\n",
      "[step: 2065] loss: 84.37556457519531\n",
      "[step: 2066] loss: 83.13853454589844\n",
      "[step: 2067] loss: 78.54985809326172\n",
      "[step: 2068] loss: 77.64251708984375\n",
      "[step: 2069] loss: 79.5997314453125\n",
      "[step: 2070] loss: 75.03466033935547\n",
      "[step: 2071] loss: 75.57644653320312\n",
      "[step: 2072] loss: 76.8095932006836\n",
      "[step: 2073] loss: 79.066650390625\n",
      "[step: 2074] loss: 72.72479248046875\n",
      "[step: 2075] loss: 78.92520141601562\n",
      "[step: 2076] loss: 78.78865051269531\n",
      "[step: 2077] loss: 85.05821990966797\n",
      "[step: 2078] loss: 79.93524169921875\n",
      "[step: 2079] loss: 78.163818359375\n",
      "[step: 2080] loss: 81.0896224975586\n",
      "[step: 2081] loss: 77.48490142822266\n",
      "[step: 2082] loss: 81.13623046875\n",
      "[step: 2083] loss: 82.5350570678711\n",
      "[step: 2084] loss: 75.02264404296875\n",
      "[step: 2085] loss: 85.83686828613281\n",
      "[step: 2086] loss: 86.05231475830078\n",
      "[step: 2087] loss: 91.43910217285156\n",
      "[step: 2088] loss: 89.90815734863281\n",
      "[step: 2089] loss: 81.53445434570312\n",
      "[step: 2090] loss: 81.87651062011719\n",
      "[step: 2091] loss: 100.37435150146484\n",
      "[step: 2092] loss: 93.84053802490234\n",
      "[step: 2093] loss: 79.37715148925781\n",
      "[step: 2094] loss: 104.87571716308594\n",
      "[step: 2095] loss: 123.69855499267578\n",
      "[step: 2096] loss: 80.47671508789062\n",
      "[step: 2097] loss: 123.87516784667969\n",
      "[step: 2098] loss: 102.08658599853516\n",
      "[step: 2099] loss: 99.16041564941406\n",
      "[step: 2100] loss: 93.30833435058594\n",
      "[step: 2101] loss: 101.56547546386719\n",
      "[step: 2102] loss: 86.66719055175781\n",
      "[step: 2103] loss: 104.10147857666016\n",
      "[step: 2104] loss: 89.21708679199219\n",
      "[step: 2105] loss: 95.74020385742188\n",
      "[step: 2106] loss: 94.30841064453125\n",
      "[step: 2107] loss: 88.64100646972656\n",
      "[step: 2108] loss: 100.60099029541016\n",
      "[step: 2109] loss: 83.54983520507812\n",
      "[step: 2110] loss: 92.70020294189453\n",
      "[step: 2111] loss: 80.41256713867188\n",
      "[step: 2112] loss: 87.76846313476562\n",
      "[step: 2113] loss: 80.66650390625\n",
      "[step: 2114] loss: 83.89848327636719\n",
      "[step: 2115] loss: 81.3998031616211\n",
      "[step: 2116] loss: 81.83025360107422\n",
      "[step: 2117] loss: 83.20510864257812\n",
      "[step: 2118] loss: 80.00480651855469\n",
      "[step: 2119] loss: 79.57477569580078\n",
      "[step: 2120] loss: 76.52989196777344\n",
      "[step: 2121] loss: 77.59788513183594\n",
      "[step: 2122] loss: 77.73245239257812\n",
      "[step: 2123] loss: 84.21369171142578\n",
      "[step: 2124] loss: 82.8876724243164\n",
      "[step: 2125] loss: 78.26866912841797\n",
      "[step: 2126] loss: 81.52233123779297\n",
      "[step: 2127] loss: 80.00537109375\n",
      "[step: 2128] loss: 72.87242126464844\n",
      "[step: 2129] loss: 79.68074035644531\n",
      "[step: 2130] loss: 81.140380859375\n",
      "[step: 2131] loss: 75.3263168334961\n",
      "[step: 2132] loss: 74.10064697265625\n",
      "[step: 2133] loss: 78.70147705078125\n",
      "[step: 2134] loss: 80.60774230957031\n",
      "[step: 2135] loss: 79.83934783935547\n",
      "[step: 2136] loss: 80.91252899169922\n",
      "[step: 2137] loss: 77.33932495117188\n",
      "[step: 2138] loss: 77.56012725830078\n",
      "[step: 2139] loss: 78.44772338867188\n",
      "[step: 2140] loss: 75.88349914550781\n",
      "[step: 2141] loss: 76.62547302246094\n",
      "[step: 2142] loss: 79.15138244628906\n",
      "[step: 2143] loss: 78.54515838623047\n",
      "[step: 2144] loss: 77.6995849609375\n",
      "[step: 2145] loss: 76.6512451171875\n",
      "[step: 2146] loss: 78.5931396484375\n",
      "[step: 2147] loss: 78.9803695678711\n",
      "[step: 2148] loss: 77.19699096679688\n",
      "[step: 2149] loss: 75.5890121459961\n",
      "[step: 2150] loss: 76.26825714111328\n",
      "[step: 2151] loss: 87.01763916015625\n",
      "[step: 2152] loss: 77.46818542480469\n",
      "[step: 2153] loss: 81.04495239257812\n",
      "[step: 2154] loss: 89.04631805419922\n",
      "[step: 2155] loss: 82.30316162109375\n",
      "[step: 2156] loss: 77.02304077148438\n",
      "[step: 2157] loss: 83.93486022949219\n",
      "[step: 2158] loss: 81.02223205566406\n",
      "[step: 2159] loss: 74.97732543945312\n",
      "[step: 2160] loss: 80.23603820800781\n",
      "[step: 2161] loss: 75.92298889160156\n",
      "[step: 2162] loss: 76.94291687011719\n",
      "[step: 2163] loss: 76.0341796875\n",
      "[step: 2164] loss: 80.641845703125\n",
      "[step: 2165] loss: 82.44187927246094\n",
      "[step: 2166] loss: 81.5632553100586\n",
      "[step: 2167] loss: 75.76338958740234\n",
      "[step: 2168] loss: 79.40777587890625\n",
      "[step: 2169] loss: 78.72701263427734\n",
      "[step: 2170] loss: 78.35548400878906\n",
      "[step: 2171] loss: 76.11756896972656\n",
      "[step: 2172] loss: 77.54368591308594\n",
      "[step: 2173] loss: 76.03916931152344\n",
      "[step: 2174] loss: 76.82209777832031\n",
      "[step: 2175] loss: 74.70204162597656\n",
      "[step: 2176] loss: 73.25558471679688\n",
      "[step: 2177] loss: 76.62799072265625\n",
      "[step: 2178] loss: 83.73066711425781\n",
      "[step: 2179] loss: 84.59170532226562\n",
      "[step: 2180] loss: 79.68002319335938\n",
      "[step: 2181] loss: 78.73600769042969\n",
      "[step: 2182] loss: 75.82844543457031\n",
      "[step: 2183] loss: 78.50475311279297\n",
      "[step: 2184] loss: 79.33064270019531\n",
      "[step: 2185] loss: 74.18356323242188\n",
      "[step: 2186] loss: 79.65260314941406\n",
      "[step: 2187] loss: 86.14549255371094\n",
      "[step: 2188] loss: 74.94908142089844\n",
      "[step: 2189] loss: 80.64236450195312\n",
      "[step: 2190] loss: 85.29289245605469\n",
      "[step: 2191] loss: 75.87596130371094\n",
      "[step: 2192] loss: 79.33560943603516\n",
      "[step: 2193] loss: 83.57803344726562\n",
      "[step: 2194] loss: 78.18595886230469\n",
      "[step: 2195] loss: 80.84256744384766\n",
      "[step: 2196] loss: 78.28816223144531\n",
      "[step: 2197] loss: 73.92915344238281\n",
      "[step: 2198] loss: 77.9487533569336\n",
      "[step: 2199] loss: 74.68862915039062\n",
      "[step: 2200] loss: 76.9946517944336\n",
      "[step: 2201] loss: 76.649658203125\n",
      "[step: 2202] loss: 74.19538116455078\n",
      "[step: 2203] loss: 81.95028686523438\n",
      "[step: 2204] loss: 73.85270690917969\n",
      "[step: 2205] loss: 77.30131530761719\n",
      "[step: 2206] loss: 77.03117370605469\n",
      "[step: 2207] loss: 78.361572265625\n",
      "[step: 2208] loss: 78.38261413574219\n",
      "[step: 2209] loss: 77.35887145996094\n",
      "[step: 2210] loss: 76.57722473144531\n",
      "[step: 2211] loss: 81.19232177734375\n",
      "[step: 2212] loss: 78.4264907836914\n",
      "[step: 2213] loss: 79.99736022949219\n",
      "[step: 2214] loss: 74.30628204345703\n",
      "[step: 2215] loss: 76.77058410644531\n",
      "[step: 2216] loss: 82.97593688964844\n",
      "[step: 2217] loss: 92.67982482910156\n",
      "[step: 2218] loss: 90.86553192138672\n",
      "[step: 2219] loss: 77.2713623046875\n",
      "[step: 2220] loss: 84.36609649658203\n",
      "[step: 2221] loss: 89.53006744384766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2222] loss: 79.92015075683594\n",
      "[step: 2223] loss: 96.00434875488281\n",
      "[step: 2224] loss: 94.00701904296875\n",
      "[step: 2225] loss: 76.91200256347656\n",
      "[step: 2226] loss: 87.22537231445312\n",
      "[step: 2227] loss: 83.74730682373047\n",
      "[step: 2228] loss: 80.23440551757812\n",
      "[step: 2229] loss: 81.7373275756836\n",
      "[step: 2230] loss: 78.38382720947266\n",
      "[step: 2231] loss: 78.77822875976562\n",
      "[step: 2232] loss: 89.1693115234375\n",
      "[step: 2233] loss: 79.1031723022461\n",
      "[step: 2234] loss: 78.19673156738281\n",
      "[step: 2235] loss: 85.29002380371094\n",
      "[step: 2236] loss: 78.64973449707031\n",
      "[step: 2237] loss: 76.66441345214844\n",
      "[step: 2238] loss: 86.42146301269531\n",
      "[step: 2239] loss: 78.36627960205078\n",
      "[step: 2240] loss: 75.11349487304688\n",
      "[step: 2241] loss: 80.32887268066406\n",
      "[step: 2242] loss: 81.02091217041016\n",
      "[step: 2243] loss: 74.86054992675781\n",
      "[step: 2244] loss: 83.95721435546875\n",
      "[step: 2245] loss: 85.07746887207031\n",
      "[step: 2246] loss: 75.864501953125\n",
      "[step: 2247] loss: 82.95436096191406\n",
      "[step: 2248] loss: 92.78482055664062\n",
      "[step: 2249] loss: 77.6192855834961\n",
      "[step: 2250] loss: 82.7015151977539\n",
      "[step: 2251] loss: 90.48588562011719\n",
      "[step: 2252] loss: 80.4266357421875\n",
      "[step: 2253] loss: 88.42143249511719\n",
      "[step: 2254] loss: 88.61366271972656\n",
      "[step: 2255] loss: 78.97637939453125\n",
      "[step: 2256] loss: 85.49884033203125\n",
      "[step: 2257] loss: 86.01445007324219\n",
      "[step: 2258] loss: 80.48286437988281\n",
      "[step: 2259] loss: 83.23146057128906\n",
      "[step: 2260] loss: 81.49766540527344\n",
      "[step: 2261] loss: 80.0862045288086\n",
      "[step: 2262] loss: 80.85708618164062\n",
      "[step: 2263] loss: 79.95896911621094\n",
      "[step: 2264] loss: 76.27685546875\n",
      "[step: 2265] loss: 85.23363494873047\n",
      "[step: 2266] loss: 76.83555603027344\n",
      "[step: 2267] loss: 79.98127746582031\n",
      "[step: 2268] loss: 81.98014831542969\n",
      "[step: 2269] loss: 75.55046081542969\n",
      "[step: 2270] loss: 77.50939178466797\n",
      "[step: 2271] loss: 76.53164672851562\n",
      "[step: 2272] loss: 78.28836059570312\n",
      "[step: 2273] loss: 76.52236938476562\n",
      "[step: 2274] loss: 76.66609191894531\n",
      "[step: 2275] loss: 76.47933959960938\n",
      "[step: 2276] loss: 80.74301147460938\n",
      "[step: 2277] loss: 82.63931274414062\n",
      "[step: 2278] loss: 82.82962036132812\n",
      "[step: 2279] loss: 78.28563690185547\n",
      "[step: 2280] loss: 75.67779541015625\n",
      "[step: 2281] loss: 77.08104705810547\n",
      "[step: 2282] loss: 78.49943542480469\n",
      "[step: 2283] loss: 77.7103042602539\n",
      "[step: 2284] loss: 76.55624389648438\n",
      "[step: 2285] loss: 80.5693588256836\n",
      "[step: 2286] loss: 76.08814239501953\n",
      "[step: 2287] loss: 77.22322082519531\n",
      "[step: 2288] loss: 80.26335144042969\n",
      "[step: 2289] loss: 74.33235168457031\n",
      "[step: 2290] loss: 78.69781494140625\n",
      "[step: 2291] loss: 76.77920532226562\n",
      "[step: 2292] loss: 81.49848937988281\n",
      "[step: 2293] loss: 83.75030517578125\n",
      "[step: 2294] loss: 82.8104248046875\n",
      "[step: 2295] loss: 79.42561340332031\n",
      "[step: 2296] loss: 74.69168090820312\n",
      "[step: 2297] loss: 76.53263854980469\n",
      "[step: 2298] loss: 78.41085052490234\n",
      "[step: 2299] loss: 78.19425964355469\n",
      "[step: 2300] loss: 74.96754455566406\n",
      "[step: 2301] loss: 76.13581848144531\n",
      "[step: 2302] loss: 76.8856201171875\n",
      "[step: 2303] loss: 73.19617462158203\n",
      "[step: 2304] loss: 76.67601013183594\n",
      "[step: 2305] loss: 81.30986022949219\n",
      "[step: 2306] loss: 80.85758972167969\n",
      "[step: 2307] loss: 78.6490249633789\n",
      "[step: 2308] loss: 75.60274505615234\n",
      "[step: 2309] loss: 76.9300537109375\n",
      "[step: 2310] loss: 80.53390502929688\n",
      "[step: 2311] loss: 75.61231231689453\n",
      "[step: 2312] loss: 74.40093994140625\n",
      "[step: 2313] loss: 76.98429107666016\n",
      "[step: 2314] loss: 74.41024780273438\n",
      "[step: 2315] loss: 72.54716491699219\n",
      "[step: 2316] loss: 76.28453063964844\n",
      "[step: 2317] loss: 78.513427734375\n",
      "[step: 2318] loss: 82.80563354492188\n",
      "[step: 2319] loss: 74.76078796386719\n",
      "[step: 2320] loss: 76.83992004394531\n",
      "[step: 2321] loss: 75.30455780029297\n",
      "[step: 2322] loss: 78.36778259277344\n",
      "[step: 2323] loss: 80.43977355957031\n",
      "[step: 2324] loss: 75.54379272460938\n",
      "[step: 2325] loss: 72.7474594116211\n",
      "[step: 2326] loss: 76.60798645019531\n",
      "[step: 2327] loss: 76.71829223632812\n",
      "[step: 2328] loss: 79.20254516601562\n",
      "[step: 2329] loss: 74.85905456542969\n",
      "[step: 2330] loss: 74.6756362915039\n",
      "[step: 2331] loss: 81.19617462158203\n",
      "[step: 2332] loss: 76.61979675292969\n",
      "[step: 2333] loss: 79.97940826416016\n",
      "[step: 2334] loss: 74.78658294677734\n",
      "[step: 2335] loss: 73.57444763183594\n",
      "[step: 2336] loss: 78.16230773925781\n",
      "[step: 2337] loss: 75.58478546142578\n",
      "[step: 2338] loss: 78.13372802734375\n",
      "[step: 2339] loss: 74.78318786621094\n",
      "[step: 2340] loss: 77.25331115722656\n",
      "[step: 2341] loss: 77.28319549560547\n",
      "[step: 2342] loss: 75.38877868652344\n",
      "[step: 2343] loss: 72.03654479980469\n",
      "[step: 2344] loss: 76.9397201538086\n",
      "[step: 2345] loss: 77.44141387939453\n",
      "[step: 2346] loss: 75.83998107910156\n",
      "[step: 2347] loss: 74.76873779296875\n",
      "[step: 2348] loss: 76.19222259521484\n",
      "[step: 2349] loss: 74.6047134399414\n",
      "[step: 2350] loss: 70.36567687988281\n",
      "[step: 2351] loss: 73.52714538574219\n",
      "[step: 2352] loss: 80.72722625732422\n",
      "[step: 2353] loss: 85.71793365478516\n",
      "[step: 2354] loss: 88.44398498535156\n",
      "[step: 2355] loss: 96.7246322631836\n",
      "[step: 2356] loss: 86.14816284179688\n",
      "[step: 2357] loss: 74.54182434082031\n",
      "[step: 2358] loss: 85.88575744628906\n",
      "[step: 2359] loss: 82.1451416015625\n",
      "[step: 2360] loss: 75.48892211914062\n",
      "[step: 2361] loss: 77.05631256103516\n",
      "[step: 2362] loss: 85.18045043945312\n",
      "[step: 2363] loss: 81.9739990234375\n",
      "[step: 2364] loss: 76.27057647705078\n",
      "[step: 2365] loss: 76.46705627441406\n",
      "[step: 2366] loss: 76.035888671875\n",
      "[step: 2367] loss: 73.53541564941406\n",
      "[step: 2368] loss: 78.09504699707031\n",
      "[step: 2369] loss: 78.5753173828125\n",
      "[step: 2370] loss: 75.13216400146484\n",
      "[step: 2371] loss: 76.80021667480469\n",
      "[step: 2372] loss: 84.59331512451172\n",
      "[step: 2373] loss: 77.88103485107422\n",
      "[step: 2374] loss: 73.38914489746094\n",
      "[step: 2375] loss: 78.390625\n",
      "[step: 2376] loss: 85.82353210449219\n",
      "[step: 2377] loss: 83.3335952758789\n",
      "[step: 2378] loss: 73.04071807861328\n",
      "[step: 2379] loss: 77.07904052734375\n",
      "[step: 2380] loss: 74.29865264892578\n",
      "[step: 2381] loss: 74.46371459960938\n",
      "[step: 2382] loss: 74.62678527832031\n",
      "[step: 2383] loss: 77.24160766601562\n",
      "[step: 2384] loss: 79.26332092285156\n",
      "[step: 2385] loss: 75.79241943359375\n",
      "[step: 2386] loss: 73.66496276855469\n",
      "[step: 2387] loss: 79.87837219238281\n",
      "[step: 2388] loss: 72.8212890625\n",
      "[step: 2389] loss: 74.81576538085938\n",
      "[step: 2390] loss: 83.8938980102539\n",
      "[step: 2391] loss: 79.80459594726562\n",
      "[step: 2392] loss: 77.43748474121094\n",
      "[step: 2393] loss: 78.59484100341797\n",
      "[step: 2394] loss: 72.91609191894531\n",
      "[step: 2395] loss: 77.6654052734375\n",
      "[step: 2396] loss: 75.59632110595703\n",
      "[step: 2397] loss: 75.42369079589844\n",
      "[step: 2398] loss: 73.83332824707031\n",
      "[step: 2399] loss: 79.01658630371094\n",
      "[step: 2400] loss: 86.3534164428711\n",
      "[step: 2401] loss: 91.07384490966797\n",
      "[step: 2402] loss: 79.31144714355469\n",
      "[step: 2403] loss: 78.6582260131836\n",
      "[step: 2404] loss: 83.01091766357422\n",
      "[step: 2405] loss: 75.12972259521484\n",
      "[step: 2406] loss: 74.38264465332031\n",
      "[step: 2407] loss: 81.22901153564453\n",
      "[step: 2408] loss: 73.50369262695312\n",
      "[step: 2409] loss: 71.48846435546875\n",
      "[step: 2410] loss: 78.04762268066406\n",
      "[step: 2411] loss: 78.26853942871094\n",
      "[step: 2412] loss: 77.73339080810547\n",
      "[step: 2413] loss: 74.83497619628906\n",
      "[step: 2414] loss: 75.48593139648438\n",
      "[step: 2415] loss: 76.44419860839844\n",
      "[step: 2416] loss: 74.83494567871094\n",
      "[step: 2417] loss: 71.97354125976562\n",
      "[step: 2418] loss: 76.07533264160156\n",
      "[step: 2419] loss: 74.48458862304688\n",
      "[step: 2420] loss: 74.1787109375\n",
      "[step: 2421] loss: 75.1074447631836\n",
      "[step: 2422] loss: 76.94790649414062\n",
      "[step: 2423] loss: 81.66959381103516\n",
      "[step: 2424] loss: 81.12940979003906\n",
      "[step: 2425] loss: 80.992919921875\n",
      "[step: 2426] loss: 72.96466827392578\n",
      "[step: 2427] loss: 77.447265625\n",
      "[step: 2428] loss: 85.57484436035156\n",
      "[step: 2429] loss: 76.30001831054688\n",
      "[step: 2430] loss: 72.160400390625\n",
      "[step: 2431] loss: 77.64017486572266\n",
      "[step: 2432] loss: 73.34722900390625\n",
      "[step: 2433] loss: 70.86685180664062\n",
      "[step: 2434] loss: 75.92808532714844\n",
      "[step: 2435] loss: 74.92212677001953\n",
      "[step: 2436] loss: 71.33151245117188\n",
      "[step: 2437] loss: 74.94728088378906\n",
      "[step: 2438] loss: 74.13999938964844\n",
      "[step: 2439] loss: 74.08567810058594\n",
      "[step: 2440] loss: 73.68206024169922\n",
      "[step: 2441] loss: 71.99593353271484\n",
      "[step: 2442] loss: 73.82057189941406\n",
      "[step: 2443] loss: 73.92578125\n",
      "[step: 2444] loss: 71.813720703125\n",
      "[step: 2445] loss: 72.44363403320312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2446] loss: 74.36994934082031\n",
      "[step: 2447] loss: 77.17882537841797\n",
      "[step: 2448] loss: 69.6752700805664\n",
      "[step: 2449] loss: 73.52237701416016\n",
      "[step: 2450] loss: 71.48114013671875\n",
      "[step: 2451] loss: 71.77558898925781\n",
      "[step: 2452] loss: 74.47550201416016\n",
      "[step: 2453] loss: 74.07595825195312\n",
      "[step: 2454] loss: 70.50616455078125\n",
      "[step: 2455] loss: 77.0867919921875\n",
      "[step: 2456] loss: 86.12541198730469\n",
      "[step: 2457] loss: 100.25247955322266\n",
      "[step: 2458] loss: 92.06298828125\n",
      "[step: 2459] loss: 78.59648132324219\n",
      "[step: 2460] loss: 79.51944732666016\n",
      "[step: 2461] loss: 85.51709747314453\n",
      "[step: 2462] loss: 79.36964416503906\n",
      "[step: 2463] loss: 74.3470458984375\n",
      "[step: 2464] loss: 73.38444519042969\n",
      "[step: 2465] loss: 76.85224914550781\n",
      "[step: 2466] loss: 75.18304443359375\n",
      "[step: 2467] loss: 74.14242553710938\n",
      "[step: 2468] loss: 78.99703979492188\n",
      "[step: 2469] loss: 76.85516357421875\n",
      "[step: 2470] loss: 75.38711547851562\n",
      "[step: 2471] loss: 75.15419006347656\n",
      "[step: 2472] loss: 75.04837799072266\n",
      "[step: 2473] loss: 75.62278747558594\n",
      "[step: 2474] loss: 71.8123779296875\n",
      "[step: 2475] loss: 71.25996398925781\n",
      "[step: 2476] loss: 71.42155456542969\n",
      "[step: 2477] loss: 74.31195068359375\n",
      "[step: 2478] loss: 75.23361206054688\n",
      "[step: 2479] loss: 69.91841125488281\n",
      "[step: 2480] loss: 72.91200256347656\n",
      "[step: 2481] loss: 70.4380111694336\n",
      "[step: 2482] loss: 72.80317687988281\n",
      "[step: 2483] loss: 72.89356231689453\n",
      "[step: 2484] loss: 69.9808578491211\n",
      "[step: 2485] loss: 72.04856872558594\n",
      "[step: 2486] loss: 72.84790802001953\n",
      "[step: 2487] loss: 70.78665161132812\n",
      "[step: 2488] loss: 70.3760986328125\n",
      "[step: 2489] loss: 78.50232696533203\n",
      "[step: 2490] loss: 77.98748779296875\n",
      "[step: 2491] loss: 81.47803497314453\n",
      "[step: 2492] loss: 82.7991943359375\n",
      "[step: 2493] loss: 73.11383056640625\n",
      "[step: 2494] loss: 73.90888977050781\n",
      "[step: 2495] loss: 90.80994415283203\n",
      "[step: 2496] loss: 76.90867614746094\n",
      "[step: 2497] loss: 73.19328308105469\n",
      "[step: 2498] loss: 83.48934173583984\n",
      "[step: 2499] loss: 74.88018798828125\n",
      "[step: 2500] loss: 74.36781311035156\n",
      "[step: 2501] loss: 82.18472290039062\n",
      "[step: 2502] loss: 73.31703186035156\n",
      "[step: 2503] loss: 74.56103515625\n",
      "[step: 2504] loss: 78.5460205078125\n",
      "[step: 2505] loss: 78.44126892089844\n",
      "[step: 2506] loss: 73.18984985351562\n",
      "[step: 2507] loss: 72.31436157226562\n",
      "[step: 2508] loss: 74.11617279052734\n",
      "[step: 2509] loss: 70.69337463378906\n",
      "[step: 2510] loss: 71.55888366699219\n",
      "[step: 2511] loss: 71.10710906982422\n",
      "[step: 2512] loss: 73.58616638183594\n",
      "[step: 2513] loss: 67.96600341796875\n",
      "[step: 2514] loss: 68.16835021972656\n",
      "[step: 2515] loss: 72.29423522949219\n",
      "[step: 2516] loss: 72.7445297241211\n",
      "[step: 2517] loss: 73.97377014160156\n",
      "[step: 2518] loss: 77.6976318359375\n",
      "[step: 2519] loss: 72.47727966308594\n",
      "[step: 2520] loss: 71.32147216796875\n",
      "[step: 2521] loss: 81.73591613769531\n",
      "[step: 2522] loss: 87.22245788574219\n",
      "[step: 2523] loss: 81.56501770019531\n",
      "[step: 2524] loss: 76.96656799316406\n",
      "[step: 2525] loss: 74.56990051269531\n",
      "[step: 2526] loss: 83.7688217163086\n",
      "[step: 2527] loss: 74.8483657836914\n",
      "[step: 2528] loss: 77.46955871582031\n",
      "[step: 2529] loss: 89.69850158691406\n",
      "[step: 2530] loss: 78.04733276367188\n",
      "[step: 2531] loss: 75.45451354980469\n",
      "[step: 2532] loss: 84.21479797363281\n",
      "[step: 2533] loss: 76.3441162109375\n",
      "[step: 2534] loss: 82.10243225097656\n",
      "[step: 2535] loss: 80.0052490234375\n",
      "[step: 2536] loss: 79.61392211914062\n",
      "[step: 2537] loss: 82.02223205566406\n",
      "[step: 2538] loss: 75.47117614746094\n",
      "[step: 2539] loss: 85.63358306884766\n",
      "[step: 2540] loss: 71.53922271728516\n",
      "[step: 2541] loss: 75.9946060180664\n",
      "[step: 2542] loss: 77.99177551269531\n",
      "[step: 2543] loss: 70.61471557617188\n",
      "[step: 2544] loss: 76.72057342529297\n",
      "[step: 2545] loss: 71.14874267578125\n",
      "[step: 2546] loss: 72.2166748046875\n",
      "[step: 2547] loss: 74.98048400878906\n",
      "[step: 2548] loss: 74.10997009277344\n",
      "[step: 2549] loss: 79.2955322265625\n",
      "[step: 2550] loss: 71.47859191894531\n",
      "[step: 2551] loss: 70.71824645996094\n",
      "[step: 2552] loss: 75.84640502929688\n",
      "[step: 2553] loss: 75.28622436523438\n",
      "[step: 2554] loss: 75.55567932128906\n",
      "[step: 2555] loss: 71.85208129882812\n",
      "[step: 2556] loss: 70.85846710205078\n",
      "[step: 2557] loss: 76.70231628417969\n",
      "[step: 2558] loss: 73.9158935546875\n",
      "[step: 2559] loss: 71.72853088378906\n",
      "[step: 2560] loss: 71.05106353759766\n",
      "[step: 2561] loss: 71.32389068603516\n",
      "[step: 2562] loss: 75.356201171875\n",
      "[step: 2563] loss: 71.49457550048828\n",
      "[step: 2564] loss: 71.00773620605469\n",
      "[step: 2565] loss: 71.40906524658203\n",
      "[step: 2566] loss: 71.04566192626953\n",
      "[step: 2567] loss: 69.62235260009766\n",
      "[step: 2568] loss: 71.82362365722656\n",
      "[step: 2569] loss: 72.91704559326172\n",
      "[step: 2570] loss: 67.21778106689453\n",
      "[step: 2571] loss: 72.46919250488281\n",
      "[step: 2572] loss: 65.60002136230469\n",
      "[step: 2573] loss: 71.25180053710938\n",
      "[step: 2574] loss: 73.61736297607422\n",
      "[step: 2575] loss: 69.9767074584961\n",
      "[step: 2576] loss: 71.32537078857422\n",
      "[step: 2577] loss: 72.64077758789062\n",
      "[step: 2578] loss: 72.09212493896484\n",
      "[step: 2579] loss: 76.58329010009766\n",
      "[step: 2580] loss: 75.53134155273438\n",
      "[step: 2581] loss: 78.95587158203125\n",
      "[step: 2582] loss: 75.36433410644531\n",
      "[step: 2583] loss: 71.93977355957031\n",
      "[step: 2584] loss: 76.84412384033203\n",
      "[step: 2585] loss: 71.46849822998047\n",
      "[step: 2586] loss: 72.58992004394531\n",
      "[step: 2587] loss: 73.92567443847656\n",
      "[step: 2588] loss: 69.0861587524414\n",
      "[step: 2589] loss: 68.83121490478516\n",
      "[step: 2590] loss: 72.07919311523438\n",
      "[step: 2591] loss: 67.48290252685547\n",
      "[step: 2592] loss: 70.97669982910156\n",
      "[step: 2593] loss: 68.95394897460938\n",
      "[step: 2594] loss: 75.72100830078125\n",
      "[step: 2595] loss: 81.98613739013672\n",
      "[step: 2596] loss: 90.37205505371094\n",
      "[step: 2597] loss: 113.24809265136719\n",
      "[step: 2598] loss: 77.14645385742188\n",
      "[step: 2599] loss: 91.40342712402344\n",
      "[step: 2600] loss: 103.27855682373047\n",
      "[step: 2601] loss: 83.85943603515625\n",
      "[step: 2602] loss: 100.8450927734375\n",
      "[step: 2603] loss: 93.60381317138672\n",
      "[step: 2604] loss: 99.87922668457031\n",
      "[step: 2605] loss: 87.32511901855469\n",
      "[step: 2606] loss: 92.386962890625\n",
      "[step: 2607] loss: 80.31890869140625\n",
      "[step: 2608] loss: 87.60848999023438\n",
      "[step: 2609] loss: 81.66608428955078\n",
      "[step: 2610] loss: 83.577392578125\n",
      "[step: 2611] loss: 88.83590698242188\n",
      "[step: 2612] loss: 80.19439697265625\n",
      "[step: 2613] loss: 83.8194351196289\n",
      "[step: 2614] loss: 77.56373596191406\n",
      "[step: 2615] loss: 79.19674682617188\n",
      "[step: 2616] loss: 72.55630493164062\n",
      "[step: 2617] loss: 83.29270935058594\n",
      "[step: 2618] loss: 78.66127014160156\n",
      "[step: 2619] loss: 75.10035705566406\n",
      "[step: 2620] loss: 76.10108947753906\n",
      "[step: 2621] loss: 73.10709381103516\n",
      "[step: 2622] loss: 77.91055297851562\n",
      "[step: 2623] loss: 72.23086547851562\n",
      "[step: 2624] loss: 76.13002014160156\n",
      "[step: 2625] loss: 71.05420684814453\n",
      "[step: 2626] loss: 74.84078979492188\n",
      "[step: 2627] loss: 71.77257537841797\n",
      "[step: 2628] loss: 68.36761474609375\n",
      "[step: 2629] loss: 72.6689453125\n",
      "[step: 2630] loss: 72.2660140991211\n",
      "[step: 2631] loss: 72.79986572265625\n",
      "[step: 2632] loss: 71.16254425048828\n",
      "[step: 2633] loss: 69.60688781738281\n",
      "[step: 2634] loss: 68.96367645263672\n",
      "[step: 2635] loss: 70.1279296875\n",
      "[step: 2636] loss: 67.97871398925781\n",
      "[step: 2637] loss: 73.26385498046875\n",
      "[step: 2638] loss: 67.7940444946289\n",
      "[step: 2639] loss: 70.99404907226562\n",
      "[step: 2640] loss: 72.75834655761719\n",
      "[step: 2641] loss: 72.7518310546875\n",
      "[step: 2642] loss: 73.01116943359375\n",
      "[step: 2643] loss: 71.44937133789062\n",
      "[step: 2644] loss: 68.59947204589844\n",
      "[step: 2645] loss: 70.87882995605469\n",
      "[step: 2646] loss: 69.59065246582031\n",
      "[step: 2647] loss: 74.87783813476562\n",
      "[step: 2648] loss: 75.06781005859375\n",
      "[step: 2649] loss: 70.36680603027344\n",
      "[step: 2650] loss: 68.12049102783203\n",
      "[step: 2651] loss: 69.78340148925781\n",
      "[step: 2652] loss: 70.05686950683594\n",
      "[step: 2653] loss: 70.38065338134766\n",
      "[step: 2654] loss: 70.7764892578125\n",
      "[step: 2655] loss: 69.35624694824219\n",
      "[step: 2656] loss: 70.23633575439453\n",
      "[step: 2657] loss: 68.81156921386719\n",
      "[step: 2658] loss: 67.26383209228516\n",
      "[step: 2659] loss: 70.05509948730469\n",
      "[step: 2660] loss: 71.05708312988281\n",
      "[step: 2661] loss: 69.25336456298828\n",
      "[step: 2662] loss: 72.16809844970703\n",
      "[step: 2663] loss: 70.96370697021484\n",
      "[step: 2664] loss: 68.99056243896484\n",
      "[step: 2665] loss: 70.10263061523438\n",
      "[step: 2666] loss: 75.21553039550781\n",
      "[step: 2667] loss: 76.5189208984375\n",
      "[step: 2668] loss: 65.63311767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2669] loss: 69.88105773925781\n",
      "[step: 2670] loss: 75.41170501708984\n",
      "[step: 2671] loss: 75.33882141113281\n",
      "[step: 2672] loss: 71.58454132080078\n",
      "[step: 2673] loss: 70.24842834472656\n",
      "[step: 2674] loss: 69.28092193603516\n",
      "[step: 2675] loss: 70.91532135009766\n",
      "[step: 2676] loss: 67.58204650878906\n",
      "[step: 2677] loss: 68.73741149902344\n",
      "[step: 2678] loss: 65.85800170898438\n",
      "[step: 2679] loss: 69.96842956542969\n",
      "[step: 2680] loss: 71.0802993774414\n",
      "[step: 2681] loss: 72.92552185058594\n",
      "[step: 2682] loss: 71.6545639038086\n",
      "[step: 2683] loss: 69.35877227783203\n",
      "[step: 2684] loss: 72.82916259765625\n",
      "[step: 2685] loss: 72.294921875\n",
      "[step: 2686] loss: 77.52998352050781\n",
      "[step: 2687] loss: 73.56597900390625\n",
      "[step: 2688] loss: 68.57286071777344\n",
      "[step: 2689] loss: 72.65962982177734\n",
      "[step: 2690] loss: 70.0762939453125\n",
      "[step: 2691] loss: 69.05220031738281\n",
      "[step: 2692] loss: 71.39729309082031\n",
      "[step: 2693] loss: 81.37763214111328\n",
      "[step: 2694] loss: 102.06260681152344\n",
      "[step: 2695] loss: 78.83208465576172\n",
      "[step: 2696] loss: 69.81338500976562\n",
      "[step: 2697] loss: 79.07669830322266\n",
      "[step: 2698] loss: 80.58009338378906\n",
      "[step: 2699] loss: 68.83271026611328\n",
      "[step: 2700] loss: 77.0372314453125\n",
      "[step: 2701] loss: 82.9669189453125\n",
      "[step: 2702] loss: 74.51972961425781\n",
      "[step: 2703] loss: 78.02232360839844\n",
      "[step: 2704] loss: 77.12374877929688\n",
      "[step: 2705] loss: 69.68350982666016\n",
      "[step: 2706] loss: 75.17353820800781\n",
      "[step: 2707] loss: 73.44149017333984\n",
      "[step: 2708] loss: 71.76924896240234\n",
      "[step: 2709] loss: 72.1558609008789\n",
      "[step: 2710] loss: 70.9324951171875\n",
      "[step: 2711] loss: 76.93151092529297\n",
      "[step: 2712] loss: 71.95864868164062\n",
      "[step: 2713] loss: 67.622802734375\n",
      "[step: 2714] loss: 72.00151062011719\n",
      "[step: 2715] loss: 73.05020904541016\n",
      "[step: 2716] loss: 69.62156677246094\n",
      "[step: 2717] loss: 67.21894836425781\n",
      "[step: 2718] loss: 68.2427978515625\n",
      "[step: 2719] loss: 66.10053253173828\n",
      "[step: 2720] loss: 67.4679183959961\n",
      "[step: 2721] loss: 68.15570068359375\n",
      "[step: 2722] loss: 69.32585906982422\n",
      "[step: 2723] loss: 71.59078979492188\n",
      "[step: 2724] loss: 72.5936279296875\n",
      "[step: 2725] loss: 67.43258666992188\n",
      "[step: 2726] loss: 71.53949737548828\n",
      "[step: 2727] loss: 72.5815200805664\n",
      "[step: 2728] loss: 70.45050811767578\n",
      "[step: 2729] loss: 66.31031036376953\n",
      "[step: 2730] loss: 73.69046020507812\n",
      "[step: 2731] loss: 79.02056121826172\n",
      "[step: 2732] loss: 71.83953857421875\n",
      "[step: 2733] loss: 70.6808853149414\n",
      "[step: 2734] loss: 76.08698272705078\n",
      "[step: 2735] loss: 78.37403869628906\n",
      "[step: 2736] loss: 74.64556121826172\n",
      "[step: 2737] loss: 71.70494079589844\n",
      "[step: 2738] loss: 73.88096618652344\n",
      "[step: 2739] loss: 76.96509552001953\n",
      "[step: 2740] loss: 72.76071166992188\n",
      "[step: 2741] loss: 67.2365493774414\n",
      "[step: 2742] loss: 72.46511840820312\n",
      "[step: 2743] loss: 70.4710693359375\n",
      "[step: 2744] loss: 67.77383422851562\n",
      "[step: 2745] loss: 72.43669128417969\n",
      "[step: 2746] loss: 73.02216339111328\n",
      "[step: 2747] loss: 74.26901245117188\n",
      "[step: 2748] loss: 70.07130432128906\n",
      "[step: 2749] loss: 65.72967529296875\n",
      "[step: 2750] loss: 75.6357421875\n",
      "[step: 2751] loss: 69.68207550048828\n",
      "[step: 2752] loss: 68.69539642333984\n",
      "[step: 2753] loss: 71.16587829589844\n",
      "[step: 2754] loss: 75.20128631591797\n",
      "[step: 2755] loss: 66.22666931152344\n",
      "[step: 2756] loss: 71.45211791992188\n",
      "[step: 2757] loss: 67.20744323730469\n",
      "[step: 2758] loss: 70.26115417480469\n",
      "[step: 2759] loss: 67.63526153564453\n",
      "[step: 2760] loss: 68.31563568115234\n",
      "[step: 2761] loss: 69.5316390991211\n",
      "[step: 2762] loss: 69.46859741210938\n",
      "[step: 2763] loss: 69.70880126953125\n",
      "[step: 2764] loss: 71.00460815429688\n",
      "[step: 2765] loss: 68.96206665039062\n",
      "[step: 2766] loss: 69.73197937011719\n",
      "[step: 2767] loss: 66.40899658203125\n",
      "[step: 2768] loss: 70.36808776855469\n",
      "[step: 2769] loss: 68.57176971435547\n",
      "[step: 2770] loss: 68.65962982177734\n",
      "[step: 2771] loss: 69.48545837402344\n",
      "[step: 2772] loss: 67.40077209472656\n",
      "[step: 2773] loss: 70.468994140625\n",
      "[step: 2774] loss: 69.05877685546875\n",
      "[step: 2775] loss: 71.41249084472656\n",
      "[step: 2776] loss: 75.58401489257812\n",
      "[step: 2777] loss: 78.29020690917969\n",
      "[step: 2778] loss: 73.45891571044922\n",
      "[step: 2779] loss: 77.55671691894531\n",
      "[step: 2780] loss: 68.51921081542969\n",
      "[step: 2781] loss: 69.35084533691406\n",
      "[step: 2782] loss: 71.54458618164062\n",
      "[step: 2783] loss: 79.61134338378906\n",
      "[step: 2784] loss: 81.89759826660156\n",
      "[step: 2785] loss: 77.3465347290039\n",
      "[step: 2786] loss: 69.65361022949219\n",
      "[step: 2787] loss: 76.98542785644531\n",
      "[step: 2788] loss: 77.59873962402344\n",
      "[step: 2789] loss: 69.90837860107422\n",
      "[step: 2790] loss: 73.81197357177734\n",
      "[step: 2791] loss: 71.05264282226562\n",
      "[step: 2792] loss: 65.58906555175781\n",
      "[step: 2793] loss: 68.626953125\n",
      "[step: 2794] loss: 70.6997299194336\n",
      "[step: 2795] loss: 69.0853271484375\n",
      "[step: 2796] loss: 71.03243255615234\n",
      "[step: 2797] loss: 68.58877563476562\n",
      "[step: 2798] loss: 68.75606536865234\n",
      "[step: 2799] loss: 68.55923461914062\n",
      "[step: 2800] loss: 65.218505859375\n",
      "[step: 2801] loss: 70.36032104492188\n",
      "[step: 2802] loss: 72.88412475585938\n",
      "[step: 2803] loss: 68.92011260986328\n",
      "[step: 2804] loss: 75.57208251953125\n",
      "[step: 2805] loss: 68.79251861572266\n",
      "[step: 2806] loss: 68.14480590820312\n",
      "[step: 2807] loss: 74.01024627685547\n",
      "[step: 2808] loss: 75.25733184814453\n",
      "[step: 2809] loss: 70.10002136230469\n",
      "[step: 2810] loss: 68.478271484375\n",
      "[step: 2811] loss: 72.08821105957031\n",
      "[step: 2812] loss: 77.66548919677734\n",
      "[step: 2813] loss: 72.31133270263672\n",
      "[step: 2814] loss: 66.63907623291016\n",
      "[step: 2815] loss: 69.80453491210938\n",
      "[step: 2816] loss: 71.16461181640625\n",
      "[step: 2817] loss: 67.92301940917969\n",
      "[step: 2818] loss: 68.81517028808594\n",
      "[step: 2819] loss: 73.84339904785156\n",
      "[step: 2820] loss: 72.92976379394531\n",
      "[step: 2821] loss: 66.5318603515625\n",
      "[step: 2822] loss: 70.11428833007812\n",
      "[step: 2823] loss: 74.23648071289062\n",
      "[step: 2824] loss: 98.3231430053711\n",
      "[step: 2825] loss: 133.61553955078125\n",
      "[step: 2826] loss: 96.46355438232422\n",
      "[step: 2827] loss: 106.5782470703125\n",
      "[step: 2828] loss: 95.94305419921875\n",
      "[step: 2829] loss: 100.62715148925781\n",
      "[step: 2830] loss: 85.49060821533203\n",
      "[step: 2831] loss: 91.7192153930664\n",
      "[step: 2832] loss: 88.01136779785156\n",
      "[step: 2833] loss: 86.12187194824219\n",
      "[step: 2834] loss: 87.57929992675781\n",
      "[step: 2835] loss: 77.24211883544922\n",
      "[step: 2836] loss: 82.3837890625\n",
      "[step: 2837] loss: 80.63613891601562\n",
      "[step: 2838] loss: 78.06768035888672\n",
      "[step: 2839] loss: 78.13581848144531\n",
      "[step: 2840] loss: 73.89493560791016\n",
      "[step: 2841] loss: 76.29378509521484\n",
      "[step: 2842] loss: 76.58039855957031\n",
      "[step: 2843] loss: 76.01834106445312\n",
      "[step: 2844] loss: 79.15508270263672\n",
      "[step: 2845] loss: 74.07099151611328\n",
      "[step: 2846] loss: 72.43025207519531\n",
      "[step: 2847] loss: 75.39779663085938\n",
      "[step: 2848] loss: 70.50631713867188\n",
      "[step: 2849] loss: 67.8664321899414\n",
      "[step: 2850] loss: 75.09971618652344\n",
      "[step: 2851] loss: 71.64774322509766\n",
      "[step: 2852] loss: 66.80375671386719\n",
      "[step: 2853] loss: 74.19001770019531\n",
      "[step: 2854] loss: 77.41973876953125\n",
      "[step: 2855] loss: 74.21924591064453\n",
      "[step: 2856] loss: 68.38716125488281\n",
      "[step: 2857] loss: 74.61398315429688\n",
      "[step: 2858] loss: 74.56730651855469\n",
      "[step: 2859] loss: 70.2630386352539\n",
      "[step: 2860] loss: 75.70866394042969\n",
      "[step: 2861] loss: 78.3777084350586\n",
      "[step: 2862] loss: 67.88410949707031\n",
      "[step: 2863] loss: 80.48788452148438\n",
      "[step: 2864] loss: 84.63018798828125\n",
      "[step: 2865] loss: 69.45126342773438\n",
      "[step: 2866] loss: 77.25216674804688\n",
      "[step: 2867] loss: 80.98619842529297\n",
      "[step: 2868] loss: 68.95404052734375\n",
      "[step: 2869] loss: 72.53439331054688\n",
      "[step: 2870] loss: 79.84231567382812\n",
      "[step: 2871] loss: 69.31532287597656\n",
      "[step: 2872] loss: 72.42943572998047\n",
      "[step: 2873] loss: 78.48585510253906\n",
      "[step: 2874] loss: 68.85952758789062\n",
      "[step: 2875] loss: 74.27816772460938\n",
      "[step: 2876] loss: 74.90656280517578\n",
      "[step: 2877] loss: 68.85143280029297\n",
      "[step: 2878] loss: 76.28886413574219\n",
      "[step: 2879] loss: 72.62873840332031\n",
      "[step: 2880] loss: 66.40863037109375\n",
      "[step: 2881] loss: 76.06790161132812\n",
      "[step: 2882] loss: 75.3262939453125\n",
      "[step: 2883] loss: 70.30743408203125\n",
      "[step: 2884] loss: 72.71302795410156\n",
      "[step: 2885] loss: 68.14724731445312\n",
      "[step: 2886] loss: 66.66439056396484\n",
      "[step: 2887] loss: 70.14977264404297\n",
      "[step: 2888] loss: 67.29649353027344\n",
      "[step: 2889] loss: 70.84519958496094\n",
      "[step: 2890] loss: 72.14031982421875\n",
      "[step: 2891] loss: 69.28555297851562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2892] loss: 71.49999237060547\n",
      "[step: 2893] loss: 69.87519836425781\n",
      "[step: 2894] loss: 68.74813842773438\n",
      "[step: 2895] loss: 70.01251220703125\n",
      "[step: 2896] loss: 66.62336730957031\n",
      "[step: 2897] loss: 67.06632232666016\n",
      "[step: 2898] loss: 69.06253051757812\n",
      "[step: 2899] loss: 68.95654296875\n",
      "[step: 2900] loss: 65.24885559082031\n",
      "[step: 2901] loss: 71.04945373535156\n",
      "[step: 2902] loss: 71.67021942138672\n",
      "[step: 2903] loss: 72.22834777832031\n",
      "[step: 2904] loss: 65.03582763671875\n",
      "[step: 2905] loss: 65.27925109863281\n",
      "[step: 2906] loss: 67.35226440429688\n",
      "[step: 2907] loss: 68.14749145507812\n",
      "[step: 2908] loss: 67.08529663085938\n",
      "[step: 2909] loss: 66.4215087890625\n",
      "[step: 2910] loss: 71.41728210449219\n",
      "[step: 2911] loss: 74.60556030273438\n",
      "[step: 2912] loss: 70.24964904785156\n",
      "[step: 2913] loss: 65.53846740722656\n",
      "[step: 2914] loss: 66.87637329101562\n",
      "[step: 2915] loss: 70.90021514892578\n",
      "[step: 2916] loss: 68.12799072265625\n",
      "[step: 2917] loss: 68.46630859375\n",
      "[step: 2918] loss: 62.93479537963867\n",
      "[step: 2919] loss: 67.74880981445312\n",
      "[step: 2920] loss: 66.18598175048828\n",
      "[step: 2921] loss: 69.99209594726562\n",
      "[step: 2922] loss: 68.19044494628906\n",
      "[step: 2923] loss: 68.23992919921875\n",
      "[step: 2924] loss: 65.31405639648438\n",
      "[step: 2925] loss: 68.18829345703125\n",
      "[step: 2926] loss: 70.65484619140625\n",
      "[step: 2927] loss: 69.91957092285156\n",
      "[step: 2928] loss: 67.49641418457031\n",
      "[step: 2929] loss: 67.18783569335938\n",
      "[step: 2930] loss: 67.9715347290039\n",
      "[step: 2931] loss: 64.22547912597656\n",
      "[step: 2932] loss: 68.4032974243164\n",
      "[step: 2933] loss: 65.752197265625\n",
      "[step: 2934] loss: 69.83695983886719\n",
      "[step: 2935] loss: 74.10621643066406\n",
      "[step: 2936] loss: 65.14360046386719\n",
      "[step: 2937] loss: 69.49176788330078\n",
      "[step: 2938] loss: 68.62390899658203\n",
      "[step: 2939] loss: 68.11915588378906\n",
      "[step: 2940] loss: 65.80970764160156\n",
      "[step: 2941] loss: 71.41082763671875\n",
      "[step: 2942] loss: 72.25709533691406\n",
      "[step: 2943] loss: 74.74803924560547\n",
      "[step: 2944] loss: 68.85713195800781\n",
      "[step: 2945] loss: 71.59546661376953\n",
      "[step: 2946] loss: 80.01699829101562\n",
      "[step: 2947] loss: 76.17475891113281\n",
      "[step: 2948] loss: 65.82939910888672\n",
      "[step: 2949] loss: 73.84747314453125\n",
      "[step: 2950] loss: 71.49842834472656\n",
      "[step: 2951] loss: 69.63005065917969\n",
      "[step: 2952] loss: 76.12857818603516\n",
      "[step: 2953] loss: 67.01112365722656\n",
      "[step: 2954] loss: 73.42315673828125\n",
      "[step: 2955] loss: 78.81155395507812\n",
      "[step: 2956] loss: 70.43496704101562\n",
      "[step: 2957] loss: 68.4160385131836\n",
      "[step: 2958] loss: 79.03619384765625\n",
      "[step: 2959] loss: 73.53887939453125\n",
      "[step: 2960] loss: 68.5888442993164\n",
      "[step: 2961] loss: 69.84014129638672\n",
      "[step: 2962] loss: 68.08087921142578\n",
      "[step: 2963] loss: 66.32438659667969\n",
      "[step: 2964] loss: 67.29412841796875\n",
      "[step: 2965] loss: 72.5049057006836\n",
      "[step: 2966] loss: 72.61769104003906\n",
      "[step: 2967] loss: 67.57487487792969\n",
      "[step: 2968] loss: 68.3245620727539\n",
      "[step: 2969] loss: 73.36930847167969\n",
      "[step: 2970] loss: 67.83833312988281\n",
      "[step: 2971] loss: 64.3331069946289\n",
      "[step: 2972] loss: 70.70297241210938\n",
      "[step: 2973] loss: 76.98497009277344\n",
      "[step: 2974] loss: 71.0288314819336\n",
      "[step: 2975] loss: 69.8434066772461\n",
      "[step: 2976] loss: 74.17182922363281\n",
      "[step: 2977] loss: 67.96578979492188\n",
      "[step: 2978] loss: 72.23553466796875\n",
      "[step: 2979] loss: 74.49301147460938\n",
      "[step: 2980] loss: 66.1522216796875\n",
      "[step: 2981] loss: 66.40812683105469\n",
      "[step: 2982] loss: 69.01542663574219\n",
      "[step: 2983] loss: 68.130859375\n",
      "[step: 2984] loss: 64.65568542480469\n",
      "[step: 2985] loss: 72.05374145507812\n",
      "[step: 2986] loss: 72.80824279785156\n",
      "[step: 2987] loss: 66.09649658203125\n",
      "[step: 2988] loss: 70.85838317871094\n",
      "[step: 2989] loss: 76.03070831298828\n",
      "[step: 2990] loss: 69.3865737915039\n",
      "[step: 2991] loss: 71.95101928710938\n",
      "[step: 2992] loss: 75.2711181640625\n",
      "[step: 2993] loss: 72.43507385253906\n",
      "[step: 2994] loss: 70.9551010131836\n",
      "[step: 2995] loss: 85.03175354003906\n",
      "[step: 2996] loss: 77.29373168945312\n",
      "[step: 2997] loss: 69.88706970214844\n",
      "[step: 2998] loss: 81.09529876708984\n",
      "[step: 2999] loss: 70.56526947021484\n",
      "[step: 3000] loss: 65.6086196899414\n",
      "[step: 3001] loss: 72.3817367553711\n",
      "[step: 3002] loss: 67.18859100341797\n",
      "[step: 3003] loss: 68.78114318847656\n",
      "[step: 3004] loss: 76.10771942138672\n",
      "[step: 3005] loss: 69.59408569335938\n",
      "[step: 3006] loss: 73.06784057617188\n",
      "[step: 3007] loss: 66.5904312133789\n",
      "[step: 3008] loss: 67.512451171875\n",
      "[step: 3009] loss: 72.96018981933594\n",
      "[step: 3010] loss: 71.00198364257812\n",
      "[step: 3011] loss: 65.06658935546875\n",
      "[step: 3012] loss: 71.6924819946289\n",
      "[step: 3013] loss: 65.55026245117188\n",
      "[step: 3014] loss: 70.43299102783203\n",
      "[step: 3015] loss: 72.23017120361328\n",
      "[step: 3016] loss: 74.17645263671875\n",
      "[step: 3017] loss: 71.90016174316406\n",
      "[step: 3018] loss: 67.41375732421875\n",
      "[step: 3019] loss: 69.83322143554688\n",
      "[step: 3020] loss: 74.2165756225586\n",
      "[step: 3021] loss: 72.8051986694336\n",
      "[step: 3022] loss: 69.95211791992188\n",
      "[step: 3023] loss: 67.08003997802734\n",
      "[step: 3024] loss: 67.34310150146484\n",
      "[step: 3025] loss: 72.81655883789062\n",
      "[step: 3026] loss: 68.57633972167969\n",
      "[step: 3027] loss: 71.59286499023438\n",
      "[step: 3028] loss: 66.49043273925781\n",
      "[step: 3029] loss: 66.12134552001953\n",
      "[step: 3030] loss: 70.07276916503906\n",
      "[step: 3031] loss: 72.29808044433594\n",
      "[step: 3032] loss: 66.32205200195312\n",
      "[step: 3033] loss: 67.97676086425781\n",
      "[step: 3034] loss: 70.32279205322266\n",
      "[step: 3035] loss: 72.66816711425781\n",
      "[step: 3036] loss: 70.19975280761719\n",
      "[step: 3037] loss: 70.00749206542969\n",
      "[step: 3038] loss: 67.23348999023438\n",
      "[step: 3039] loss: 70.87633514404297\n",
      "[step: 3040] loss: 66.18312072753906\n",
      "[step: 3041] loss: 64.97355651855469\n",
      "[step: 3042] loss: 71.00199890136719\n",
      "[step: 3043] loss: 65.2148208618164\n",
      "[step: 3044] loss: 64.78821563720703\n",
      "[step: 3045] loss: 66.09823608398438\n",
      "[step: 3046] loss: 65.81299591064453\n",
      "[step: 3047] loss: 64.65769958496094\n",
      "[step: 3048] loss: 64.56611633300781\n",
      "[step: 3049] loss: 64.39881134033203\n",
      "[step: 3050] loss: 65.8069839477539\n",
      "[step: 3051] loss: 66.83436584472656\n",
      "[step: 3052] loss: 66.67143249511719\n",
      "[step: 3053] loss: 62.82524108886719\n",
      "[step: 3054] loss: 62.812705993652344\n",
      "[step: 3055] loss: 65.75468444824219\n",
      "[step: 3056] loss: 67.9530029296875\n",
      "[step: 3057] loss: 68.55098724365234\n",
      "[step: 3058] loss: 66.19348907470703\n",
      "[step: 3059] loss: 65.58042907714844\n",
      "[step: 3060] loss: 65.79829406738281\n",
      "[step: 3061] loss: 68.46308898925781\n",
      "[step: 3062] loss: 64.53741455078125\n",
      "[step: 3063] loss: 63.890220642089844\n",
      "[step: 3064] loss: 65.93587493896484\n",
      "[step: 3065] loss: 66.42407989501953\n",
      "[step: 3066] loss: 65.97638702392578\n",
      "[step: 3067] loss: 67.32457733154297\n",
      "[step: 3068] loss: 70.1559829711914\n",
      "[step: 3069] loss: 65.70449829101562\n",
      "[step: 3070] loss: 65.38175964355469\n",
      "[step: 3071] loss: 64.87136840820312\n",
      "[step: 3072] loss: 64.30735778808594\n",
      "[step: 3073] loss: 65.02456665039062\n",
      "[step: 3074] loss: 66.90192413330078\n",
      "[step: 3075] loss: 66.77925109863281\n",
      "[step: 3076] loss: 66.72511291503906\n",
      "[step: 3077] loss: 67.32435607910156\n",
      "[step: 3078] loss: 63.856422424316406\n",
      "[step: 3079] loss: 65.27969360351562\n",
      "[step: 3080] loss: 64.105712890625\n",
      "[step: 3081] loss: 71.83303833007812\n",
      "[step: 3082] loss: 65.37472534179688\n",
      "[step: 3083] loss: 65.11111450195312\n",
      "[step: 3084] loss: 67.17388916015625\n",
      "[step: 3085] loss: 66.74464416503906\n",
      "[step: 3086] loss: 71.93716430664062\n",
      "[step: 3087] loss: 75.95695495605469\n",
      "[step: 3088] loss: 68.78602600097656\n",
      "[step: 3089] loss: 65.93846130371094\n",
      "[step: 3090] loss: 66.74085998535156\n",
      "[step: 3091] loss: 68.52301025390625\n",
      "[step: 3092] loss: 74.1832275390625\n",
      "[step: 3093] loss: 74.7886962890625\n",
      "[step: 3094] loss: 68.96906280517578\n",
      "[step: 3095] loss: 63.985389709472656\n",
      "[step: 3096] loss: 67.736328125\n",
      "[step: 3097] loss: 71.21257019042969\n",
      "[step: 3098] loss: 70.5350341796875\n",
      "[step: 3099] loss: 66.71656799316406\n",
      "[step: 3100] loss: 73.45240783691406\n",
      "[step: 3101] loss: 76.48326873779297\n",
      "[step: 3102] loss: 67.57696533203125\n",
      "[step: 3103] loss: 64.50909423828125\n",
      "[step: 3104] loss: 71.41470336914062\n",
      "[step: 3105] loss: 66.34140014648438\n",
      "[step: 3106] loss: 69.09281921386719\n",
      "[step: 3107] loss: 71.87748718261719\n",
      "[step: 3108] loss: 65.9290542602539\n",
      "[step: 3109] loss: 73.83305358886719\n",
      "[step: 3110] loss: 80.05127716064453\n",
      "[step: 3111] loss: 66.87377166748047\n",
      "[step: 3112] loss: 70.57975006103516\n",
      "[step: 3113] loss: 81.19224548339844\n",
      "[step: 3114] loss: 70.30265045166016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3115] loss: 64.34233093261719\n",
      "[step: 3116] loss: 75.65908813476562\n",
      "[step: 3117] loss: 74.49488830566406\n",
      "[step: 3118] loss: 65.8626708984375\n",
      "[step: 3119] loss: 68.79972839355469\n",
      "[step: 3120] loss: 80.02474975585938\n",
      "[step: 3121] loss: 69.42919921875\n",
      "[step: 3122] loss: 66.03258514404297\n",
      "[step: 3123] loss: 74.93244934082031\n",
      "[step: 3124] loss: 61.283935546875\n",
      "[step: 3125] loss: 69.60735321044922\n",
      "[step: 3126] loss: 64.81637573242188\n",
      "[step: 3127] loss: 66.87118530273438\n",
      "[step: 3128] loss: 67.17525482177734\n",
      "[step: 3129] loss: 68.06747436523438\n",
      "[step: 3130] loss: 63.93009948730469\n",
      "[step: 3131] loss: 64.17430877685547\n",
      "[step: 3132] loss: 65.08183288574219\n",
      "[step: 3133] loss: 66.47816467285156\n",
      "[step: 3134] loss: 67.7530288696289\n",
      "[step: 3135] loss: 62.301300048828125\n",
      "[step: 3136] loss: 61.24790954589844\n",
      "[step: 3137] loss: 65.10920715332031\n",
      "[step: 3138] loss: 62.88028335571289\n",
      "[step: 3139] loss: 65.64373779296875\n",
      "[step: 3140] loss: 65.27367401123047\n",
      "[step: 3141] loss: 63.26871871948242\n",
      "[step: 3142] loss: 65.8929443359375\n",
      "[step: 3143] loss: 65.0050048828125\n",
      "[step: 3144] loss: 61.778717041015625\n",
      "[step: 3145] loss: 63.099998474121094\n",
      "[step: 3146] loss: 63.44057083129883\n",
      "[step: 3147] loss: 63.791900634765625\n",
      "[step: 3148] loss: 66.58275604248047\n",
      "[step: 3149] loss: 63.02098083496094\n",
      "[step: 3150] loss: 66.0103759765625\n",
      "[step: 3151] loss: 62.204505920410156\n",
      "[step: 3152] loss: 63.16868591308594\n",
      "[step: 3153] loss: 68.35662841796875\n",
      "[step: 3154] loss: 67.43295288085938\n",
      "[step: 3155] loss: 61.97901153564453\n",
      "[step: 3156] loss: 66.16838073730469\n",
      "[step: 3157] loss: 68.68193054199219\n",
      "[step: 3158] loss: 72.92416381835938\n",
      "[step: 3159] loss: 86.341796875\n",
      "[step: 3160] loss: 73.40697479248047\n",
      "[step: 3161] loss: 64.49571228027344\n",
      "[step: 3162] loss: 72.66838073730469\n",
      "[step: 3163] loss: 70.32108306884766\n",
      "[step: 3164] loss: 68.26722717285156\n",
      "[step: 3165] loss: 67.07142639160156\n",
      "[step: 3166] loss: 64.54490661621094\n",
      "[step: 3167] loss: 65.0817642211914\n",
      "[step: 3168] loss: 65.47430419921875\n",
      "[step: 3169] loss: 64.53236389160156\n",
      "[step: 3170] loss: 64.97791290283203\n",
      "[step: 3171] loss: 63.03144073486328\n",
      "[step: 3172] loss: 64.83026123046875\n",
      "[step: 3173] loss: 68.18968200683594\n",
      "[step: 3174] loss: 70.67311096191406\n",
      "[step: 3175] loss: 65.43058776855469\n",
      "[step: 3176] loss: 66.72676086425781\n",
      "[step: 3177] loss: 65.62605285644531\n",
      "[step: 3178] loss: 67.7611312866211\n",
      "[step: 3179] loss: 67.15227508544922\n",
      "[step: 3180] loss: 65.1785888671875\n",
      "[step: 3181] loss: 71.54475402832031\n",
      "[step: 3182] loss: 70.91310119628906\n",
      "[step: 3183] loss: 62.974945068359375\n",
      "[step: 3184] loss: 68.94526672363281\n",
      "[step: 3185] loss: 77.4432373046875\n",
      "[step: 3186] loss: 69.71034240722656\n",
      "[step: 3187] loss: 65.15664672851562\n",
      "[step: 3188] loss: 70.25829315185547\n",
      "[step: 3189] loss: 76.17222595214844\n",
      "[step: 3190] loss: 68.61268615722656\n",
      "[step: 3191] loss: 62.04877471923828\n",
      "[step: 3192] loss: 68.26502990722656\n",
      "[step: 3193] loss: 66.24811553955078\n",
      "[step: 3194] loss: 62.78941345214844\n",
      "[step: 3195] loss: 73.39936828613281\n",
      "[step: 3196] loss: 69.450439453125\n",
      "[step: 3197] loss: 64.64569091796875\n",
      "[step: 3198] loss: 63.60038375854492\n",
      "[step: 3199] loss: 69.8819580078125\n",
      "[step: 3200] loss: 64.89032745361328\n",
      "[step: 3201] loss: 63.2635383605957\n",
      "[step: 3202] loss: 70.55900573730469\n",
      "[step: 3203] loss: 70.12954711914062\n",
      "[step: 3204] loss: 69.41584777832031\n",
      "[step: 3205] loss: 66.1507568359375\n",
      "[step: 3206] loss: 64.72181701660156\n",
      "[step: 3207] loss: 66.79297637939453\n",
      "[step: 3208] loss: 59.92341995239258\n",
      "[step: 3209] loss: 61.76112365722656\n",
      "[step: 3210] loss: 67.40005493164062\n",
      "[step: 3211] loss: 65.71250915527344\n",
      "[step: 3212] loss: 64.76004028320312\n",
      "[step: 3213] loss: 64.03082275390625\n",
      "[step: 3214] loss: 64.05332946777344\n",
      "[step: 3215] loss: 69.24089050292969\n",
      "[step: 3216] loss: 68.14370727539062\n",
      "[step: 3217] loss: 67.18881225585938\n",
      "[step: 3218] loss: 62.663787841796875\n",
      "[step: 3219] loss: 67.33778381347656\n",
      "[step: 3220] loss: 71.62984466552734\n",
      "[step: 3221] loss: 72.4737548828125\n",
      "[step: 3222] loss: 64.37869262695312\n",
      "[step: 3223] loss: 63.76626205444336\n",
      "[step: 3224] loss: 63.19032287597656\n",
      "[step: 3225] loss: 65.24226379394531\n",
      "[step: 3226] loss: 63.5223274230957\n",
      "[step: 3227] loss: 63.5797004699707\n",
      "[step: 3228] loss: 62.21635437011719\n",
      "[step: 3229] loss: 63.12014389038086\n",
      "[step: 3230] loss: 62.49298858642578\n",
      "[step: 3231] loss: 64.02094268798828\n",
      "[step: 3232] loss: 65.5216293334961\n",
      "[step: 3233] loss: 61.27598571777344\n",
      "[step: 3234] loss: 68.84135437011719\n",
      "[step: 3235] loss: 90.63029479980469\n",
      "[step: 3236] loss: 89.84956359863281\n",
      "[step: 3237] loss: 84.66578674316406\n",
      "[step: 3238] loss: 77.12571716308594\n",
      "[step: 3239] loss: 89.22122192382812\n",
      "[step: 3240] loss: 74.95259094238281\n",
      "[step: 3241] loss: 79.87435150146484\n",
      "[step: 3242] loss: 72.80635070800781\n",
      "[step: 3243] loss: 77.66536712646484\n",
      "[step: 3244] loss: 71.46786499023438\n",
      "[step: 3245] loss: 71.96292114257812\n",
      "[step: 3246] loss: 72.74226379394531\n",
      "[step: 3247] loss: 68.95162963867188\n",
      "[step: 3248] loss: 67.73050689697266\n",
      "[step: 3249] loss: 65.5650634765625\n",
      "[step: 3250] loss: 67.68450164794922\n",
      "[step: 3251] loss: 69.74334716796875\n",
      "[step: 3252] loss: 64.12570190429688\n",
      "[step: 3253] loss: 70.35091400146484\n",
      "[step: 3254] loss: 65.83709716796875\n",
      "[step: 3255] loss: 67.02521514892578\n",
      "[step: 3256] loss: 63.65950012207031\n",
      "[step: 3257] loss: 69.40963745117188\n",
      "[step: 3258] loss: 65.09774017333984\n",
      "[step: 3259] loss: 68.22114562988281\n",
      "[step: 3260] loss: 64.5718765258789\n",
      "[step: 3261] loss: 67.99214172363281\n",
      "[step: 3262] loss: 65.93391418457031\n",
      "[step: 3263] loss: 64.6353759765625\n",
      "[step: 3264] loss: 66.36370086669922\n",
      "[step: 3265] loss: 62.57910919189453\n",
      "[step: 3266] loss: 61.24937057495117\n",
      "[step: 3267] loss: 64.81983184814453\n",
      "[step: 3268] loss: 67.23414611816406\n",
      "[step: 3269] loss: 65.08811950683594\n",
      "[step: 3270] loss: 65.86466979980469\n",
      "[step: 3271] loss: 65.41659545898438\n",
      "[step: 3272] loss: 67.28617858886719\n",
      "[step: 3273] loss: 65.39268493652344\n",
      "[step: 3274] loss: 63.37013244628906\n",
      "[step: 3275] loss: 65.02275085449219\n",
      "[step: 3276] loss: 68.84542846679688\n",
      "[step: 3277] loss: 64.51263427734375\n",
      "[step: 3278] loss: 62.48023986816406\n",
      "[step: 3279] loss: 66.07228088378906\n",
      "[step: 3280] loss: 65.30406188964844\n",
      "[step: 3281] loss: 64.80887603759766\n",
      "[step: 3282] loss: 74.674560546875\n",
      "[step: 3283] loss: 72.88424682617188\n",
      "[step: 3284] loss: 66.06040954589844\n",
      "[step: 3285] loss: 61.474021911621094\n",
      "[step: 3286] loss: 65.08636474609375\n",
      "[step: 3287] loss: 65.50970458984375\n",
      "[step: 3288] loss: 63.32444763183594\n",
      "[step: 3289] loss: 66.18833923339844\n",
      "[step: 3290] loss: 68.61705017089844\n",
      "[step: 3291] loss: 63.44391632080078\n",
      "[step: 3292] loss: 63.46342468261719\n",
      "[step: 3293] loss: 63.2883415222168\n",
      "[step: 3294] loss: 61.511619567871094\n",
      "[step: 3295] loss: 62.454063415527344\n",
      "[step: 3296] loss: 64.57070922851562\n",
      "[step: 3297] loss: 65.40587615966797\n",
      "[step: 3298] loss: 71.22174072265625\n",
      "[step: 3299] loss: 68.60450744628906\n",
      "[step: 3300] loss: 62.672119140625\n",
      "[step: 3301] loss: 66.20731353759766\n",
      "[step: 3302] loss: 64.64376831054688\n",
      "[step: 3303] loss: 65.48635864257812\n",
      "[step: 3304] loss: 64.76365661621094\n",
      "[step: 3305] loss: 63.302764892578125\n",
      "[step: 3306] loss: 62.094459533691406\n",
      "[step: 3307] loss: 62.85527038574219\n",
      "[step: 3308] loss: 59.616172790527344\n",
      "[step: 3309] loss: 61.99542999267578\n",
      "[step: 3310] loss: 63.451744079589844\n",
      "[step: 3311] loss: 60.615108489990234\n",
      "[step: 3312] loss: 63.41679763793945\n",
      "[step: 3313] loss: 63.642616271972656\n",
      "[step: 3314] loss: 60.38179016113281\n",
      "[step: 3315] loss: 64.16361999511719\n",
      "[step: 3316] loss: 64.16334533691406\n",
      "[step: 3317] loss: 65.06808471679688\n",
      "[step: 3318] loss: 62.01254653930664\n",
      "[step: 3319] loss: 64.6903305053711\n",
      "[step: 3320] loss: 62.424354553222656\n",
      "[step: 3321] loss: 66.30979919433594\n",
      "[step: 3322] loss: 65.30056762695312\n",
      "[step: 3323] loss: 66.10429382324219\n",
      "[step: 3324] loss: 63.907867431640625\n",
      "[step: 3325] loss: 62.50277328491211\n",
      "[step: 3326] loss: 63.59774398803711\n",
      "[step: 3327] loss: 64.69718170166016\n",
      "[step: 3328] loss: 65.16732788085938\n",
      "[step: 3329] loss: 61.27015686035156\n",
      "[step: 3330] loss: 69.37850189208984\n",
      "[step: 3331] loss: 71.94351196289062\n",
      "[step: 3332] loss: 74.34126281738281\n",
      "[step: 3333] loss: 67.83828735351562\n",
      "[step: 3334] loss: 63.01091766357422\n",
      "[step: 3335] loss: 69.14749908447266\n",
      "[step: 3336] loss: 73.16959381103516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3337] loss: 69.97055053710938\n",
      "[step: 3338] loss: 65.10556030273438\n",
      "[step: 3339] loss: 73.065185546875\n",
      "[step: 3340] loss: 72.54435729980469\n",
      "[step: 3341] loss: 60.62964630126953\n",
      "[step: 3342] loss: 74.40435791015625\n",
      "[step: 3343] loss: 70.55754089355469\n",
      "[step: 3344] loss: 63.755393981933594\n",
      "[step: 3345] loss: 72.13233947753906\n",
      "[step: 3346] loss: 69.48735046386719\n",
      "[step: 3347] loss: 62.218162536621094\n",
      "[step: 3348] loss: 62.13855743408203\n",
      "[step: 3349] loss: 63.576148986816406\n",
      "[step: 3350] loss: 63.14203643798828\n",
      "[step: 3351] loss: 68.01477813720703\n",
      "[step: 3352] loss: 63.65373611450195\n",
      "[step: 3353] loss: 62.068359375\n",
      "[step: 3354] loss: 62.62921905517578\n",
      "[step: 3355] loss: 68.90426635742188\n",
      "[step: 3356] loss: 67.92849731445312\n",
      "[step: 3357] loss: 62.88325119018555\n",
      "[step: 3358] loss: 67.80168151855469\n",
      "[step: 3359] loss: 70.52336120605469\n",
      "[step: 3360] loss: 62.06474304199219\n",
      "[step: 3361] loss: 67.71249389648438\n",
      "[step: 3362] loss: 67.71908569335938\n",
      "[step: 3363] loss: 63.79596710205078\n",
      "[step: 3364] loss: 67.17472839355469\n",
      "[step: 3365] loss: 69.64732360839844\n",
      "[step: 3366] loss: 68.35443115234375\n",
      "[step: 3367] loss: 63.0372314453125\n",
      "[step: 3368] loss: 63.50701141357422\n",
      "[step: 3369] loss: 62.55241394042969\n",
      "[step: 3370] loss: 63.09452438354492\n",
      "[step: 3371] loss: 63.045555114746094\n",
      "[step: 3372] loss: 60.6907958984375\n",
      "[step: 3373] loss: 63.01698684692383\n",
      "[step: 3374] loss: 60.78546905517578\n",
      "[step: 3375] loss: 65.28278350830078\n",
      "[step: 3376] loss: 63.17918395996094\n",
      "[step: 3377] loss: 59.66254425048828\n",
      "[step: 3378] loss: 62.30829620361328\n",
      "[step: 3379] loss: 60.86955642700195\n",
      "[step: 3380] loss: 68.64366149902344\n",
      "[step: 3381] loss: 71.30267333984375\n",
      "[step: 3382] loss: 73.1944580078125\n",
      "[step: 3383] loss: 65.54805755615234\n",
      "[step: 3384] loss: 64.62554168701172\n",
      "[step: 3385] loss: 68.56401062011719\n",
      "[step: 3386] loss: 71.21685028076172\n",
      "[step: 3387] loss: 63.81951141357422\n",
      "[step: 3388] loss: 63.965667724609375\n",
      "[step: 3389] loss: 62.431182861328125\n",
      "[step: 3390] loss: 63.1546630859375\n",
      "[step: 3391] loss: 63.76721954345703\n",
      "[step: 3392] loss: 62.13209533691406\n",
      "[step: 3393] loss: 65.19892883300781\n",
      "[step: 3394] loss: 62.81581497192383\n",
      "[step: 3395] loss: 62.36332702636719\n",
      "[step: 3396] loss: 60.15550994873047\n",
      "[step: 3397] loss: 60.42554473876953\n",
      "[step: 3398] loss: 59.04878234863281\n",
      "[step: 3399] loss: 62.33810806274414\n",
      "[step: 3400] loss: 73.39726257324219\n",
      "[step: 3401] loss: 70.1421127319336\n",
      "[step: 3402] loss: 71.11445617675781\n",
      "[step: 3403] loss: 66.7873306274414\n",
      "[step: 3404] loss: 61.51792526245117\n",
      "[step: 3405] loss: 67.36698150634766\n",
      "[step: 3406] loss: 61.53819274902344\n",
      "[step: 3407] loss: 63.426963806152344\n",
      "[step: 3408] loss: 70.38450622558594\n",
      "[step: 3409] loss: 65.68186950683594\n",
      "[step: 3410] loss: 66.46639251708984\n",
      "[step: 3411] loss: 68.92095184326172\n",
      "[step: 3412] loss: 65.95674133300781\n",
      "[step: 3413] loss: 65.171142578125\n",
      "[step: 3414] loss: 61.62043762207031\n",
      "[step: 3415] loss: 65.52867126464844\n",
      "[step: 3416] loss: 66.35822296142578\n",
      "[step: 3417] loss: 59.68645477294922\n",
      "[step: 3418] loss: 65.7308578491211\n",
      "[step: 3419] loss: 67.15520477294922\n",
      "[step: 3420] loss: 65.9021224975586\n",
      "[step: 3421] loss: 63.338096618652344\n",
      "[step: 3422] loss: 60.3532829284668\n",
      "[step: 3423] loss: 64.25980377197266\n",
      "[step: 3424] loss: 66.5059814453125\n",
      "[step: 3425] loss: 65.9261474609375\n",
      "[step: 3426] loss: 64.67889404296875\n",
      "[step: 3427] loss: 69.16960144042969\n",
      "[step: 3428] loss: 74.86320495605469\n",
      "[step: 3429] loss: 70.62068176269531\n",
      "[step: 3430] loss: 65.19415283203125\n",
      "[step: 3431] loss: 68.19078063964844\n",
      "[step: 3432] loss: 64.49566650390625\n",
      "[step: 3433] loss: 72.37339782714844\n",
      "[step: 3434] loss: 68.537109375\n",
      "[step: 3435] loss: 67.1171646118164\n",
      "[step: 3436] loss: 73.9946060180664\n",
      "[step: 3437] loss: 67.80784606933594\n",
      "[step: 3438] loss: 63.62541198730469\n",
      "[step: 3439] loss: 69.91969299316406\n",
      "[step: 3440] loss: 62.672542572021484\n",
      "[step: 3441] loss: 64.96807861328125\n",
      "[step: 3442] loss: 62.48880386352539\n",
      "[step: 3443] loss: 64.23869323730469\n",
      "[step: 3444] loss: 67.18436431884766\n",
      "[step: 3445] loss: 64.22205352783203\n",
      "[step: 3446] loss: 65.78829193115234\n",
      "[step: 3447] loss: 66.59327697753906\n",
      "[step: 3448] loss: 64.21144104003906\n",
      "[step: 3449] loss: 64.04815673828125\n",
      "[step: 3450] loss: 64.81935119628906\n",
      "[step: 3451] loss: 60.237571716308594\n",
      "[step: 3452] loss: 60.949729919433594\n",
      "[step: 3453] loss: 62.83794021606445\n",
      "[step: 3454] loss: 61.878807067871094\n",
      "[step: 3455] loss: 60.984127044677734\n",
      "[step: 3456] loss: 59.7991943359375\n",
      "[step: 3457] loss: 61.63490676879883\n",
      "[step: 3458] loss: 61.04237365722656\n",
      "[step: 3459] loss: 62.91523742675781\n",
      "[step: 3460] loss: 65.5135498046875\n",
      "[step: 3461] loss: 60.2572135925293\n",
      "[step: 3462] loss: 63.45794677734375\n",
      "[step: 3463] loss: 65.00993347167969\n",
      "[step: 3464] loss: 61.39276123046875\n",
      "[step: 3465] loss: 65.63126373291016\n",
      "[step: 3466] loss: 60.08706283569336\n",
      "[step: 3467] loss: 62.009361267089844\n",
      "[step: 3468] loss: 63.85396194458008\n",
      "[step: 3469] loss: 66.95919036865234\n",
      "[step: 3470] loss: 66.69268798828125\n",
      "[step: 3471] loss: 62.19496154785156\n",
      "[step: 3472] loss: 62.1806526184082\n",
      "[step: 3473] loss: 66.82301330566406\n",
      "[step: 3474] loss: 61.886634826660156\n",
      "[step: 3475] loss: 60.307640075683594\n",
      "[step: 3476] loss: 60.17231750488281\n",
      "[step: 3477] loss: 60.33644104003906\n",
      "[step: 3478] loss: 60.9430046081543\n",
      "[step: 3479] loss: 59.89228439331055\n",
      "[step: 3480] loss: 60.360313415527344\n",
      "[step: 3481] loss: 63.375770568847656\n",
      "[step: 3482] loss: 62.494110107421875\n",
      "[step: 3483] loss: 60.847267150878906\n",
      "[step: 3484] loss: 65.00263214111328\n",
      "[step: 3485] loss: 65.11467742919922\n",
      "[step: 3486] loss: 61.557273864746094\n",
      "[step: 3487] loss: 65.0654296875\n",
      "[step: 3488] loss: 66.70088958740234\n",
      "[step: 3489] loss: 73.39786529541016\n",
      "[step: 3490] loss: 68.65225219726562\n",
      "[step: 3491] loss: 59.09531021118164\n",
      "[step: 3492] loss: 65.21115112304688\n",
      "[step: 3493] loss: 72.32977294921875\n",
      "[step: 3494] loss: 61.51827621459961\n",
      "[step: 3495] loss: 61.025794982910156\n",
      "[step: 3496] loss: 68.2029037475586\n",
      "[step: 3497] loss: 65.41338348388672\n",
      "[step: 3498] loss: 63.255672454833984\n",
      "[step: 3499] loss: 61.201087951660156\n",
      "[step: 3500] loss: 62.86753463745117\n",
      "[step: 3501] loss: 63.989498138427734\n",
      "[step: 3502] loss: 60.38886642456055\n",
      "[step: 3503] loss: 59.488746643066406\n",
      "[step: 3504] loss: 61.99534225463867\n",
      "[step: 3505] loss: 63.842811584472656\n",
      "[step: 3506] loss: 63.9920539855957\n",
      "[step: 3507] loss: 62.512977600097656\n",
      "[step: 3508] loss: 60.30522918701172\n",
      "[step: 3509] loss: 63.58457565307617\n",
      "[step: 3510] loss: 61.63309860229492\n",
      "[step: 3511] loss: 61.83339309692383\n",
      "[step: 3512] loss: 68.30900573730469\n",
      "[step: 3513] loss: 68.04943084716797\n",
      "[step: 3514] loss: 61.41650390625\n",
      "[step: 3515] loss: 67.67488098144531\n",
      "[step: 3516] loss: 62.28361892700195\n",
      "[step: 3517] loss: 61.198829650878906\n",
      "[step: 3518] loss: 61.130027770996094\n",
      "[step: 3519] loss: 60.01266098022461\n",
      "[step: 3520] loss: 59.478790283203125\n",
      "[step: 3521] loss: 66.5036849975586\n",
      "[step: 3522] loss: 58.03486633300781\n",
      "[step: 3523] loss: 61.10084533691406\n",
      "[step: 3524] loss: 63.09602737426758\n",
      "[step: 3525] loss: 62.140724182128906\n",
      "[step: 3526] loss: 58.00210952758789\n",
      "[step: 3527] loss: 63.20848083496094\n",
      "[step: 3528] loss: 59.840232849121094\n",
      "[step: 3529] loss: 63.013427734375\n",
      "[step: 3530] loss: 63.08163070678711\n",
      "[step: 3531] loss: 66.38975524902344\n",
      "[step: 3532] loss: 71.5438232421875\n",
      "[step: 3533] loss: 66.5259017944336\n",
      "[step: 3534] loss: 60.32256317138672\n",
      "[step: 3535] loss: 62.2754020690918\n",
      "[step: 3536] loss: 62.97632598876953\n",
      "[step: 3537] loss: 59.249229431152344\n",
      "[step: 3538] loss: 60.53201675415039\n",
      "[step: 3539] loss: 62.13911437988281\n",
      "[step: 3540] loss: 60.9127311706543\n",
      "[step: 3541] loss: 65.43215942382812\n",
      "[step: 3542] loss: 64.67655181884766\n",
      "[step: 3543] loss: 63.903358459472656\n",
      "[step: 3544] loss: 62.581520080566406\n",
      "[step: 3545] loss: 61.15540313720703\n",
      "[step: 3546] loss: 59.75914764404297\n",
      "[step: 3547] loss: 62.472869873046875\n",
      "[step: 3548] loss: 59.21832275390625\n",
      "[step: 3549] loss: 61.83412170410156\n",
      "[step: 3550] loss: 65.29457092285156\n",
      "[step: 3551] loss: 68.49456024169922\n",
      "[step: 3552] loss: 59.72441864013672\n",
      "[step: 3553] loss: 62.56869888305664\n",
      "[step: 3554] loss: 60.7210693359375\n",
      "[step: 3555] loss: 59.716705322265625\n",
      "[step: 3556] loss: 62.56089782714844\n",
      "[step: 3557] loss: 61.436546325683594\n",
      "[step: 3558] loss: 60.627098083496094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3559] loss: 62.81969451904297\n",
      "[step: 3560] loss: 64.78141021728516\n",
      "[step: 3561] loss: 63.433353424072266\n",
      "[step: 3562] loss: 62.18376922607422\n",
      "[step: 3563] loss: 60.692283630371094\n",
      "[step: 3564] loss: 61.874717712402344\n",
      "[step: 3565] loss: 63.305477142333984\n",
      "[step: 3566] loss: 62.46942138671875\n",
      "[step: 3567] loss: 58.86699676513672\n",
      "[step: 3568] loss: 60.43608093261719\n",
      "[step: 3569] loss: 60.80526351928711\n",
      "[step: 3570] loss: 58.93825912475586\n",
      "[step: 3571] loss: 62.34556579589844\n",
      "[step: 3572] loss: 62.88862609863281\n",
      "[step: 3573] loss: 64.0010986328125\n",
      "[step: 3574] loss: 72.27804565429688\n",
      "[step: 3575] loss: 66.13421630859375\n",
      "[step: 3576] loss: 59.598873138427734\n",
      "[step: 3577] loss: 62.92044448852539\n",
      "[step: 3578] loss: 77.07185363769531\n",
      "[step: 3579] loss: 90.93097686767578\n",
      "[step: 3580] loss: 66.45863342285156\n",
      "[step: 3581] loss: 77.08177185058594\n",
      "[step: 3582] loss: 83.68306732177734\n",
      "[step: 3583] loss: 63.8825569152832\n",
      "[step: 3584] loss: 78.66728210449219\n",
      "[step: 3585] loss: 77.94886016845703\n",
      "[step: 3586] loss: 71.16065979003906\n",
      "[step: 3587] loss: 73.12897491455078\n",
      "[step: 3588] loss: 67.27690124511719\n",
      "[step: 3589] loss: 71.07804870605469\n",
      "[step: 3590] loss: 63.20556640625\n",
      "[step: 3591] loss: 73.43537902832031\n",
      "[step: 3592] loss: 64.1239013671875\n",
      "[step: 3593] loss: 65.10037231445312\n",
      "[step: 3594] loss: 70.54676055908203\n",
      "[step: 3595] loss: 67.822998046875\n",
      "[step: 3596] loss: 68.72035217285156\n",
      "[step: 3597] loss: 68.9979019165039\n",
      "[step: 3598] loss: 59.36827087402344\n",
      "[step: 3599] loss: 72.69483947753906\n",
      "[step: 3600] loss: 67.01892852783203\n",
      "[step: 3601] loss: 60.537837982177734\n",
      "[step: 3602] loss: 67.74905395507812\n",
      "[step: 3603] loss: 62.86986541748047\n",
      "[step: 3604] loss: 66.19204711914062\n",
      "[step: 3605] loss: 76.12670135498047\n",
      "[step: 3606] loss: 60.21428680419922\n",
      "[step: 3607] loss: 72.23233795166016\n",
      "[step: 3608] loss: 70.66806030273438\n",
      "[step: 3609] loss: 64.22488403320312\n",
      "[step: 3610] loss: 70.16529083251953\n",
      "[step: 3611] loss: 65.91082000732422\n",
      "[step: 3612] loss: 63.012229919433594\n",
      "[step: 3613] loss: 62.58390426635742\n",
      "[step: 3614] loss: 62.80452346801758\n",
      "[step: 3615] loss: 63.13801574707031\n",
      "[step: 3616] loss: 57.74519348144531\n",
      "[step: 3617] loss: 61.698638916015625\n",
      "[step: 3618] loss: 60.37115478515625\n",
      "[step: 3619] loss: 63.178245544433594\n",
      "[step: 3620] loss: 61.6958122253418\n",
      "[step: 3621] loss: 57.946990966796875\n",
      "[step: 3622] loss: 61.014705657958984\n",
      "[step: 3623] loss: 63.78862762451172\n",
      "[step: 3624] loss: 61.79002380371094\n",
      "[step: 3625] loss: 64.21768951416016\n",
      "[step: 3626] loss: 62.260719299316406\n",
      "[step: 3627] loss: 60.24479675292969\n",
      "[step: 3628] loss: 61.92711639404297\n",
      "[step: 3629] loss: 59.255126953125\n",
      "[step: 3630] loss: 59.365074157714844\n",
      "[step: 3631] loss: 62.55364990234375\n",
      "[step: 3632] loss: 59.06702423095703\n",
      "[step: 3633] loss: 58.68318176269531\n",
      "[step: 3634] loss: 60.75460433959961\n",
      "[step: 3635] loss: 66.88175964355469\n",
      "[step: 3636] loss: 64.61056518554688\n",
      "[step: 3637] loss: 58.791343688964844\n",
      "[step: 3638] loss: 59.438812255859375\n",
      "[step: 3639] loss: 65.46489715576172\n",
      "[step: 3640] loss: 61.84874725341797\n",
      "[step: 3641] loss: 62.528564453125\n",
      "[step: 3642] loss: 62.85028839111328\n",
      "[step: 3643] loss: 61.18767547607422\n",
      "[step: 3644] loss: 61.929771423339844\n",
      "[step: 3645] loss: 60.05774688720703\n",
      "[step: 3646] loss: 61.255775451660156\n",
      "[step: 3647] loss: 60.23644256591797\n",
      "[step: 3648] loss: 61.84837341308594\n",
      "[step: 3649] loss: 59.097999572753906\n",
      "[step: 3650] loss: 61.38416290283203\n",
      "[step: 3651] loss: 59.36271667480469\n",
      "[step: 3652] loss: 57.674896240234375\n",
      "[step: 3653] loss: 59.45264434814453\n",
      "[step: 3654] loss: 59.64946746826172\n",
      "[step: 3655] loss: 59.050453186035156\n",
      "[step: 3656] loss: 62.62201690673828\n",
      "[step: 3657] loss: 57.88100814819336\n",
      "[step: 3658] loss: 60.736473083496094\n",
      "[step: 3659] loss: 59.99125289916992\n",
      "[step: 3660] loss: 58.95576858520508\n",
      "[step: 3661] loss: 58.947914123535156\n",
      "[step: 3662] loss: 60.59393310546875\n",
      "[step: 3663] loss: 60.64714813232422\n",
      "[step: 3664] loss: 58.74391174316406\n",
      "[step: 3665] loss: 57.49775695800781\n",
      "[step: 3666] loss: 57.058677673339844\n",
      "[step: 3667] loss: 59.8991813659668\n",
      "[step: 3668] loss: 59.72895812988281\n",
      "[step: 3669] loss: 58.62144470214844\n",
      "[step: 3670] loss: 61.39940643310547\n",
      "[step: 3671] loss: 59.330535888671875\n",
      "[step: 3672] loss: 59.25629425048828\n",
      "[step: 3673] loss: 57.17089080810547\n",
      "[step: 3674] loss: 59.05575942993164\n",
      "[step: 3675] loss: 60.894935607910156\n",
      "[step: 3676] loss: 57.97167205810547\n",
      "[step: 3677] loss: 58.83976364135742\n",
      "[step: 3678] loss: 59.90232467651367\n",
      "[step: 3679] loss: 63.94597625732422\n",
      "[step: 3680] loss: 60.386863708496094\n",
      "[step: 3681] loss: 62.213340759277344\n",
      "[step: 3682] loss: 63.077674865722656\n",
      "[step: 3683] loss: 60.94350051879883\n",
      "[step: 3684] loss: 59.30398178100586\n",
      "[step: 3685] loss: 61.67707824707031\n",
      "[step: 3686] loss: 59.73835754394531\n",
      "[step: 3687] loss: 58.184051513671875\n",
      "[step: 3688] loss: 63.279762268066406\n",
      "[step: 3689] loss: 65.4581298828125\n",
      "[step: 3690] loss: 62.14311981201172\n",
      "[step: 3691] loss: 61.28516387939453\n",
      "[step: 3692] loss: 62.36316680908203\n",
      "[step: 3693] loss: 60.986045837402344\n",
      "[step: 3694] loss: 64.64715576171875\n",
      "[step: 3695] loss: 59.75778579711914\n",
      "[step: 3696] loss: 62.85462188720703\n",
      "[step: 3697] loss: 64.1061019897461\n",
      "[step: 3698] loss: 66.62484741210938\n",
      "[step: 3699] loss: 59.618465423583984\n",
      "[step: 3700] loss: 59.37702178955078\n",
      "[step: 3701] loss: 60.34120178222656\n",
      "[step: 3702] loss: 61.30223846435547\n",
      "[step: 3703] loss: 60.86808776855469\n",
      "[step: 3704] loss: 59.14373779296875\n",
      "[step: 3705] loss: 63.609375\n",
      "[step: 3706] loss: 68.97940826416016\n",
      "[step: 3707] loss: 70.101318359375\n",
      "[step: 3708] loss: 64.476806640625\n",
      "[step: 3709] loss: 58.09502410888672\n",
      "[step: 3710] loss: 60.74491500854492\n",
      "[step: 3711] loss: 60.1409912109375\n",
      "[step: 3712] loss: 61.15803146362305\n",
      "[step: 3713] loss: 61.380611419677734\n",
      "[step: 3714] loss: 62.94443893432617\n",
      "[step: 3715] loss: 58.641761779785156\n",
      "[step: 3716] loss: 62.000389099121094\n",
      "[step: 3717] loss: 60.20945739746094\n",
      "[step: 3718] loss: 57.317527770996094\n",
      "[step: 3719] loss: 58.662986755371094\n",
      "[step: 3720] loss: 61.23807907104492\n",
      "[step: 3721] loss: 58.78892517089844\n",
      "[step: 3722] loss: 60.38715362548828\n",
      "[step: 3723] loss: 67.61308288574219\n",
      "[step: 3724] loss: 62.586341857910156\n",
      "[step: 3725] loss: 57.51240158081055\n",
      "[step: 3726] loss: 61.78959274291992\n",
      "[step: 3727] loss: 63.97809600830078\n",
      "[step: 3728] loss: 59.555030822753906\n",
      "[step: 3729] loss: 59.20343780517578\n",
      "[step: 3730] loss: 63.0971794128418\n",
      "[step: 3731] loss: 64.13404846191406\n",
      "[step: 3732] loss: 77.35760498046875\n",
      "[step: 3733] loss: 66.45681762695312\n",
      "[step: 3734] loss: 61.60704803466797\n",
      "[step: 3735] loss: 67.43302917480469\n",
      "[step: 3736] loss: 68.8065185546875\n",
      "[step: 3737] loss: 57.708106994628906\n",
      "[step: 3738] loss: 65.54566955566406\n",
      "[step: 3739] loss: 69.04801940917969\n",
      "[step: 3740] loss: 57.31944274902344\n",
      "[step: 3741] loss: 65.00653076171875\n",
      "[step: 3742] loss: 67.25083923339844\n",
      "[step: 3743] loss: 59.58898162841797\n",
      "[step: 3744] loss: 67.34687805175781\n",
      "[step: 3745] loss: 68.00373077392578\n",
      "[step: 3746] loss: 63.22768783569336\n",
      "[step: 3747] loss: 62.0373420715332\n",
      "[step: 3748] loss: 68.15061950683594\n",
      "[step: 3749] loss: 65.36636352539062\n",
      "[step: 3750] loss: 67.69853210449219\n",
      "[step: 3751] loss: 70.90919494628906\n",
      "[step: 3752] loss: 60.38195037841797\n",
      "[step: 3753] loss: 66.29501342773438\n",
      "[step: 3754] loss: 65.09248352050781\n",
      "[step: 3755] loss: 61.49650573730469\n",
      "[step: 3756] loss: 65.54034423828125\n",
      "[step: 3757] loss: 64.4009780883789\n",
      "[step: 3758] loss: 59.00390625\n",
      "[step: 3759] loss: 63.83958053588867\n",
      "[step: 3760] loss: 64.32984924316406\n",
      "[step: 3761] loss: 61.23634719848633\n",
      "[step: 3762] loss: 60.87477111816406\n",
      "[step: 3763] loss: 64.80449676513672\n",
      "[step: 3764] loss: 61.11648941040039\n",
      "[step: 3765] loss: 63.10199737548828\n",
      "[step: 3766] loss: 61.05769348144531\n",
      "[step: 3767] loss: 64.24795532226562\n",
      "[step: 3768] loss: 59.61302947998047\n",
      "[step: 3769] loss: 62.51373291015625\n",
      "[step: 3770] loss: 61.53434753417969\n",
      "[step: 3771] loss: 64.12896728515625\n",
      "[step: 3772] loss: 58.34583282470703\n",
      "[step: 3773] loss: 60.49153518676758\n",
      "[step: 3774] loss: 61.051780700683594\n",
      "[step: 3775] loss: 60.24999237060547\n",
      "[step: 3776] loss: 58.53761291503906\n",
      "[step: 3777] loss: 59.00389099121094\n",
      "[step: 3778] loss: 58.44584655761719\n",
      "[step: 3779] loss: 62.84233093261719\n",
      "[step: 3780] loss: 63.642784118652344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3781] loss: 63.96527099609375\n",
      "[step: 3782] loss: 63.8216552734375\n",
      "[step: 3783] loss: 58.656463623046875\n",
      "[step: 3784] loss: 56.242210388183594\n",
      "[step: 3785] loss: 58.225650787353516\n",
      "[step: 3786] loss: 59.31947326660156\n",
      "[step: 3787] loss: 61.51908493041992\n",
      "[step: 3788] loss: 60.226470947265625\n",
      "[step: 3789] loss: 59.762298583984375\n",
      "[step: 3790] loss: 61.0732421875\n",
      "[step: 3791] loss: 64.62940979003906\n",
      "[step: 3792] loss: 74.37984466552734\n",
      "[step: 3793] loss: 67.07571411132812\n",
      "[step: 3794] loss: 58.53966522216797\n",
      "[step: 3795] loss: 59.34843444824219\n",
      "[step: 3796] loss: 62.43701171875\n",
      "[step: 3797] loss: 60.51819610595703\n",
      "[step: 3798] loss: 60.66337585449219\n",
      "[step: 3799] loss: 58.59236145019531\n",
      "[step: 3800] loss: 58.59233093261719\n",
      "[step: 3801] loss: 63.36058807373047\n",
      "[step: 3802] loss: 58.817726135253906\n",
      "[step: 3803] loss: 57.93736267089844\n",
      "[step: 3804] loss: 61.516029357910156\n",
      "[step: 3805] loss: 61.58709716796875\n",
      "[step: 3806] loss: 58.513580322265625\n",
      "[step: 3807] loss: 58.850494384765625\n",
      "[step: 3808] loss: 60.21802520751953\n",
      "[step: 3809] loss: 59.76155090332031\n",
      "[step: 3810] loss: 56.34431076049805\n",
      "[step: 3811] loss: 57.450069427490234\n",
      "[step: 3812] loss: 62.092491149902344\n",
      "[step: 3813] loss: 56.492347717285156\n",
      "[step: 3814] loss: 57.48775863647461\n",
      "[step: 3815] loss: 58.85816955566406\n",
      "[step: 3816] loss: 57.80489730834961\n",
      "[step: 3817] loss: 58.08074188232422\n",
      "[step: 3818] loss: 61.248435974121094\n",
      "[step: 3819] loss: 63.25035095214844\n",
      "[step: 3820] loss: 62.57008743286133\n",
      "[step: 3821] loss: 65.6889877319336\n",
      "[step: 3822] loss: 62.49494934082031\n",
      "[step: 3823] loss: 58.300926208496094\n",
      "[step: 3824] loss: 60.69573211669922\n",
      "[step: 3825] loss: 61.796241760253906\n",
      "[step: 3826] loss: 64.95645141601562\n",
      "[step: 3827] loss: 58.50181579589844\n",
      "[step: 3828] loss: 60.34239959716797\n",
      "[step: 3829] loss: 63.33165740966797\n",
      "[step: 3830] loss: 68.94281768798828\n",
      "[step: 3831] loss: 69.23153686523438\n",
      "[step: 3832] loss: 61.60160827636719\n",
      "[step: 3833] loss: 58.131099700927734\n",
      "[step: 3834] loss: 61.39512634277344\n",
      "[step: 3835] loss: 59.921356201171875\n",
      "[step: 3836] loss: 59.68665313720703\n",
      "[step: 3837] loss: 65.08992004394531\n",
      "[step: 3838] loss: 66.41932678222656\n",
      "[step: 3839] loss: 56.67443084716797\n",
      "[step: 3840] loss: 59.49602508544922\n",
      "[step: 3841] loss: 72.16751098632812\n",
      "[step: 3842] loss: 69.99581909179688\n",
      "[step: 3843] loss: 61.16607666015625\n",
      "[step: 3844] loss: 71.6342544555664\n",
      "[step: 3845] loss: 72.13399505615234\n",
      "[step: 3846] loss: 62.67955017089844\n",
      "[step: 3847] loss: 63.64664840698242\n",
      "[step: 3848] loss: 71.33897399902344\n",
      "[step: 3849] loss: 71.56472778320312\n",
      "[step: 3850] loss: 62.37030792236328\n",
      "[step: 3851] loss: 69.83734130859375\n",
      "[step: 3852] loss: 71.27169799804688\n",
      "[step: 3853] loss: 60.42708206176758\n",
      "[step: 3854] loss: 67.10896301269531\n",
      "[step: 3855] loss: 69.80604553222656\n",
      "[step: 3856] loss: 63.470951080322266\n",
      "[step: 3857] loss: 62.763694763183594\n",
      "[step: 3858] loss: 64.45242309570312\n",
      "[step: 3859] loss: 64.48173522949219\n",
      "[step: 3860] loss: 60.339324951171875\n",
      "[step: 3861] loss: 60.2142333984375\n",
      "[step: 3862] loss: 64.23654174804688\n",
      "[step: 3863] loss: 62.51679611206055\n",
      "[step: 3864] loss: 59.68191909790039\n",
      "[step: 3865] loss: 63.53097915649414\n",
      "[step: 3866] loss: 63.8009033203125\n",
      "[step: 3867] loss: 66.60859680175781\n",
      "[step: 3868] loss: 62.990318298339844\n",
      "[step: 3869] loss: 59.89680480957031\n",
      "[step: 3870] loss: 60.58345031738281\n",
      "[step: 3871] loss: 60.05058288574219\n",
      "[step: 3872] loss: 59.684776306152344\n",
      "[step: 3873] loss: 58.8491096496582\n",
      "[step: 3874] loss: 62.364463806152344\n",
      "[step: 3875] loss: 60.13753128051758\n",
      "[step: 3876] loss: 59.12208938598633\n",
      "[step: 3877] loss: 64.9384994506836\n",
      "[step: 3878] loss: 62.10708236694336\n",
      "[step: 3879] loss: 58.40446090698242\n",
      "[step: 3880] loss: 59.10356140136719\n",
      "[step: 3881] loss: 63.1485595703125\n",
      "[step: 3882] loss: 58.3032341003418\n",
      "[step: 3883] loss: 59.358978271484375\n",
      "[step: 3884] loss: 64.56060791015625\n",
      "[step: 3885] loss: 63.21842956542969\n",
      "[step: 3886] loss: 58.90226745605469\n",
      "[step: 3887] loss: 59.507896423339844\n",
      "[step: 3888] loss: 59.69216537475586\n",
      "[step: 3889] loss: 61.51493835449219\n",
      "[step: 3890] loss: 59.90682601928711\n",
      "[step: 3891] loss: 60.764793395996094\n",
      "[step: 3892] loss: 61.15058898925781\n",
      "[step: 3893] loss: 59.24076461791992\n",
      "[step: 3894] loss: 57.135257720947266\n",
      "[step: 3895] loss: 58.4432487487793\n",
      "[step: 3896] loss: 58.97221374511719\n",
      "[step: 3897] loss: 58.83325958251953\n",
      "[step: 3898] loss: 57.323089599609375\n",
      "[step: 3899] loss: 54.95039367675781\n",
      "[step: 3900] loss: 59.78520202636719\n",
      "[step: 3901] loss: 59.241798400878906\n",
      "[step: 3902] loss: 60.0726432800293\n",
      "[step: 3903] loss: 62.843109130859375\n",
      "[step: 3904] loss: 63.005126953125\n",
      "[step: 3905] loss: 55.56414031982422\n",
      "[step: 3906] loss: 59.50674819946289\n",
      "[step: 3907] loss: 60.61346435546875\n",
      "[step: 3908] loss: 58.149009704589844\n",
      "[step: 3909] loss: 58.579917907714844\n",
      "[step: 3910] loss: 59.629478454589844\n",
      "[step: 3911] loss: 56.88664627075195\n",
      "[step: 3912] loss: 58.09382629394531\n",
      "[step: 3913] loss: 60.104286193847656\n",
      "[step: 3914] loss: 56.397926330566406\n",
      "[step: 3915] loss: 58.87995910644531\n",
      "[step: 3916] loss: 63.62607955932617\n",
      "[step: 3917] loss: 59.280426025390625\n",
      "[step: 3918] loss: 57.93062973022461\n",
      "[step: 3919] loss: 61.42486572265625\n",
      "[step: 3920] loss: 56.294410705566406\n",
      "[step: 3921] loss: 63.95356750488281\n",
      "[step: 3922] loss: 67.49696350097656\n",
      "[step: 3923] loss: 65.33419799804688\n",
      "[step: 3924] loss: 62.26697540283203\n",
      "[step: 3925] loss: 65.17594146728516\n",
      "[step: 3926] loss: 58.437103271484375\n",
      "[step: 3927] loss: 58.91177749633789\n",
      "[step: 3928] loss: 65.73123168945312\n",
      "[step: 3929] loss: 62.266929626464844\n",
      "[step: 3930] loss: 64.59346008300781\n",
      "[step: 3931] loss: 69.74215698242188\n",
      "[step: 3932] loss: 58.966651916503906\n",
      "[step: 3933] loss: 55.633323669433594\n",
      "[step: 3934] loss: 62.739681243896484\n",
      "[step: 3935] loss: 60.835243225097656\n",
      "[step: 3936] loss: 59.892147064208984\n",
      "[step: 3937] loss: 60.21689224243164\n",
      "[step: 3938] loss: 56.21747589111328\n",
      "[step: 3939] loss: 61.4544792175293\n",
      "[step: 3940] loss: 57.331947326660156\n",
      "[step: 3941] loss: 56.64407730102539\n",
      "[step: 3942] loss: 59.945838928222656\n",
      "[step: 3943] loss: 58.828495025634766\n",
      "[step: 3944] loss: 58.96857833862305\n",
      "[step: 3945] loss: 60.75654602050781\n",
      "[step: 3946] loss: 56.15610122680664\n",
      "[step: 3947] loss: 54.811973571777344\n",
      "[step: 3948] loss: 63.31134796142578\n",
      "[step: 3949] loss: 56.912235260009766\n",
      "[step: 3950] loss: 58.47637176513672\n",
      "[step: 3951] loss: 59.899166107177734\n",
      "[step: 3952] loss: 61.28575134277344\n",
      "[step: 3953] loss: 56.64143753051758\n",
      "[step: 3954] loss: 60.30860137939453\n",
      "[step: 3955] loss: 58.0768928527832\n",
      "[step: 3956] loss: 57.878196716308594\n",
      "[step: 3957] loss: 55.94050598144531\n",
      "[step: 3958] loss: 58.36322784423828\n",
      "[step: 3959] loss: 59.630496978759766\n",
      "[step: 3960] loss: 56.99408721923828\n",
      "[step: 3961] loss: 57.881683349609375\n",
      "[step: 3962] loss: 55.845943450927734\n",
      "[step: 3963] loss: 58.20637512207031\n",
      "[step: 3964] loss: 54.821311950683594\n",
      "[step: 3965] loss: 56.22337341308594\n",
      "[step: 3966] loss: 55.30698776245117\n",
      "[step: 3967] loss: 56.094993591308594\n",
      "[step: 3968] loss: 55.05736541748047\n",
      "[step: 3969] loss: 56.647430419921875\n",
      "[step: 3970] loss: 56.59437561035156\n",
      "[step: 3971] loss: 58.28584289550781\n",
      "[step: 3972] loss: 54.466590881347656\n",
      "[step: 3973] loss: 58.25040054321289\n",
      "[step: 3974] loss: 55.605186462402344\n",
      "[step: 3975] loss: 57.229248046875\n",
      "[step: 3976] loss: 58.4672966003418\n",
      "[step: 3977] loss: 58.53045654296875\n",
      "[step: 3978] loss: 62.46891403198242\n",
      "[step: 3979] loss: 60.39352798461914\n",
      "[step: 3980] loss: 59.87592315673828\n",
      "[step: 3981] loss: 58.006263732910156\n",
      "[step: 3982] loss: 56.83050537109375\n",
      "[step: 3983] loss: 60.01935577392578\n",
      "[step: 3984] loss: 68.20189666748047\n",
      "[step: 3985] loss: 70.74411010742188\n",
      "[step: 3986] loss: 60.624855041503906\n",
      "[step: 3987] loss: 57.4512825012207\n",
      "[step: 3988] loss: 56.4613037109375\n",
      "[step: 3989] loss: 57.006324768066406\n",
      "[step: 3990] loss: 58.01521682739258\n",
      "[step: 3991] loss: 56.50416564941406\n",
      "[step: 3992] loss: 61.21601867675781\n",
      "[step: 3993] loss: 59.749725341796875\n",
      "[step: 3994] loss: 58.99017333984375\n",
      "[step: 3995] loss: 58.03632736206055\n",
      "[step: 3996] loss: 58.35231399536133\n",
      "[step: 3997] loss: 56.396873474121094\n",
      "[step: 3998] loss: 59.13063430786133\n",
      "[step: 3999] loss: 59.316078186035156\n",
      "RMSE: 0.1525181382894516\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "##xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "##xy_with_noise = xy_with_noise[:,:27]\n",
    "##a_with_noise = xy_with_noise[:,:2]\n",
    "##b_with_noise = xy_with_noise[:,2:]\n",
    "##b_with_noise = MinMaxScaler(b_with_noise)\n",
    "##xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 54\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 4000\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "##train_set_with_noise = xy_with_noise[:train_size]\n",
    "##validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "##test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "##trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "##validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "##testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "##X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "##Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    lstm_1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout1 = tf.contrib.rnn.DropoutWrapper(lstm_1,keep_prob)\n",
    "    lstm_2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim*2, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout2 = tf.contrib.rnn.DropoutWrapper(lstm_2,keep_prob)\n",
    "    lstm_3 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    dropout3 = tf.contrib.rnn.DropoutWrapper(lstm_3,keep_prob)\n",
    "    cell1 = tf.contrib.rnn.MultiRNNCell([dropout1,dropout2,dropout3])\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "##with tf.variable_scope(\"rnn2\"):\n",
    "    ##cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    ##outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    ##Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    ##loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    ##train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE \n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY, keep_prob : 0.7})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "       ## _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        ##print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    validation_predict = sess.run(Y_pred, feed_dict = {X1:validationX, keep_prob : 1})\n",
    "    ##validation_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:testX, keep_prob : 1})\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: validation_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
