{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-96de89488fab>:89: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 19893.150390625\n",
      "[step: 0] loss: 0.3186686038970947\n",
      "[step: 1] loss: 19067.734375\n",
      "[step: 1] loss: 0.30558547377586365\n",
      "[step: 2] loss: 18279.875\n",
      "[step: 2] loss: 0.2931113541126251\n",
      "[step: 3] loss: 17528.69140625\n",
      "[step: 3] loss: 0.2812255918979645\n",
      "[step: 4] loss: 16812.955078125\n",
      "[step: 4] loss: 0.26991498470306396\n",
      "[step: 5] loss: 16131.158203125\n",
      "[step: 5] loss: 0.25920605659484863\n",
      "[step: 6] loss: 15481.6103515625\n",
      "[step: 6] loss: 0.2492152601480484\n",
      "[step: 7] loss: 14862.478515625\n",
      "[step: 7] loss: 0.23991283774375916\n",
      "[step: 8] loss: 14271.810546875\n",
      "[step: 8] loss: 0.23092059791088104\n",
      "[step: 9] loss: 13707.6875\n",
      "[step: 9] loss: 0.22228389978408813\n",
      "[step: 10] loss: 13168.359375\n",
      "[step: 10] loss: 0.2140980362892151\n",
      "[step: 11] loss: 12652.1640625\n",
      "[step: 11] loss: 0.20638006925582886\n",
      "[step: 12] loss: 12157.4970703125\n",
      "[step: 12] loss: 0.199093297123909\n",
      "[step: 13] loss: 11682.7724609375\n",
      "[step: 13] loss: 0.1921897977590561\n",
      "[step: 14] loss: 11226.537109375\n",
      "[step: 14] loss: 0.18563157320022583\n",
      "[step: 15] loss: 10787.4814453125\n",
      "[step: 15] loss: 0.17939037084579468\n",
      "[step: 16] loss: 10364.544921875\n",
      "[step: 16] loss: 0.17344266176223755\n",
      "[step: 17] loss: 9957.0126953125\n",
      "[step: 17] loss: 0.1677672266960144\n",
      "[step: 18] loss: 9564.4453125\n",
      "[step: 18] loss: 0.16234540939331055\n",
      "[step: 19] loss: 9186.4501953125\n",
      "[step: 19] loss: 0.15715861320495605\n",
      "[step: 20] loss: 8822.59765625\n",
      "[step: 20] loss: 0.1521897166967392\n",
      "[step: 21] loss: 8472.33984375\n",
      "[step: 21] loss: 0.14742209017276764\n",
      "[step: 22] loss: 8135.0146484375\n",
      "[step: 22] loss: 0.14283999800682068\n",
      "[step: 23] loss: 7809.9345703125\n",
      "[step: 23] loss: 0.13842856884002686\n",
      "[step: 24] loss: 7496.5078125\n",
      "[step: 24] loss: 0.13417281210422516\n",
      "[step: 25] loss: 7194.33935546875\n",
      "[step: 25] loss: 0.13005928695201874\n",
      "[step: 26] loss: 6903.13916015625\n",
      "[step: 26] loss: 0.12607508897781372\n",
      "[step: 27] loss: 6622.7900390625\n",
      "[step: 27] loss: 0.12220841646194458\n",
      "[step: 28] loss: 6353.2021484375\n",
      "[step: 28] loss: 0.11844862252473831\n",
      "[step: 29] loss: 6094.2998046875\n",
      "[step: 29] loss: 0.11478637903928757\n",
      "[step: 30] loss: 5846.025390625\n",
      "[step: 30] loss: 0.11121367663145065\n",
      "[step: 31] loss: 5608.232421875\n",
      "[step: 31] loss: 0.10772334784269333\n",
      "[step: 32] loss: 5380.71240234375\n",
      "[step: 32] loss: 0.10430905222892761\n",
      "[step: 33] loss: 5163.1513671875\n",
      "[step: 33] loss: 0.10096524655818939\n",
      "[step: 34] loss: 4955.1083984375\n",
      "[step: 34] loss: 0.09768743067979813\n",
      "[step: 35] loss: 4756.02099609375\n",
      "[step: 35] loss: 0.0944715365767479\n",
      "[step: 36] loss: 4565.240234375\n",
      "[step: 36] loss: 0.09131432324647903\n",
      "[step: 37] loss: 4382.08837890625\n",
      "[step: 37] loss: 0.08821320533752441\n",
      "[step: 38] loss: 4205.9345703125\n",
      "[step: 38] loss: 0.08516709506511688\n",
      "[step: 39] loss: 4036.3095703125\n",
      "[step: 39] loss: 0.08217493444681168\n",
      "[step: 40] loss: 3872.91015625\n",
      "[step: 40] loss: 0.07923663407564163\n",
      "[step: 41] loss: 3715.603515625\n",
      "[step: 41] loss: 0.07635288685560226\n",
      "[step: 42] loss: 3564.339599609375\n",
      "[step: 42] loss: 0.07352487742900848\n",
      "[step: 43] loss: 3419.09326171875\n",
      "[step: 43] loss: 0.07075504213571548\n",
      "[step: 44] loss: 3279.790283203125\n",
      "[step: 44] loss: 0.06804662197828293\n",
      "[step: 45] loss: 3146.295166015625\n",
      "[step: 45] loss: 0.06540316343307495\n",
      "[step: 46] loss: 3018.434814453125\n",
      "[step: 46] loss: 0.06282861530780792\n",
      "[step: 47] loss: 2896.03564453125\n",
      "[step: 47] loss: 0.060326676815748215\n",
      "[step: 48] loss: 2778.999755859375\n",
      "[step: 48] loss: 0.05790037661790848\n",
      "[step: 49] loss: 2667.32177734375\n",
      "[step: 49] loss: 0.05555271357297897\n",
      "[step: 50] loss: 2561.103759765625\n",
      "[step: 50] loss: 0.05328615382313728\n",
      "[step: 51] loss: 2460.521484375\n",
      "[step: 51] loss: 0.05110291764140129\n",
      "[step: 52] loss: 2365.76513671875\n",
      "[step: 52] loss: 0.04900512099266052\n",
      "[step: 53] loss: 2276.9814453125\n",
      "[step: 53] loss: 0.04699459671974182\n",
      "[step: 54] loss: 2194.1953125\n",
      "[step: 54] loss: 0.04507244750857353\n",
      "[step: 55] loss: 2117.284423828125\n",
      "[step: 55] loss: 0.04323973506689072\n",
      "[step: 56] loss: 2045.9453125\n",
      "[step: 56] loss: 0.04149625077843666\n",
      "[step: 57] loss: 1979.7412109375\n",
      "[step: 57] loss: 0.03984152525663376\n",
      "[step: 58] loss: 1918.19384765625\n",
      "[step: 58] loss: 0.03827415779232979\n",
      "[step: 59] loss: 1860.854248046875\n",
      "[step: 59] loss: 0.036792393773794174\n",
      "[step: 60] loss: 1807.378662109375\n",
      "[step: 60] loss: 0.03539407253265381\n",
      "[step: 61] loss: 1757.508056640625\n",
      "[step: 61] loss: 0.03407678008079529\n",
      "[step: 62] loss: 1711.025634765625\n",
      "[step: 62] loss: 0.03283795341849327\n",
      "[step: 63] loss: 1667.693115234375\n",
      "[step: 63] loss: 0.03167487308382988\n",
      "[step: 64] loss: 1627.24072265625\n",
      "[step: 64] loss: 0.03058467246592045\n",
      "[step: 65] loss: 1589.36376953125\n",
      "[step: 65] loss: 0.029564490541815758\n",
      "[step: 66] loss: 1553.77587890625\n",
      "[step: 66] loss: 0.02861141227185726\n",
      "[step: 67] loss: 1520.220947265625\n",
      "[step: 67] loss: 0.027722487226128578\n",
      "[step: 68] loss: 1488.5157470703125\n",
      "[step: 68] loss: 0.02689487487077713\n",
      "[step: 69] loss: 1458.541259765625\n",
      "[step: 69] loss: 0.026125742122530937\n",
      "[step: 70] loss: 1430.233154296875\n",
      "[step: 70] loss: 0.025412067770957947\n",
      "[step: 71] loss: 1403.538818359375\n",
      "[step: 71] loss: 0.024750834330916405\n",
      "[step: 72] loss: 1378.3668212890625\n",
      "[step: 72] loss: 0.02413894049823284\n",
      "[step: 73] loss: 1354.5654296875\n",
      "[step: 73] loss: 0.02357320673763752\n",
      "[step: 74] loss: 1331.923095703125\n",
      "[step: 74] loss: 0.02305065281689167\n",
      "[step: 75] loss: 1310.239013671875\n",
      "[step: 75] loss: 0.022568387910723686\n",
      "[step: 76] loss: 1289.36767578125\n",
      "[step: 76] loss: 0.022123804315924644\n",
      "[step: 77] loss: 1269.243408203125\n",
      "[step: 77] loss: 0.021714352071285248\n",
      "[step: 78] loss: 1249.852783203125\n",
      "[step: 78] loss: 0.02133767493069172\n",
      "[step: 79] loss: 1231.203125\n",
      "[step: 79] loss: 0.020991317927837372\n",
      "[step: 80] loss: 1213.267333984375\n",
      "[step: 80] loss: 0.020672744140028954\n",
      "[step: 81] loss: 1195.9813232421875\n",
      "[step: 81] loss: 0.02037923038005829\n",
      "[step: 82] loss: 1179.2529296875\n",
      "[step: 82] loss: 0.02010786347091198\n",
      "[step: 83] loss: 1162.999267578125\n",
      "[step: 83] loss: 0.01985570788383484\n",
      "[step: 84] loss: 1147.179443359375\n",
      "[step: 84] loss: 0.01961996965110302\n",
      "[step: 85] loss: 1131.788330078125\n",
      "[step: 85] loss: 0.019398121163249016\n",
      "[step: 86] loss: 1116.853759765625\n",
      "[step: 86] loss: 0.019188079982995987\n",
      "[step: 87] loss: 1102.39697265625\n",
      "[step: 87] loss: 0.018988022580742836\n",
      "[step: 88] loss: 1088.3997802734375\n",
      "[step: 88] loss: 0.018796514719724655\n",
      "[step: 89] loss: 1074.806884765625\n",
      "[step: 89] loss: 0.018612468615174294\n",
      "[step: 90] loss: 1061.5467529296875\n",
      "[step: 90] loss: 0.018435023725032806\n",
      "[step: 91] loss: 1048.5634765625\n",
      "[step: 91] loss: 0.01826370880007744\n",
      "[step: 92] loss: 1035.8316650390625\n",
      "[step: 92] loss: 0.018098291009664536\n",
      "[step: 93] loss: 1023.3521728515625\n",
      "[step: 93] loss: 0.017938679084181786\n",
      "[step: 94] loss: 1011.125\n",
      "[step: 94] loss: 0.017784906551241875\n",
      "[step: 95] loss: 999.1387329101562\n",
      "[step: 95] loss: 0.01763693429529667\n",
      "[step: 96] loss: 987.373046875\n",
      "[step: 96] loss: 0.01749478466808796\n",
      "[step: 97] loss: 975.8162841796875\n",
      "[step: 97] loss: 0.01735832542181015\n",
      "[step: 98] loss: 964.4816284179688\n",
      "[step: 98] loss: 0.017227349802851677\n",
      "[step: 99] loss: 953.3923950195312\n",
      "[step: 99] loss: 0.017101693898439407\n",
      "[step: 100] loss: 942.5435180664062\n",
      "[step: 100] loss: 0.016981137916445732\n",
      "[step: 101] loss: 931.912841796875\n",
      "[step: 101] loss: 0.016865387558937073\n",
      "[step: 102] loss: 921.47900390625\n",
      "[step: 102] loss: 0.01675420068204403\n",
      "[step: 103] loss: 911.242919921875\n",
      "[step: 103] loss: 0.016647275537252426\n",
      "[step: 104] loss: 901.2066040039062\n",
      "[step: 104] loss: 0.016544358804821968\n",
      "[step: 105] loss: 891.3531494140625\n",
      "[step: 105] loss: 0.016445159912109375\n",
      "[step: 106] loss: 881.6614990234375\n",
      "[step: 106] loss: 0.016349442303180695\n",
      "[step: 107] loss: 872.1253662109375\n",
      "[step: 107] loss: 0.016256939619779587\n",
      "[step: 108] loss: 862.7620239257812\n",
      "[step: 108] loss: 0.01616738922894001\n",
      "[step: 109] loss: 853.5775756835938\n",
      "[step: 109] loss: 0.016080589964985847\n",
      "[step: 110] loss: 844.557373046875\n",
      "[step: 110] loss: 0.01599627360701561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 111] loss: 835.6884765625\n",
      "[step: 111] loss: 0.015914304181933403\n",
      "[step: 112] loss: 826.9771728515625\n",
      "[step: 112] loss: 0.01583445630967617\n",
      "[step: 113] loss: 818.4353637695312\n",
      "[step: 113] loss: 0.01575656607747078\n",
      "[step: 114] loss: 810.056396484375\n",
      "[step: 114] loss: 0.015680545940995216\n",
      "[step: 115] loss: 801.8283081054688\n",
      "[step: 115] loss: 0.015606320463120937\n",
      "[step: 116] loss: 793.755859375\n",
      "[step: 116] loss: 0.015533847734332085\n",
      "[step: 117] loss: 785.8515014648438\n",
      "[step: 117] loss: 0.015463097020983696\n",
      "[step: 118] loss: 778.1062622070312\n",
      "[step: 118] loss: 0.015393983572721481\n",
      "[step: 119] loss: 770.498779296875\n",
      "[step: 119] loss: 0.015326494351029396\n",
      "[step: 120] loss: 763.010986328125\n",
      "[step: 120] loss: 0.015260529704391956\n",
      "[step: 121] loss: 755.6219482421875\n",
      "[step: 121] loss: 0.015196087770164013\n",
      "[step: 122] loss: 748.3184204101562\n",
      "[step: 122] loss: 0.015133103355765343\n",
      "[step: 123] loss: 741.1001586914062\n",
      "[step: 123] loss: 0.015071611851453781\n",
      "[step: 124] loss: 733.966552734375\n",
      "[step: 124] loss: 0.015011606737971306\n",
      "[step: 125] loss: 726.9200439453125\n",
      "[step: 125] loss: 0.01495304238051176\n",
      "[step: 126] loss: 719.9751586914062\n",
      "[step: 126] loss: 0.014895914122462273\n",
      "[step: 127] loss: 713.1290283203125\n",
      "[step: 127] loss: 0.014840184710919857\n",
      "[step: 128] loss: 706.3786010742188\n",
      "[step: 128] loss: 0.014785794541239738\n",
      "[step: 129] loss: 699.7230224609375\n",
      "[step: 129] loss: 0.014732735231518745\n",
      "[step: 130] loss: 693.1469116210938\n",
      "[step: 130] loss: 0.014680902473628521\n",
      "[step: 131] loss: 686.6502685546875\n",
      "[step: 131] loss: 0.014630261808633804\n",
      "[step: 132] loss: 680.2310791015625\n",
      "[step: 132] loss: 0.014580722898244858\n",
      "[step: 133] loss: 673.8953247070312\n",
      "[step: 133] loss: 0.014532233588397503\n",
      "[step: 134] loss: 667.6485595703125\n",
      "[step: 134] loss: 0.014484734274446964\n",
      "[step: 135] loss: 661.4903564453125\n",
      "[step: 135] loss: 0.014438122510910034\n",
      "[step: 136] loss: 655.4183349609375\n",
      "[step: 136] loss: 0.01439239364117384\n",
      "[step: 137] loss: 649.422119140625\n",
      "[step: 137] loss: 0.0143474405631423\n",
      "[step: 138] loss: 643.4937744140625\n",
      "[step: 138] loss: 0.014303219504654408\n",
      "[step: 139] loss: 637.6312866210938\n",
      "[step: 139] loss: 0.014259721152484417\n",
      "[step: 140] loss: 631.8284301757812\n",
      "[step: 140] loss: 0.01421686727553606\n",
      "[step: 141] loss: 626.0894165039062\n",
      "[step: 141] loss: 0.014174601063132286\n",
      "[step: 142] loss: 620.41552734375\n",
      "[step: 142] loss: 0.014132904820144176\n",
      "[step: 143] loss: 614.804443359375\n",
      "[step: 143] loss: 0.014091737568378448\n",
      "[step: 144] loss: 609.2551879882812\n",
      "[step: 144] loss: 0.014051059260964394\n",
      "[step: 145] loss: 603.760986328125\n",
      "[step: 145] loss: 0.014010840095579624\n",
      "[step: 146] loss: 598.3154907226562\n",
      "[step: 146] loss: 0.013971013016998768\n",
      "[step: 147] loss: 592.9141845703125\n",
      "[step: 147] loss: 0.013931594789028168\n",
      "[step: 148] loss: 587.5562744140625\n",
      "[step: 148] loss: 0.013892533257603645\n",
      "[step: 149] loss: 582.2393798828125\n",
      "[step: 149] loss: 0.013853794895112514\n",
      "[step: 150] loss: 576.958251953125\n",
      "[step: 150] loss: 0.013815370388329029\n",
      "[step: 151] loss: 571.70703125\n",
      "[step: 151] loss: 0.013777224346995354\n",
      "[step: 152] loss: 566.4783935546875\n",
      "[step: 152] loss: 0.013739344663918018\n",
      "[step: 153] loss: 561.264404296875\n",
      "[step: 153] loss: 0.013701717369258404\n",
      "[step: 154] loss: 556.0617065429688\n",
      "[step: 154] loss: 0.01366432011127472\n",
      "[step: 155] loss: 550.866943359375\n",
      "[step: 155] loss: 0.013627159409224987\n",
      "[step: 156] loss: 545.6854248046875\n",
      "[step: 156] loss: 0.01359018124639988\n",
      "[step: 157] loss: 540.508056640625\n",
      "[step: 157] loss: 0.01355341449379921\n",
      "[step: 158] loss: 535.3614501953125\n",
      "[step: 158] loss: 0.013516824692487717\n",
      "[step: 159] loss: 530.2890014648438\n",
      "[step: 159] loss: 0.01348042394965887\n",
      "[step: 160] loss: 525.3137817382812\n",
      "[step: 160] loss: 0.013444174081087112\n",
      "[step: 161] loss: 520.41552734375\n",
      "[step: 161] loss: 0.013408059254288673\n",
      "[step: 162] loss: 515.5587158203125\n",
      "[step: 162] loss: 0.013372071087360382\n",
      "[step: 163] loss: 510.7579345703125\n",
      "[step: 163] loss: 0.01333623006939888\n",
      "[step: 164] loss: 506.0204772949219\n",
      "[step: 164] loss: 0.013300472870469093\n",
      "[step: 165] loss: 501.32147216796875\n",
      "[step: 165] loss: 0.013264812529087067\n",
      "[step: 166] loss: 496.6385498046875\n",
      "[step: 166] loss: 0.013229237869381905\n",
      "[step: 167] loss: 491.9841003417969\n",
      "[step: 167] loss: 0.013193737715482712\n",
      "[step: 168] loss: 487.3692626953125\n",
      "[step: 168] loss: 0.013158285990357399\n",
      "[step: 169] loss: 482.7824401855469\n",
      "[step: 169] loss: 0.013122894801199436\n",
      "[step: 170] loss: 478.21563720703125\n",
      "[step: 170] loss: 0.013087532483041286\n",
      "[step: 171] loss: 473.71185302734375\n",
      "[step: 171] loss: 0.013052187860012054\n",
      "[step: 172] loss: 469.34228515625\n",
      "[step: 172] loss: 0.013016853481531143\n",
      "[step: 173] loss: 465.16534423828125\n",
      "[step: 173] loss: 0.012981513515114784\n",
      "[step: 174] loss: 461.1938781738281\n",
      "[step: 174] loss: 0.012946161441504955\n",
      "[step: 175] loss: 457.34967041015625\n",
      "[step: 175] loss: 0.012910758145153522\n",
      "[step: 176] loss: 453.5631103515625\n",
      "[step: 176] loss: 0.012875321321189404\n",
      "[step: 177] loss: 449.7638244628906\n",
      "[step: 177] loss: 0.01283983513712883\n",
      "[step: 178] loss: 445.9473876953125\n",
      "[step: 178] loss: 0.012804265134036541\n",
      "[step: 179] loss: 442.164794921875\n",
      "[step: 179] loss: 0.012768592685461044\n",
      "[step: 180] loss: 438.4295349121094\n",
      "[step: 180] loss: 0.012732820585370064\n",
      "[step: 181] loss: 434.7582092285156\n",
      "[step: 181] loss: 0.01269691251218319\n",
      "[step: 182] loss: 431.1690979003906\n",
      "[step: 182] loss: 0.012660831212997437\n",
      "[step: 183] loss: 427.6915588378906\n",
      "[step: 183] loss: 0.012624581344425678\n",
      "[step: 184] loss: 424.34674072265625\n",
      "[step: 184] loss: 0.012588108889758587\n",
      "[step: 185] loss: 421.09625244140625\n",
      "[step: 185] loss: 0.012551389634609222\n",
      "[step: 186] loss: 417.8822021484375\n",
      "[step: 186] loss: 0.012514394707977772\n",
      "[step: 187] loss: 414.6634521484375\n",
      "[step: 187] loss: 0.012477075681090355\n",
      "[step: 188] loss: 411.45001220703125\n",
      "[step: 188] loss: 0.012439440004527569\n",
      "[step: 189] loss: 408.28314208984375\n",
      "[step: 189] loss: 0.012401447631418705\n",
      "[step: 190] loss: 405.1852722167969\n",
      "[step: 190] loss: 0.012363113462924957\n",
      "[step: 191] loss: 402.15997314453125\n",
      "[step: 191] loss: 0.012324467301368713\n",
      "[step: 192] loss: 399.1909484863281\n",
      "[step: 192] loss: 0.012285572476685047\n",
      "[step: 193] loss: 396.274658203125\n",
      "[step: 193] loss: 0.012246442027390003\n",
      "[step: 194] loss: 393.4068603515625\n",
      "[step: 194] loss: 0.012207195162773132\n",
      "[step: 195] loss: 390.58642578125\n",
      "[step: 195] loss: 0.012167884036898613\n",
      "[step: 196] loss: 387.798583984375\n",
      "[step: 196] loss: 0.01212859433144331\n",
      "[step: 197] loss: 385.039306640625\n",
      "[step: 197] loss: 0.012089421972632408\n",
      "[step: 198] loss: 382.31805419921875\n",
      "[step: 198] loss: 0.012050461955368519\n",
      "[step: 199] loss: 379.6453552246094\n",
      "[step: 199] loss: 0.012011772021651268\n",
      "[step: 200] loss: 377.023193359375\n",
      "[step: 200] loss: 0.011973287910223007\n",
      "[step: 201] loss: 374.4368896484375\n",
      "[step: 201] loss: 0.01193503849208355\n",
      "[step: 202] loss: 371.8769836425781\n",
      "[step: 202] loss: 0.011896919459104538\n",
      "[step: 203] loss: 369.34228515625\n",
      "[step: 203] loss: 0.011858953163027763\n",
      "[step: 204] loss: 366.8447265625\n",
      "[step: 204] loss: 0.011821259744465351\n",
      "[step: 205] loss: 364.38848876953125\n",
      "[step: 205] loss: 0.011785740964114666\n",
      "[step: 206] loss: 361.97125244140625\n",
      "[step: 206] loss: 0.011764208786189556\n",
      "[step: 207] loss: 359.58990478515625\n",
      "[step: 207] loss: 0.011712419800460339\n",
      "[step: 208] loss: 357.241455078125\n",
      "[step: 208] loss: 0.011685695499181747\n",
      "[step: 209] loss: 354.9259033203125\n",
      "[step: 209] loss: 0.011649317108094692\n",
      "[step: 210] loss: 352.6375732421875\n",
      "[step: 210] loss: 0.011608222499489784\n",
      "[step: 211] loss: 350.3736267089844\n",
      "[step: 211] loss: 0.011577878147363663\n",
      "[step: 212] loss: 348.1312255859375\n",
      "[step: 212] loss: 0.011534501798450947\n",
      "[step: 213] loss: 345.9161376953125\n",
      "[step: 213] loss: 0.011506134644150734\n",
      "[step: 214] loss: 343.73309326171875\n",
      "[step: 214] loss: 0.011464696377515793\n",
      "[step: 215] loss: 341.5833435058594\n",
      "[step: 215] loss: 0.011436725966632366\n",
      "[step: 216] loss: 339.46380615234375\n",
      "[step: 216] loss: 0.011394360102713108\n",
      "[step: 217] loss: 337.372802734375\n",
      "[step: 217] loss: 0.011366809718310833\n",
      "[step: 218] loss: 335.3092956542969\n",
      "[step: 218] loss: 0.011325856670737267\n",
      "[step: 219] loss: 333.2721252441406\n",
      "[step: 219] loss: 0.011298432014882565\n",
      "[step: 220] loss: 331.260009765625\n",
      "[step: 220] loss: 0.01125872228294611\n",
      "[step: 221] loss: 329.271484375\n",
      "[step: 221] loss: 0.011229395866394043\n",
      "[step: 222] loss: 327.30865478515625\n",
      "[step: 222] loss: 0.011192305944859982\n",
      "[step: 223] loss: 325.36688232421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 223] loss: 0.011162039823830128\n",
      "[step: 224] loss: 323.44622802734375\n",
      "[step: 224] loss: 0.011127067729830742\n",
      "[step: 225] loss: 321.53778076171875\n",
      "[step: 225] loss: 0.011094952002167702\n",
      "[step: 226] loss: 319.6458435058594\n",
      "[step: 226] loss: 0.011063022539019585\n",
      "[step: 227] loss: 317.772705078125\n",
      "[step: 227] loss: 0.011028674431145191\n",
      "[step: 228] loss: 315.9273681640625\n",
      "[step: 228] loss: 0.010999062098562717\n",
      "[step: 229] loss: 314.112548828125\n",
      "[step: 229] loss: 0.01096464041620493\n",
      "[step: 230] loss: 312.3265686035156\n",
      "[step: 230] loss: 0.010935207828879356\n",
      "[step: 231] loss: 310.566162109375\n",
      "[step: 231] loss: 0.010902639478445053\n",
      "[step: 232] loss: 308.82781982421875\n",
      "[step: 232] loss: 0.010871323756873608\n",
      "[step: 233] loss: 307.10894775390625\n",
      "[step: 233] loss: 0.010841521434485912\n",
      "[step: 234] loss: 305.4039001464844\n",
      "[step: 234] loss: 0.010809320025146008\n",
      "[step: 235] loss: 303.7149658203125\n",
      "[step: 235] loss: 0.010780150070786476\n",
      "[step: 236] loss: 302.0397644042969\n",
      "[step: 236] loss: 0.010749583132565022\n",
      "[step: 237] loss: 300.38409423828125\n",
      "[step: 237] loss: 0.010719210840761662\n",
      "[step: 238] loss: 298.74932861328125\n",
      "[step: 238] loss: 0.010690598748624325\n",
      "[step: 239] loss: 297.138427734375\n",
      "[step: 239] loss: 0.010660393163561821\n",
      "[step: 240] loss: 295.55181884765625\n",
      "[step: 240] loss: 0.010631521232426167\n",
      "[step: 241] loss: 293.98907470703125\n",
      "[step: 241] loss: 0.01060329657047987\n",
      "[step: 242] loss: 292.4480895996094\n",
      "[step: 242] loss: 0.010573843494057655\n",
      "[step: 243] loss: 290.9290466308594\n",
      "[step: 243] loss: 0.010545949451625347\n",
      "[step: 244] loss: 289.432373046875\n",
      "[step: 244] loss: 0.010518185794353485\n",
      "[step: 245] loss: 287.9580383300781\n",
      "[step: 245] loss: 0.01048967707902193\n",
      "[step: 246] loss: 286.50823974609375\n",
      "[step: 246] loss: 0.010462398640811443\n",
      "[step: 247] loss: 285.0787353515625\n",
      "[step: 247] loss: 0.010435361415147781\n",
      "[step: 248] loss: 283.6710205078125\n",
      "[step: 248] loss: 0.01040778961032629\n",
      "[step: 249] loss: 282.2687072753906\n",
      "[step: 249] loss: 0.010380825959146023\n",
      "[step: 250] loss: 280.87286376953125\n",
      "[step: 250] loss: 0.0103545431047678\n",
      "[step: 251] loss: 279.4741516113281\n",
      "[step: 251] loss: 0.010327954776585102\n",
      "[step: 252] loss: 278.0926513671875\n",
      "[step: 252] loss: 0.010301390662789345\n",
      "[step: 253] loss: 276.7412414550781\n",
      "[step: 253] loss: 0.010275456123054028\n",
      "[step: 254] loss: 275.4267578125\n",
      "[step: 254] loss: 0.010249876417219639\n",
      "[step: 255] loss: 274.1427001953125\n",
      "[step: 255] loss: 0.010224134661257267\n",
      "[step: 256] loss: 272.87744140625\n",
      "[step: 256] loss: 0.010198519565165043\n",
      "[step: 257] loss: 271.6226806640625\n",
      "[step: 257] loss: 0.010173298418521881\n",
      "[step: 258] loss: 270.36920166015625\n",
      "[step: 258] loss: 0.010148489847779274\n",
      "[step: 259] loss: 269.1211242675781\n",
      "[step: 259] loss: 0.010123723186552525\n",
      "[step: 260] loss: 267.8801574707031\n",
      "[step: 260] loss: 0.01009901612997055\n",
      "[step: 261] loss: 266.6573486328125\n",
      "[step: 261] loss: 0.010074485093355179\n",
      "[step: 262] loss: 265.45758056640625\n",
      "[step: 262] loss: 0.010050270706415176\n",
      "[step: 263] loss: 264.28076171875\n",
      "[step: 263] loss: 0.010026350617408752\n",
      "[step: 264] loss: 263.12353515625\n",
      "[step: 264] loss: 0.010002677328884602\n",
      "[step: 265] loss: 261.9828186035156\n",
      "[step: 265] loss: 0.009979158639907837\n",
      "[step: 266] loss: 260.8555603027344\n",
      "[step: 266] loss: 0.009955795481801033\n",
      "[step: 267] loss: 259.73870849609375\n",
      "[step: 267] loss: 0.009932608343660831\n",
      "[step: 268] loss: 258.63287353515625\n",
      "[step: 268] loss: 0.009909607470035553\n",
      "[step: 269] loss: 257.5349426269531\n",
      "[step: 269] loss: 0.00988680962473154\n",
      "[step: 270] loss: 256.4476318359375\n",
      "[step: 270] loss: 0.00986423809081316\n",
      "[step: 271] loss: 255.36929321289062\n",
      "[step: 271] loss: 0.009841891936957836\n",
      "[step: 272] loss: 254.30392456054688\n",
      "[step: 272] loss: 0.009819797240197659\n",
      "[step: 273] loss: 253.25137329101562\n",
      "[step: 273] loss: 0.009797980077564716\n",
      "[step: 274] loss: 252.21383666992188\n",
      "[step: 274] loss: 0.0097765251994133\n",
      "[step: 275] loss: 251.19017028808594\n",
      "[step: 275] loss: 0.009755706414580345\n",
      "[step: 276] loss: 250.1810302734375\n",
      "[step: 276] loss: 0.00973588414490223\n",
      "[step: 277] loss: 249.18515014648438\n",
      "[step: 277] loss: 0.009718072600662708\n",
      "[step: 278] loss: 248.2014923095703\n",
      "[step: 278] loss: 0.009700929746031761\n",
      "[step: 279] loss: 247.2308349609375\n",
      "[step: 279] loss: 0.009682737290859222\n",
      "[step: 280] loss: 246.27301025390625\n",
      "[step: 280] loss: 0.009655638597905636\n",
      "[step: 281] loss: 245.32904052734375\n",
      "[step: 281] loss: 0.00962983537465334\n",
      "[step: 282] loss: 244.40113830566406\n",
      "[step: 282] loss: 0.009612387046217918\n",
      "[step: 283] loss: 243.49606323242188\n",
      "[step: 283] loss: 0.00959659181535244\n",
      "[step: 284] loss: 242.62086486816406\n",
      "[step: 284] loss: 0.00957498885691166\n",
      "[step: 285] loss: 241.79481506347656\n",
      "[step: 285] loss: 0.00955114420503378\n",
      "[step: 286] loss: 241.00376892089844\n",
      "[step: 286] loss: 0.009533926844596863\n",
      "[step: 287] loss: 240.2340087890625\n",
      "[step: 287] loss: 0.009517914615571499\n",
      "[step: 288] loss: 239.30810546875\n",
      "[step: 288] loss: 0.009496055543422699\n",
      "[step: 289] loss: 238.27362060546875\n",
      "[step: 289] loss: 0.00947536900639534\n",
      "[step: 290] loss: 237.27685546875\n",
      "[step: 290] loss: 0.009459320455789566\n",
      "[step: 291] loss: 236.48382568359375\n",
      "[step: 291] loss: 0.009441522881388664\n",
      "[step: 292] loss: 235.78854370117188\n",
      "[step: 292] loss: 0.009421074762940407\n",
      "[step: 293] loss: 234.97360229492188\n",
      "[step: 293] loss: 0.009402906522154808\n",
      "[step: 294] loss: 234.0489959716797\n",
      "[step: 294] loss: 0.009386887773871422\n",
      "[step: 295] loss: 233.17547607421875\n",
      "[step: 295] loss: 0.009368916042149067\n",
      "[step: 296] loss: 232.44573974609375\n",
      "[step: 296] loss: 0.009349873289465904\n",
      "[step: 297] loss: 231.7414093017578\n",
      "[step: 297] loss: 0.009333179332315922\n",
      "[step: 298] loss: 230.931884765625\n",
      "[step: 298] loss: 0.009317153133451939\n",
      "[step: 299] loss: 230.09092712402344\n",
      "[step: 299] loss: 0.009299364872276783\n",
      "[step: 300] loss: 229.33181762695312\n",
      "[step: 300] loss: 0.009281735867261887\n",
      "[step: 301] loss: 228.64492797851562\n",
      "[step: 301] loss: 0.009265786036849022\n",
      "[step: 302] loss: 227.93368530273438\n",
      "[step: 302] loss: 0.009249820373952389\n",
      "[step: 303] loss: 227.16168212890625\n",
      "[step: 303] loss: 0.009232861921191216\n",
      "[step: 304] loss: 226.4051055908203\n",
      "[step: 304] loss: 0.009216222912073135\n",
      "[step: 305] loss: 225.71263122558594\n",
      "[step: 305] loss: 0.009200699627399445\n",
      "[step: 306] loss: 225.04779052734375\n",
      "[step: 306] loss: 0.00918522384017706\n",
      "[step: 307] loss: 224.35787963867188\n",
      "[step: 307] loss: 0.009169081225991249\n",
      "[step: 308] loss: 223.64138793945312\n",
      "[step: 308] loss: 0.009153147228062153\n",
      "[step: 309] loss: 222.94522094726562\n",
      "[step: 309] loss: 0.009137997403740883\n",
      "[step: 310] loss: 222.28982543945312\n",
      "[step: 310] loss: 0.00912304874509573\n",
      "[step: 311] loss: 221.65101623535156\n",
      "[step: 311] loss: 0.009107744321227074\n",
      "[step: 312] loss: 221.00106811523438\n",
      "[step: 312] loss: 0.009092438966035843\n",
      "[step: 313] loss: 220.3370819091797\n",
      "[step: 313] loss: 0.009077641181647778\n",
      "[step: 314] loss: 219.68292236328125\n",
      "[step: 314] loss: 0.009063211269676685\n",
      "[step: 315] loss: 219.05279541015625\n",
      "[step: 315] loss: 0.009048683568835258\n",
      "[step: 316] loss: 218.4417724609375\n",
      "[step: 316] loss: 0.009034044109284878\n",
      "[step: 317] loss: 217.83413696289062\n",
      "[step: 317] loss: 0.00901962909847498\n",
      "[step: 318] loss: 217.22055053710938\n",
      "[step: 318] loss: 0.009005564264953136\n",
      "[step: 319] loss: 216.60598754882812\n",
      "[step: 319] loss: 0.008991695940494537\n",
      "[step: 320] loss: 215.99957275390625\n",
      "[step: 320] loss: 0.008977795951068401\n",
      "[step: 321] loss: 215.4073028564453\n",
      "[step: 321] loss: 0.00896390713751316\n",
      "[step: 322] loss: 214.8279266357422\n",
      "[step: 322] loss: 0.008950191549956799\n",
      "[step: 323] loss: 214.2565460205078\n",
      "[step: 323] loss: 0.008936739526689053\n",
      "[step: 324] loss: 213.688232421875\n",
      "[step: 324] loss: 0.008923438377678394\n",
      "[step: 325] loss: 213.120849609375\n",
      "[step: 325] loss: 0.008910229429602623\n",
      "[step: 326] loss: 212.55621337890625\n",
      "[step: 326] loss: 0.008897032588720322\n",
      "[step: 327] loss: 211.9962921142578\n",
      "[step: 327] loss: 0.008883949369192123\n",
      "[step: 328] loss: 211.44290161132812\n",
      "[step: 328] loss: 0.008871058002114296\n",
      "[step: 329] loss: 210.8974609375\n",
      "[step: 329] loss: 0.008858331479132175\n",
      "[step: 330] loss: 210.35968017578125\n",
      "[step: 330] loss: 0.008845738135278225\n",
      "[step: 331] loss: 209.82879638671875\n",
      "[step: 331] loss: 0.008833219297230244\n",
      "[step: 332] loss: 209.30459594726562\n",
      "[step: 332] loss: 0.008820790797472\n",
      "[step: 333] loss: 208.78631591796875\n",
      "[step: 333] loss: 0.008808458223938942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 334] loss: 208.27456665039062\n",
      "[step: 334] loss: 0.008796237409114838\n",
      "[step: 335] loss: 207.7695770263672\n",
      "[step: 335] loss: 0.00878418143838644\n",
      "[step: 336] loss: 207.27243041992188\n",
      "[step: 336] loss: 0.008772255852818489\n",
      "[step: 337] loss: 206.7857208251953\n",
      "[step: 337] loss: 0.00876044575124979\n",
      "[step: 338] loss: 206.31143188476562\n",
      "[step: 338] loss: 0.008748720400035381\n",
      "[step: 339] loss: 205.85809326171875\n",
      "[step: 339] loss: 0.008737083524465561\n",
      "[step: 340] loss: 205.42576599121094\n",
      "[step: 340] loss: 0.008725551888346672\n",
      "[step: 341] loss: 205.03073120117188\n",
      "[step: 341] loss: 0.008714125491678715\n",
      "[step: 342] loss: 204.63812255859375\n",
      "[step: 342] loss: 0.008702811785042286\n",
      "[step: 343] loss: 204.24407958984375\n",
      "[step: 343] loss: 0.008691626600921154\n",
      "[step: 344] loss: 203.7297821044922\n",
      "[step: 344] loss: 0.008680528961122036\n",
      "[step: 345] loss: 203.1422119140625\n",
      "[step: 345] loss: 0.008669549599289894\n",
      "[step: 346] loss: 202.53485107421875\n",
      "[step: 346] loss: 0.008658667095005512\n",
      "[step: 347] loss: 202.03866577148438\n",
      "[step: 347] loss: 0.008647896349430084\n",
      "[step: 348] loss: 201.66319274902344\n",
      "[step: 348] loss: 0.008637205697596073\n",
      "[step: 349] loss: 201.3194580078125\n",
      "[step: 349] loss: 0.008626618422567844\n",
      "[step: 350] loss: 200.9253692626953\n",
      "[step: 350] loss: 0.008616117760539055\n",
      "[step: 351] loss: 200.4299774169922\n",
      "[step: 351] loss: 0.008605715818703175\n",
      "[step: 352] loss: 199.91334533691406\n",
      "[step: 352] loss: 0.008595412597060204\n",
      "[step: 353] loss: 199.452392578125\n",
      "[step: 353] loss: 0.008585203438997269\n",
      "[step: 354] loss: 199.07073974609375\n",
      "[step: 354] loss: 0.008575085550546646\n",
      "[step: 355] loss: 198.7206573486328\n",
      "[step: 355] loss: 0.00856506172567606\n",
      "[step: 356] loss: 198.33644104003906\n",
      "[step: 356] loss: 0.008555133827030659\n",
      "[step: 357] loss: 197.90621948242188\n",
      "[step: 357] loss: 0.008545299991965294\n",
      "[step: 358] loss: 197.45126342773438\n",
      "[step: 358] loss: 0.008535551838576794\n",
      "[step: 359] loss: 197.02369689941406\n",
      "[step: 359] loss: 0.00852589588612318\n",
      "[step: 360] loss: 196.6410369873047\n",
      "[step: 360] loss: 0.008516333065927029\n",
      "[step: 361] loss: 196.28599548339844\n",
      "[step: 361] loss: 0.008506863377988338\n",
      "[step: 362] loss: 195.929443359375\n",
      "[step: 362] loss: 0.008497496135532856\n",
      "[step: 363] loss: 195.5491943359375\n",
      "[step: 363] loss: 0.008488240651786327\n",
      "[step: 364] loss: 195.1507568359375\n",
      "[step: 364] loss: 0.008479099720716476\n",
      "[step: 365] loss: 194.74862670898438\n",
      "[step: 365] loss: 0.008470118045806885\n",
      "[step: 366] loss: 194.3607940673828\n",
      "[step: 366] loss: 0.008461346849799156\n",
      "[step: 367] loss: 193.99459838867188\n",
      "[step: 367] loss: 0.008452905341982841\n",
      "[step: 368] loss: 193.6446533203125\n",
      "[step: 368] loss: 0.008444842882454395\n",
      "[step: 369] loss: 193.30197143554688\n",
      "[step: 369] loss: 0.00843740999698639\n",
      "[step: 370] loss: 192.95693969726562\n",
      "[step: 370] loss: 0.008430233225226402\n",
      "[step: 371] loss: 192.60801696777344\n",
      "[step: 371] loss: 0.0084231561049819\n",
      "[step: 372] loss: 192.2525634765625\n",
      "[step: 372] loss: 0.008414107374846935\n",
      "[step: 373] loss: 191.8953094482422\n",
      "[step: 373] loss: 0.008403480052947998\n",
      "[step: 374] loss: 191.53854370117188\n",
      "[step: 374] loss: 0.008392155170440674\n",
      "[step: 375] loss: 191.1859588623047\n",
      "[step: 375] loss: 0.00838290061801672\n",
      "[step: 376] loss: 190.8392333984375\n",
      "[step: 376] loss: 0.008375974372029305\n",
      "[step: 377] loss: 190.49822998046875\n",
      "[step: 377] loss: 0.008369391784071922\n",
      "[step: 378] loss: 190.1629180908203\n",
      "[step: 378] loss: 0.008361363783478737\n",
      "[step: 379] loss: 189.8321533203125\n",
      "[step: 379] loss: 0.008351671509444714\n",
      "[step: 380] loss: 189.50559997558594\n",
      "[step: 380] loss: 0.008342498913407326\n",
      "[step: 381] loss: 189.1834259033203\n",
      "[step: 381] loss: 0.00833501759916544\n",
      "[step: 382] loss: 188.86557006835938\n",
      "[step: 382] loss: 0.008328303694725037\n",
      "[step: 383] loss: 188.55426025390625\n",
      "[step: 383] loss: 0.008320833556354046\n",
      "[step: 384] loss: 188.25302124023438\n",
      "[step: 384] loss: 0.008312230929732323\n",
      "[step: 385] loss: 187.96725463867188\n",
      "[step: 385] loss: 0.008303815498948097\n",
      "[step: 386] loss: 187.71287536621094\n",
      "[step: 386] loss: 0.008296464569866657\n",
      "[step: 387] loss: 187.50396728515625\n",
      "[step: 387] loss: 0.008289657533168793\n",
      "[step: 388] loss: 187.38784790039062\n",
      "[step: 388] loss: 0.008282425813376904\n",
      "[step: 389] loss: 187.31263732910156\n",
      "[step: 389] loss: 0.008274523541331291\n",
      "[step: 390] loss: 187.26974487304688\n",
      "[step: 390] loss: 0.00826675072312355\n",
      "[step: 391] loss: 186.84571838378906\n",
      "[step: 391] loss: 0.008259620517492294\n",
      "[step: 392] loss: 186.17138671875\n",
      "[step: 392] loss: 0.008252856321632862\n",
      "[step: 393] loss: 185.50604248046875\n",
      "[step: 393] loss: 0.008245869539678097\n",
      "[step: 394] loss: 185.26048278808594\n",
      "[step: 394] loss: 0.00823851116001606\n",
      "[step: 395] loss: 185.271484375\n",
      "[step: 395] loss: 0.00823119841516018\n",
      "[step: 396] loss: 185.0677032470703\n",
      "[step: 396] loss: 0.008224274031817913\n",
      "[step: 397] loss: 184.5721435546875\n",
      "[step: 397] loss: 0.008217629976570606\n",
      "[step: 398] loss: 184.05264282226562\n",
      "[step: 398] loss: 0.008210908621549606\n",
      "[step: 399] loss: 183.83087158203125\n",
      "[step: 399] loss: 0.008203983306884766\n",
      "[step: 400] loss: 183.7498321533203\n",
      "[step: 400] loss: 0.008197042159736156\n",
      "[step: 401] loss: 183.47036743164062\n",
      "[step: 401] loss: 0.008190336637198925\n",
      "[step: 402] loss: 183.02391052246094\n",
      "[step: 402] loss: 0.008183836005628109\n",
      "[step: 403] loss: 182.65501403808594\n",
      "[step: 403] loss: 0.008177374489605427\n",
      "[step: 404] loss: 182.4744873046875\n",
      "[step: 404] loss: 0.008170809596776962\n",
      "[step: 405] loss: 182.30960083007812\n",
      "[step: 405] loss: 0.008164208382368088\n",
      "[step: 406] loss: 181.98855590820312\n",
      "[step: 406] loss: 0.008157707750797272\n",
      "[step: 407] loss: 181.610107421875\n",
      "[step: 407] loss: 0.008151354268193245\n",
      "[step: 408] loss: 181.32518005371094\n",
      "[step: 408] loss: 0.008145105093717575\n",
      "[step: 409] loss: 181.13661193847656\n",
      "[step: 409] loss: 0.008138840086758137\n",
      "[step: 410] loss: 180.92236328125\n",
      "[step: 410] loss: 0.008132549934089184\n",
      "[step: 411] loss: 180.61331176757812\n",
      "[step: 411] loss: 0.008126277476549149\n",
      "[step: 412] loss: 180.2890625\n",
      "[step: 412] loss: 0.008120091632008553\n",
      "[step: 413] loss: 180.02896118164062\n",
      "[step: 413] loss: 0.00811400543898344\n",
      "[step: 414] loss: 179.8211669921875\n",
      "[step: 414] loss: 0.008107983507215977\n",
      "[step: 415] loss: 179.59622192382812\n",
      "[step: 415] loss: 0.008101953193545341\n",
      "[step: 416] loss: 179.3173828125\n",
      "[step: 416] loss: 0.008095928467810154\n",
      "[step: 417] loss: 179.024658203125\n",
      "[step: 417] loss: 0.008089940994977951\n",
      "[step: 418] loss: 178.76394653320312\n",
      "[step: 418] loss: 0.008084007538855076\n",
      "[step: 419] loss: 178.539306640625\n",
      "[step: 419] loss: 0.00807814672589302\n",
      "[step: 420] loss: 178.31857299804688\n",
      "[step: 420] loss: 0.008072332479059696\n",
      "[step: 421] loss: 178.0728302001953\n",
      "[step: 421] loss: 0.00806653406471014\n",
      "[step: 422] loss: 177.8073272705078\n",
      "[step: 422] loss: 0.008060767315328121\n",
      "[step: 423] loss: 177.54393005371094\n",
      "[step: 423] loss: 0.008055012673139572\n",
      "[step: 424] loss: 177.3001251220703\n",
      "[step: 424] loss: 0.008049306459724903\n",
      "[step: 425] loss: 177.0736083984375\n",
      "[step: 425] loss: 0.00804363563656807\n",
      "[step: 426] loss: 176.84991455078125\n",
      "[step: 426] loss: 0.008038019761443138\n",
      "[step: 427] loss: 176.61769104003906\n",
      "[step: 427] loss: 0.008032434619963169\n",
      "[step: 428] loss: 176.37393188476562\n",
      "[step: 428] loss: 0.00802688393741846\n",
      "[step: 429] loss: 176.1266632080078\n",
      "[step: 429] loss: 0.00802135095000267\n",
      "[step: 430] loss: 175.88356018066406\n",
      "[step: 430] loss: 0.00801585242152214\n",
      "[step: 431] loss: 175.64932250976562\n",
      "[step: 431] loss: 0.008010385558009148\n",
      "[step: 432] loss: 175.42269897460938\n",
      "[step: 432] loss: 0.008004957810044289\n",
      "[step: 433] loss: 175.20045471191406\n",
      "[step: 433] loss: 0.007999569177627563\n",
      "[step: 434] loss: 174.9788055419922\n",
      "[step: 434] loss: 0.00799421314150095\n",
      "[step: 435] loss: 174.75503540039062\n",
      "[step: 435] loss: 0.007988887839019299\n",
      "[step: 436] loss: 174.52957153320312\n",
      "[step: 436] loss: 0.007983589544892311\n",
      "[step: 437] loss: 174.30209350585938\n",
      "[step: 437] loss: 0.007978327572345734\n",
      "[step: 438] loss: 174.07470703125\n",
      "[step: 438] loss: 0.007973105646669865\n",
      "[step: 439] loss: 173.84811401367188\n",
      "[step: 439] loss: 0.007967902347445488\n",
      "[step: 440] loss: 173.62283325195312\n",
      "[step: 440] loss: 0.007962738163769245\n",
      "[step: 441] loss: 173.39944458007812\n",
      "[step: 441] loss: 0.007957623340189457\n",
      "[step: 442] loss: 173.177734375\n",
      "[step: 442] loss: 0.007952533662319183\n",
      "[step: 443] loss: 172.9578857421875\n",
      "[step: 443] loss: 0.007947484962642193\n",
      "[step: 444] loss: 172.7398681640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 444] loss: 0.007942479103803635\n",
      "[step: 445] loss: 172.52330017089844\n",
      "[step: 445] loss: 0.007937511429190636\n",
      "[step: 446] loss: 172.30874633789062\n",
      "[step: 446] loss: 0.007932553067803383\n",
      "[step: 447] loss: 172.09658813476562\n",
      "[step: 447] loss: 0.007927648723125458\n",
      "[step: 448] loss: 171.8876953125\n",
      "[step: 448] loss: 0.007922790944576263\n",
      "[step: 449] loss: 171.6846466064453\n",
      "[step: 449] loss: 0.007917962968349457\n",
      "[step: 450] loss: 171.49270629882812\n",
      "[step: 450] loss: 0.007913175038993359\n",
      "[step: 451] loss: 171.32167053222656\n",
      "[step: 451] loss: 0.007908430881798267\n",
      "[step: 452] loss: 171.198486328125\n",
      "[step: 452] loss: 0.007903714664280415\n",
      "[step: 453] loss: 171.1573486328125\n",
      "[step: 453] loss: 0.007899043150246143\n",
      "[step: 454] loss: 171.3051300048828\n",
      "[step: 454] loss: 0.007894409820437431\n",
      "[step: 455] loss: 171.59942626953125\n",
      "[step: 455] loss: 0.007889819331467152\n",
      "[step: 456] loss: 172.07046508789062\n",
      "[step: 456] loss: 0.007885261438786983\n",
      "[step: 457] loss: 171.67373657226562\n",
      "[step: 457] loss: 0.007880735211074352\n",
      "[step: 458] loss: 170.64300537109375\n",
      "[step: 458] loss: 0.007876253686845303\n",
      "[step: 459] loss: 169.6421661376953\n",
      "[step: 459] loss: 0.007871802896261215\n",
      "[step: 460] loss: 169.72238159179688\n",
      "[step: 460] loss: 0.007867398671805859\n",
      "[step: 461] loss: 170.22463989257812\n",
      "[step: 461] loss: 0.007863018661737442\n",
      "[step: 462] loss: 169.84329223632812\n",
      "[step: 462] loss: 0.007858668453991413\n",
      "[step: 463] loss: 168.99551391601562\n",
      "[step: 463] loss: 0.007854362018406391\n",
      "[step: 464] loss: 168.70521545410156\n",
      "[step: 464] loss: 0.00785007607191801\n",
      "[step: 465] loss: 168.971435546875\n",
      "[step: 465] loss: 0.00784582830965519\n",
      "[step: 466] loss: 168.874755859375\n",
      "[step: 466] loss: 0.007841612212359905\n",
      "[step: 467] loss: 168.21478271484375\n",
      "[step: 467] loss: 0.007837423123419285\n",
      "[step: 468] loss: 167.91513061523438\n",
      "[step: 468] loss: 0.007833271287381649\n",
      "[step: 469] loss: 168.0535125732422\n",
      "[step: 469] loss: 0.007829132489860058\n",
      "[step: 470] loss: 167.8948974609375\n",
      "[step: 470] loss: 0.007825040258467197\n",
      "[step: 471] loss: 167.41708374023438\n",
      "[step: 471] loss: 0.007820967584848404\n",
      "[step: 472] loss: 167.15689086914062\n",
      "[step: 472] loss: 0.007816929370164871\n",
      "[step: 473] loss: 167.18466186523438\n",
      "[step: 473] loss: 0.007812910713255405\n",
      "[step: 474] loss: 167.0455780029297\n",
      "[step: 474] loss: 0.007808919530361891\n",
      "[step: 475] loss: 166.6528778076172\n",
      "[step: 475] loss: 0.007804960943758488\n",
      "[step: 476] loss: 166.4168243408203\n",
      "[step: 476] loss: 0.0078010279685258865\n",
      "[step: 477] loss: 166.38143920898438\n",
      "[step: 477] loss: 0.007797114551067352\n",
      "[step: 478] loss: 166.2265167236328\n",
      "[step: 478] loss: 0.0077932290732860565\n",
      "[step: 479] loss: 165.91648864746094\n",
      "[step: 479] loss: 0.007789374329149723\n",
      "[step: 480] loss: 165.68568420410156\n",
      "[step: 480] loss: 0.007785547524690628\n",
      "[step: 481] loss: 165.59622192382812\n",
      "[step: 481] loss: 0.007781749125570059\n",
      "[step: 482] loss: 165.46295166015625\n",
      "[step: 482] loss: 0.007777966558933258\n",
      "[step: 483] loss: 165.20413208007812\n",
      "[step: 483] loss: 0.007774221245199442\n",
      "[step: 484] loss: 164.9714813232422\n",
      "[step: 484] loss: 0.007770500145852566\n",
      "[step: 485] loss: 164.840576171875\n",
      "[step: 485] loss: 0.0077668153680861\n",
      "[step: 486] loss: 164.7122802734375\n",
      "[step: 486] loss: 0.007763170637190342\n",
      "[step: 487] loss: 164.50750732421875\n",
      "[step: 487] loss: 0.007759572938084602\n",
      "[step: 488] loss: 164.27874755859375\n",
      "[step: 488] loss: 0.0077560460194945335\n",
      "[step: 489] loss: 164.1094970703125\n",
      "[step: 489] loss: 0.007752624806016684\n",
      "[step: 490] loss: 163.97805786132812\n",
      "[step: 490] loss: 0.00774935120716691\n",
      "[step: 491] loss: 163.81246948242188\n",
      "[step: 491] loss: 0.007746327202767134\n",
      "[step: 492] loss: 163.607666015625\n",
      "[step: 492] loss: 0.007743599824607372\n",
      "[step: 493] loss: 163.41250610351562\n",
      "[step: 493] loss: 0.007741324603557587\n",
      "[step: 494] loss: 163.2554931640625\n",
      "[step: 494] loss: 0.0077392058447003365\n",
      "[step: 495] loss: 163.1099853515625\n",
      "[step: 495] loss: 0.007737010717391968\n",
      "[step: 496] loss: 162.9404296875\n",
      "[step: 496] loss: 0.00773335387930274\n",
      "[step: 497] loss: 162.75082397460938\n",
      "[step: 497] loss: 0.007728294935077429\n",
      "[step: 498] loss: 162.5682830810547\n",
      "[step: 498] loss: 0.0077222599647939205\n",
      "[step: 499] loss: 162.40728759765625\n",
      "[step: 499] loss: 0.007717378903180361\n",
      "[step: 500] loss: 162.25558471679688\n",
      "[step: 500] loss: 0.007714465726166964\n",
      "[step: 501] loss: 162.093994140625\n",
      "[step: 501] loss: 0.007712582591921091\n",
      "[step: 502] loss: 161.91934204101562\n",
      "[step: 502] loss: 0.007710123900324106\n",
      "[step: 503] loss: 161.74224853515625\n",
      "[step: 503] loss: 0.007706074975430965\n",
      "[step: 504] loss: 161.57421875\n",
      "[step: 504] loss: 0.007701417896896601\n",
      "[step: 505] loss: 161.41647338867188\n",
      "[step: 505] loss: 0.007697515655308962\n",
      "[step: 506] loss: 161.26126098632812\n",
      "[step: 506] loss: 0.007694808766245842\n",
      "[step: 507] loss: 161.1011962890625\n",
      "[step: 507] loss: 0.007692428771406412\n",
      "[step: 508] loss: 160.9346923828125\n",
      "[step: 508] loss: 0.007689279969781637\n",
      "[step: 509] loss: 160.76637268066406\n",
      "[step: 509] loss: 0.007685391698032618\n",
      "[step: 510] loss: 160.60073852539062\n",
      "[step: 510] loss: 0.007681598886847496\n",
      "[step: 511] loss: 160.4404296875\n",
      "[step: 511] loss: 0.007678543217480183\n",
      "[step: 512] loss: 160.28379821777344\n",
      "[step: 512] loss: 0.00767594063654542\n",
      "[step: 513] loss: 160.1284942626953\n",
      "[step: 513] loss: 0.0076730502769351006\n",
      "[step: 514] loss: 159.97216796875\n",
      "[step: 514] loss: 0.007669645827263594\n",
      "[step: 515] loss: 159.81378173828125\n",
      "[step: 515] loss: 0.007666128221899271\n",
      "[step: 516] loss: 159.65408325195312\n",
      "[step: 516] loss: 0.007663003634661436\n",
      "[step: 517] loss: 159.49386596679688\n",
      "[step: 517] loss: 0.007660247851163149\n",
      "[step: 518] loss: 159.33477783203125\n",
      "[step: 518] loss: 0.007657437585294247\n",
      "[step: 519] loss: 159.1765594482422\n",
      "[step: 519] loss: 0.007654326967895031\n",
      "[step: 520] loss: 159.0199737548828\n",
      "[step: 520] loss: 0.007651068735867739\n",
      "[step: 521] loss: 158.86480712890625\n",
      "[step: 521] loss: 0.007647977210581303\n",
      "[step: 522] loss: 158.71084594726562\n",
      "[step: 522] loss: 0.007645150180906057\n",
      "[step: 523] loss: 158.55760192871094\n",
      "[step: 523] loss: 0.007642375770956278\n",
      "[step: 524] loss: 158.4052734375\n",
      "[step: 524] loss: 0.007639448158442974\n",
      "[step: 525] loss: 158.25399780273438\n",
      "[step: 525] loss: 0.007636405061930418\n",
      "[step: 526] loss: 158.10372924804688\n",
      "[step: 526] loss: 0.007633409462869167\n",
      "[step: 527] loss: 157.95553588867188\n",
      "[step: 527] loss: 0.007630551233887672\n",
      "[step: 528] loss: 157.8099365234375\n",
      "[step: 528] loss: 0.007627802900969982\n",
      "[step: 529] loss: 157.6693878173828\n",
      "[step: 529] loss: 0.007625000551342964\n",
      "[step: 530] loss: 157.53662109375\n",
      "[step: 530] loss: 0.007622113451361656\n",
      "[step: 531] loss: 157.42062377929688\n",
      "[step: 531] loss: 0.007619233336299658\n",
      "[step: 532] loss: 157.33184814453125\n",
      "[step: 532] loss: 0.007616396062076092\n",
      "[step: 533] loss: 157.30364990234375\n",
      "[step: 533] loss: 0.007613660767674446\n",
      "[step: 534] loss: 157.3620147705078\n",
      "[step: 534] loss: 0.007610934786498547\n",
      "[step: 535] loss: 157.6166229248047\n",
      "[step: 535] loss: 0.007608186453580856\n",
      "[step: 536] loss: 157.98158264160156\n",
      "[step: 536] loss: 0.007605408784002066\n",
      "[step: 537] loss: 158.54214477539062\n",
      "[step: 537] loss: 0.007602641358971596\n",
      "[step: 538] loss: 158.3497314453125\n",
      "[step: 538] loss: 0.007599933072924614\n",
      "[step: 539] loss: 157.57269287109375\n",
      "[step: 539] loss: 0.007597259245812893\n",
      "[step: 540] loss: 156.32933044433594\n",
      "[step: 540] loss: 0.007594606373459101\n",
      "[step: 541] loss: 155.92764282226562\n",
      "[step: 541] loss: 0.007591929752379656\n",
      "[step: 542] loss: 156.39889526367188\n",
      "[step: 542] loss: 0.007589260581880808\n",
      "[step: 543] loss: 156.71420288085938\n",
      "[step: 543] loss: 0.007586603052914143\n",
      "[step: 544] loss: 156.33018493652344\n",
      "[step: 544] loss: 0.007583966478705406\n",
      "[step: 545] loss: 155.49574279785156\n",
      "[step: 545] loss: 0.007581355515867472\n",
      "[step: 546] loss: 155.25360107421875\n",
      "[step: 546] loss: 0.0075787692330777645\n",
      "[step: 547] loss: 155.56973266601562\n",
      "[step: 547] loss: 0.007576181087642908\n",
      "[step: 548] loss: 155.60182189941406\n",
      "[step: 548] loss: 0.007573613431304693\n",
      "[step: 549] loss: 155.14013671875\n",
      "[step: 549] loss: 0.007571043446660042\n",
      "[step: 550] loss: 154.6622314453125\n",
      "[step: 550] loss: 0.007568491157144308\n",
      "[step: 551] loss: 154.65576171875\n",
      "[step: 551] loss: 0.007565949112176895\n",
      "[step: 552] loss: 154.8091583251953\n",
      "[step: 552] loss: 0.007563428953289986\n",
      "[step: 553] loss: 154.6072540283203\n",
      "[step: 553] loss: 0.007560910191386938\n",
      "[step: 554] loss: 154.20118713378906\n",
      "[step: 554] loss: 0.0075584170408546925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 555] loss: 153.98977661132812\n",
      "[step: 555] loss: 0.00755594065412879\n",
      "[step: 556] loss: 154.02932739257812\n",
      "[step: 556] loss: 0.00755347590893507\n",
      "[step: 557] loss: 154.0093994140625\n",
      "[step: 557] loss: 0.007551025133579969\n",
      "[step: 558] loss: 153.74484252929688\n",
      "[step: 558] loss: 0.007548573426902294\n",
      "[step: 559] loss: 153.46139526367188\n",
      "[step: 559] loss: 0.007546132896095514\n",
      "[step: 560] loss: 153.35391235351562\n",
      "[step: 560] loss: 0.007543707732111216\n",
      "[step: 561] loss: 153.3401336669922\n",
      "[step: 561] loss: 0.007541289087384939\n",
      "[step: 562] loss: 153.23431396484375\n",
      "[step: 562] loss: 0.007538899313658476\n",
      "[step: 563] loss: 153.00088500976562\n",
      "[step: 563] loss: 0.007536509074270725\n",
      "[step: 564] loss: 152.79461669921875\n",
      "[step: 564] loss: 0.007534137926995754\n",
      "[step: 565] loss: 152.69723510742188\n",
      "[step: 565] loss: 0.007531772833317518\n",
      "[step: 566] loss: 152.63778686523438\n",
      "[step: 566] loss: 0.007529424503445625\n",
      "[step: 567] loss: 152.51712036132812\n",
      "[step: 567] loss: 0.007527089677751064\n",
      "[step: 568] loss: 152.3262939453125\n",
      "[step: 568] loss: 0.0075247641652822495\n",
      "[step: 569] loss: 152.14920043945312\n",
      "[step: 569] loss: 0.007522456347942352\n",
      "[step: 570] loss: 152.03309631347656\n",
      "[step: 570] loss: 0.007520170882344246\n",
      "[step: 571] loss: 151.9481658935547\n",
      "[step: 571] loss: 0.007517899386584759\n",
      "[step: 572] loss: 151.8389892578125\n",
      "[step: 572] loss: 0.0075156791135668755\n",
      "[step: 573] loss: 151.6859130859375\n",
      "[step: 573] loss: 0.0075135198421776295\n",
      "[step: 574] loss: 151.52186584472656\n",
      "[step: 574] loss: 0.0075115058571100235\n",
      "[step: 575] loss: 151.38265991210938\n",
      "[step: 575] loss: 0.007509755901992321\n",
      "[step: 576] loss: 151.2730712890625\n",
      "[step: 576] loss: 0.007508570794016123\n",
      "[step: 577] loss: 151.17062377929688\n",
      "[step: 577] loss: 0.007508080452680588\n",
      "[step: 578] loss: 151.0509033203125\n",
      "[step: 578] loss: 0.007508483249694109\n",
      "[step: 579] loss: 150.91217041015625\n",
      "[step: 579] loss: 0.007507354486733675\n",
      "[step: 580] loss: 150.76641845703125\n",
      "[step: 580] loss: 0.007504378445446491\n",
      "[step: 581] loss: 150.63031005859375\n",
      "[step: 581] loss: 0.0075000738725066185\n",
      "[step: 582] loss: 150.50924682617188\n",
      "[step: 582] loss: 0.007496831938624382\n",
      "[step: 583] loss: 150.3970489501953\n",
      "[step: 583] loss: 0.007493229117244482\n",
      "[step: 584] loss: 150.2841796875\n",
      "[step: 584] loss: 0.00748993456363678\n",
      "[step: 585] loss: 150.16339111328125\n",
      "[step: 585] loss: 0.007488875184208155\n",
      "[step: 586] loss: 150.0348358154297\n",
      "[step: 586] loss: 0.007488586939871311\n",
      "[step: 587] loss: 149.902587890625\n",
      "[step: 587] loss: 0.007486145943403244\n",
      "[step: 588] loss: 149.77133178710938\n",
      "[step: 588] loss: 0.007481880020350218\n",
      "[step: 589] loss: 149.6448516845703\n",
      "[step: 589] loss: 0.007479242980480194\n",
      "[step: 590] loss: 149.52334594726562\n",
      "[step: 590] loss: 0.007478063926100731\n",
      "[step: 591] loss: 149.40545654296875\n",
      "[step: 591] loss: 0.007476171012967825\n",
      "[step: 592] loss: 149.28958129882812\n",
      "[step: 592] loss: 0.007473808713257313\n",
      "[step: 593] loss: 149.17355346679688\n",
      "[step: 593] loss: 0.0074719879776239395\n",
      "[step: 594] loss: 149.0570068359375\n",
      "[step: 594] loss: 0.0074700163677334785\n",
      "[step: 595] loss: 148.93939208984375\n",
      "[step: 595] loss: 0.007467132993042469\n",
      "[step: 596] loss: 148.82113647460938\n",
      "[step: 596] loss: 0.007464804220944643\n",
      "[step: 597] loss: 148.70301818847656\n",
      "[step: 597] loss: 0.007463508751243353\n",
      "[step: 598] loss: 148.58493041992188\n",
      "[step: 598] loss: 0.007461842615157366\n",
      "[step: 599] loss: 148.46832275390625\n",
      "[step: 599] loss: 0.00745945330709219\n",
      "[step: 600] loss: 148.353271484375\n",
      "[step: 600] loss: 0.007457253057509661\n",
      "[step: 601] loss: 148.24151611328125\n",
      "[step: 601] loss: 0.007455401122570038\n",
      "[step: 602] loss: 148.13385009765625\n",
      "[step: 602] loss: 0.007453280501067638\n",
      "[step: 603] loss: 148.03469848632812\n",
      "[step: 603] loss: 0.007451205514371395\n",
      "[step: 604] loss: 147.94631958007812\n",
      "[step: 604] loss: 0.007449581753462553\n",
      "[step: 605] loss: 147.882080078125\n",
      "[step: 605] loss: 0.0074478634633123875\n",
      "[step: 606] loss: 147.84933471679688\n",
      "[step: 606] loss: 0.007445721887052059\n",
      "[step: 607] loss: 147.88864135742188\n",
      "[step: 607] loss: 0.007443644106388092\n",
      "[step: 608] loss: 148.00405883789062\n",
      "[step: 608] loss: 0.0074418275617063046\n",
      "[step: 609] loss: 148.3007049560547\n",
      "[step: 609] loss: 0.0074399253353476524\n",
      "[step: 610] loss: 148.63485717773438\n",
      "[step: 610] loss: 0.00743792112916708\n",
      "[step: 611] loss: 149.10870361328125\n",
      "[step: 611] loss: 0.007436136715114117\n",
      "[step: 612] loss: 148.9556121826172\n",
      "[step: 612] loss: 0.007434448692947626\n",
      "[step: 613] loss: 148.38900756835938\n",
      "[step: 613] loss: 0.007432606536895037\n",
      "[step: 614] loss: 147.2719268798828\n",
      "[step: 614] loss: 0.007430702447891235\n",
      "[step: 615] loss: 146.6126251220703\n",
      "[step: 615] loss: 0.0074288975447416306\n",
      "[step: 616] loss: 146.71182250976562\n",
      "[step: 616] loss: 0.007427085656672716\n",
      "[step: 617] loss: 147.15805053710938\n",
      "[step: 617] loss: 0.007425177376717329\n",
      "[step: 618] loss: 147.3634033203125\n",
      "[step: 618] loss: 0.00742332125082612\n",
      "[step: 619] loss: 146.8738250732422\n",
      "[step: 619] loss: 0.007421561051160097\n",
      "[step: 620] loss: 146.2314453125\n",
      "[step: 620] loss: 0.007419790606945753\n",
      "[step: 621] loss: 145.95440673828125\n",
      "[step: 621] loss: 0.0074179754592478275\n",
      "[step: 622] loss: 146.1309356689453\n",
      "[step: 622] loss: 0.007416213862597942\n",
      "[step: 623] loss: 146.34458923339844\n",
      "[step: 623] loss: 0.007414514198899269\n",
      "[step: 624] loss: 146.15724182128906\n",
      "[step: 624] loss: 0.007412787992507219\n",
      "[step: 625] loss: 145.7344512939453\n",
      "[step: 625] loss: 0.007411043159663677\n",
      "[step: 626] loss: 145.41383361816406\n",
      "[step: 626] loss: 0.007409356068819761\n",
      "[step: 627] loss: 145.4002685546875\n",
      "[step: 627] loss: 0.007407705299556255\n",
      "[step: 628] loss: 145.51319885253906\n",
      "[step: 628] loss: 0.0074060698971152306\n",
      "[step: 629] loss: 145.45245361328125\n",
      "[step: 629] loss: 0.0074044764041900635\n",
      "[step: 630] loss: 145.19973754882812\n",
      "[step: 630] loss: 0.007403023540973663\n",
      "[step: 631] loss: 144.9129180908203\n",
      "[step: 631] loss: 0.007401714101433754\n",
      "[step: 632] loss: 144.78482055664062\n",
      "[step: 632] loss: 0.007400699891149998\n",
      "[step: 633] loss: 144.79331970214844\n",
      "[step: 633] loss: 0.007400055881589651\n",
      "[step: 634] loss: 144.779296875\n",
      "[step: 634] loss: 0.007400150410830975\n",
      "[step: 635] loss: 144.65414428710938\n",
      "[step: 635] loss: 0.0074006859213113785\n",
      "[step: 636] loss: 144.4384307861328\n",
      "[step: 636] loss: 0.007401714101433754\n",
      "[step: 637] loss: 144.2537384033203\n",
      "[step: 637] loss: 0.007400968577712774\n",
      "[step: 638] loss: 144.15951538085938\n",
      "[step: 638] loss: 0.007398067973554134\n",
      "[step: 639] loss: 144.12225341796875\n",
      "[step: 639] loss: 0.007392114959657192\n",
      "[step: 640] loss: 144.0688018798828\n",
      "[step: 640] loss: 0.007386764977127314\n",
      "[step: 641] loss: 143.95068359375\n",
      "[step: 641] loss: 0.007384430151432753\n",
      "[step: 642] loss: 143.79244995117188\n",
      "[step: 642] loss: 0.007384938187897205\n",
      "[step: 643] loss: 143.6376953125\n",
      "[step: 643] loss: 0.0073857600800693035\n",
      "[step: 644] loss: 143.52259826660156\n",
      "[step: 644] loss: 0.007384187541902065\n",
      "[step: 645] loss: 143.44500732421875\n",
      "[step: 645] loss: 0.007380584254860878\n",
      "[step: 646] loss: 143.37698364257812\n",
      "[step: 646] loss: 0.007376824505627155\n",
      "[step: 647] loss: 143.29380798339844\n",
      "[step: 647] loss: 0.007374997716397047\n",
      "[step: 648] loss: 143.1824493408203\n",
      "[step: 648] loss: 0.00737485196441412\n",
      "[step: 649] loss: 143.0548095703125\n",
      "[step: 649] loss: 0.007374465931206942\n",
      "[step: 650] loss: 142.92526245117188\n",
      "[step: 650] loss: 0.007372693158686161\n",
      "[step: 651] loss: 142.80776977539062\n",
      "[step: 651] loss: 0.007369765546172857\n",
      "[step: 652] loss: 142.70635986328125\n",
      "[step: 652] loss: 0.0073672328144311905\n",
      "[step: 653] loss: 142.6166229248047\n",
      "[step: 653] loss: 0.0073659117333590984\n",
      "[step: 654] loss: 142.53143310546875\n",
      "[step: 654] loss: 0.007365285884588957\n",
      "[step: 655] loss: 142.443359375\n",
      "[step: 655] loss: 0.007364295423030853\n",
      "[step: 656] loss: 142.35003662109375\n",
      "[step: 656] loss: 0.007362413220107555\n",
      "[step: 657] loss: 142.24923706054688\n",
      "[step: 657] loss: 0.007360135205090046\n",
      "[step: 658] loss: 142.14456176757812\n",
      "[step: 658] loss: 0.007358172908425331\n",
      "[step: 659] loss: 142.03616333007812\n",
      "[step: 659] loss: 0.007356863934546709\n",
      "[step: 660] loss: 141.92794799804688\n",
      "[step: 660] loss: 0.007355891168117523\n",
      "[step: 661] loss: 141.82029724121094\n",
      "[step: 661] loss: 0.007354691158980131\n",
      "[step: 662] loss: 141.71450805664062\n",
      "[step: 662] loss: 0.007353071589022875\n",
      "[step: 663] loss: 141.6104736328125\n",
      "[step: 663] loss: 0.007351189386099577\n",
      "[step: 664] loss: 141.50823974609375\n",
      "[step: 664] loss: 0.0073494138196110725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 665] loss: 141.40707397460938\n",
      "[step: 665] loss: 0.007347951177507639\n",
      "[step: 666] loss: 141.30703735351562\n",
      "[step: 666] loss: 0.007346730679273605\n",
      "[step: 667] loss: 141.20779418945312\n",
      "[step: 667] loss: 0.007345525547862053\n",
      "[step: 668] loss: 141.10906982421875\n",
      "[step: 668] loss: 0.007344162557274103\n",
      "[step: 669] loss: 141.0107421875\n",
      "[step: 669] loss: 0.007342605385929346\n",
      "[step: 670] loss: 140.9129638671875\n",
      "[step: 670] loss: 0.007340962532907724\n",
      "[step: 671] loss: 140.8157958984375\n",
      "[step: 671] loss: 0.007339388132095337\n",
      "[step: 672] loss: 140.7197265625\n",
      "[step: 672] loss: 0.007337943185120821\n",
      "[step: 673] loss: 140.62542724609375\n",
      "[step: 673] loss: 0.007336617447435856\n",
      "[step: 674] loss: 140.53439331054688\n",
      "[step: 674] loss: 0.007335338741540909\n",
      "[step: 675] loss: 140.450439453125\n",
      "[step: 675] loss: 0.007334026973694563\n",
      "[step: 676] loss: 140.38101196289062\n",
      "[step: 676] loss: 0.007332663983106613\n",
      "[step: 677] loss: 140.3421630859375\n",
      "[step: 677] loss: 0.0073312316089868546\n",
      "[step: 678] loss: 140.37586975097656\n",
      "[step: 678] loss: 0.007329766638576984\n",
      "[step: 679] loss: 140.55160522460938\n",
      "[step: 679] loss: 0.007328297011554241\n",
      "[step: 680] loss: 141.08294677734375\n",
      "[step: 680] loss: 0.007326880469918251\n",
      "[step: 681] loss: 142.11439514160156\n",
      "[step: 681] loss: 0.007325487211346626\n",
      "[step: 682] loss: 144.22439575195312\n",
      "[step: 682] loss: 0.007324135396629572\n",
      "[step: 683] loss: 145.5201416015625\n",
      "[step: 683] loss: 0.00732281431555748\n",
      "[step: 684] loss: 145.45545959472656\n",
      "[step: 684] loss: 0.007321505807340145\n",
      "[step: 685] loss: 141.3543243408203\n",
      "[step: 685] loss: 0.0073202126659452915\n",
      "[step: 686] loss: 139.43206787109375\n",
      "[step: 686] loss: 0.0073189083486795425\n",
      "[step: 687] loss: 141.3430633544922\n",
      "[step: 687] loss: 0.00731761334463954\n",
      "[step: 688] loss: 142.54299926757812\n",
      "[step: 688] loss: 0.007316318806260824\n",
      "[step: 689] loss: 140.910400390625\n",
      "[step: 689] loss: 0.007315023802220821\n",
      "[step: 690] loss: 139.07583618164062\n",
      "[step: 690] loss: 0.007313745561987162\n",
      "[step: 691] loss: 140.25401306152344\n",
      "[step: 691] loss: 0.007312486879527569\n",
      "[step: 692] loss: 141.25436401367188\n",
      "[step: 692] loss: 0.007311250548809767\n",
      "[step: 693] loss: 139.51400756835938\n",
      "[step: 693] loss: 0.007310059387236834\n",
      "[step: 694] loss: 138.82415771484375\n",
      "[step: 694] loss: 0.007308914791792631\n",
      "[step: 695] loss: 139.96099853515625\n",
      "[step: 695] loss: 0.007307865656912327\n",
      "[step: 696] loss: 139.72752380371094\n",
      "[step: 696] loss: 0.0073069240897893906\n",
      "[step: 697] loss: 138.5686798095703\n",
      "[step: 697] loss: 0.007306178566068411\n",
      "[step: 698] loss: 138.72705078125\n",
      "[step: 698] loss: 0.007305616978555918\n",
      "[step: 699] loss: 139.2736053466797\n",
      "[step: 699] loss: 0.007305428851395845\n",
      "[step: 700] loss: 138.69082641601562\n",
      "[step: 700] loss: 0.007305350620299578\n",
      "[step: 701] loss: 138.12933349609375\n",
      "[step: 701] loss: 0.007305535487830639\n",
      "[step: 702] loss: 138.55001831054688\n",
      "[step: 702] loss: 0.007304989267140627\n",
      "[step: 703] loss: 138.62330627441406\n",
      "[step: 703] loss: 0.007303770165890455\n",
      "[step: 704] loss: 137.9737091064453\n",
      "[step: 704] loss: 0.007300838828086853\n",
      "[step: 705] loss: 137.90603637695312\n",
      "[step: 705] loss: 0.007297267206013203\n",
      "[step: 706] loss: 138.2100830078125\n",
      "[step: 706] loss: 0.007293878123164177\n",
      "[step: 707] loss: 137.90330505371094\n",
      "[step: 707] loss: 0.007291816174983978\n",
      "[step: 708] loss: 137.53048706054688\n",
      "[step: 708] loss: 0.007291153073310852\n",
      "[step: 709] loss: 137.65625\n",
      "[step: 709] loss: 0.00729120010510087\n",
      "[step: 710] loss: 137.6945037841797\n",
      "[step: 710] loss: 0.007291092537343502\n",
      "[step: 711] loss: 137.37376403808594\n",
      "[step: 711] loss: 0.007290024310350418\n",
      "[step: 712] loss: 137.21102905273438\n",
      "[step: 712] loss: 0.00728810578584671\n",
      "[step: 713] loss: 137.31268310546875\n",
      "[step: 713] loss: 0.0072856745682656765\n",
      "[step: 714] loss: 137.22735595703125\n",
      "[step: 714] loss: 0.00728357071056962\n",
      "[step: 715] loss: 136.96441650390625\n",
      "[step: 715] loss: 0.0072821807116270065\n",
      "[step: 716] loss: 136.89971923828125\n",
      "[step: 716] loss: 0.007281437981873751\n",
      "[step: 717] loss: 136.93963623046875\n",
      "[step: 717] loss: 0.007280918303877115\n",
      "[step: 718] loss: 136.79949951171875\n",
      "[step: 718] loss: 0.007280170451849699\n",
      "[step: 719] loss: 136.60931396484375\n",
      "[step: 719] loss: 0.007278988603502512\n",
      "[step: 720] loss: 136.5684814453125\n",
      "[step: 720] loss: 0.007277381140738726\n",
      "[step: 721] loss: 136.55450439453125\n",
      "[step: 721] loss: 0.007275660987943411\n",
      "[step: 722] loss: 136.42202758789062\n",
      "[step: 722] loss: 0.007274067495018244\n",
      "[step: 723] loss: 136.27325439453125\n",
      "[step: 723] loss: 0.007272779475897551\n",
      "[step: 724] loss: 136.224365234375\n",
      "[step: 724] loss: 0.007271769922226667\n",
      "[step: 725] loss: 136.18743896484375\n",
      "[step: 725] loss: 0.00727090472355485\n",
      "[step: 726] loss: 136.06924438476562\n",
      "[step: 726] loss: 0.007270032539963722\n",
      "[step: 727] loss: 135.94198608398438\n",
      "[step: 727] loss: 0.00726901413872838\n",
      "[step: 728] loss: 135.87860107421875\n",
      "[step: 728] loss: 0.007267857901751995\n",
      "[step: 729] loss: 135.8287353515625\n",
      "[step: 729] loss: 0.007266547530889511\n",
      "[step: 730] loss: 135.73101806640625\n",
      "[step: 730] loss: 0.007265195250511169\n",
      "[step: 731] loss: 135.61500549316406\n",
      "[step: 731] loss: 0.007263851817697287\n",
      "[step: 732] loss: 135.53636169433594\n",
      "[step: 732] loss: 0.007262573577463627\n",
      "[step: 733] loss: 135.47911071777344\n",
      "[step: 733] loss: 0.0072613968513906\n",
      "[step: 734] loss: 135.39671325683594\n",
      "[step: 734] loss: 0.007260289043188095\n",
      "[step: 735] loss: 135.29244995117188\n",
      "[step: 735] loss: 0.007259241305291653\n",
      "[step: 736] loss: 135.20184326171875\n",
      "[step: 736] loss: 0.007258220110088587\n",
      "[step: 737] loss: 135.13369750976562\n",
      "[step: 737] loss: 0.00725721986964345\n",
      "[step: 738] loss: 135.06224060058594\n",
      "[step: 738] loss: 0.007256214041262865\n",
      "[step: 739] loss: 134.9716796875\n",
      "[step: 739] loss: 0.007255218457430601\n",
      "[step: 740] loss: 134.877197265625\n",
      "[step: 740] loss: 0.007254214491695166\n",
      "[step: 741] loss: 134.79632568359375\n",
      "[step: 741] loss: 0.007253209128975868\n",
      "[step: 742] loss: 134.724853515625\n",
      "[step: 742] loss: 0.007252219133079052\n",
      "[step: 743] loss: 134.64715576171875\n",
      "[step: 743] loss: 0.007251253351569176\n",
      "[step: 744] loss: 134.55926513671875\n",
      "[step: 744] loss: 0.007250307127833366\n",
      "[step: 745] loss: 134.47096252441406\n",
      "[step: 745] loss: 0.007249407470226288\n",
      "[step: 746] loss: 134.39102172851562\n",
      "[step: 746] loss: 0.007248545531183481\n",
      "[step: 747] loss: 134.31600952148438\n",
      "[step: 747] loss: 0.00724776741117239\n",
      "[step: 748] loss: 134.238037109375\n",
      "[step: 748] loss: 0.007247021421790123\n",
      "[step: 749] loss: 134.15423583984375\n",
      "[step: 749] loss: 0.007246396504342556\n",
      "[step: 750] loss: 134.06874084472656\n",
      "[step: 750] loss: 0.007245740853250027\n",
      "[step: 751] loss: 133.9872283935547\n",
      "[step: 751] loss: 0.007245186250656843\n",
      "[step: 752] loss: 133.90985107421875\n",
      "[step: 752] loss: 0.007244419772177935\n",
      "[step: 753] loss: 133.83248901367188\n",
      "[step: 753] loss: 0.007243590895086527\n",
      "[step: 754] loss: 133.7523956298828\n",
      "[step: 754] loss: 0.0072423131205141544\n",
      "[step: 755] loss: 133.669921875\n",
      "[step: 755] loss: 0.0072408257983624935\n",
      "[step: 756] loss: 133.58767700195312\n",
      "[step: 756] loss: 0.0072389403358101845\n",
      "[step: 757] loss: 133.50747680664062\n",
      "[step: 757] loss: 0.007237033918499947\n",
      "[step: 758] loss: 133.42918395996094\n",
      "[step: 758] loss: 0.0072352043353021145\n",
      "[step: 759] loss: 133.35137939453125\n",
      "[step: 759] loss: 0.007233666721731424\n",
      "[step: 760] loss: 133.27218627929688\n",
      "[step: 760] loss: 0.0072324699722230434\n",
      "[step: 761] loss: 133.1918182373047\n",
      "[step: 761] loss: 0.007231563329696655\n",
      "[step: 762] loss: 133.1111602783203\n",
      "[step: 762] loss: 0.007230855990201235\n",
      "[step: 763] loss: 133.03094482421875\n",
      "[step: 763] loss: 0.0072302184998989105\n",
      "[step: 764] loss: 132.95172119140625\n",
      "[step: 764] loss: 0.007229591719806194\n",
      "[step: 765] loss: 132.873291015625\n",
      "[step: 765] loss: 0.00722886947914958\n",
      "[step: 766] loss: 132.79527282714844\n",
      "[step: 766] loss: 0.007228081580251455\n",
      "[step: 767] loss: 132.7168426513672\n",
      "[step: 767] loss: 0.007227116730064154\n",
      "[step: 768] loss: 132.63787841796875\n",
      "[step: 768] loss: 0.007226082030683756\n",
      "[step: 769] loss: 132.55874633789062\n",
      "[step: 769] loss: 0.007224886678159237\n",
      "[step: 770] loss: 132.47952270507812\n",
      "[step: 770] loss: 0.0072236452251672745\n",
      "[step: 771] loss: 132.4005126953125\n",
      "[step: 771] loss: 0.007222354877740145\n",
      "[step: 772] loss: 132.32156372070312\n",
      "[step: 772] loss: 0.007221090607345104\n",
      "[step: 773] loss: 132.24317932128906\n",
      "[step: 773] loss: 0.007219856604933739\n",
      "[step: 774] loss: 132.16513061523438\n",
      "[step: 774] loss: 0.0072186896577477455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 775] loss: 132.0870361328125\n",
      "[step: 775] loss: 0.007217588368803263\n",
      "[step: 776] loss: 132.00930786132812\n",
      "[step: 776] loss: 0.0072165364399552345\n",
      "[step: 777] loss: 131.93148803710938\n",
      "[step: 777] loss: 0.007215545047074556\n",
      "[step: 778] loss: 131.85374450683594\n",
      "[step: 778] loss: 0.0072145843878388405\n",
      "[step: 779] loss: 131.77618408203125\n",
      "[step: 779] loss: 0.007213653065264225\n",
      "[step: 780] loss: 131.69857788085938\n",
      "[step: 780] loss: 0.007212745491415262\n",
      "[step: 781] loss: 131.6212158203125\n",
      "[step: 781] loss: 0.00721186026930809\n",
      "[step: 782] loss: 131.5439910888672\n",
      "[step: 782] loss: 0.007211010903120041\n",
      "[step: 783] loss: 131.46719360351562\n",
      "[step: 783] loss: 0.0072102113626897335\n",
      "[step: 784] loss: 131.39071655273438\n",
      "[step: 784] loss: 0.007209478411823511\n",
      "[step: 785] loss: 131.31503295898438\n",
      "[step: 785] loss: 0.007208876311779022\n",
      "[step: 786] loss: 131.24069213867188\n",
      "[step: 786] loss: 0.007208444643765688\n",
      "[step: 787] loss: 131.1686248779297\n",
      "[step: 787] loss: 0.007208321243524551\n",
      "[step: 788] loss: 131.10055541992188\n",
      "[step: 788] loss: 0.007208560593426228\n",
      "[step: 789] loss: 131.03944396972656\n",
      "[step: 789] loss: 0.00720947002992034\n",
      "[step: 790] loss: 130.99313354492188\n",
      "[step: 790] loss: 0.007210673298686743\n",
      "[step: 791] loss: 130.97238159179688\n",
      "[step: 791] loss: 0.007212448865175247\n",
      "[step: 792] loss: 131.00906372070312\n",
      "[step: 792] loss: 0.007212825119495392\n",
      "[step: 793] loss: 131.1432342529297\n",
      "[step: 793] loss: 0.007211724296212196\n",
      "[step: 794] loss: 131.50917053222656\n",
      "[step: 794] loss: 0.007207250688225031\n",
      "[step: 795] loss: 132.18795776367188\n",
      "[step: 795] loss: 0.0072019584476947784\n",
      "[step: 796] loss: 133.617919921875\n",
      "[step: 796] loss: 0.0071980319917202\n",
      "[step: 797] loss: 135.1498260498047\n",
      "[step: 797] loss: 0.007197157479822636\n",
      "[step: 798] loss: 137.01913452148438\n",
      "[step: 798] loss: 0.007198480423539877\n",
      "[step: 799] loss: 135.33409118652344\n",
      "[step: 799] loss: 0.00719969579949975\n",
      "[step: 800] loss: 132.33773803710938\n",
      "[step: 800] loss: 0.007199238054454327\n",
      "[step: 801] loss: 130.15219116210938\n",
      "[step: 801] loss: 0.0071965050883591175\n",
      "[step: 802] loss: 131.2199249267578\n",
      "[step: 802] loss: 0.007193346507847309\n",
      "[step: 803] loss: 133.29139709472656\n",
      "[step: 803] loss: 0.007191411219537258\n",
      "[step: 804] loss: 132.5262451171875\n",
      "[step: 804] loss: 0.007191189099103212\n",
      "[step: 805] loss: 130.45657348632812\n",
      "[step: 805] loss: 0.007191705517470837\n",
      "[step: 806] loss: 129.9280242919922\n",
      "[step: 806] loss: 0.007191524375230074\n",
      "[step: 807] loss: 131.21469116210938\n",
      "[step: 807] loss: 0.007190185133367777\n",
      "[step: 808] loss: 131.67337036132812\n",
      "[step: 808] loss: 0.007188055664300919\n",
      "[step: 809] loss: 130.21522521972656\n",
      "[step: 809] loss: 0.0071862610056996346\n",
      "[step: 810] loss: 129.53378295898438\n",
      "[step: 810] loss: 0.007185364607721567\n",
      "[step: 811] loss: 130.33299255371094\n",
      "[step: 811] loss: 0.007185142487287521\n",
      "[step: 812] loss: 130.5926971435547\n",
      "[step: 812] loss: 0.007184925023466349\n",
      "[step: 813] loss: 129.7633056640625\n",
      "[step: 813] loss: 0.007184137124568224\n",
      "[step: 814] loss: 129.23480224609375\n",
      "[step: 814] loss: 0.0071828062646090984\n",
      "[step: 815] loss: 129.711181640625\n",
      "[step: 815] loss: 0.007181307300925255\n",
      "[step: 816] loss: 129.9754638671875\n",
      "[step: 816] loss: 0.007180097978562117\n",
      "[step: 817] loss: 129.34722900390625\n",
      "[step: 817] loss: 0.007179322652518749\n",
      "[step: 818] loss: 128.9530487060547\n",
      "[step: 818] loss: 0.00717881741002202\n",
      "[step: 819] loss: 129.24314880371094\n",
      "[step: 819] loss: 0.007178278174251318\n",
      "[step: 820] loss: 129.37005615234375\n",
      "[step: 820] loss: 0.0071774739772081375\n",
      "[step: 821] loss: 128.9835205078125\n",
      "[step: 821] loss: 0.007176434621214867\n",
      "[step: 822] loss: 128.672607421875\n",
      "[step: 822] loss: 0.007175266742706299\n",
      "[step: 823] loss: 128.8152313232422\n",
      "[step: 823] loss: 0.00717418035492301\n",
      "[step: 824] loss: 128.93191528320312\n",
      "[step: 824] loss: 0.007173281628638506\n",
      "[step: 825] loss: 128.66055297851562\n",
      "[step: 825] loss: 0.007172541227191687\n",
      "[step: 826] loss: 128.39755249023438\n",
      "[step: 826] loss: 0.007171873468905687\n",
      "[step: 827] loss: 128.43310546875\n",
      "[step: 827] loss: 0.007171161938458681\n",
      "[step: 828] loss: 128.5081024169922\n",
      "[step: 828] loss: 0.007170368451625109\n",
      "[step: 829] loss: 128.36190795898438\n",
      "[step: 829] loss: 0.007169464137405157\n",
      "[step: 830] loss: 128.1355438232422\n",
      "[step: 830] loss: 0.007168509531766176\n",
      "[step: 831] loss: 128.083984375\n",
      "[step: 831] loss: 0.00716755073517561\n",
      "[step: 832] loss: 128.13140869140625\n",
      "[step: 832] loss: 0.007166633382439613\n",
      "[step: 833] loss: 128.0588836669922\n",
      "[step: 833] loss: 0.00716578122228384\n",
      "[step: 834] loss: 127.88489532470703\n",
      "[step: 834] loss: 0.007164977490901947\n",
      "[step: 835] loss: 127.77438354492188\n",
      "[step: 835] loss: 0.007164210081100464\n",
      "[step: 836] loss: 127.76887512207031\n",
      "[step: 836] loss: 0.007163450121879578\n",
      "[step: 837] loss: 127.748046875\n",
      "[step: 837] loss: 0.007162683177739382\n",
      "[step: 838] loss: 127.63528442382812\n",
      "[step: 838] loss: 0.00716189481317997\n",
      "[step: 839] loss: 127.50491333007812\n",
      "[step: 839] loss: 0.007161093410104513\n",
      "[step: 840] loss: 127.4383544921875\n",
      "[step: 840] loss: 0.007160269655287266\n",
      "[step: 841] loss: 127.41495513916016\n",
      "[step: 841] loss: 0.0071594370529055595\n",
      "[step: 842] loss: 127.36166381835938\n",
      "[step: 842] loss: 0.007158597931265831\n",
      "[step: 843] loss: 127.25831604003906\n",
      "[step: 843] loss: 0.007157761137932539\n",
      "[step: 844] loss: 127.15596008300781\n",
      "[step: 844] loss: 0.007156923413276672\n",
      "[step: 845] loss: 127.09243774414062\n",
      "[step: 845] loss: 0.0071561033837497234\n",
      "[step: 846] loss: 127.05108642578125\n",
      "[step: 846] loss: 0.007155278231948614\n",
      "[step: 847] loss: 126.992919921875\n",
      "[step: 847] loss: 0.00715445913374424\n",
      "[step: 848] loss: 126.90585327148438\n",
      "[step: 848] loss: 0.007153654936701059\n",
      "[step: 849] loss: 126.81488037109375\n",
      "[step: 849] loss: 0.007152842823415995\n",
      "[step: 850] loss: 126.74363708496094\n",
      "[step: 850] loss: 0.007152047473937273\n",
      "[step: 851] loss: 126.68943786621094\n",
      "[step: 851] loss: 0.007151258643716574\n",
      "[step: 852] loss: 126.633056640625\n",
      "[step: 852] loss: 0.007150463759899139\n",
      "[step: 853] loss: 126.56120300292969\n",
      "[step: 853] loss: 0.00714968703687191\n",
      "[step: 854] loss: 126.47955322265625\n",
      "[step: 854] loss: 0.007148921489715576\n",
      "[step: 855] loss: 126.4013442993164\n",
      "[step: 855] loss: 0.007148161996155977\n",
      "[step: 856] loss: 126.3344497680664\n",
      "[step: 856] loss: 0.0071474346332252026\n",
      "[step: 857] loss: 126.27470397949219\n",
      "[step: 857] loss: 0.007146749645471573\n",
      "[step: 858] loss: 126.21273803710938\n",
      "[step: 858] loss: 0.007146129384636879\n",
      "[step: 859] loss: 126.14350891113281\n",
      "[step: 859] loss: 0.007145637646317482\n",
      "[step: 860] loss: 126.06898498535156\n",
      "[step: 860] loss: 0.0071453554555773735\n",
      "[step: 861] loss: 125.99432373046875\n",
      "[step: 861] loss: 0.007145477458834648\n",
      "[step: 862] loss: 125.92440795898438\n",
      "[step: 862] loss: 0.007146194111555815\n",
      "[step: 863] loss: 125.85894775390625\n",
      "[step: 863] loss: 0.0071481033228337765\n",
      "[step: 864] loss: 125.79542541503906\n",
      "[step: 864] loss: 0.00715118320658803\n",
      "[step: 865] loss: 125.73039245605469\n",
      "[step: 865] loss: 0.007156236097216606\n",
      "[step: 866] loss: 125.66246032714844\n",
      "[step: 866] loss: 0.007159871514886618\n",
      "[step: 867] loss: 125.59202575683594\n",
      "[step: 867] loss: 0.007160958368331194\n",
      "[step: 868] loss: 125.52095031738281\n",
      "[step: 868] loss: 0.007153159007430077\n",
      "[step: 869] loss: 125.45084381103516\n",
      "[step: 869] loss: 0.007142763119190931\n",
      "[step: 870] loss: 125.38236999511719\n",
      "[step: 870] loss: 0.007136636879295111\n",
      "[step: 871] loss: 125.31568908691406\n",
      "[step: 871] loss: 0.007138640619814396\n",
      "[step: 872] loss: 125.24992370605469\n",
      "[step: 872] loss: 0.007143900729715824\n",
      "[step: 873] loss: 125.1841049194336\n",
      "[step: 873] loss: 0.007144440896809101\n",
      "[step: 874] loss: 125.11775207519531\n",
      "[step: 874] loss: 0.007139428053051233\n",
      "[step: 875] loss: 125.05094909667969\n",
      "[step: 875] loss: 0.007133621256798506\n",
      "[step: 876] loss: 124.9833984375\n",
      "[step: 876] loss: 0.007132900413125753\n",
      "[step: 877] loss: 124.91539764404297\n",
      "[step: 877] loss: 0.007135824766010046\n",
      "[step: 878] loss: 124.84719848632812\n",
      "[step: 878] loss: 0.007136653643101454\n",
      "[step: 879] loss: 124.77897644042969\n",
      "[step: 879] loss: 0.007133623585104942\n",
      "[step: 880] loss: 124.7110595703125\n",
      "[step: 880] loss: 0.007129755802452564\n",
      "[step: 881] loss: 124.64300537109375\n",
      "[step: 881] loss: 0.0071289860643446445\n",
      "[step: 882] loss: 124.57552337646484\n",
      "[step: 882] loss: 0.007130487821996212\n",
      "[step: 883] loss: 124.50802612304688\n",
      "[step: 883] loss: 0.0071306838653981686\n",
      "[step: 884] loss: 124.44088745117188\n",
      "[step: 884] loss: 0.007128493394702673\n",
      "[step: 885] loss: 124.37362670898438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 885] loss: 0.007125932723283768\n",
      "[step: 886] loss: 124.30665588378906\n",
      "[step: 886] loss: 0.007125257048755884\n",
      "[step: 887] loss: 124.23974609375\n",
      "[step: 887] loss: 0.007125903852283955\n",
      "[step: 888] loss: 124.17301177978516\n",
      "[step: 888] loss: 0.007125713396817446\n",
      "[step: 889] loss: 124.1065444946289\n",
      "[step: 889] loss: 0.007124109193682671\n",
      "[step: 890] loss: 124.04060363769531\n",
      "[step: 890] loss: 0.007122281473129988\n",
      "[step: 891] loss: 123.97541046142578\n",
      "[step: 891] loss: 0.007121562026441097\n",
      "[step: 892] loss: 123.91162109375\n",
      "[step: 892] loss: 0.007121656555682421\n",
      "[step: 893] loss: 123.85064697265625\n",
      "[step: 893] loss: 0.007121341768652201\n",
      "[step: 894] loss: 123.7950439453125\n",
      "[step: 894] loss: 0.007120173890143633\n",
      "[step: 895] loss: 123.7508316040039\n",
      "[step: 895] loss: 0.007118761073797941\n",
      "[step: 896] loss: 123.72932434082031\n",
      "[step: 896] loss: 0.007117928471416235\n",
      "[step: 897] loss: 123.75949096679688\n",
      "[step: 897] loss: 0.007117649540305138\n",
      "[step: 898] loss: 123.89002990722656\n",
      "[step: 898] loss: 0.0071172937750816345\n",
      "[step: 899] loss: 124.26823425292969\n",
      "[step: 899] loss: 0.007116454653441906\n",
      "[step: 900] loss: 125.05938720703125\n",
      "[step: 900] loss: 0.007115328684449196\n",
      "[step: 901] loss: 126.90348815917969\n",
      "[step: 901] loss: 0.0071144006215035915\n",
      "[step: 902] loss: 129.43777465820312\n",
      "[step: 902] loss: 0.0071138436906039715\n",
      "[step: 903] loss: 133.48475646972656\n",
      "[step: 903] loss: 0.007113417610526085\n",
      "[step: 904] loss: 132.202392578125\n",
      "[step: 904] loss: 0.007112790830433369\n",
      "[step: 905] loss: 128.00820922851562\n",
      "[step: 905] loss: 0.007111909333616495\n",
      "[step: 906] loss: 123.25827026367188\n",
      "[step: 906] loss: 0.007110997103154659\n",
      "[step: 907] loss: 124.7969970703125\n",
      "[step: 907] loss: 0.007110255304723978\n",
      "[step: 908] loss: 128.66415405273438\n",
      "[step: 908] loss: 0.007109675090759993\n",
      "[step: 909] loss: 126.6994400024414\n",
      "[step: 909] loss: 0.007109125144779682\n",
      "[step: 910] loss: 123.1860580444336\n",
      "[step: 910] loss: 0.0071084522642195225\n",
      "[step: 911] loss: 123.68862915039062\n",
      "[step: 911] loss: 0.007107648998498917\n",
      "[step: 912] loss: 125.82282257080078\n",
      "[step: 912] loss: 0.007106850389391184\n",
      "[step: 913] loss: 124.86373138427734\n",
      "[step: 913] loss: 0.00710612628608942\n",
      "[step: 914] loss: 122.62831115722656\n",
      "[step: 914] loss: 0.007105504162609577\n",
      "[step: 915] loss: 123.67681884765625\n",
      "[step: 915] loss: 0.007104899734258652\n",
      "[step: 916] loss: 124.91902160644531\n",
      "[step: 916] loss: 0.007104239426553249\n",
      "[step: 917] loss: 123.18003845214844\n",
      "[step: 917] loss: 0.007103524170815945\n",
      "[step: 918] loss: 122.45094299316406\n",
      "[step: 918] loss: 0.007102771196514368\n",
      "[step: 919] loss: 123.67880249023438\n",
      "[step: 919] loss: 0.007102048955857754\n",
      "[step: 920] loss: 123.4151840209961\n",
      "[step: 920] loss: 0.007101379334926605\n",
      "[step: 921] loss: 122.24534606933594\n",
      "[step: 921] loss: 0.007100735791027546\n",
      "[step: 922] loss: 122.505859375\n",
      "[step: 922] loss: 0.007100098300725222\n",
      "[step: 923] loss: 123.07955932617188\n",
      "[step: 923] loss: 0.007099432405084372\n",
      "[step: 924] loss: 122.4631118774414\n",
      "[step: 924] loss: 0.007098741829395294\n",
      "[step: 925] loss: 121.93485260009766\n",
      "[step: 925] loss: 0.007098033558577299\n",
      "[step: 926] loss: 122.41412353515625\n",
      "[step: 926] loss: 0.007097336463630199\n",
      "[step: 927] loss: 122.4916000366211\n",
      "[step: 927] loss: 0.0070966617204248905\n",
      "[step: 928] loss: 121.84536743164062\n",
      "[step: 928] loss: 0.007095999550074339\n",
      "[step: 929] loss: 121.8155288696289\n",
      "[step: 929] loss: 0.007095350883901119\n",
      "[step: 930] loss: 122.15811157226562\n",
      "[step: 930] loss: 0.007094694301486015\n",
      "[step: 931] loss: 121.87261962890625\n",
      "[step: 931] loss: 0.0070940288715064526\n",
      "[step: 932] loss: 121.5049057006836\n",
      "[step: 932] loss: 0.007093354128301144\n",
      "[step: 933] loss: 121.65126037597656\n",
      "[step: 933] loss: 0.007092672865837812\n",
      "[step: 934] loss: 121.73407745361328\n",
      "[step: 934] loss: 0.007092001382261515\n",
      "[step: 935] loss: 121.43779754638672\n",
      "[step: 935] loss: 0.00709132943302393\n",
      "[step: 936] loss: 121.27415466308594\n",
      "[step: 936] loss: 0.0070906709879636765\n",
      "[step: 937] loss: 121.40184020996094\n",
      "[step: 937] loss: 0.007090010680258274\n",
      "[step: 938] loss: 121.3660659790039\n",
      "[step: 938] loss: 0.007089357823133469\n",
      "[step: 939] loss: 121.11894226074219\n",
      "[step: 939] loss: 0.007088702172040939\n",
      "[step: 940] loss: 121.05316162109375\n",
      "[step: 940] loss: 0.007088042329996824\n",
      "[step: 941] loss: 121.12507629394531\n",
      "[step: 941] loss: 0.007087386678904295\n",
      "[step: 942] loss: 121.02987670898438\n",
      "[step: 942] loss: 0.007086725439876318\n",
      "[step: 943] loss: 120.85191345214844\n",
      "[step: 943] loss: 0.0070860665291547775\n",
      "[step: 944] loss: 120.81320190429688\n",
      "[step: 944] loss: 0.007085402961820364\n",
      "[step: 945] loss: 120.83406066894531\n",
      "[step: 945] loss: 0.007084743119776249\n",
      "[step: 946] loss: 120.74270629882812\n",
      "[step: 946] loss: 0.00708408560603857\n",
      "[step: 947] loss: 120.60530090332031\n",
      "[step: 947] loss: 0.007083426229655743\n",
      "[step: 948] loss: 120.56324768066406\n",
      "[step: 948] loss: 0.007082771044224501\n",
      "[step: 949] loss: 120.55824279785156\n",
      "[step: 949] loss: 0.00708211911842227\n",
      "[step: 950] loss: 120.47698974609375\n",
      "[step: 950] loss: 0.00708146532997489\n",
      "[step: 951] loss: 120.36476135253906\n",
      "[step: 951] loss: 0.007080817595124245\n",
      "[step: 952] loss: 120.31047821044922\n",
      "[step: 952] loss: 0.007080170791596174\n",
      "[step: 953] loss: 120.28782653808594\n",
      "[step: 953] loss: 0.0070795174688100815\n",
      "[step: 954] loss: 120.22404479980469\n",
      "[step: 954] loss: 0.007078870199620724\n",
      "[step: 955] loss: 120.12789154052734\n",
      "[step: 955] loss: 0.007078223861753941\n",
      "[step: 956] loss: 120.06095123291016\n",
      "[step: 956] loss: 0.007077583111822605\n",
      "[step: 957] loss: 120.02532958984375\n",
      "[step: 957] loss: 0.007076932583004236\n",
      "[step: 958] loss: 119.97315979003906\n",
      "[step: 958] loss: 0.007076295558363199\n",
      "[step: 959] loss: 119.89305114746094\n",
      "[step: 959] loss: 0.007075656671077013\n",
      "[step: 960] loss: 119.81813049316406\n",
      "[step: 960] loss: 0.007075023837387562\n",
      "[step: 961] loss: 119.76808166503906\n",
      "[step: 961] loss: 0.007074388209730387\n",
      "[step: 962] loss: 119.72152709960938\n",
      "[step: 962] loss: 0.007073772605508566\n",
      "[step: 963] loss: 119.6566162109375\n",
      "[step: 963] loss: 0.007073169108480215\n",
      "[step: 964] loss: 119.58245849609375\n",
      "[step: 964] loss: 0.007072587963193655\n",
      "[step: 965] loss: 119.51919555664062\n",
      "[step: 965] loss: 0.0070720543153584\n",
      "[step: 966] loss: 119.46833038330078\n",
      "[step: 966] loss: 0.007071592845022678\n",
      "[step: 967] loss: 119.41413879394531\n",
      "[step: 967] loss: 0.007071253843605518\n",
      "[step: 968] loss: 119.34892272949219\n",
      "[step: 968] loss: 0.007071156986057758\n",
      "[step: 969] loss: 119.28062438964844\n",
      "[step: 969] loss: 0.007071439642459154\n",
      "[step: 970] loss: 119.2198486328125\n",
      "[step: 970] loss: 0.007072465028613806\n",
      "[step: 971] loss: 119.16554260253906\n",
      "[step: 971] loss: 0.007074461318552494\n",
      "[step: 972] loss: 119.10926818847656\n",
      "[step: 972] loss: 0.007078306283801794\n",
      "[step: 973] loss: 119.04652404785156\n",
      "[step: 973] loss: 0.007083156146109104\n",
      "[step: 974] loss: 118.98164367675781\n",
      "[step: 974] loss: 0.007089266553521156\n",
      "[step: 975] loss: 118.92029571533203\n",
      "[step: 975] loss: 0.007089931983500719\n",
      "[step: 976] loss: 118.86322021484375\n",
      "[step: 976] loss: 0.007084897719323635\n",
      "[step: 977] loss: 118.8062744140625\n",
      "[step: 977] loss: 0.007072774693369865\n",
      "[step: 978] loss: 118.74632263183594\n",
      "[step: 978] loss: 0.007064300123602152\n",
      "[step: 979] loss: 118.68374633789062\n",
      "[step: 979] loss: 0.007064757868647575\n",
      "[step: 980] loss: 118.62161254882812\n",
      "[step: 980] loss: 0.007070532534271479\n",
      "[step: 981] loss: 118.56205749511719\n",
      "[step: 981] loss: 0.007073813583701849\n",
      "[step: 982] loss: 118.50408935546875\n",
      "[step: 982] loss: 0.007069327402859926\n",
      "[step: 983] loss: 118.44572448730469\n",
      "[step: 983] loss: 0.007062563672661781\n",
      "[step: 984] loss: 118.3857421875\n",
      "[step: 984] loss: 0.007060198113322258\n",
      "[step: 985] loss: 118.32461547851562\n",
      "[step: 985] loss: 0.007063003722578287\n",
      "[step: 986] loss: 118.26351928710938\n",
      "[step: 986] loss: 0.007065543904900551\n",
      "[step: 987] loss: 118.20354461669922\n",
      "[step: 987] loss: 0.007063295226544142\n",
      "[step: 988] loss: 118.14460754394531\n",
      "[step: 988] loss: 0.007058960385620594\n",
      "[step: 989] loss: 118.08580017089844\n",
      "[step: 989] loss: 0.007057103328406811\n",
      "[step: 990] loss: 118.02659606933594\n",
      "[step: 990] loss: 0.007058547809720039\n",
      "[step: 991] loss: 117.9666976928711\n",
      "[step: 991] loss: 0.007059847004711628\n",
      "[step: 992] loss: 117.90621185302734\n",
      "[step: 992] loss: 0.007058244198560715\n",
      "[step: 993] loss: 117.84590148925781\n",
      "[step: 993] loss: 0.007055401802062988\n",
      "[step: 994] loss: 117.78596496582031\n",
      "[step: 994] loss: 0.007054093759506941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 995] loss: 117.72652435302734\n",
      "[step: 995] loss: 0.007054754067212343\n",
      "[step: 996] loss: 117.66729736328125\n",
      "[step: 996] loss: 0.007055283058434725\n",
      "[step: 997] loss: 117.60818481445312\n",
      "[step: 997] loss: 0.007054096553474665\n",
      "[step: 998] loss: 117.54869079589844\n",
      "[step: 998] loss: 0.007052146829664707\n",
      "[step: 999] loss: 117.48896789550781\n",
      "[step: 999] loss: 0.007051099557429552\n",
      "[step: 1000] loss: 117.4291763305664\n",
      "[step: 1000] loss: 0.007051229476928711\n",
      "[step: 1001] loss: 117.36930847167969\n",
      "[step: 1001] loss: 0.0070513589307665825\n",
      "[step: 1002] loss: 117.3094482421875\n",
      "[step: 1002] loss: 0.007050495129078627\n",
      "[step: 1003] loss: 117.24960327148438\n",
      "[step: 1003] loss: 0.0070491028018295765\n",
      "[step: 1004] loss: 117.19001770019531\n",
      "[step: 1004] loss: 0.007048126310110092\n",
      "[step: 1005] loss: 117.1303482055664\n",
      "[step: 1005] loss: 0.007047885097563267\n",
      "[step: 1006] loss: 117.07086181640625\n",
      "[step: 1006] loss: 0.007047777529805899\n",
      "[step: 1007] loss: 117.01152038574219\n",
      "[step: 1007] loss: 0.007047176361083984\n",
      "[step: 1008] loss: 116.95199584960938\n",
      "[step: 1008] loss: 0.00704614631831646\n",
      "[step: 1009] loss: 116.89250946044922\n",
      "[step: 1009] loss: 0.007045218721032143\n",
      "[step: 1010] loss: 116.83316040039062\n",
      "[step: 1010] loss: 0.007044703233987093\n",
      "[step: 1011] loss: 116.77378845214844\n",
      "[step: 1011] loss: 0.007044418714940548\n",
      "[step: 1012] loss: 116.71444702148438\n",
      "[step: 1012] loss: 0.007043969817459583\n",
      "[step: 1013] loss: 116.65519714355469\n",
      "[step: 1013] loss: 0.00704321451485157\n",
      "[step: 1014] loss: 116.59593200683594\n",
      "[step: 1014] loss: 0.007042371202260256\n",
      "[step: 1015] loss: 116.53689575195312\n",
      "[step: 1015] loss: 0.007041702046990395\n",
      "[step: 1016] loss: 116.47837829589844\n",
      "[step: 1016] loss: 0.007041234523057938\n",
      "[step: 1017] loss: 116.42013549804688\n",
      "[step: 1017] loss: 0.007040802855044603\n",
      "[step: 1018] loss: 116.3628921508789\n",
      "[step: 1018] loss: 0.007040240336209536\n",
      "[step: 1019] loss: 116.3072509765625\n",
      "[step: 1019] loss: 0.0070395381189882755\n",
      "[step: 1020] loss: 116.2545166015625\n",
      "[step: 1020] loss: 0.007038822863250971\n",
      "[step: 1021] loss: 116.20721435546875\n",
      "[step: 1021] loss: 0.007038217503577471\n",
      "[step: 1022] loss: 116.16978454589844\n",
      "[step: 1022] loss: 0.007037709001451731\n",
      "[step: 1023] loss: 116.152587890625\n",
      "[step: 1023] loss: 0.0070372167974710464\n",
      "[step: 1024] loss: 116.1713638305664\n",
      "[step: 1024] loss: 0.007036641240119934\n",
      "[step: 1025] loss: 116.27113342285156\n",
      "[step: 1025] loss: 0.007035999093204737\n",
      "[step: 1026] loss: 116.50682067871094\n",
      "[step: 1026] loss: 0.007035364862531424\n",
      "[step: 1027] loss: 117.07473754882812\n",
      "[step: 1027] loss: 0.007034752052277327\n",
      "[step: 1028] loss: 118.08160400390625\n",
      "[step: 1028] loss: 0.007034203968942165\n",
      "[step: 1029] loss: 120.22864532470703\n",
      "[step: 1029] loss: 0.007033669855445623\n",
      "[step: 1030] loss: 122.59587860107422\n",
      "[step: 1030] loss: 0.007033120840787888\n",
      "[step: 1031] loss: 126.02240753173828\n",
      "[step: 1031] loss: 0.007032525725662708\n",
      "[step: 1032] loss: 124.05511474609375\n",
      "[step: 1032] loss: 0.007031913381069899\n",
      "[step: 1033] loss: 119.96719360351562\n",
      "[step: 1033] loss: 0.007031315006315708\n",
      "[step: 1034] loss: 115.73883819580078\n",
      "[step: 1034] loss: 0.007030735723674297\n",
      "[step: 1035] loss: 116.70842742919922\n",
      "[step: 1035] loss: 0.007030182983726263\n",
      "[step: 1036] loss: 120.19340515136719\n",
      "[step: 1036] loss: 0.00702963350340724\n",
      "[step: 1037] loss: 119.62083435058594\n",
      "[step: 1037] loss: 0.007029068656265736\n",
      "[step: 1038] loss: 116.56649780273438\n",
      "[step: 1038] loss: 0.007028494495898485\n",
      "[step: 1039] loss: 115.32194519042969\n",
      "[step: 1039] loss: 0.007027905900031328\n",
      "[step: 1040] loss: 117.26524353027344\n",
      "[step: 1040] loss: 0.007027320563793182\n",
      "[step: 1041] loss: 118.27555847167969\n",
      "[step: 1041] loss: 0.007026745937764645\n",
      "[step: 1042] loss: 116.08796691894531\n",
      "[step: 1042] loss: 0.007026187609881163\n",
      "[step: 1043] loss: 115.04446411132812\n",
      "[step: 1043] loss: 0.007025623228400946\n",
      "[step: 1044] loss: 116.35336303710938\n",
      "[step: 1044] loss: 0.007025066297501326\n",
      "[step: 1045] loss: 116.71592712402344\n",
      "[step: 1045] loss: 0.007024508435279131\n",
      "[step: 1046] loss: 115.43220520019531\n",
      "[step: 1046] loss: 0.007023936603218317\n",
      "[step: 1047] loss: 114.82850646972656\n",
      "[step: 1047] loss: 0.007023365702480078\n",
      "[step: 1048] loss: 115.69731140136719\n",
      "[step: 1048] loss: 0.007022794336080551\n",
      "[step: 1049] loss: 115.96083068847656\n",
      "[step: 1049] loss: 0.007022224832326174\n",
      "[step: 1050] loss: 114.94358825683594\n",
      "[step: 1050] loss: 0.007021655794233084\n",
      "[step: 1051] loss: 114.62882995605469\n",
      "[step: 1051] loss: 0.0070210970006883144\n",
      "[step: 1052] loss: 115.24406433105469\n",
      "[step: 1052] loss: 0.007020541932433844\n",
      "[step: 1053] loss: 115.2200698852539\n",
      "[step: 1053] loss: 0.007019973825663328\n",
      "[step: 1054] loss: 114.56013488769531\n",
      "[step: 1054] loss: 0.007019416429102421\n",
      "[step: 1055] loss: 114.4081039428711\n",
      "[step: 1055] loss: 0.007018860429525375\n",
      "[step: 1056] loss: 114.788330078125\n",
      "[step: 1056] loss: 0.007018295116722584\n",
      "[step: 1057] loss: 114.75794982910156\n",
      "[step: 1057] loss: 0.007017733063548803\n",
      "[step: 1058] loss: 114.2740478515625\n",
      "[step: 1058] loss: 0.007017170079052448\n",
      "[step: 1059] loss: 114.17080688476562\n",
      "[step: 1059] loss: 0.007016611285507679\n",
      "[step: 1060] loss: 114.41551208496094\n",
      "[step: 1060] loss: 0.0070160552859306335\n",
      "[step: 1061] loss: 114.3491439819336\n",
      "[step: 1061] loss: 0.00701548857614398\n",
      "[step: 1062] loss: 114.02261352539062\n",
      "[step: 1062] loss: 0.007014932110905647\n",
      "[step: 1063] loss: 113.92233276367188\n",
      "[step: 1063] loss: 0.0070143770426511765\n",
      "[step: 1064] loss: 114.05743408203125\n",
      "[step: 1064] loss: 0.00701382290571928\n",
      "[step: 1065] loss: 114.03370666503906\n",
      "[step: 1065] loss: 0.007013262249529362\n",
      "[step: 1066] loss: 113.7958755493164\n",
      "[step: 1066] loss: 0.007012706249952316\n",
      "[step: 1067] loss: 113.67938232421875\n",
      "[step: 1067] loss: 0.007012150716036558\n",
      "[step: 1068] loss: 113.7435531616211\n",
      "[step: 1068] loss: 0.007011602632701397\n",
      "[step: 1069] loss: 113.73434448242188\n",
      "[step: 1069] loss: 0.007011051755398512\n",
      "[step: 1070] loss: 113.57764434814453\n",
      "[step: 1070] loss: 0.007010499481111765\n",
      "[step: 1071] loss: 113.44828033447266\n",
      "[step: 1071] loss: 0.007009947206825018\n",
      "[step: 1072] loss: 113.44827270507812\n",
      "[step: 1072] loss: 0.007009392604231834\n",
      "[step: 1073] loss: 113.45590209960938\n",
      "[step: 1073] loss: 0.007008844520896673\n",
      "[step: 1074] loss: 113.35951232910156\n",
      "[step: 1074] loss: 0.007008287589997053\n",
      "[step: 1075] loss: 113.2337646484375\n",
      "[step: 1075] loss: 0.0070077464915812016\n",
      "[step: 1076] loss: 113.18050384521484\n",
      "[step: 1076] loss: 0.007007190492004156\n",
      "[step: 1077] loss: 113.17539978027344\n",
      "[step: 1077] loss: 0.007006650324910879\n",
      "[step: 1078] loss: 113.13035583496094\n",
      "[step: 1078] loss: 0.007006105966866016\n",
      "[step: 1079] loss: 113.03046417236328\n",
      "[step: 1079] loss: 0.007005567196756601\n",
      "[step: 1080] loss: 112.94396209716797\n",
      "[step: 1080] loss: 0.007005045190453529\n",
      "[step: 1081] loss: 112.90575408935547\n",
      "[step: 1081] loss: 0.007004525046795607\n",
      "[step: 1082] loss: 112.87837219238281\n",
      "[step: 1082] loss: 0.007004023529589176\n",
      "[step: 1083] loss: 112.8189697265625\n",
      "[step: 1083] loss: 0.007003550883382559\n",
      "[step: 1084] loss: 112.73416137695312\n",
      "[step: 1084] loss: 0.007003129459917545\n",
      "[step: 1085] loss: 112.66336059570312\n",
      "[step: 1085] loss: 0.007002778816968203\n",
      "[step: 1086] loss: 112.6197509765625\n",
      "[step: 1086] loss: 0.007002571132034063\n",
      "[step: 1087] loss: 112.58039093017578\n",
      "[step: 1087] loss: 0.007002576254308224\n",
      "[step: 1088] loss: 112.52293395996094\n",
      "[step: 1088] loss: 0.007002984639257193\n",
      "[step: 1089] loss: 112.4503173828125\n",
      "[step: 1089] loss: 0.007003962993621826\n",
      "[step: 1090] loss: 112.38304138183594\n",
      "[step: 1090] loss: 0.007006034720689058\n",
      "[step: 1091] loss: 112.33094787597656\n",
      "[step: 1091] loss: 0.007009189110249281\n",
      "[step: 1092] loss: 112.28567504882812\n",
      "[step: 1092] loss: 0.007014251314103603\n",
      "[step: 1093] loss: 112.23314666748047\n",
      "[step: 1093] loss: 0.007018767762929201\n",
      "[step: 1094] loss: 112.16990661621094\n",
      "[step: 1094] loss: 0.007022278383374214\n",
      "[step: 1095] loss: 112.10435485839844\n",
      "[step: 1095] loss: 0.0070179724134504795\n",
      "[step: 1096] loss: 112.04539489746094\n",
      "[step: 1096] loss: 0.007008926942944527\n",
      "[step: 1097] loss: 111.99366760253906\n",
      "[step: 1097] loss: 0.006998827680945396\n",
      "[step: 1098] loss: 111.94291687011719\n",
      "[step: 1098] loss: 0.006995438598096371\n",
      "[step: 1099] loss: 111.88751220703125\n",
      "[step: 1099] loss: 0.006999128498136997\n",
      "[step: 1100] loss: 111.8271255493164\n",
      "[step: 1100] loss: 0.007003959268331528\n",
      "[step: 1101] loss: 111.76568603515625\n",
      "[step: 1101] loss: 0.007004464976489544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1102] loss: 111.70735168457031\n",
      "[step: 1102] loss: 0.006999026983976364\n",
      "[step: 1103] loss: 111.65258026123047\n",
      "[step: 1103] loss: 0.006993602961301804\n",
      "[step: 1104] loss: 111.59931945800781\n",
      "[step: 1104] loss: 0.006992723327130079\n",
      "[step: 1105] loss: 111.54467010498047\n",
      "[step: 1105] loss: 0.006995487958192825\n",
      "[step: 1106] loss: 111.48751831054688\n",
      "[step: 1106] loss: 0.006997363176196814\n",
      "[step: 1107] loss: 111.42860412597656\n",
      "[step: 1107] loss: 0.00699528818950057\n",
      "[step: 1108] loss: 111.36970520019531\n",
      "[step: 1108] loss: 0.006991548463702202\n",
      "[step: 1109] loss: 111.31216430664062\n",
      "[step: 1109] loss: 0.006989676039665937\n",
      "[step: 1110] loss: 111.25621032714844\n",
      "[step: 1110] loss: 0.006990605499595404\n",
      "[step: 1111] loss: 111.20109558105469\n",
      "[step: 1111] loss: 0.006991987582296133\n",
      "[step: 1112] loss: 111.14579772949219\n",
      "[step: 1112] loss: 0.006991321686655283\n",
      "[step: 1113] loss: 111.08961486816406\n",
      "[step: 1113] loss: 0.0069890376180410385\n",
      "[step: 1114] loss: 111.03265380859375\n",
      "[step: 1114] loss: 0.006987179163843393\n",
      "[step: 1115] loss: 110.97496032714844\n",
      "[step: 1115] loss: 0.006987040396779776\n",
      "[step: 1116] loss: 110.91720581054688\n",
      "[step: 1116] loss: 0.006987735629081726\n",
      "[step: 1117] loss: 110.85971069335938\n",
      "[step: 1117] loss: 0.006987640634179115\n",
      "[step: 1118] loss: 110.80266571044922\n",
      "[step: 1118] loss: 0.006986355409026146\n",
      "[step: 1119] loss: 110.74612426757812\n",
      "[step: 1119] loss: 0.006984810344874859\n",
      "[step: 1120] loss: 110.68978118896484\n",
      "[step: 1120] loss: 0.006984102074056864\n",
      "[step: 1121] loss: 110.63357543945312\n",
      "[step: 1121] loss: 0.006984190084040165\n",
      "[step: 1122] loss: 110.57733917236328\n",
      "[step: 1122] loss: 0.006984220817685127\n",
      "[step: 1123] loss: 110.52084350585938\n",
      "[step: 1123] loss: 0.006983592174947262\n",
      "[step: 1124] loss: 110.46424865722656\n",
      "[step: 1124] loss: 0.006982471328228712\n",
      "[step: 1125] loss: 110.40742492675781\n",
      "[step: 1125] loss: 0.006981541868299246\n",
      "[step: 1126] loss: 110.35061645507812\n",
      "[step: 1126] loss: 0.006981143727898598\n",
      "[step: 1127] loss: 110.29364776611328\n",
      "[step: 1127] loss: 0.006981024984270334\n",
      "[step: 1128] loss: 110.23661804199219\n",
      "[step: 1128] loss: 0.0069807181134819984\n",
      "[step: 1129] loss: 110.17985534667969\n",
      "[step: 1129] loss: 0.006980027072131634\n",
      "[step: 1130] loss: 110.12291717529297\n",
      "[step: 1130] loss: 0.00697918189689517\n",
      "[step: 1131] loss: 110.066162109375\n",
      "[step: 1131] loss: 0.00697850389406085\n",
      "[step: 1132] loss: 110.00961303710938\n",
      "[step: 1132] loss: 0.00697809923440218\n",
      "[step: 1133] loss: 109.95317077636719\n",
      "[step: 1133] loss: 0.0069778007455170155\n",
      "[step: 1134] loss: 109.89732360839844\n",
      "[step: 1134] loss: 0.006977392360568047\n",
      "[step: 1135] loss: 109.84217834472656\n",
      "[step: 1135] loss: 0.0069767930544912815\n",
      "[step: 1136] loss: 109.78825378417969\n",
      "[step: 1136] loss: 0.006976103875786066\n",
      "[step: 1137] loss: 109.73640441894531\n",
      "[step: 1137] loss: 0.0069754961878061295\n",
      "[step: 1138] loss: 109.68826293945312\n",
      "[step: 1138] loss: 0.006975028198212385\n",
      "[step: 1139] loss: 109.64729309082031\n",
      "[step: 1139] loss: 0.006974636577069759\n",
      "[step: 1140] loss: 109.61846923828125\n",
      "[step: 1140] loss: 0.006974205374717712\n",
      "[step: 1141] loss: 109.61476135253906\n",
      "[step: 1141] loss: 0.006973684299737215\n",
      "[step: 1142] loss: 109.65389251708984\n",
      "[step: 1142] loss: 0.006973091047257185\n",
      "[step: 1143] loss: 109.78862762451172\n",
      "[step: 1143] loss: 0.006972518749535084\n",
      "[step: 1144] loss: 110.0751953125\n",
      "[step: 1144] loss: 0.006972003262490034\n",
      "[step: 1145] loss: 110.73334503173828\n",
      "[step: 1145] loss: 0.0069715422578155994\n",
      "[step: 1146] loss: 111.841552734375\n",
      "[step: 1146] loss: 0.006971095688641071\n",
      "[step: 1147] loss: 114.15438079833984\n",
      "[step: 1147] loss: 0.006970621645450592\n",
      "[step: 1148] loss: 116.59623718261719\n",
      "[step: 1148] loss: 0.006970107089728117\n",
      "[step: 1149] loss: 120.20243072509766\n",
      "[step: 1149] loss: 0.0069695631973445415\n",
      "[step: 1150] loss: 118.33892822265625\n",
      "[step: 1150] loss: 0.006969028618186712\n",
      "[step: 1151] loss: 114.36317443847656\n",
      "[step: 1151] loss: 0.006968518253415823\n",
      "[step: 1152] loss: 109.49787902832031\n",
      "[step: 1152] loss: 0.006968047469854355\n",
      "[step: 1153] loss: 109.78865051269531\n",
      "[step: 1153] loss: 0.006967579480260611\n",
      "[step: 1154] loss: 113.36692810058594\n",
      "[step: 1154] loss: 0.006967096123844385\n",
      "[step: 1155] loss: 113.69551849365234\n",
      "[step: 1155] loss: 0.0069665987975895405\n",
      "[step: 1156] loss: 110.95341491699219\n",
      "[step: 1156] loss: 0.00696609215810895\n",
      "[step: 1157] loss: 108.71424865722656\n",
      "[step: 1157] loss: 0.006965578068047762\n",
      "[step: 1158] loss: 110.17835235595703\n",
      "[step: 1158] loss: 0.0069650704972445965\n",
      "[step: 1159] loss: 111.98370361328125\n",
      "[step: 1159] loss: 0.006964581552892923\n",
      "[step: 1160] loss: 110.34059143066406\n",
      "[step: 1160] loss: 0.006964090745896101\n",
      "[step: 1161] loss: 108.54004669189453\n",
      "[step: 1161] loss: 0.0069636073894798756\n",
      "[step: 1162] loss: 109.19454956054688\n",
      "[step: 1162] loss: 0.006963122636079788\n",
      "[step: 1163] loss: 110.30537414550781\n",
      "[step: 1163] loss: 0.006962642073631287\n",
      "[step: 1164] loss: 109.64407348632812\n",
      "[step: 1164] loss: 0.006962139159440994\n",
      "[step: 1165] loss: 108.33490753173828\n",
      "[step: 1165] loss: 0.006961648352444172\n",
      "[step: 1166] loss: 108.69035339355469\n",
      "[step: 1166] loss: 0.006961144506931305\n",
      "[step: 1167] loss: 109.55223083496094\n",
      "[step: 1167] loss: 0.0069606490433216095\n",
      "[step: 1168] loss: 108.97642517089844\n",
      "[step: 1168] loss: 0.006960154511034489\n",
      "[step: 1169] loss: 108.10063171386719\n",
      "[step: 1169] loss: 0.006959677208214998\n",
      "[step: 1170] loss: 108.28616333007812\n",
      "[step: 1170] loss: 0.0069591933861374855\n",
      "[step: 1171] loss: 108.79180145263672\n",
      "[step: 1171] loss: 0.006958707235753536\n",
      "[step: 1172] loss: 108.50567626953125\n",
      "[step: 1172] loss: 0.006958217825740576\n",
      "[step: 1173] loss: 107.88226318359375\n",
      "[step: 1173] loss: 0.0069577363319695\n",
      "[step: 1174] loss: 107.95875549316406\n",
      "[step: 1174] loss: 0.006957246921956539\n",
      "[step: 1175] loss: 108.31716918945312\n",
      "[step: 1175] loss: 0.006956756114959717\n",
      "[step: 1176] loss: 108.10148620605469\n",
      "[step: 1176] loss: 0.006956270895898342\n",
      "[step: 1177] loss: 107.6651382446289\n",
      "[step: 1177] loss: 0.006955777760595083\n",
      "[step: 1178] loss: 107.64787292480469\n",
      "[step: 1178] loss: 0.006955297663807869\n",
      "[step: 1179] loss: 107.86368560791016\n",
      "[step: 1179] loss: 0.006954808719456196\n",
      "[step: 1180] loss: 107.78150939941406\n",
      "[step: 1180] loss: 0.006954316981136799\n",
      "[step: 1181] loss: 107.46017456054688\n",
      "[step: 1181] loss: 0.006953840609639883\n",
      "[step: 1182] loss: 107.37223815917969\n",
      "[step: 1182] loss: 0.0069533539935946465\n",
      "[step: 1183] loss: 107.50183868408203\n",
      "[step: 1183] loss: 0.006952876690775156\n",
      "[step: 1184] loss: 107.47486877441406\n",
      "[step: 1184] loss: 0.006952387280762196\n",
      "[step: 1185] loss: 107.2602767944336\n",
      "[step: 1185] loss: 0.006951913703233004\n",
      "[step: 1186] loss: 107.12608337402344\n",
      "[step: 1186] loss: 0.006951423361897469\n",
      "[step: 1187] loss: 107.16797637939453\n",
      "[step: 1187] loss: 0.006950946059077978\n",
      "[step: 1188] loss: 107.18875122070312\n",
      "[step: 1188] loss: 0.006950465962290764\n",
      "[step: 1189] loss: 107.05836486816406\n",
      "[step: 1189] loss: 0.006949980277568102\n",
      "[step: 1190] loss: 106.91073608398438\n",
      "[step: 1190] loss: 0.006949499249458313\n",
      "[step: 1191] loss: 106.87458801269531\n",
      "[step: 1191] loss: 0.006949024274945259\n",
      "[step: 1192] loss: 106.89088439941406\n",
      "[step: 1192] loss: 0.006948536727577448\n",
      "[step: 1193] loss: 106.83935546875\n",
      "[step: 1193] loss: 0.006948072463274002\n",
      "[step: 1194] loss: 106.71753692626953\n",
      "[step: 1194] loss: 0.006947584915906191\n",
      "[step: 1195] loss: 106.62701416015625\n",
      "[step: 1195] loss: 0.006947110872715712\n",
      "[step: 1196] loss: 106.60424041748047\n",
      "[step: 1196] loss: 0.006946641486138105\n",
      "[step: 1197] loss: 106.58622741699219\n",
      "[step: 1197] loss: 0.006946162320673466\n",
      "[step: 1198] loss: 106.51902770996094\n",
      "[step: 1198] loss: 0.006945697590708733\n",
      "[step: 1199] loss: 106.42198181152344\n",
      "[step: 1199] loss: 0.00694523099809885\n",
      "[step: 1200] loss: 106.35090637207031\n",
      "[step: 1200] loss: 0.006944774184376001\n",
      "[step: 1201] loss: 106.31672668457031\n",
      "[step: 1201] loss: 0.006944333203136921\n",
      "[step: 1202] loss: 106.283203125\n",
      "[step: 1202] loss: 0.0069439043290913105\n",
      "[step: 1203] loss: 106.22200012207031\n",
      "[step: 1203] loss: 0.006943505257368088\n",
      "[step: 1204] loss: 106.14199829101562\n",
      "[step: 1204] loss: 0.00694315368309617\n",
      "[step: 1205] loss: 106.07325744628906\n",
      "[step: 1205] loss: 0.0069428738206624985\n",
      "[step: 1206] loss: 106.02635955810547\n",
      "[step: 1206] loss: 0.006942718755453825\n",
      "[step: 1207] loss: 105.98593139648438\n",
      "[step: 1207] loss: 0.00694276811555028\n",
      "[step: 1208] loss: 105.93328094482422\n",
      "[step: 1208] loss: 0.006943120155483484\n",
      "[step: 1209] loss: 105.86607360839844\n",
      "[step: 1209] loss: 0.006944066379219294\n",
      "[step: 1210] loss: 105.79795837402344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1210] loss: 0.0069457245990633965\n",
      "[step: 1211] loss: 105.74037170410156\n",
      "[step: 1211] loss: 0.006948723923414946\n",
      "[step: 1212] loss: 105.69219207763672\n",
      "[step: 1212] loss: 0.006952632684260607\n",
      "[step: 1213] loss: 105.64389038085938\n",
      "[step: 1213] loss: 0.006958062294870615\n",
      "[step: 1214] loss: 105.58781433105469\n",
      "[step: 1214] loss: 0.006961225997656584\n",
      "[step: 1215] loss: 105.52543640136719\n",
      "[step: 1215] loss: 0.006961605977267027\n",
      "[step: 1216] loss: 105.4627685546875\n",
      "[step: 1216] loss: 0.006954153999686241\n",
      "[step: 1217] loss: 105.4053726196289\n",
      "[step: 1217] loss: 0.006944257766008377\n",
      "[step: 1218] loss: 105.35285949707031\n",
      "[step: 1218] loss: 0.006937050260603428\n",
      "[step: 1219] loss: 105.3012466430664\n",
      "[step: 1219] loss: 0.006936950609087944\n",
      "[step: 1220] loss: 105.2470703125\n",
      "[step: 1220] loss: 0.006941674277186394\n",
      "[step: 1221] loss: 105.18912506103516\n",
      "[step: 1221] loss: 0.006945137400180101\n",
      "[step: 1222] loss: 105.12930297851562\n",
      "[step: 1222] loss: 0.00694388197734952\n",
      "[step: 1223] loss: 105.07035064697266\n",
      "[step: 1223] loss: 0.006938283331692219\n",
      "[step: 1224] loss: 105.01373291015625\n",
      "[step: 1224] loss: 0.006933963391929865\n",
      "[step: 1225] loss: 104.95916748046875\n",
      "[step: 1225] loss: 0.006933967582881451\n",
      "[step: 1226] loss: 104.90522003173828\n",
      "[step: 1226] loss: 0.006936618126928806\n",
      "[step: 1227] loss: 104.85050201416016\n",
      "[step: 1227] loss: 0.006938086356967688\n",
      "[step: 1228] loss: 104.7943344116211\n",
      "[step: 1228] loss: 0.0069361100904643536\n",
      "[step: 1229] loss: 104.73690795898438\n",
      "[step: 1229] loss: 0.006932748481631279\n",
      "[step: 1230] loss: 104.67893981933594\n",
      "[step: 1230] loss: 0.006930912844836712\n",
      "[step: 1231] loss: 104.62124633789062\n",
      "[step: 1231] loss: 0.006931533571332693\n",
      "[step: 1232] loss: 104.5644302368164\n",
      "[step: 1232] loss: 0.006932871416211128\n",
      "[step: 1233] loss: 104.50823974609375\n",
      "[step: 1233] loss: 0.006932719610631466\n",
      "[step: 1234] loss: 104.45265197753906\n",
      "[step: 1234] loss: 0.006930958013981581\n",
      "[step: 1235] loss: 104.397216796875\n",
      "[step: 1235] loss: 0.0069289919920265675\n",
      "[step: 1236] loss: 104.34153747558594\n",
      "[step: 1236] loss: 0.006928320974111557\n",
      "[step: 1237] loss: 104.28559875488281\n",
      "[step: 1237] loss: 0.006928787101060152\n",
      "[step: 1238] loss: 104.22915649414062\n",
      "[step: 1238] loss: 0.006929138209670782\n",
      "[step: 1239] loss: 104.17251586914062\n",
      "[step: 1239] loss: 0.006928559858351946\n",
      "[step: 1240] loss: 104.1156005859375\n",
      "[step: 1240] loss: 0.006927209440618753\n",
      "[step: 1241] loss: 104.05867767333984\n",
      "[step: 1241] loss: 0.0069260732270777225\n",
      "[step: 1242] loss: 104.00165557861328\n",
      "[step: 1242] loss: 0.006925688590854406\n",
      "[step: 1243] loss: 103.94477844238281\n",
      "[step: 1243] loss: 0.006925793830305338\n",
      "[step: 1244] loss: 103.88783264160156\n",
      "[step: 1244] loss: 0.006925730500370264\n",
      "[step: 1245] loss: 103.8309326171875\n",
      "[step: 1245] loss: 0.006925112102180719\n",
      "[step: 1246] loss: 103.77405548095703\n",
      "[step: 1246] loss: 0.006924176588654518\n",
      "[step: 1247] loss: 103.71733093261719\n",
      "[step: 1247] loss: 0.006923374719917774\n",
      "[step: 1248] loss: 103.66047668457031\n",
      "[step: 1248] loss: 0.006922977510839701\n",
      "[step: 1249] loss: 103.60374450683594\n",
      "[step: 1249] loss: 0.006922836881130934\n",
      "[step: 1250] loss: 103.54693603515625\n",
      "[step: 1250] loss: 0.006922602187842131\n",
      "[step: 1251] loss: 103.4901351928711\n",
      "[step: 1251] loss: 0.006922103930264711\n",
      "[step: 1252] loss: 103.43333435058594\n",
      "[step: 1252] loss: 0.006921405903995037\n",
      "[step: 1253] loss: 103.37648010253906\n",
      "[step: 1253] loss: 0.006920750252902508\n",
      "[step: 1254] loss: 103.31964111328125\n",
      "[step: 1254] loss: 0.0069202762097120285\n",
      "[step: 1255] loss: 103.26295471191406\n",
      "[step: 1255] loss: 0.006919977255165577\n",
      "[step: 1256] loss: 103.2061767578125\n",
      "[step: 1256] loss: 0.006919678766280413\n",
      "[step: 1257] loss: 103.14991760253906\n",
      "[step: 1257] loss: 0.006919264327734709\n",
      "[step: 1258] loss: 103.09388732910156\n",
      "[step: 1258] loss: 0.006918733008205891\n",
      "[step: 1259] loss: 103.03874969482422\n",
      "[step: 1259] loss: 0.006918154191225767\n",
      "[step: 1260] loss: 102.98539733886719\n",
      "[step: 1260] loss: 0.006917642895132303\n",
      "[step: 1261] loss: 102.9354476928711\n",
      "[step: 1261] loss: 0.006917223334312439\n",
      "[step: 1262] loss: 102.89266967773438\n",
      "[step: 1262] loss: 0.006916864309459925\n",
      "[step: 1263] loss: 102.86415100097656\n",
      "[step: 1263] loss: 0.006916482001543045\n",
      "[step: 1264] loss: 102.867919921875\n",
      "[step: 1264] loss: 0.006916052661836147\n",
      "[step: 1265] loss: 102.93504333496094\n",
      "[step: 1265] loss: 0.006915566511452198\n",
      "[step: 1266] loss: 103.15458679199219\n",
      "[step: 1266] loss: 0.006915057543665171\n",
      "[step: 1267] loss: 103.65824890136719\n",
      "[step: 1267] loss: 0.006914593745023012\n",
      "[step: 1268] loss: 104.89692687988281\n",
      "[step: 1268] loss: 0.006914154160767794\n",
      "[step: 1269] loss: 107.1602554321289\n",
      "[step: 1269] loss: 0.006913750432431698\n",
      "[step: 1270] loss: 112.17848205566406\n",
      "[step: 1270] loss: 0.00691335229203105\n",
      "[step: 1271] loss: 116.78445434570312\n",
      "[step: 1271] loss: 0.006912930868566036\n",
      "[step: 1272] loss: 122.1711196899414\n",
      "[step: 1272] loss: 0.006912484299391508\n",
      "[step: 1273] loss: 113.31094360351562\n",
      "[step: 1273] loss: 0.006912013981491327\n",
      "[step: 1274] loss: 104.059326171875\n",
      "[step: 1274] loss: 0.0069115543738007545\n",
      "[step: 1275] loss: 103.5255355834961\n",
      "[step: 1275] loss: 0.006911116652190685\n",
      "[step: 1276] loss: 109.70146179199219\n",
      "[step: 1276] loss: 0.006910684984177351\n",
      "[step: 1277] loss: 111.24000549316406\n",
      "[step: 1277] loss: 0.006910264957696199\n",
      "[step: 1278] loss: 103.90213012695312\n",
      "[step: 1278] loss: 0.006909848656505346\n",
      "[step: 1279] loss: 103.03297424316406\n",
      "[step: 1279] loss: 0.006909423507750034\n",
      "[step: 1280] loss: 107.83550262451172\n",
      "[step: 1280] loss: 0.0069089834578335285\n",
      "[step: 1281] loss: 105.80329895019531\n",
      "[step: 1281] loss: 0.006908540613949299\n",
      "[step: 1282] loss: 102.03907775878906\n",
      "[step: 1282] loss: 0.006908104754984379\n",
      "[step: 1283] loss: 103.80307006835938\n",
      "[step: 1283] loss: 0.0069076684303581715\n",
      "[step: 1284] loss: 105.32038879394531\n",
      "[step: 1284] loss: 0.006907227449119091\n",
      "[step: 1285] loss: 102.88658905029297\n",
      "[step: 1285] loss: 0.0069067999720573425\n",
      "[step: 1286] loss: 101.94680786132812\n",
      "[step: 1286] loss: 0.00690637668594718\n",
      "[step: 1287] loss: 103.9115219116211\n",
      "[step: 1287] loss: 0.006905949208885431\n",
      "[step: 1288] loss: 103.46916198730469\n",
      "[step: 1288] loss: 0.006905526388436556\n",
      "[step: 1289] loss: 101.59352111816406\n",
      "[step: 1289] loss: 0.0069051021710038185\n",
      "[step: 1290] loss: 102.59930419921875\n",
      "[step: 1290] loss: 0.006904672831296921\n",
      "[step: 1291] loss: 103.36986541748047\n",
      "[step: 1291] loss: 0.006904236041009426\n",
      "[step: 1292] loss: 101.7757568359375\n",
      "[step: 1292] loss: 0.006903808563947678\n",
      "[step: 1293] loss: 101.65348815917969\n",
      "[step: 1293] loss: 0.006903378758579493\n",
      "[step: 1294] loss: 102.67970275878906\n",
      "[step: 1294] loss: 0.006902937777340412\n",
      "[step: 1295] loss: 101.92520141601562\n",
      "[step: 1295] loss: 0.0069025126285851\n",
      "[step: 1296] loss: 101.21195983886719\n",
      "[step: 1296] loss: 0.006902085617184639\n",
      "[step: 1297] loss: 101.86761474609375\n",
      "[step: 1297] loss: 0.006901663728058338\n",
      "[step: 1298] loss: 101.84980773925781\n",
      "[step: 1298] loss: 0.006901242304593325\n",
      "[step: 1299] loss: 101.10543823242188\n",
      "[step: 1299] loss: 0.006900814361870289\n",
      "[step: 1300] loss: 101.21063995361328\n",
      "[step: 1300] loss: 0.006900388281792402\n",
      "[step: 1301] loss: 101.55055236816406\n",
      "[step: 1301] loss: 0.006899958476424217\n",
      "[step: 1302] loss: 101.12899780273438\n",
      "[step: 1302] loss: 0.006899539846926928\n",
      "[step: 1303] loss: 100.83457946777344\n",
      "[step: 1303] loss: 0.0068991174921393394\n",
      "[step: 1304] loss: 101.12980651855469\n",
      "[step: 1304] loss: 0.006898695137351751\n",
      "[step: 1305] loss: 101.08003997802734\n",
      "[step: 1305] loss: 0.006898265331983566\n",
      "[step: 1306] loss: 100.68247985839844\n",
      "[step: 1306] loss: 0.006897840648889542\n",
      "[step: 1307] loss: 100.7242431640625\n",
      "[step: 1307] loss: 0.006897423416376114\n",
      "[step: 1308] loss: 100.87586975097656\n",
      "[step: 1308] loss: 0.006896992679685354\n",
      "[step: 1309] loss: 100.62411499023438\n",
      "[step: 1309] loss: 0.006896571256220341\n",
      "[step: 1310] loss: 100.4418716430664\n",
      "[step: 1310] loss: 0.006896147504448891\n",
      "[step: 1311] loss: 100.55715942382812\n",
      "[step: 1311] loss: 0.006895725149661303\n",
      "[step: 1312] loss: 100.52217864990234\n",
      "[step: 1312] loss: 0.006895300466567278\n",
      "[step: 1313] loss: 100.30219268798828\n",
      "[step: 1313] loss: 0.006894885096698999\n",
      "[step: 1314] loss: 100.25183868408203\n",
      "[step: 1314] loss: 0.0068944660015404224\n",
      "[step: 1315] loss: 100.31202697753906\n",
      "[step: 1315] loss: 0.006894041318446398\n",
      "[step: 1316] loss: 100.21328735351562\n",
      "[step: 1316] loss: 0.006893625482916832\n",
      "[step: 1317] loss: 100.05793762207031\n",
      "[step: 1317] loss: 0.006893209647387266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1318] loss: 100.04779052734375\n",
      "[step: 1318] loss: 0.0068927970714867115\n",
      "[step: 1319] loss: 100.05670166015625\n",
      "[step: 1319] loss: 0.006892379838973284\n",
      "[step: 1320] loss: 99.94419860839844\n",
      "[step: 1320] loss: 0.006891977041959763\n",
      "[step: 1321] loss: 99.8372573852539\n",
      "[step: 1321] loss: 0.006891576573252678\n",
      "[step: 1322] loss: 99.82626342773438\n",
      "[step: 1322] loss: 0.0068911905400455\n",
      "[step: 1323] loss: 99.80287170410156\n",
      "[step: 1323] loss: 0.006890813820064068\n",
      "[step: 1324] loss: 99.70526123046875\n",
      "[step: 1324] loss: 0.00689047621563077\n",
      "[step: 1325] loss: 99.6207504272461\n",
      "[step: 1325] loss: 0.006890169344842434\n",
      "[step: 1326] loss: 99.59744262695312\n",
      "[step: 1326] loss: 0.006889945827424526\n",
      "[step: 1327] loss: 99.56314849853516\n",
      "[step: 1327] loss: 0.006889847107231617\n",
      "[step: 1328] loss: 99.47932434082031\n",
      "[step: 1328] loss: 0.006889938842505217\n",
      "[step: 1329] loss: 99.40409851074219\n",
      "[step: 1329] loss: 0.006890387274324894\n",
      "[step: 1330] loss: 99.36874389648438\n",
      "[step: 1330] loss: 0.006891347002238035\n",
      "[step: 1331] loss: 99.329833984375\n",
      "[step: 1331] loss: 0.006893256679177284\n",
      "[step: 1332] loss: 99.259033203125\n",
      "[step: 1332] loss: 0.006896211765706539\n",
      "[step: 1333] loss: 99.1876220703125\n",
      "[step: 1333] loss: 0.006901055108755827\n",
      "[step: 1334] loss: 99.14234924316406\n",
      "[step: 1334] loss: 0.0069063338451087475\n",
      "[step: 1335] loss: 99.10165405273438\n",
      "[step: 1335] loss: 0.006912176962941885\n",
      "[step: 1336] loss: 99.04073333740234\n",
      "[step: 1336] loss: 0.006912142038345337\n",
      "[step: 1337] loss: 98.97265625\n",
      "[step: 1337] loss: 0.006906731519848108\n",
      "[step: 1338] loss: 98.9190673828125\n",
      "[step: 1338] loss: 0.006894873920828104\n",
      "[step: 1339] loss: 98.87493896484375\n",
      "[step: 1339] loss: 0.006885726936161518\n",
      "[step: 1340] loss: 98.82186889648438\n",
      "[step: 1340] loss: 0.006884418893605471\n",
      "[step: 1341] loss: 98.75865173339844\n",
      "[step: 1341] loss: 0.006889462471008301\n",
      "[step: 1342] loss: 98.6998291015625\n",
      "[step: 1342] loss: 0.006894521880894899\n",
      "[step: 1343] loss: 98.650390625\n",
      "[step: 1343] loss: 0.006893385201692581\n",
      "[step: 1344] loss: 98.60087585449219\n",
      "[step: 1344] loss: 0.006887630093842745\n",
      "[step: 1345] loss: 98.54374694824219\n",
      "[step: 1345] loss: 0.006882363930344582\n",
      "[step: 1346] loss: 98.48387908935547\n",
      "[step: 1346] loss: 0.0068821352906525135\n",
      "[step: 1347] loss: 98.42884826660156\n",
      "[step: 1347] loss: 0.006885349750518799\n",
      "[step: 1348] loss: 98.37818908691406\n",
      "[step: 1348] loss: 0.006887136027216911\n",
      "[step: 1349] loss: 98.32575988769531\n",
      "[step: 1349] loss: 0.006885295268148184\n",
      "[step: 1350] loss: 98.26896667480469\n",
      "[step: 1350] loss: 0.00688141817227006\n",
      "[step: 1351] loss: 98.2113037109375\n",
      "[step: 1351] loss: 0.006879383232444525\n",
      "[step: 1352] loss: 98.15682983398438\n",
      "[step: 1352] loss: 0.006880262400954962\n",
      "[step: 1353] loss: 98.1048355102539\n",
      "[step: 1353] loss: 0.0068818917497992516\n",
      "[step: 1354] loss: 98.05140686035156\n",
      "[step: 1354] loss: 0.006881886627525091\n",
      "[step: 1355] loss: 97.99549865722656\n",
      "[step: 1355] loss: 0.006879826541990042\n",
      "[step: 1356] loss: 97.93905639648438\n",
      "[step: 1356] loss: 0.006877706851810217\n",
      "[step: 1357] loss: 97.88427734375\n",
      "[step: 1357] loss: 0.0068771615624427795\n",
      "[step: 1358] loss: 97.83094787597656\n",
      "[step: 1358] loss: 0.006877889856696129\n",
      "[step: 1359] loss: 97.7773666381836\n",
      "[step: 1359] loss: 0.006878404412418604\n",
      "[step: 1360] loss: 97.72239685058594\n",
      "[step: 1360] loss: 0.006877671927213669\n",
      "[step: 1361] loss: 97.66650390625\n",
      "[step: 1361] loss: 0.006876194849610329\n",
      "[step: 1362] loss: 97.61123657226562\n",
      "[step: 1362] loss: 0.006875067949295044\n",
      "[step: 1363] loss: 97.55695343017578\n",
      "[step: 1363] loss: 0.006874896120280027\n",
      "[step: 1364] loss: 97.50309753417969\n",
      "[step: 1364] loss: 0.006875187158584595\n",
      "[step: 1365] loss: 97.44863891601562\n",
      "[step: 1365] loss: 0.006875117775052786\n",
      "[step: 1366] loss: 97.39344024658203\n",
      "[step: 1366] loss: 0.006874410901218653\n",
      "[step: 1367] loss: 97.33810424804688\n",
      "[step: 1367] loss: 0.0068733952939510345\n",
      "[step: 1368] loss: 97.28305053710938\n",
      "[step: 1368] loss: 0.006872696336358786\n",
      "[step: 1369] loss: 97.2283935546875\n",
      "[step: 1369] loss: 0.00687248632311821\n",
      "[step: 1370] loss: 97.17398834228516\n",
      "[step: 1370] loss: 0.006872459314763546\n",
      "[step: 1371] loss: 97.11934661865234\n",
      "[step: 1371] loss: 0.006872236728668213\n",
      "[step: 1372] loss: 97.06442260742188\n",
      "[step: 1372] loss: 0.006871652789413929\n",
      "[step: 1373] loss: 97.00914001464844\n",
      "[step: 1373] loss: 0.006870927754789591\n",
      "[step: 1374] loss: 96.95392608642578\n",
      "[step: 1374] loss: 0.006870361510664225\n",
      "[step: 1375] loss: 96.89910888671875\n",
      "[step: 1375] loss: 0.00687004067003727\n",
      "[step: 1376] loss: 96.84417724609375\n",
      "[step: 1376] loss: 0.006869848817586899\n",
      "[step: 1377] loss: 96.78938293457031\n",
      "[step: 1377] loss: 0.0068695745430886745\n",
      "[step: 1378] loss: 96.73446655273438\n",
      "[step: 1378] loss: 0.006869125179946423\n",
      "[step: 1379] loss: 96.6793441772461\n",
      "[step: 1379] loss: 0.006868571508675814\n",
      "[step: 1380] loss: 96.62406921386719\n",
      "[step: 1380] loss: 0.006868050899356604\n",
      "[step: 1381] loss: 96.56878662109375\n",
      "[step: 1381] loss: 0.0068676406517624855\n",
      "[step: 1382] loss: 96.5136489868164\n",
      "[step: 1382] loss: 0.006867335177958012\n",
      "[step: 1383] loss: 96.45843505859375\n",
      "[step: 1383] loss: 0.006867042277008295\n",
      "[step: 1384] loss: 96.40336608886719\n",
      "[step: 1384] loss: 0.006866677198559046\n",
      "[step: 1385] loss: 96.34829711914062\n",
      "[step: 1385] loss: 0.0068662287667393684\n",
      "[step: 1386] loss: 96.29302978515625\n",
      "[step: 1386] loss: 0.006865762174129486\n",
      "[step: 1387] loss: 96.23783874511719\n",
      "[step: 1387] loss: 0.006865315604954958\n",
      "[step: 1388] loss: 96.18247985839844\n",
      "[step: 1388] loss: 0.0068649291060864925\n",
      "[step: 1389] loss: 96.1269760131836\n",
      "[step: 1389] loss: 0.006864588242024183\n",
      "[step: 1390] loss: 96.07154846191406\n",
      "[step: 1390] loss: 0.006864241790026426\n",
      "[step: 1391] loss: 96.01615905761719\n",
      "[step: 1391] loss: 0.006863866932690144\n",
      "[step: 1392] loss: 95.96076965332031\n",
      "[step: 1392] loss: 0.006863465066999197\n",
      "[step: 1393] loss: 95.90534973144531\n",
      "[step: 1393] loss: 0.006863035727292299\n",
      "[step: 1394] loss: 95.84988403320312\n",
      "[step: 1394] loss: 0.0068626184947788715\n",
      "[step: 1395] loss: 95.79438781738281\n",
      "[step: 1395] loss: 0.0068622250109910965\n",
      "[step: 1396] loss: 95.73890686035156\n",
      "[step: 1396] loss: 0.006861844565719366\n",
      "[step: 1397] loss: 95.68334197998047\n",
      "[step: 1397] loss: 0.00686148926615715\n",
      "[step: 1398] loss: 95.62771606445312\n",
      "[step: 1398] loss: 0.006861117668449879\n",
      "[step: 1399] loss: 95.57220458984375\n",
      "[step: 1399] loss: 0.006860736757516861\n",
      "[step: 1400] loss: 95.51638793945312\n",
      "[step: 1400] loss: 0.0068603390827775\n",
      "[step: 1401] loss: 95.46075439453125\n",
      "[step: 1401] loss: 0.006859939079731703\n",
      "[step: 1402] loss: 95.40501403808594\n",
      "[step: 1402] loss: 0.006859533488750458\n",
      "[step: 1403] loss: 95.34923553466797\n",
      "[step: 1403] loss: 0.006859148386865854\n",
      "[step: 1404] loss: 95.29342651367188\n",
      "[step: 1404] loss: 0.006858771201223135\n",
      "[step: 1405] loss: 95.23773193359375\n",
      "[step: 1405] loss: 0.006858392618596554\n",
      "[step: 1406] loss: 95.18186950683594\n",
      "[step: 1406] loss: 0.006858023814857006\n",
      "[step: 1407] loss: 95.1259765625\n",
      "[step: 1407] loss: 0.006857641972601414\n",
      "[step: 1408] loss: 95.07013702392578\n",
      "[step: 1408] loss: 0.0068572573363780975\n",
      "[step: 1409] loss: 95.01433563232422\n",
      "[step: 1409] loss: 0.006856870837509632\n",
      "[step: 1410] loss: 94.95860290527344\n",
      "[step: 1410] loss: 0.006856472697108984\n",
      "[step: 1411] loss: 94.90283203125\n",
      "[step: 1411] loss: 0.0068560875952243805\n",
      "[step: 1412] loss: 94.84729766845703\n",
      "[step: 1412] loss: 0.006855699233710766\n",
      "[step: 1413] loss: 94.79202270507812\n",
      "[step: 1413] loss: 0.006855320651084185\n",
      "[step: 1414] loss: 94.73755645751953\n",
      "[step: 1414] loss: 0.006854938808828592\n",
      "[step: 1415] loss: 94.68423461914062\n",
      "[step: 1415] loss: 0.006854561157524586\n",
      "[step: 1416] loss: 94.6331558227539\n",
      "[step: 1416] loss: 0.006854180246591568\n",
      "[step: 1417] loss: 94.58624267578125\n",
      "[step: 1417] loss: 0.006853802129626274\n",
      "[step: 1418] loss: 94.54753112792969\n",
      "[step: 1418] loss: 0.006853420753031969\n",
      "[step: 1419] loss: 94.52556610107422\n",
      "[step: 1419] loss: 0.006853037513792515\n",
      "[step: 1420] loss: 94.5349349975586\n",
      "[step: 1420] loss: 0.006852658465504646\n",
      "[step: 1421] loss: 94.61368560791016\n",
      "[step: 1421] loss: 0.0068522728979587555\n",
      "[step: 1422] loss: 94.8183822631836\n",
      "[step: 1422] loss: 0.006851886864751577\n",
      "[step: 1423] loss: 95.32469940185547\n",
      "[step: 1423] loss: 0.006851502228528261\n",
      "[step: 1424] loss: 96.31065368652344\n",
      "[step: 1424] loss: 0.006851122248917818\n",
      "[step: 1425] loss: 98.54193115234375\n",
      "[step: 1425] loss: 0.006850743666291237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1426] loss: 101.86717987060547\n",
      "[step: 1426] loss: 0.006850364618003368\n",
      "[step: 1427] loss: 108.26329040527344\n",
      "[step: 1427] loss: 0.006849986035376787\n",
      "[step: 1428] loss: 110.57057189941406\n",
      "[step: 1428] loss: 0.006849601864814758\n",
      "[step: 1429] loss: 110.40934753417969\n",
      "[step: 1429] loss: 0.006849219091236591\n",
      "[step: 1430] loss: 99.48822784423828\n",
      "[step: 1430] loss: 0.006848846096545458\n",
      "[step: 1431] loss: 93.97883605957031\n",
      "[step: 1431] loss: 0.006848463788628578\n",
      "[step: 1432] loss: 98.19306945800781\n",
      "[step: 1432] loss: 0.0068480949848890305\n",
      "[step: 1433] loss: 102.21321105957031\n",
      "[step: 1433] loss: 0.006847716867923737\n",
      "[step: 1434] loss: 99.77422332763672\n",
      "[step: 1434] loss: 0.006847338285297155\n",
      "[step: 1435] loss: 94.11930847167969\n",
      "[step: 1435] loss: 0.0068469480611383915\n",
      "[step: 1436] loss: 95.69408416748047\n",
      "[step: 1436] loss: 0.006846575066447258\n",
      "[step: 1437] loss: 99.51179504394531\n",
      "[step: 1437] loss: 0.006846197880804539\n",
      "[step: 1438] loss: 96.47724914550781\n",
      "[step: 1438] loss: 0.006845819763839245\n",
      "[step: 1439] loss: 93.55793762207031\n",
      "[step: 1439] loss: 0.0068454435095191\n",
      "[step: 1440] loss: 95.6456298828125\n",
      "[step: 1440] loss: 0.006845066323876381\n",
      "[step: 1441] loss: 96.71026611328125\n",
      "[step: 1441] loss: 0.006844689603894949\n",
      "[step: 1442] loss: 94.39668273925781\n",
      "[step: 1442] loss: 0.006844307761639357\n",
      "[step: 1443] loss: 93.4820556640625\n",
      "[step: 1443] loss: 0.006843945477157831\n",
      "[step: 1444] loss: 95.2488784790039\n",
      "[step: 1444] loss: 0.006843573413789272\n",
      "[step: 1445] loss: 95.19439697265625\n",
      "[step: 1445] loss: 0.00684320880100131\n",
      "[step: 1446] loss: 93.32234954833984\n",
      "[step: 1446] loss: 0.00684284558519721\n",
      "[step: 1447] loss: 93.7289810180664\n",
      "[step: 1447] loss: 0.006842492613941431\n",
      "[step: 1448] loss: 94.82868957519531\n",
      "[step: 1448] loss: 0.0068421512842178345\n",
      "[step: 1449] loss: 93.77043151855469\n",
      "[step: 1449] loss: 0.006841820664703846\n",
      "[step: 1450] loss: 92.96516418457031\n",
      "[step: 1450] loss: 0.006841541733592749\n",
      "[step: 1451] loss: 93.75174713134766\n",
      "[step: 1451] loss: 0.006841287948191166\n",
      "[step: 1452] loss: 93.85722351074219\n",
      "[step: 1452] loss: 0.006841129157692194\n",
      "[step: 1453] loss: 92.96739196777344\n",
      "[step: 1453] loss: 0.006841105408966541\n",
      "[step: 1454] loss: 92.8873519897461\n",
      "[step: 1454] loss: 0.006841300521045923\n",
      "[step: 1455] loss: 93.41641235351562\n",
      "[step: 1455] loss: 0.006841826718300581\n",
      "[step: 1456] loss: 93.14125061035156\n",
      "[step: 1456] loss: 0.006842965260148048\n",
      "[step: 1457] loss: 92.57888793945312\n",
      "[step: 1457] loss: 0.006844882387667894\n",
      "[step: 1458] loss: 92.79328918457031\n",
      "[step: 1458] loss: 0.0068482328206300735\n",
      "[step: 1459] loss: 93.02177429199219\n",
      "[step: 1459] loss: 0.006852686870843172\n",
      "[step: 1460] loss: 92.5993423461914\n",
      "[step: 1460] loss: 0.006859000772237778\n",
      "[step: 1461] loss: 92.3637924194336\n",
      "[step: 1461] loss: 0.006863562855869532\n",
      "[step: 1462] loss: 92.59700012207031\n",
      "[step: 1462] loss: 0.006865767762064934\n",
      "[step: 1463] loss: 92.57095336914062\n",
      "[step: 1463] loss: 0.006859150715172291\n",
      "[step: 1464] loss: 92.24283599853516\n",
      "[step: 1464] loss: 0.006848325952887535\n",
      "[step: 1465] loss: 92.17707061767578\n",
      "[step: 1465] loss: 0.006838222499936819\n",
      "[step: 1466] loss: 92.31524658203125\n",
      "[step: 1466] loss: 0.006835739593952894\n",
      "[step: 1467] loss: 92.21443176269531\n",
      "[step: 1467] loss: 0.006840307265520096\n",
      "[step: 1468] loss: 91.98040771484375\n",
      "[step: 1468] loss: 0.006845609284937382\n",
      "[step: 1469] loss: 91.96693420410156\n",
      "[step: 1469] loss: 0.006846328265964985\n",
      "[step: 1470] loss: 92.03448486328125\n",
      "[step: 1470] loss: 0.006840892601758242\n",
      "[step: 1471] loss: 91.91424560546875\n",
      "[step: 1471] loss: 0.006835034117102623\n",
      "[step: 1472] loss: 91.749755859375\n",
      "[step: 1472] loss: 0.006833511404693127\n",
      "[step: 1473] loss: 91.73632049560547\n",
      "[step: 1473] loss: 0.006836198270320892\n",
      "[step: 1474] loss: 91.75579833984375\n",
      "[step: 1474] loss: 0.0068390145897865295\n",
      "[step: 1475] loss: 91.65736389160156\n",
      "[step: 1475] loss: 0.006838252302259207\n",
      "[step: 1476] loss: 91.52963256835938\n",
      "[step: 1476] loss: 0.006834791507571936\n",
      "[step: 1477] loss: 91.49961853027344\n",
      "[step: 1477] loss: 0.006831774953752756\n",
      "[step: 1478] loss: 91.49816131591797\n",
      "[step: 1478] loss: 0.006831564474850893\n",
      "[step: 1479] loss: 91.4194564819336\n",
      "[step: 1479] loss: 0.006833235267549753\n",
      "[step: 1480] loss: 91.31311798095703\n",
      "[step: 1480] loss: 0.006834183819591999\n",
      "[step: 1481] loss: 91.26466369628906\n",
      "[step: 1481] loss: 0.006833130028098822\n",
      "[step: 1482] loss: 91.24732208251953\n",
      "[step: 1482] loss: 0.006830848287791014\n",
      "[step: 1483] loss: 91.18952178955078\n",
      "[step: 1483] loss: 0.006829351652413607\n",
      "[step: 1484] loss: 91.09904479980469\n",
      "[step: 1484] loss: 0.006829427555203438\n",
      "[step: 1485] loss: 91.03592681884766\n",
      "[step: 1485] loss: 0.006830229423940182\n",
      "[step: 1486] loss: 91.00473022460938\n",
      "[step: 1486] loss: 0.006830430589616299\n",
      "[step: 1487] loss: 90.95932006835938\n",
      "[step: 1487] loss: 0.006829461082816124\n",
      "[step: 1488] loss: 90.88557434082031\n",
      "[step: 1488] loss: 0.006828054320067167\n",
      "[step: 1489] loss: 90.81497192382812\n",
      "[step: 1489] loss: 0.0068271830677986145\n",
      "[step: 1490] loss: 90.76873779296875\n",
      "[step: 1490] loss: 0.006827122997492552\n",
      "[step: 1491] loss: 90.72782897949219\n",
      "[step: 1491] loss: 0.006827402859926224\n",
      "[step: 1492] loss: 90.66928100585938\n",
      "[step: 1492] loss: 0.006827313452959061\n",
      "[step: 1493] loss: 90.60033416748047\n",
      "[step: 1493] loss: 0.0068266382440924644\n",
      "[step: 1494] loss: 90.54153442382812\n",
      "[step: 1494] loss: 0.006825713906437159\n",
      "[step: 1495] loss: 90.49528503417969\n",
      "[step: 1495] loss: 0.006825046613812447\n",
      "[step: 1496] loss: 90.44640350341797\n",
      "[step: 1496] loss: 0.006824804004281759\n",
      "[step: 1497] loss: 90.38627624511719\n",
      "[step: 1497] loss: 0.006824782583862543\n",
      "[step: 1498] loss: 90.3233413696289\n",
      "[step: 1498] loss: 0.006824626121670008\n",
      "[step: 1499] loss: 90.26778411865234\n",
      "[step: 1499] loss: 0.006824164651334286\n",
      "[step: 1500] loss: 90.21858215332031\n",
      "[step: 1500] loss: 0.006823535542935133\n",
      "[step: 1501] loss: 90.16665649414062\n",
      "[step: 1501] loss: 0.006822940427809954\n",
      "[step: 1502] loss: 90.10834503173828\n",
      "[step: 1502] loss: 0.006822556257247925\n",
      "[step: 1503] loss: 90.04853057861328\n",
      "[step: 1503] loss: 0.0068223443813622\n",
      "[step: 1504] loss: 89.99300384521484\n",
      "[step: 1504] loss: 0.00682214368134737\n",
      "[step: 1505] loss: 89.9411392211914\n",
      "[step: 1505] loss: 0.0068218279629945755\n",
      "[step: 1506] loss: 89.88844299316406\n",
      "[step: 1506] loss: 0.0068213739432394505\n",
      "[step: 1507] loss: 89.83210754394531\n",
      "[step: 1507] loss: 0.00682087242603302\n",
      "[step: 1508] loss: 89.77401733398438\n",
      "[step: 1508] loss: 0.006820422597229481\n",
      "[step: 1509] loss: 89.71775817871094\n",
      "[step: 1509] loss: 0.006820072885602713\n",
      "[step: 1510] loss: 89.66395568847656\n",
      "[step: 1510] loss: 0.006819792091846466\n",
      "[step: 1511] loss: 89.61077880859375\n",
      "[step: 1511] loss: 0.006819511763751507\n",
      "[step: 1512] loss: 89.55594635009766\n",
      "[step: 1512] loss: 0.006819172762334347\n",
      "[step: 1513] loss: 89.4995346069336\n",
      "[step: 1513] loss: 0.006818775553256273\n",
      "[step: 1514] loss: 89.44287109375\n",
      "[step: 1514] loss: 0.0068183476105332375\n",
      "[step: 1515] loss: 89.38761901855469\n",
      "[step: 1515] loss: 0.0068179406225681305\n",
      "[step: 1516] loss: 89.33334350585938\n",
      "[step: 1516] loss: 0.0068175760097801685\n",
      "[step: 1517] loss: 89.27923583984375\n",
      "[step: 1517] loss: 0.006817246321588755\n",
      "[step: 1518] loss: 89.22407531738281\n",
      "[step: 1518] loss: 0.006816933397203684\n",
      "[step: 1519] loss: 89.1682357788086\n",
      "[step: 1519] loss: 0.006816595792770386\n",
      "[step: 1520] loss: 89.11229705810547\n",
      "[step: 1520] loss: 0.006816240027546883\n",
      "[step: 1521] loss: 89.05689239501953\n",
      "[step: 1521] loss: 0.006815859116613865\n",
      "[step: 1522] loss: 89.00216674804688\n",
      "[step: 1522] loss: 0.006815478205680847\n",
      "[step: 1523] loss: 88.94746398925781\n",
      "[step: 1523] loss: 0.00681510241702199\n",
      "[step: 1524] loss: 88.892578125\n",
      "[step: 1524] loss: 0.00681474432349205\n",
      "[step: 1525] loss: 88.83711242675781\n",
      "[step: 1525] loss: 0.006814394146203995\n",
      "[step: 1526] loss: 88.78142547607422\n",
      "[step: 1526] loss: 0.006814056076109409\n",
      "[step: 1527] loss: 88.72593688964844\n",
      "[step: 1527] loss: 0.0068137152120471\n",
      "[step: 1528] loss: 88.67066955566406\n",
      "[step: 1528] loss: 0.006813368294388056\n",
      "[step: 1529] loss: 88.61558532714844\n",
      "[step: 1529] loss: 0.006813008338212967\n",
      "[step: 1530] loss: 88.56069946289062\n",
      "[step: 1530] loss: 0.00681264465674758\n",
      "[step: 1531] loss: 88.505615234375\n",
      "[step: 1531] loss: 0.006812275853008032\n",
      "[step: 1532] loss: 88.45025634765625\n",
      "[step: 1532] loss: 0.006811916828155518\n",
      "[step: 1533] loss: 88.39482116699219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1533] loss: 0.006811557337641716\n",
      "[step: 1534] loss: 88.33936309814453\n",
      "[step: 1534] loss: 0.006811206229031086\n",
      "[step: 1535] loss: 88.28401184082031\n",
      "[step: 1535] loss: 0.006810854654759169\n",
      "[step: 1536] loss: 88.2288818359375\n",
      "[step: 1536] loss: 0.006810508202761412\n",
      "[step: 1537] loss: 88.17359924316406\n",
      "[step: 1537] loss: 0.006810164079070091\n",
      "[step: 1538] loss: 88.11845397949219\n",
      "[step: 1538] loss: 0.006809805985540152\n",
      "[step: 1539] loss: 88.06320190429688\n",
      "[step: 1539] loss: 0.006809458136558533\n",
      "[step: 1540] loss: 88.00810241699219\n",
      "[step: 1540] loss: 0.0068090991117060184\n",
      "[step: 1541] loss: 87.95268249511719\n",
      "[step: 1541] loss: 0.006808747537434101\n",
      "[step: 1542] loss: 87.89739990234375\n",
      "[step: 1542] loss: 0.0068083833903074265\n",
      "[step: 1543] loss: 87.84205627441406\n",
      "[step: 1543] loss: 0.006808024365454912\n",
      "[step: 1544] loss: 87.78662109375\n",
      "[step: 1544] loss: 0.006807667203247547\n",
      "[step: 1545] loss: 87.73136901855469\n",
      "[step: 1545] loss: 0.00680732075124979\n",
      "[step: 1546] loss: 87.67599487304688\n",
      "[step: 1546] loss: 0.006806966383010149\n",
      "[step: 1547] loss: 87.62083435058594\n",
      "[step: 1547] loss: 0.006806614343076944\n",
      "[step: 1548] loss: 87.56565856933594\n",
      "[step: 1548] loss: 0.006806264631450176\n",
      "[step: 1549] loss: 87.51043701171875\n",
      "[step: 1549] loss: 0.006805906072258949\n",
      "[step: 1550] loss: 87.45513916015625\n",
      "[step: 1550] loss: 0.0068055554293096066\n",
      "[step: 1551] loss: 87.39989471435547\n",
      "[step: 1551] loss: 0.006805198732763529\n",
      "[step: 1552] loss: 87.34468078613281\n",
      "[step: 1552] loss: 0.006804856937378645\n",
      "[step: 1553] loss: 87.2894287109375\n",
      "[step: 1553] loss: 0.006804501637816429\n",
      "[step: 1554] loss: 87.23413848876953\n",
      "[step: 1554] loss: 0.006804151460528374\n",
      "[step: 1555] loss: 87.17898559570312\n",
      "[step: 1555] loss: 0.006803799420595169\n",
      "[step: 1556] loss: 87.12373352050781\n",
      "[step: 1556] loss: 0.006803445052355528\n",
      "[step: 1557] loss: 87.06855010986328\n",
      "[step: 1557] loss: 0.006803090218454599\n",
      "[step: 1558] loss: 87.01329040527344\n",
      "[step: 1558] loss: 0.006802743300795555\n",
      "[step: 1559] loss: 86.95813751220703\n",
      "[step: 1559] loss: 0.006802392657846212\n",
      "[step: 1560] loss: 86.90296173095703\n",
      "[step: 1560] loss: 0.00680203502997756\n",
      "[step: 1561] loss: 86.84786987304688\n",
      "[step: 1561] loss: 0.006801689974963665\n",
      "[step: 1562] loss: 86.79292297363281\n",
      "[step: 1562] loss: 0.006801337003707886\n",
      "[step: 1563] loss: 86.73784637451172\n",
      "[step: 1563] loss: 0.006800986360758543\n",
      "[step: 1564] loss: 86.68309020996094\n",
      "[step: 1564] loss: 0.006800638046115637\n",
      "[step: 1565] loss: 86.62857055664062\n",
      "[step: 1565] loss: 0.006800295319408178\n",
      "[step: 1566] loss: 86.57429504394531\n",
      "[step: 1566] loss: 0.0067999474704265594\n",
      "[step: 1567] loss: 86.52070617675781\n",
      "[step: 1567] loss: 0.0067996070720255375\n",
      "[step: 1568] loss: 86.4681625366211\n",
      "[step: 1568] loss: 0.0067992787808179855\n",
      "[step: 1569] loss: 86.41758728027344\n",
      "[step: 1569] loss: 0.006798952352255583\n",
      "[step: 1570] loss: 86.37031555175781\n",
      "[step: 1570] loss: 0.006798642221838236\n",
      "[step: 1571] loss: 86.32939147949219\n",
      "[step: 1571] loss: 0.006798357702791691\n",
      "[step: 1572] loss: 86.30044555664062\n",
      "[step: 1572] loss: 0.006798114627599716\n",
      "[step: 1573] loss: 86.29290771484375\n",
      "[step: 1573] loss: 0.0067979213781654835\n",
      "[step: 1574] loss: 86.33026885986328\n",
      "[step: 1574] loss: 0.0067978366278111935\n",
      "[step: 1575] loss: 86.44561767578125\n",
      "[step: 1575] loss: 0.006797892972826958\n",
      "[step: 1576] loss: 86.73663330078125\n",
      "[step: 1576] loss: 0.006798228714615107\n",
      "[step: 1577] loss: 87.30815124511719\n",
      "[step: 1577] loss: 0.006798946764320135\n",
      "[step: 1578] loss: 88.56611633300781\n",
      "[step: 1578] loss: 0.006800367962568998\n",
      "[step: 1579] loss: 90.62076568603516\n",
      "[step: 1579] loss: 0.006802656222134829\n",
      "[step: 1580] loss: 94.8041763305664\n",
      "[step: 1580] loss: 0.006806533318012953\n",
      "[step: 1581] loss: 98.77410125732422\n",
      "[step: 1581] loss: 0.00681151682510972\n",
      "[step: 1582] loss: 104.03884887695312\n",
      "[step: 1582] loss: 0.006818272173404694\n",
      "[step: 1583] loss: 99.26254272460938\n",
      "[step: 1583] loss: 0.006822597701102495\n",
      "[step: 1584] loss: 91.60636901855469\n",
      "[step: 1584] loss: 0.006823831703513861\n",
      "[step: 1585] loss: 85.8048324584961\n",
      "[step: 1585] loss: 0.006815800443291664\n",
      "[step: 1586] loss: 88.73311614990234\n",
      "[step: 1586] loss: 0.006804087199270725\n",
      "[step: 1587] loss: 94.20166015625\n",
      "[step: 1587] loss: 0.006794416811317205\n",
      "[step: 1588] loss: 91.73489379882812\n",
      "[step: 1588] loss: 0.006793029140681028\n",
      "[step: 1589] loss: 86.57865905761719\n",
      "[step: 1589] loss: 0.006798359099775553\n",
      "[step: 1590] loss: 86.10169982910156\n",
      "[step: 1590] loss: 0.0068035549484193325\n",
      "[step: 1591] loss: 89.51190185546875\n",
      "[step: 1591] loss: 0.00680359173566103\n",
      "[step: 1592] loss: 89.75196838378906\n",
      "[step: 1592] loss: 0.006797627080231905\n",
      "[step: 1593] loss: 86.03242492675781\n",
      "[step: 1593] loss: 0.006791825406253338\n",
      "[step: 1594] loss: 85.81568908691406\n",
      "[step: 1594] loss: 0.006790696177631617\n",
      "[step: 1595] loss: 88.30462646484375\n",
      "[step: 1595] loss: 0.006793669890612364\n",
      "[step: 1596] loss: 87.53760528564453\n",
      "[step: 1596] loss: 0.0067965188063681126\n",
      "[step: 1597] loss: 85.29948425292969\n",
      "[step: 1597] loss: 0.0067956289276480675\n",
      "[step: 1598] loss: 85.61321258544922\n",
      "[step: 1598] loss: 0.006792070344090462\n",
      "[step: 1599] loss: 86.97813415527344\n",
      "[step: 1599] loss: 0.006788973230868578\n",
      "[step: 1600] loss: 86.30073547363281\n",
      "[step: 1600] loss: 0.006788719445466995\n",
      "[step: 1601] loss: 84.9106216430664\n",
      "[step: 1601] loss: 0.006790442857891321\n",
      "[step: 1602] loss: 85.46036529541016\n",
      "[step: 1602] loss: 0.0067915464751422405\n",
      "[step: 1603] loss: 86.27254486083984\n",
      "[step: 1603] loss: 0.006790678016841412\n",
      "[step: 1604] loss: 85.38471984863281\n",
      "[step: 1604] loss: 0.006788400933146477\n",
      "[step: 1605] loss: 84.65814208984375\n",
      "[step: 1605] loss: 0.006786709185689688\n",
      "[step: 1606] loss: 85.22309875488281\n",
      "[step: 1606] loss: 0.0067865923047065735\n",
      "[step: 1607] loss: 85.4805908203125\n",
      "[step: 1607] loss: 0.006787407677620649\n",
      "[step: 1608] loss: 84.80604553222656\n",
      "[step: 1608] loss: 0.006787834223359823\n",
      "[step: 1609] loss: 84.47262573242188\n",
      "[step: 1609] loss: 0.006787124555557966\n",
      "[step: 1610] loss: 84.89303588867188\n",
      "[step: 1610] loss: 0.0067857480607926846\n",
      "[step: 1611] loss: 84.94962310791016\n",
      "[step: 1611] loss: 0.006784665863960981\n",
      "[step: 1612] loss: 84.41574096679688\n",
      "[step: 1612] loss: 0.006784398574382067\n",
      "[step: 1613] loss: 84.28974914550781\n",
      "[step: 1613] loss: 0.006784672383219004\n",
      "[step: 1614] loss: 84.58087158203125\n",
      "[step: 1614] loss: 0.006784792058169842\n",
      "[step: 1615] loss: 84.49407958984375\n",
      "[step: 1615] loss: 0.006784355733543634\n",
      "[step: 1616] loss: 84.13154602050781\n",
      "[step: 1616] loss: 0.0067834933288395405\n",
      "[step: 1617] loss: 84.08101654052734\n",
      "[step: 1617] loss: 0.006782685872167349\n",
      "[step: 1618] loss: 84.24666595458984\n",
      "[step: 1618] loss: 0.006782268173992634\n",
      "[step: 1619] loss: 84.1597900390625\n",
      "[step: 1619] loss: 0.006782184354960918\n",
      "[step: 1620] loss: 83.89791107177734\n",
      "[step: 1620] loss: 0.006782157346606255\n",
      "[step: 1621] loss: 83.85676574707031\n",
      "[step: 1621] loss: 0.006781893782317638\n",
      "[step: 1622] loss: 83.95374298095703\n",
      "[step: 1622] loss: 0.006781369913369417\n",
      "[step: 1623] loss: 83.8746566772461\n",
      "[step: 1623] loss: 0.00678075198084116\n",
      "[step: 1624] loss: 83.68546295166016\n",
      "[step: 1624] loss: 0.006780244410037994\n",
      "[step: 1625] loss: 83.62599182128906\n",
      "[step: 1625] loss: 0.006779931951314211\n",
      "[step: 1626] loss: 83.6715087890625\n",
      "[step: 1626] loss: 0.006779746152460575\n",
      "[step: 1627] loss: 83.62657165527344\n",
      "[step: 1627] loss: 0.006779548712074757\n",
      "[step: 1628] loss: 83.48342895507812\n",
      "[step: 1628] loss: 0.006779227405786514\n",
      "[step: 1629] loss: 83.40455627441406\n",
      "[step: 1629] loss: 0.006778787821531296\n",
      "[step: 1630] loss: 83.41316223144531\n",
      "[step: 1630] loss: 0.006778318900614977\n",
      "[step: 1631] loss: 83.38726806640625\n",
      "[step: 1631] loss: 0.006777897011488676\n",
      "[step: 1632] loss: 83.28666687011719\n",
      "[step: 1632] loss: 0.006777561269700527\n",
      "[step: 1633] loss: 83.1956787109375\n",
      "[step: 1633] loss: 0.006777298636734486\n",
      "[step: 1634] loss: 83.1694107055664\n",
      "[step: 1634] loss: 0.006777031347155571\n",
      "[step: 1635] loss: 83.15234375\n",
      "[step: 1635] loss: 0.006776717025786638\n",
      "[step: 1636] loss: 83.08578491210938\n",
      "[step: 1636] loss: 0.006776357535272837\n",
      "[step: 1637] loss: 82.99910736083984\n",
      "[step: 1637] loss: 0.006775956600904465\n",
      "[step: 1638] loss: 82.94477844238281\n",
      "[step: 1638] loss: 0.006775564048439264\n",
      "[step: 1639] loss: 82.91883850097656\n",
      "[step: 1639] loss: 0.006775202229619026\n",
      "[step: 1640] loss: 82.87734985351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1640] loss: 0.0067748804576694965\n",
      "[step: 1641] loss: 82.8072280883789\n",
      "[step: 1641] loss: 0.006774574983865023\n",
      "[step: 1642] loss: 82.73933410644531\n",
      "[step: 1642] loss: 0.006774269510060549\n",
      "[step: 1643] loss: 82.69483947753906\n",
      "[step: 1643] loss: 0.0067739468067884445\n",
      "[step: 1644] loss: 82.65936279296875\n",
      "[step: 1644] loss: 0.0067736003547906876\n",
      "[step: 1645] loss: 82.60957336425781\n",
      "[step: 1645] loss: 0.006773240864276886\n",
      "[step: 1646] loss: 82.54578399658203\n",
      "[step: 1646] loss: 0.006772880908101797\n",
      "[step: 1647] loss: 82.48706817626953\n",
      "[step: 1647] loss: 0.0067725274711847305\n",
      "[step: 1648] loss: 82.44203186035156\n",
      "[step: 1648] loss: 0.006772184278815985\n",
      "[step: 1649] loss: 82.40054321289062\n",
      "[step: 1649] loss: 0.00677185645326972\n",
      "[step: 1650] loss: 82.35028076171875\n",
      "[step: 1650] loss: 0.006771528627723455\n",
      "[step: 1651] loss: 82.29222106933594\n",
      "[step: 1651] loss: 0.006771211978048086\n",
      "[step: 1652] loss: 82.23683166503906\n",
      "[step: 1652] loss: 0.0067708855494856834\n",
      "[step: 1653] loss: 82.1893539428711\n",
      "[step: 1653] loss: 0.006770542822778225\n",
      "[step: 1654] loss: 82.1448974609375\n",
      "[step: 1654] loss: 0.0067702098749578\n",
      "[step: 1655] loss: 82.09622192382812\n",
      "[step: 1655] loss: 0.006769861560314894\n",
      "[step: 1656] loss: 82.04240417480469\n",
      "[step: 1656] loss: 0.006769516039639711\n",
      "[step: 1657] loss: 81.98860168457031\n",
      "[step: 1657] loss: 0.006769170984625816\n",
      "[step: 1658] loss: 81.93885803222656\n",
      "[step: 1658] loss: 0.006768837571144104\n",
      "[step: 1659] loss: 81.89220428466797\n",
      "[step: 1659] loss: 0.006768505088984966\n",
      "[step: 1660] loss: 81.8448257446289\n",
      "[step: 1660] loss: 0.006768163293600082\n",
      "[step: 1661] loss: 81.79429626464844\n",
      "[step: 1661] loss: 0.006767828017473221\n",
      "[step: 1662] loss: 81.74221801757812\n",
      "[step: 1662] loss: 0.006767503451555967\n",
      "[step: 1663] loss: 81.69129943847656\n",
      "[step: 1663] loss: 0.006767169572412968\n",
      "[step: 1664] loss: 81.64262390136719\n",
      "[step: 1664] loss: 0.0067668333649635315\n",
      "[step: 1665] loss: 81.59501647949219\n",
      "[step: 1665] loss: 0.006766504608094692\n",
      "[step: 1666] loss: 81.54670715332031\n",
      "[step: 1666] loss: 0.006766162812709808\n",
      "[step: 1667] loss: 81.49698638916016\n",
      "[step: 1667] loss: 0.0067658270709216595\n",
      "[step: 1668] loss: 81.44646453857422\n",
      "[step: 1668] loss: 0.006765487138181925\n",
      "[step: 1669] loss: 81.39645385742188\n",
      "[step: 1669] loss: 0.006765154656022787\n",
      "[step: 1670] loss: 81.34776306152344\n",
      "[step: 1670] loss: 0.006764818448573351\n",
      "[step: 1671] loss: 81.29987335205078\n",
      "[step: 1671] loss: 0.006764482241123915\n",
      "[step: 1672] loss: 81.25166320800781\n",
      "[step: 1672] loss: 0.006764144171029329\n",
      "[step: 1673] loss: 81.20274353027344\n",
      "[step: 1673] loss: 0.006763806100934744\n",
      "[step: 1674] loss: 81.15339660644531\n",
      "[step: 1674] loss: 0.006763472221791744\n",
      "[step: 1675] loss: 81.10409545898438\n",
      "[step: 1675] loss: 0.006763129029422998\n",
      "[step: 1676] loss: 81.05525207519531\n",
      "[step: 1676] loss: 0.006762797478586435\n",
      "[step: 1677] loss: 81.0070571899414\n",
      "[step: 1677] loss: 0.00676246453076601\n",
      "[step: 1678] loss: 80.95909881591797\n",
      "[step: 1678] loss: 0.0067621273919939995\n",
      "[step: 1679] loss: 80.91095733642578\n",
      "[step: 1679] loss: 0.006761793978512287\n",
      "[step: 1680] loss: 80.8625259399414\n",
      "[step: 1680] loss: 0.006761453580111265\n",
      "[step: 1681] loss: 80.81398010253906\n",
      "[step: 1681] loss: 0.00676111551001668\n",
      "[step: 1682] loss: 80.76530456542969\n",
      "[step: 1682] loss: 0.006760791409760714\n",
      "[step: 1683] loss: 80.71703338623047\n",
      "[step: 1683] loss: 0.006760453339666128\n",
      "[step: 1684] loss: 80.6690673828125\n",
      "[step: 1684] loss: 0.006760114338248968\n",
      "[step: 1685] loss: 80.62123107910156\n",
      "[step: 1685] loss: 0.006759778130799532\n",
      "[step: 1686] loss: 80.57339477539062\n",
      "[step: 1686] loss: 0.006759449373930693\n",
      "[step: 1687] loss: 80.52560424804688\n",
      "[step: 1687] loss: 0.006759110372513533\n",
      "[step: 1688] loss: 80.47773742675781\n",
      "[step: 1688] loss: 0.006758771371096373\n",
      "[step: 1689] loss: 80.42985534667969\n",
      "[step: 1689] loss: 0.006758439354598522\n",
      "[step: 1690] loss: 80.38197326660156\n",
      "[step: 1690] loss: 0.006758106406778097\n",
      "[step: 1691] loss: 80.334228515625\n",
      "[step: 1691] loss: 0.006757768336683512\n",
      "[step: 1692] loss: 80.28672790527344\n",
      "[step: 1692] loss: 0.00675742793828249\n",
      "[step: 1693] loss: 80.23916625976562\n",
      "[step: 1693] loss: 0.006757098250091076\n",
      "[step: 1694] loss: 80.19169616699219\n",
      "[step: 1694] loss: 0.006756765302270651\n",
      "[step: 1695] loss: 80.1445083618164\n",
      "[step: 1695] loss: 0.006756432354450226\n",
      "[step: 1696] loss: 80.09719848632812\n",
      "[step: 1696] loss: 0.006756102200597525\n",
      "[step: 1697] loss: 80.05001068115234\n",
      "[step: 1697] loss: 0.006755777634680271\n",
      "[step: 1698] loss: 80.00285339355469\n",
      "[step: 1698] loss: 0.006755453068763018\n",
      "[step: 1699] loss: 79.95571899414062\n",
      "[step: 1699] loss: 0.006755148060619831\n",
      "[step: 1700] loss: 79.90853881835938\n",
      "[step: 1700] loss: 0.006754860281944275\n",
      "[step: 1701] loss: 79.86156463623047\n",
      "[step: 1701] loss: 0.006754612550139427\n",
      "[step: 1702] loss: 79.81466674804688\n",
      "[step: 1702] loss: 0.006754437927156687\n",
      "[step: 1703] loss: 79.76786804199219\n",
      "[step: 1703] loss: 0.00675439415499568\n",
      "[step: 1704] loss: 79.72102355957031\n",
      "[step: 1704] loss: 0.006754609756171703\n",
      "[step: 1705] loss: 79.67436981201172\n",
      "[step: 1705] loss: 0.00675528310239315\n",
      "[step: 1706] loss: 79.62773132324219\n",
      "[step: 1706] loss: 0.0067568873055279255\n",
      "[step: 1707] loss: 79.5811767578125\n",
      "[step: 1707] loss: 0.006760023068636656\n",
      "[step: 1708] loss: 79.53469848632812\n",
      "[step: 1708] loss: 0.006766209378838539\n",
      "[step: 1709] loss: 79.48822021484375\n",
      "[step: 1709] loss: 0.006776097696274519\n",
      "[step: 1710] loss: 79.44194030761719\n",
      "[step: 1710] loss: 0.006792089436203241\n",
      "[step: 1711] loss: 79.39573669433594\n",
      "[step: 1711] loss: 0.006806108169257641\n",
      "[step: 1712] loss: 79.34954833984375\n",
      "[step: 1712] loss: 0.006813227664679289\n",
      "[step: 1713] loss: 79.30345153808594\n",
      "[step: 1713] loss: 0.006792809814214706\n",
      "[step: 1714] loss: 79.25733184814453\n",
      "[step: 1714] loss: 0.006763631012290716\n",
      "[step: 1715] loss: 79.2114486694336\n",
      "[step: 1715] loss: 0.006750189233571291\n",
      "[step: 1716] loss: 79.16554260253906\n",
      "[step: 1716] loss: 0.0067622778005898\n",
      "[step: 1717] loss: 79.11984252929688\n",
      "[step: 1717] loss: 0.006778194103389978\n",
      "[step: 1718] loss: 79.07420349121094\n",
      "[step: 1718] loss: 0.006772493477910757\n",
      "[step: 1719] loss: 79.02870178222656\n",
      "[step: 1719] loss: 0.006754954811185598\n",
      "[step: 1720] loss: 78.98332214355469\n",
      "[step: 1720] loss: 0.006749320309609175\n",
      "[step: 1721] loss: 78.93822479248047\n",
      "[step: 1721] loss: 0.0067596896551549435\n",
      "[step: 1722] loss: 78.89353942871094\n",
      "[step: 1722] loss: 0.006766066886484623\n",
      "[step: 1723] loss: 78.84934997558594\n",
      "[step: 1723] loss: 0.006756967399269342\n",
      "[step: 1724] loss: 78.80609130859375\n",
      "[step: 1724] loss: 0.006747725885361433\n",
      "[step: 1725] loss: 78.76416015625\n",
      "[step: 1725] loss: 0.00675048166885972\n",
      "[step: 1726] loss: 78.72488403320312\n",
      "[step: 1726] loss: 0.0067571233958005905\n",
      "[step: 1727] loss: 78.6898193359375\n",
      "[step: 1727] loss: 0.006755606736987829\n",
      "[step: 1728] loss: 78.66254425048828\n",
      "[step: 1728] loss: 0.006748069543391466\n",
      "[step: 1729] loss: 78.64888763427734\n",
      "[step: 1729] loss: 0.006746113300323486\n",
      "[step: 1730] loss: 78.66224670410156\n",
      "[step: 1730] loss: 0.00675040390342474\n",
      "[step: 1731] loss: 78.72100830078125\n",
      "[step: 1731] loss: 0.006751957815140486\n",
      "[step: 1732] loss: 78.876708984375\n",
      "[step: 1732] loss: 0.006748002488166094\n",
      "[step: 1733] loss: 79.1875\n",
      "[step: 1733] loss: 0.006744444370269775\n",
      "[step: 1734] loss: 79.85459899902344\n",
      "[step: 1734] loss: 0.0067457165569067\n",
      "[step: 1735] loss: 80.99789428710938\n",
      "[step: 1735] loss: 0.006748151499778032\n",
      "[step: 1736] loss: 83.33692932128906\n",
      "[step: 1736] loss: 0.006746950093656778\n",
      "[step: 1737] loss: 86.40121459960938\n",
      "[step: 1737] loss: 0.006743848789483309\n",
      "[step: 1738] loss: 91.72416687011719\n",
      "[step: 1738] loss: 0.006743012461811304\n",
      "[step: 1739] loss: 93.39434051513672\n",
      "[step: 1739] loss: 0.006744523532688618\n",
      "[step: 1740] loss: 93.06768798828125\n",
      "[step: 1740] loss: 0.006745040416717529\n",
      "[step: 1741] loss: 84.38130950927734\n",
      "[step: 1741] loss: 0.006743274163454771\n",
      "[step: 1742] loss: 78.49612426757812\n",
      "[step: 1742] loss: 0.006741615477949381\n",
      "[step: 1743] loss: 79.95635986328125\n",
      "[step: 1743] loss: 0.006741803139448166\n",
      "[step: 1744] loss: 84.48812866210938\n",
      "[step: 1744] loss: 0.006742606405168772\n",
      "[step: 1745] loss: 85.42933654785156\n",
      "[step: 1745] loss: 0.0067421915009617805\n",
      "[step: 1746] loss: 80.33163452148438\n",
      "[step: 1746] loss: 0.006740775890648365\n",
      "[step: 1747] loss: 78.00569152832031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1747] loss: 0.006740011274814606\n",
      "[step: 1748] loss: 80.70137786865234\n",
      "[step: 1748] loss: 0.006740297190845013\n",
      "[step: 1749] loss: 82.09986877441406\n",
      "[step: 1749] loss: 0.006740501616150141\n",
      "[step: 1750] loss: 79.82653045654297\n",
      "[step: 1750] loss: 0.00673985481262207\n",
      "[step: 1751] loss: 77.77934265136719\n",
      "[step: 1751] loss: 0.006738896016031504\n",
      "[step: 1752] loss: 79.27594757080078\n",
      "[step: 1752] loss: 0.006738502066582441\n",
      "[step: 1753] loss: 80.73085021972656\n",
      "[step: 1753] loss: 0.006738621275871992\n",
      "[step: 1754] loss: 78.99855041503906\n",
      "[step: 1754] loss: 0.00673852302134037\n",
      "[step: 1755] loss: 77.59989929199219\n",
      "[step: 1755] loss: 0.006737919058650732\n",
      "[step: 1756] loss: 78.58332824707031\n",
      "[step: 1756] loss: 0.006737250369042158\n",
      "[step: 1757] loss: 79.3299331665039\n",
      "[step: 1757] loss: 0.006736957002431154\n",
      "[step: 1758] loss: 78.33116149902344\n",
      "[step: 1758] loss: 0.006736914161592722\n",
      "[step: 1759] loss: 77.43956756591797\n",
      "[step: 1759] loss: 0.00673668785020709\n",
      "[step: 1760] loss: 78.10140991210938\n",
      "[step: 1760] loss: 0.006736184936016798\n",
      "[step: 1761] loss: 78.62397766113281\n",
      "[step: 1761] loss: 0.00673566572368145\n",
      "[step: 1762] loss: 77.81687927246094\n",
      "[step: 1762] loss: 0.006735384464263916\n",
      "[step: 1763] loss: 77.2735824584961\n",
      "[step: 1763] loss: 0.006735231261700392\n",
      "[step: 1764] loss: 77.74368286132812\n",
      "[step: 1764] loss: 0.006734972819685936\n",
      "[step: 1765] loss: 77.97327423095703\n",
      "[step: 1765] loss: 0.006734542548656464\n",
      "[step: 1766] loss: 77.44615936279297\n",
      "[step: 1766] loss: 0.006734108552336693\n",
      "[step: 1767] loss: 77.11094665527344\n",
      "[step: 1767] loss: 0.006733804475516081\n",
      "[step: 1768] loss: 77.42486572265625\n",
      "[step: 1768] loss: 0.0067335874773561954\n",
      "[step: 1769] loss: 77.56448364257812\n",
      "[step: 1769] loss: 0.0067333257757127285\n",
      "[step: 1770] loss: 77.16452026367188\n",
      "[step: 1770] loss: 0.006732952315360308\n",
      "[step: 1771] loss: 76.94619750976562\n",
      "[step: 1771] loss: 0.006732561159878969\n",
      "[step: 1772] loss: 77.146484375\n",
      "[step: 1772] loss: 0.0067322333343327045\n",
      "[step: 1773] loss: 77.20916748046875\n",
      "[step: 1773] loss: 0.006731977686285973\n",
      "[step: 1774] loss: 76.94821166992188\n",
      "[step: 1774] loss: 0.006731703411787748\n",
      "[step: 1775] loss: 76.77540588378906\n",
      "[step: 1775] loss: 0.006731375120580196\n",
      "[step: 1776] loss: 76.8824234008789\n",
      "[step: 1776] loss: 0.006731016095727682\n",
      "[step: 1777] loss: 76.93785095214844\n",
      "[step: 1777] loss: 0.006730679888278246\n",
      "[step: 1778] loss: 76.75996398925781\n",
      "[step: 1778] loss: 0.006730386987328529\n",
      "[step: 1779] loss: 76.6106948852539\n",
      "[step: 1779] loss: 0.006730100605636835\n",
      "[step: 1780] loss: 76.64772033691406\n",
      "[step: 1780] loss: 0.006729800254106522\n",
      "[step: 1781] loss: 76.69126892089844\n",
      "[step: 1781] loss: 0.0067294687032699585\n",
      "[step: 1782] loss: 76.59083557128906\n",
      "[step: 1782] loss: 0.006729135289788246\n",
      "[step: 1783] loss: 76.4568862915039\n",
      "[step: 1783] loss: 0.00672881631180644\n",
      "[step: 1784] loss: 76.43751525878906\n",
      "[step: 1784] loss: 0.006728520616889\n",
      "[step: 1785] loss: 76.46871185302734\n",
      "[step: 1785] loss: 0.006728228647261858\n",
      "[step: 1786] loss: 76.4197998046875\n",
      "[step: 1786] loss: 0.006727910600602627\n",
      "[step: 1787] loss: 76.31343078613281\n",
      "[step: 1787] loss: 0.006727592088282108\n",
      "[step: 1788] loss: 76.25479125976562\n",
      "[step: 1788] loss: 0.006727267988026142\n",
      "[step: 1789] loss: 76.25799560546875\n",
      "[step: 1789] loss: 0.006726963445544243\n",
      "[step: 1790] loss: 76.24236297607422\n",
      "[step: 1790] loss: 0.006726657971739769\n",
      "[step: 1791] loss: 76.17072296142578\n",
      "[step: 1791] loss: 0.006726356223225594\n",
      "[step: 1792] loss: 76.09707641601562\n",
      "[step: 1792] loss: 0.006726042367517948\n",
      "[step: 1793] loss: 76.06706237792969\n",
      "[step: 1793] loss: 0.006725722458213568\n",
      "[step: 1794] loss: 76.05613708496094\n",
      "[step: 1794] loss: 0.006725408136844635\n",
      "[step: 1795] loss: 76.01822662353516\n",
      "[step: 1795] loss: 0.006725093815475702\n",
      "[step: 1796] loss: 75.95399475097656\n",
      "[step: 1796] loss: 0.0067247869446873665\n",
      "[step: 1797] loss: 75.90086364746094\n",
      "[step: 1797] loss: 0.006724483333528042\n",
      "[step: 1798] loss: 75.87381744384766\n",
      "[step: 1798] loss: 0.006724176928400993\n",
      "[step: 1799] loss: 75.85057830810547\n",
      "[step: 1799] loss: 0.006723861675709486\n",
      "[step: 1800] loss: 75.80848693847656\n",
      "[step: 1800] loss: 0.006723545957356691\n",
      "[step: 1801] loss: 75.75410461425781\n",
      "[step: 1801] loss: 0.0067232344299554825\n",
      "[step: 1802] loss: 75.70892333984375\n",
      "[step: 1802] loss: 0.0067229242995381355\n",
      "[step: 1803] loss: 75.67840576171875\n",
      "[step: 1803] loss: 0.006722620222717524\n",
      "[step: 1804] loss: 75.6491470336914\n",
      "[step: 1804] loss: 0.006722309160977602\n",
      "[step: 1805] loss: 75.60903930664062\n",
      "[step: 1805] loss: 0.006721991579979658\n",
      "[step: 1806] loss: 75.5615234375\n",
      "[step: 1806] loss: 0.006721682380884886\n",
      "[step: 1807] loss: 75.5186538696289\n",
      "[step: 1807] loss: 0.006721378304064274\n",
      "[step: 1808] loss: 75.48422241210938\n",
      "[step: 1808] loss: 0.006721070036292076\n",
      "[step: 1809] loss: 75.45223236083984\n",
      "[step: 1809] loss: 0.006720759440213442\n",
      "[step: 1810] loss: 75.41508483886719\n",
      "[step: 1810] loss: 0.006720446515828371\n",
      "[step: 1811] loss: 75.37248229980469\n",
      "[step: 1811] loss: 0.006720136385411024\n",
      "[step: 1812] loss: 75.33052825927734\n",
      "[step: 1812] loss: 0.006719827651977539\n",
      "[step: 1813] loss: 75.2930908203125\n",
      "[step: 1813] loss: 0.006719518918544054\n",
      "[step: 1814] loss: 75.25901794433594\n",
      "[step: 1814] loss: 0.006719206925481558\n",
      "[step: 1815] loss: 75.22369384765625\n",
      "[step: 1815] loss: 0.006718897260725498\n",
      "[step: 1816] loss: 75.18499755859375\n",
      "[step: 1816] loss: 0.006718586198985577\n",
      "[step: 1817] loss: 75.14456176757812\n",
      "[step: 1817] loss: 0.00671827606856823\n",
      "[step: 1818] loss: 75.10565185546875\n",
      "[step: 1818] loss: 0.00671796128153801\n",
      "[step: 1819] loss: 75.06919860839844\n",
      "[step: 1819] loss: 0.0067176613956689835\n",
      "[step: 1820] loss: 75.03395080566406\n",
      "[step: 1820] loss: 0.006717347074300051\n",
      "[step: 1821] loss: 74.99776458740234\n",
      "[step: 1821] loss: 0.006717042997479439\n",
      "[step: 1822] loss: 74.95987701416016\n",
      "[step: 1822] loss: 0.006716728210449219\n",
      "[step: 1823] loss: 74.92121124267578\n",
      "[step: 1823] loss: 0.006716418080031872\n",
      "[step: 1824] loss: 74.88336181640625\n",
      "[step: 1824] loss: 0.0067161149345338345\n",
      "[step: 1825] loss: 74.84681701660156\n",
      "[step: 1825] loss: 0.006715801544487476\n",
      "[step: 1826] loss: 74.81108093261719\n",
      "[step: 1826] loss: 0.006715489085763693\n",
      "[step: 1827] loss: 74.77493286132812\n",
      "[step: 1827] loss: 0.006715182214975357\n",
      "[step: 1828] loss: 74.73799133300781\n",
      "[step: 1828] loss: 0.006714869290590286\n",
      "[step: 1829] loss: 74.70063781738281\n",
      "[step: 1829] loss: 0.0067145610228180885\n",
      "[step: 1830] loss: 74.66323852539062\n",
      "[step: 1830] loss: 0.006714254152029753\n",
      "[step: 1831] loss: 74.6265869140625\n",
      "[step: 1831] loss: 0.006713947746902704\n",
      "[step: 1832] loss: 74.59042358398438\n",
      "[step: 1832] loss: 0.006713633891195059\n",
      "[step: 1833] loss: 74.55450439453125\n",
      "[step: 1833] loss: 0.006713326089084148\n",
      "[step: 1834] loss: 74.51840209960938\n",
      "[step: 1834] loss: 0.006713017821311951\n",
      "[step: 1835] loss: 74.48190307617188\n",
      "[step: 1835] loss: 0.006712704431265593\n",
      "[step: 1836] loss: 74.44524383544922\n",
      "[step: 1836] loss: 0.006712398491799831\n",
      "[step: 1837] loss: 74.40867614746094\n",
      "[step: 1837] loss: 0.006712086033076048\n",
      "[step: 1838] loss: 74.37236022949219\n",
      "[step: 1838] loss: 0.006711780093610287\n",
      "[step: 1839] loss: 74.33633422851562\n",
      "[step: 1839] loss: 0.006711466237902641\n",
      "[step: 1840] loss: 74.30046081542969\n",
      "[step: 1840] loss: 0.006711162161082029\n",
      "[step: 1841] loss: 74.26458740234375\n",
      "[step: 1841] loss: 0.006710853893309832\n",
      "[step: 1842] loss: 74.22871398925781\n",
      "[step: 1842] loss: 0.0067105479538440704\n",
      "[step: 1843] loss: 74.19270324707031\n",
      "[step: 1843] loss: 0.006710227578878403\n",
      "[step: 1844] loss: 74.15668487548828\n",
      "[step: 1844] loss: 0.006709928624331951\n",
      "[step: 1845] loss: 74.12069702148438\n",
      "[step: 1845] loss: 0.006709614768624306\n",
      "[step: 1846] loss: 74.08480834960938\n",
      "[step: 1846] loss: 0.006709306035190821\n",
      "[step: 1847] loss: 74.0490493774414\n",
      "[step: 1847] loss: 0.006708999164402485\n",
      "[step: 1848] loss: 74.01345825195312\n",
      "[step: 1848] loss: 0.0067086853086948395\n",
      "[step: 1849] loss: 73.97793579101562\n",
      "[step: 1849] loss: 0.006708374246954918\n",
      "[step: 1850] loss: 73.94241333007812\n",
      "[step: 1850] loss: 0.006708066910505295\n",
      "[step: 1851] loss: 73.90691375732422\n",
      "[step: 1851] loss: 0.00670775817707181\n",
      "[step: 1852] loss: 73.87138366699219\n",
      "[step: 1852] loss: 0.006707450374960899\n",
      "[step: 1853] loss: 73.83585357666016\n",
      "[step: 1853] loss: 0.006707149092108011\n",
      "[step: 1854] loss: 73.80036163330078\n",
      "[step: 1854] loss: 0.006706839427351952\n",
      "[step: 1855] loss: 73.76494598388672\n",
      "[step: 1855] loss: 0.006706530228257179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1856] loss: 73.72958374023438\n",
      "[step: 1856] loss: 0.006706220097839832\n",
      "[step: 1857] loss: 73.6943359375\n",
      "[step: 1857] loss: 0.006705909036099911\n",
      "[step: 1858] loss: 73.65914916992188\n",
      "[step: 1858] loss: 0.006705601233989\n",
      "[step: 1859] loss: 73.62388610839844\n",
      "[step: 1859] loss: 0.00670528644695878\n",
      "[step: 1860] loss: 73.58877563476562\n",
      "[step: 1860] loss: 0.006704983301460743\n",
      "[step: 1861] loss: 73.55363464355469\n",
      "[step: 1861] loss: 0.006704682018607855\n",
      "[step: 1862] loss: 73.5186767578125\n",
      "[step: 1862] loss: 0.006704364903271198\n",
      "[step: 1863] loss: 73.48368835449219\n",
      "[step: 1863] loss: 0.006704061757773161\n",
      "[step: 1864] loss: 73.44876861572266\n",
      "[step: 1864] loss: 0.0067037553526461124\n",
      "[step: 1865] loss: 73.41384887695312\n",
      "[step: 1865] loss: 0.0067034452222287655\n",
      "[step: 1866] loss: 73.37894439697266\n",
      "[step: 1866] loss: 0.006703137420117855\n",
      "[step: 1867] loss: 73.34414672851562\n",
      "[step: 1867] loss: 0.006702831480652094\n",
      "[step: 1868] loss: 73.30926513671875\n",
      "[step: 1868] loss: 0.006702520418912172\n",
      "[step: 1869] loss: 73.27456665039062\n",
      "[step: 1869] loss: 0.006702215876430273\n",
      "[step: 1870] loss: 73.23981475830078\n",
      "[step: 1870] loss: 0.006701915990561247\n",
      "[step: 1871] loss: 73.20518493652344\n",
      "[step: 1871] loss: 0.006701606325805187\n",
      "[step: 1872] loss: 73.17063903808594\n",
      "[step: 1872] loss: 0.006701296661049128\n",
      "[step: 1873] loss: 73.13601684570312\n",
      "[step: 1873] loss: 0.006700994446873665\n",
      "[step: 1874] loss: 73.1015625\n",
      "[step: 1874] loss: 0.00670070294290781\n",
      "[step: 1875] loss: 73.06712341308594\n",
      "[step: 1875] loss: 0.00670041237026453\n",
      "[step: 1876] loss: 73.03282165527344\n",
      "[step: 1876] loss: 0.006700138561427593\n",
      "[step: 1877] loss: 72.99858093261719\n",
      "[step: 1877] loss: 0.006699893157929182\n",
      "[step: 1878] loss: 72.96450805664062\n",
      "[step: 1878] loss: 0.006699683144688606\n",
      "[step: 1879] loss: 72.93075561523438\n",
      "[step: 1879] loss: 0.006699565332382917\n",
      "[step: 1880] loss: 72.89732360839844\n",
      "[step: 1880] loss: 0.006699577439576387\n",
      "[step: 1881] loss: 72.86449432373047\n",
      "[step: 1881] loss: 0.006699852645397186\n",
      "[step: 1882] loss: 72.8326644897461\n",
      "[step: 1882] loss: 0.006700587924569845\n",
      "[step: 1883] loss: 72.802490234375\n",
      "[step: 1883] loss: 0.0067022135481238365\n",
      "[step: 1884] loss: 72.77528381347656\n",
      "[step: 1884] loss: 0.0067052580416202545\n",
      "[step: 1885] loss: 72.75311279296875\n",
      "[step: 1885] loss: 0.0067110611125826836\n",
      "[step: 1886] loss: 72.74006652832031\n",
      "[step: 1886] loss: 0.0067201582714915276\n",
      "[step: 1887] loss: 72.74434661865234\n",
      "[step: 1887] loss: 0.0067348056472837925\n",
      "[step: 1888] loss: 72.77842712402344\n",
      "[step: 1888] loss: 0.006748632062226534\n",
      "[step: 1889] loss: 72.8736572265625\n",
      "[step: 1889] loss: 0.006758356001228094\n",
      "[step: 1890] loss: 73.07041931152344\n",
      "[step: 1890] loss: 0.0067439004778862\n",
      "[step: 1891] loss: 73.49132537841797\n",
      "[step: 1891] loss: 0.006717078387737274\n",
      "[step: 1892] loss: 74.24452209472656\n",
      "[step: 1892] loss: 0.006696833297610283\n",
      "[step: 1893] loss: 75.79476165771484\n",
      "[step: 1893] loss: 0.006701115518808365\n",
      "[step: 1894] loss: 78.1397933959961\n",
      "[step: 1894] loss: 0.00671799574047327\n",
      "[step: 1895] loss: 82.59506225585938\n",
      "[step: 1895] loss: 0.0067217713221907616\n",
      "[step: 1896] loss: 86.28445434570312\n",
      "[step: 1896] loss: 0.006708563771098852\n",
      "[step: 1897] loss: 90.4849853515625\n",
      "[step: 1897] loss: 0.006695068906992674\n",
      "[step: 1898] loss: 85.14643859863281\n",
      "[step: 1898] loss: 0.006698147859424353\n",
      "[step: 1899] loss: 77.46942901611328\n",
      "[step: 1899] loss: 0.006708793807774782\n",
      "[step: 1900] loss: 72.4598388671875\n",
      "[step: 1900] loss: 0.006708723492920399\n",
      "[step: 1901] loss: 75.42903900146484\n",
      "[step: 1901] loss: 0.006698859389871359\n",
      "[step: 1902] loss: 80.51683044433594\n",
      "[step: 1902] loss: 0.006692720111459494\n",
      "[step: 1903] loss: 78.52676391601562\n",
      "[step: 1903] loss: 0.006696945987641811\n",
      "[step: 1904] loss: 73.64630126953125\n",
      "[step: 1904] loss: 0.006702605169266462\n",
      "[step: 1905] loss: 72.60154724121094\n",
      "[step: 1905] loss: 0.006700115278363228\n",
      "[step: 1906] loss: 75.78147888183594\n",
      "[step: 1906] loss: 0.006693512201309204\n",
      "[step: 1907] loss: 76.729736328125\n",
      "[step: 1907] loss: 0.006691389251500368\n",
      "[step: 1908] loss: 73.36304473876953\n",
      "[step: 1908] loss: 0.006694834213703871\n",
      "[step: 1909] loss: 72.26993560791016\n",
      "[step: 1909] loss: 0.006697244476526976\n",
      "[step: 1910] loss: 74.45684814453125\n",
      "[step: 1910] loss: 0.006694524083286524\n",
      "[step: 1911] loss: 74.78791809082031\n",
      "[step: 1911] loss: 0.006690623704344034\n",
      "[step: 1912] loss: 72.779541015625\n",
      "[step: 1912] loss: 0.006690156180411577\n",
      "[step: 1913] loss: 72.06448364257812\n",
      "[step: 1913] loss: 0.006692368071526289\n",
      "[step: 1914] loss: 73.43397521972656\n",
      "[step: 1914] loss: 0.006693160627037287\n",
      "[step: 1915] loss: 73.75464630126953\n",
      "[step: 1915] loss: 0.0066909948363900185\n",
      "[step: 1916] loss: 72.29872131347656\n",
      "[step: 1916] loss: 0.006688746623694897\n",
      "[step: 1917] loss: 71.96560668945312\n",
      "[step: 1917] loss: 0.006688761059194803\n",
      "[step: 1918] loss: 72.92903137207031\n",
      "[step: 1918] loss: 0.0066900113597512245\n",
      "[step: 1919] loss: 72.86359405517578\n",
      "[step: 1919] loss: 0.006690113805234432\n",
      "[step: 1920] loss: 71.92522430419922\n",
      "[step: 1920] loss: 0.006688577588647604\n",
      "[step: 1921] loss: 71.83647918701172\n",
      "[step: 1921] loss: 0.0066872090101242065\n",
      "[step: 1922] loss: 72.4375228881836\n",
      "[step: 1922] loss: 0.006687220651656389\n",
      "[step: 1923] loss: 72.33085632324219\n",
      "[step: 1923] loss: 0.0066878534853458405\n",
      "[step: 1924] loss: 71.68757629394531\n",
      "[step: 1924] loss: 0.006687742657959461\n",
      "[step: 1925] loss: 71.71394348144531\n",
      "[step: 1925] loss: 0.006686696782708168\n",
      "[step: 1926] loss: 72.10893249511719\n",
      "[step: 1926] loss: 0.006685777101665735\n",
      "[step: 1927] loss: 71.91857147216797\n",
      "[step: 1927] loss: 0.006685655098408461\n",
      "[step: 1928] loss: 71.50349426269531\n",
      "[step: 1928] loss: 0.00668591633439064\n",
      "[step: 1929] loss: 71.5577392578125\n",
      "[step: 1929] loss: 0.006685777101665735\n",
      "[step: 1930] loss: 71.78475952148438\n",
      "[step: 1930] loss: 0.006685072556138039\n",
      "[step: 1931] loss: 71.64695739746094\n",
      "[step: 1931] loss: 0.00668437359854579\n",
      "[step: 1932] loss: 71.36160278320312\n",
      "[step: 1932] loss: 0.006684125401079655\n",
      "[step: 1933] loss: 71.38540649414062\n",
      "[step: 1933] loss: 0.006684161256998777\n",
      "[step: 1934] loss: 71.53375244140625\n",
      "[step: 1934] loss: 0.006684021558612585\n",
      "[step: 1935] loss: 71.43316650390625\n",
      "[step: 1935] loss: 0.006683534011244774\n",
      "[step: 1936] loss: 71.2314453125\n",
      "[step: 1936] loss: 0.006682973820716143\n",
      "[step: 1937] loss: 71.21719360351562\n",
      "[step: 1937] loss: 0.006682646460831165\n",
      "[step: 1938] loss: 71.30648040771484\n",
      "[step: 1938] loss: 0.0066825225949287415\n",
      "[step: 1939] loss: 71.26280212402344\n",
      "[step: 1939] loss: 0.006682382430881262\n",
      "[step: 1940] loss: 71.11346435546875\n",
      "[step: 1940] loss: 0.006682038772851229\n",
      "[step: 1941] loss: 71.06168365478516\n",
      "[step: 1941] loss: 0.00668159918859601\n",
      "[step: 1942] loss: 71.11038208007812\n",
      "[step: 1942] loss: 0.006681225262582302\n",
      "[step: 1943] loss: 71.09896850585938\n",
      "[step: 1943] loss: 0.00668099382892251\n",
      "[step: 1944] loss: 70.99917602539062\n",
      "[step: 1944] loss: 0.006680811755359173\n",
      "[step: 1945] loss: 70.92581939697266\n",
      "[step: 1945] loss: 0.006680556107312441\n",
      "[step: 1946] loss: 70.9324951171875\n",
      "[step: 1946] loss: 0.006680207327008247\n",
      "[step: 1947] loss: 70.93975830078125\n",
      "[step: 1947] loss: 0.006679838057607412\n",
      "[step: 1948] loss: 70.88162231445312\n",
      "[step: 1948] loss: 0.0066795386373996735\n",
      "[step: 1949] loss: 70.80672454833984\n",
      "[step: 1949] loss: 0.006679304409772158\n",
      "[step: 1950] loss: 70.77789306640625\n",
      "[step: 1950] loss: 0.006679070182144642\n",
      "[step: 1951] loss: 70.77896118164062\n",
      "[step: 1951] loss: 0.006678787525743246\n",
      "[step: 1952] loss: 70.7546157836914\n",
      "[step: 1952] loss: 0.006678471341729164\n",
      "[step: 1953] loss: 70.69613647460938\n",
      "[step: 1953] loss: 0.006678140722215176\n",
      "[step: 1954] loss: 70.64646911621094\n",
      "[step: 1954] loss: 0.006677847355604172\n",
      "[step: 1955] loss: 70.62712860107422\n",
      "[step: 1955] loss: 0.006677599158138037\n",
      "[step: 1956] loss: 70.61424255371094\n",
      "[step: 1956] loss: 0.0066773477010428905\n",
      "[step: 1957] loss: 70.5807113647461\n",
      "[step: 1957] loss: 0.0066770645789802074\n",
      "[step: 1958] loss: 70.53192138671875\n",
      "[step: 1958] loss: 0.006676762364804745\n",
      "[step: 1959] loss: 70.4931411743164\n",
      "[step: 1959] loss: 0.006676455028355122\n",
      "[step: 1960] loss: 70.47149658203125\n",
      "[step: 1960] loss: 0.006676170043647289\n",
      "[step: 1961] loss: 70.45094299316406\n",
      "[step: 1961] loss: 0.006675902288407087\n",
      "[step: 1962] loss: 70.41789245605469\n",
      "[step: 1962] loss: 0.006675632204860449\n",
      "[step: 1963] loss: 70.37667083740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1963] loss: 0.006675358861684799\n",
      "[step: 1964] loss: 70.3410873413086\n",
      "[step: 1964] loss: 0.00667506642639637\n",
      "[step: 1965] loss: 70.31548309326172\n",
      "[step: 1965] loss: 0.0066747767850756645\n",
      "[step: 1966] loss: 70.2915267944336\n",
      "[step: 1966] loss: 0.006674488075077534\n",
      "[step: 1967] loss: 70.26113891601562\n",
      "[step: 1967] loss: 0.006674213334918022\n",
      "[step: 1968] loss: 70.2249526977539\n",
      "[step: 1968] loss: 0.006673941854387522\n",
      "[step: 1969] loss: 70.19033813476562\n",
      "[step: 1969] loss: 0.006673659197986126\n",
      "[step: 1970] loss: 70.16134643554688\n",
      "[step: 1970] loss: 0.006673390511423349\n",
      "[step: 1971] loss: 70.13520812988281\n",
      "[step: 1971] loss: 0.006673098541796207\n",
      "[step: 1972] loss: 70.1070556640625\n",
      "[step: 1972] loss: 0.006672816816717386\n",
      "[step: 1973] loss: 70.07479858398438\n",
      "[step: 1973] loss: 0.006672536488622427\n",
      "[step: 1974] loss: 70.04129028320312\n",
      "[step: 1974] loss: 0.006672253832221031\n",
      "[step: 1975] loss: 70.01011657714844\n",
      "[step: 1975] loss: 0.006671975366771221\n",
      "[step: 1976] loss: 69.98171997070312\n",
      "[step: 1976] loss: 0.006671704351902008\n",
      "[step: 1977] loss: 69.95396423339844\n",
      "[step: 1977] loss: 0.006671429146081209\n",
      "[step: 1978] loss: 69.92457580566406\n",
      "[step: 1978] loss: 0.0066711436957120895\n",
      "[step: 1979] loss: 69.89324951171875\n",
      "[step: 1979] loss: 0.0066708666272461414\n",
      "[step: 1980] loss: 69.86163330078125\n",
      "[step: 1980] loss: 0.006670584436506033\n",
      "[step: 1981] loss: 69.83134460449219\n",
      "[step: 1981] loss: 0.0066702961921691895\n",
      "[step: 1982] loss: 69.80247497558594\n",
      "[step: 1982] loss: 0.006670026574283838\n",
      "[step: 1983] loss: 69.77395629882812\n",
      "[step: 1983] loss: 0.00666974950581789\n",
      "[step: 1984] loss: 69.74470520019531\n",
      "[step: 1984] loss: 0.00666947104036808\n",
      "[step: 1985] loss: 69.71454620361328\n",
      "[step: 1985] loss: 0.006669189780950546\n",
      "[step: 1986] loss: 69.68386840820312\n",
      "[step: 1986] loss: 0.006668915972113609\n",
      "[step: 1987] loss: 69.65383911132812\n",
      "[step: 1987] loss: 0.00666863052174449\n",
      "[step: 1988] loss: 69.62442779541016\n",
      "[step: 1988] loss: 0.00666835717856884\n",
      "[step: 1989] loss: 69.59552001953125\n",
      "[step: 1989] loss: 0.006668074522167444\n",
      "[step: 1990] loss: 69.56655883789062\n",
      "[step: 1990] loss: 0.00666780024766922\n",
      "[step: 1991] loss: 69.53718566894531\n",
      "[step: 1991] loss: 0.0066675166599452496\n",
      "[step: 1992] loss: 69.50745391845703\n",
      "[step: 1992] loss: 0.0066672433167696\n",
      "[step: 1993] loss: 69.47765350341797\n",
      "[step: 1993] loss: 0.006666966248303652\n",
      "[step: 1994] loss: 69.44804382324219\n",
      "[step: 1994] loss: 0.006666685454547405\n",
      "[step: 1995] loss: 69.41871643066406\n",
      "[step: 1995] loss: 0.006666410248726606\n",
      "[step: 1996] loss: 69.3897476196289\n",
      "[step: 1996] loss: 0.006666130851954222\n",
      "[step: 1997] loss: 69.36074829101562\n",
      "[step: 1997] loss: 0.006665854714810848\n",
      "[step: 1998] loss: 69.3316650390625\n",
      "[step: 1998] loss: 0.006665574386715889\n",
      "[step: 1999] loss: 69.30239868164062\n",
      "[step: 1999] loss: 0.006665302440524101\n",
      "[step: 2000] loss: 69.27305603027344\n",
      "[step: 2000] loss: 0.006665022578090429\n",
      "[step: 2001] loss: 69.24374389648438\n",
      "[step: 2001] loss: 0.006664743646979332\n",
      "[step: 2002] loss: 69.21446990966797\n",
      "[step: 2002] loss: 0.00666446890681982\n",
      "[step: 2003] loss: 69.18537902832031\n",
      "[step: 2003] loss: 0.00666419044137001\n",
      "[step: 2004] loss: 69.15644073486328\n",
      "[step: 2004] loss: 0.006663911044597626\n",
      "[step: 2005] loss: 69.12752532958984\n",
      "[step: 2005] loss: 0.006663633044809103\n",
      "[step: 2006] loss: 69.09872436523438\n",
      "[step: 2006] loss: 0.0066633508540689945\n",
      "[step: 2007] loss: 69.06974792480469\n",
      "[step: 2007] loss: 0.006663074251264334\n",
      "[step: 2008] loss: 69.04080963134766\n",
      "[step: 2008] loss: 0.006662807427346706\n",
      "[step: 2009] loss: 69.01179504394531\n",
      "[step: 2009] loss: 0.006662528961896896\n",
      "[step: 2010] loss: 68.98284912109375\n",
      "[step: 2010] loss: 0.006662257015705109\n",
      "[step: 2011] loss: 68.95387268066406\n",
      "[step: 2011] loss: 0.006661974359303713\n",
      "[step: 2012] loss: 68.925048828125\n",
      "[step: 2012] loss: 0.006661700550466776\n",
      "[step: 2013] loss: 68.8962173461914\n",
      "[step: 2013] loss: 0.0066614290699362755\n",
      "[step: 2014] loss: 68.8674545288086\n",
      "[step: 2014] loss: 0.00666115153580904\n",
      "[step: 2015] loss: 68.83882904052734\n",
      "[step: 2015] loss: 0.006660874467343092\n",
      "[step: 2016] loss: 68.81001281738281\n",
      "[step: 2016] loss: 0.006660593207925558\n",
      "[step: 2017] loss: 68.78136444091797\n",
      "[step: 2017] loss: 0.006660322193056345\n",
      "[step: 2018] loss: 68.75273132324219\n",
      "[step: 2018] loss: 0.006660042330622673\n",
      "[step: 2019] loss: 68.7240982055664\n",
      "[step: 2019] loss: 0.0066597675904631615\n",
      "[step: 2020] loss: 68.69544982910156\n",
      "[step: 2020] loss: 0.006659490521997213\n",
      "[step: 2021] loss: 68.66683197021484\n",
      "[step: 2021] loss: 0.006659216247498989\n",
      "[step: 2022] loss: 68.63825988769531\n",
      "[step: 2022] loss: 0.0066589415073394775\n",
      "[step: 2023] loss: 68.6097412109375\n",
      "[step: 2023] loss: 0.00665866956114769\n",
      "[step: 2024] loss: 68.5811767578125\n",
      "[step: 2024] loss: 0.006658389698714018\n",
      "[step: 2025] loss: 68.55264282226562\n",
      "[step: 2025] loss: 0.006658115424215794\n",
      "[step: 2026] loss: 68.52412414550781\n",
      "[step: 2026] loss: 0.006657841615378857\n",
      "[step: 2027] loss: 68.4957504272461\n",
      "[step: 2027] loss: 0.006657562218606472\n",
      "[step: 2028] loss: 68.46723175048828\n",
      "[step: 2028] loss: 0.0066572995856404305\n",
      "[step: 2029] loss: 68.43888854980469\n",
      "[step: 2029] loss: 0.006657018326222897\n",
      "[step: 2030] loss: 68.41051483154297\n",
      "[step: 2030] loss: 0.006656743120402098\n",
      "[step: 2031] loss: 68.38226318359375\n",
      "[step: 2031] loss: 0.0066564688459038734\n",
      "[step: 2032] loss: 68.35388946533203\n",
      "[step: 2032] loss: 0.00665619969367981\n",
      "[step: 2033] loss: 68.32569885253906\n",
      "[step: 2033] loss: 0.006655923556536436\n",
      "[step: 2034] loss: 68.29759216308594\n",
      "[step: 2034] loss: 0.006655648350715637\n",
      "[step: 2035] loss: 68.26951599121094\n",
      "[step: 2035] loss: 0.006655379664152861\n",
      "[step: 2036] loss: 68.24166870117188\n",
      "[step: 2036] loss: 0.006655110511928797\n",
      "[step: 2037] loss: 68.21411895751953\n",
      "[step: 2037] loss: 0.006654841359704733\n",
      "[step: 2038] loss: 68.18690490722656\n",
      "[step: 2038] loss: 0.006654584780335426\n",
      "[step: 2039] loss: 68.16033172607422\n",
      "[step: 2039] loss: 0.0066543337889015675\n",
      "[step: 2040] loss: 68.13504791259766\n",
      "[step: 2040] loss: 0.006654099561274052\n",
      "[step: 2041] loss: 68.1116943359375\n",
      "[step: 2041] loss: 0.006653885822743177\n",
      "[step: 2042] loss: 68.0920181274414\n",
      "[step: 2042] loss: 0.0066537135280668736\n",
      "[step: 2043] loss: 68.07861328125\n",
      "[step: 2043] loss: 0.006653619464486837\n",
      "[step: 2044] loss: 68.07757568359375\n",
      "[step: 2044] loss: 0.006653649732470512\n",
      "[step: 2045] loss: 68.09765625\n",
      "[step: 2045] loss: 0.006653901655226946\n",
      "[step: 2046] loss: 68.16102600097656\n",
      "[step: 2046] loss: 0.006654536351561546\n",
      "[step: 2047] loss: 68.29875946044922\n",
      "[step: 2047] loss: 0.006655878387391567\n",
      "[step: 2048] loss: 68.59724426269531\n",
      "[step: 2048] loss: 0.006658324506133795\n",
      "[step: 2049] loss: 69.15288543701172\n",
      "[step: 2049] loss: 0.006662885658442974\n",
      "[step: 2050] loss: 70.3071517944336\n",
      "[step: 2050] loss: 0.0066700163297355175\n",
      "[step: 2051] loss: 72.20710754394531\n",
      "[step: 2051] loss: 0.0066817281767725945\n",
      "[step: 2052] loss: 75.97545623779297\n",
      "[step: 2052] loss: 0.006694507785141468\n",
      "[step: 2053] loss: 80.19989013671875\n",
      "[step: 2053] loss: 0.006707559805363417\n",
      "[step: 2054] loss: 86.3979721069336\n",
      "[step: 2054] loss: 0.006703940685838461\n",
      "[step: 2055] loss: 84.67070770263672\n",
      "[step: 2055] loss: 0.0066863978281617165\n",
      "[step: 2056] loss: 78.84092712402344\n",
      "[step: 2056] loss: 0.00666044419631362\n",
      "[step: 2057] loss: 69.5988540649414\n",
      "[step: 2057] loss: 0.006649876479059458\n",
      "[step: 2058] loss: 68.853759765625\n",
      "[step: 2058] loss: 0.006659315899014473\n",
      "[step: 2059] loss: 74.8094482421875\n",
      "[step: 2059] loss: 0.006672595627605915\n",
      "[step: 2060] loss: 76.05812072753906\n",
      "[step: 2060] loss: 0.006673401687294245\n",
      "[step: 2061] loss: 71.57630920410156\n",
      "[step: 2061] loss: 0.006659308448433876\n",
      "[step: 2062] loss: 67.75717163085938\n",
      "[step: 2062] loss: 0.006648913957178593\n",
      "[step: 2063] loss: 70.39788818359375\n",
      "[step: 2063] loss: 0.006652382202446461\n",
      "[step: 2064] loss: 73.21875762939453\n",
      "[step: 2064] loss: 0.006661265157163143\n",
      "[step: 2065] loss: 70.09642028808594\n",
      "[step: 2065] loss: 0.006662711966782808\n",
      "[step: 2066] loss: 67.62651824951172\n",
      "[step: 2066] loss: 0.006654366850852966\n",
      "[step: 2067] loss: 69.56050109863281\n",
      "[step: 2067] loss: 0.0066475155763328075\n",
      "[step: 2068] loss: 70.80050659179688\n",
      "[step: 2068] loss: 0.0066490694880485535\n",
      "[step: 2069] loss: 68.87092590332031\n",
      "[step: 2069] loss: 0.006654552184045315\n",
      "[step: 2070] loss: 67.510009765625\n",
      "[step: 2070] loss: 0.0066558076068758965\n",
      "[step: 2071] loss: 68.95684051513672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2071] loss: 0.006650879513472319\n",
      "[step: 2072] loss: 69.64576721191406\n",
      "[step: 2072] loss: 0.006646253168582916\n",
      "[step: 2073] loss: 68.00616455078125\n",
      "[step: 2073] loss: 0.006646501366049051\n",
      "[step: 2074] loss: 67.51222229003906\n",
      "[step: 2074] loss: 0.006649653427302837\n",
      "[step: 2075] loss: 68.63655090332031\n",
      "[step: 2075] loss: 0.006650841329246759\n",
      "[step: 2076] loss: 68.5556640625\n",
      "[step: 2076] loss: 0.006648198701441288\n",
      "[step: 2077] loss: 67.47685241699219\n",
      "[step: 2077] loss: 0.0066450550220906734\n",
      "[step: 2078] loss: 67.4924545288086\n",
      "[step: 2078] loss: 0.006644555367529392\n",
      "[step: 2079] loss: 68.18526458740234\n",
      "[step: 2079] loss: 0.00664620753377676\n",
      "[step: 2080] loss: 67.9073257446289\n",
      "[step: 2080] loss: 0.006647175643593073\n",
      "[step: 2081] loss: 67.22283935546875\n",
      "[step: 2081] loss: 0.00664591183885932\n",
      "[step: 2082] loss: 67.45297241210938\n",
      "[step: 2082] loss: 0.006643855944275856\n",
      "[step: 2083] loss: 67.8349380493164\n",
      "[step: 2083] loss: 0.006643026135861874\n",
      "[step: 2084] loss: 67.43475341796875\n",
      "[step: 2084] loss: 0.006643715314567089\n",
      "[step: 2085] loss: 67.08186340332031\n",
      "[step: 2085] loss: 0.0066444119438529015\n",
      "[step: 2086] loss: 67.33821105957031\n",
      "[step: 2086] loss: 0.006643894128501415\n",
      "[step: 2087] loss: 67.47393798828125\n",
      "[step: 2087] loss: 0.006642613094300032\n",
      "[step: 2088] loss: 67.15435028076172\n",
      "[step: 2088] loss: 0.00664173997938633\n",
      "[step: 2089] loss: 66.9765625\n",
      "[step: 2089] loss: 0.006641804240643978\n",
      "[step: 2090] loss: 67.16597747802734\n",
      "[step: 2090] loss: 0.006642207037657499\n",
      "[step: 2091] loss: 67.21673583984375\n",
      "[step: 2091] loss: 0.006642082240432501\n",
      "[step: 2092] loss: 66.97403717041016\n",
      "[step: 2092] loss: 0.006641338113695383\n",
      "[step: 2093] loss: 66.86796569824219\n",
      "[step: 2093] loss: 0.006640568841248751\n",
      "[step: 2094] loss: 66.99212646484375\n",
      "[step: 2094] loss: 0.006640271283686161\n",
      "[step: 2095] loss: 67.00444793701172\n",
      "[step: 2095] loss: 0.00664037000387907\n",
      "[step: 2096] loss: 66.83798217773438\n",
      "[step: 2096] loss: 0.0066403718665242195\n",
      "[step: 2097] loss: 66.75365447998047\n",
      "[step: 2097] loss: 0.0066400025971233845\n",
      "[step: 2098] loss: 66.82170867919922\n",
      "[step: 2098] loss: 0.006639419123530388\n",
      "[step: 2099] loss: 66.83660888671875\n",
      "[step: 2099] loss: 0.006638968363404274\n",
      "[step: 2100] loss: 66.72250366210938\n",
      "[step: 2100] loss: 0.006638799328356981\n",
      "[step: 2101] loss: 66.64247131347656\n",
      "[step: 2101] loss: 0.006638761144131422\n",
      "[step: 2102] loss: 66.66992950439453\n",
      "[step: 2102] loss: 0.006638587452471256\n",
      "[step: 2103] loss: 66.68457794189453\n",
      "[step: 2103] loss: 0.006638222839683294\n",
      "[step: 2104] loss: 66.61396789550781\n",
      "[step: 2104] loss: 0.0066377874463796616\n",
      "[step: 2105] loss: 66.53848266601562\n",
      "[step: 2105] loss: 0.0066374544985592365\n",
      "[step: 2106] loss: 66.5323257446289\n",
      "[step: 2106] loss: 0.006637267302721739\n",
      "[step: 2107] loss: 66.545166015625\n",
      "[step: 2107] loss: 0.0066371336579322815\n",
      "[step: 2108] loss: 66.50616455078125\n",
      "[step: 2108] loss: 0.0066369217820465565\n",
      "[step: 2109] loss: 66.44093322753906\n",
      "[step: 2109] loss: 0.00663659768179059\n",
      "[step: 2110] loss: 66.4100570678711\n",
      "[step: 2110] loss: 0.00663625355809927\n",
      "[step: 2111] loss: 66.41072082519531\n",
      "[step: 2111] loss: 0.006635955534875393\n",
      "[step: 2112] loss: 66.39425659179688\n",
      "[step: 2112] loss: 0.006635730154812336\n",
      "[step: 2113] loss: 66.34676361083984\n",
      "[step: 2113] loss: 0.0066355434246361256\n",
      "[step: 2114] loss: 66.30308532714844\n",
      "[step: 2114] loss: 0.006635318044573069\n",
      "[step: 2115] loss: 66.2857437133789\n",
      "[step: 2115] loss: 0.006635046098381281\n",
      "[step: 2116] loss: 66.27570343017578\n",
      "[step: 2116] loss: 0.006634749472141266\n",
      "[step: 2117] loss: 66.24800109863281\n",
      "[step: 2117] loss: 0.0066344644874334335\n",
      "[step: 2118] loss: 66.20703125\n",
      "[step: 2118] loss: 0.006634213495999575\n",
      "[step: 2119] loss: 66.17449951171875\n",
      "[step: 2119] loss: 0.006633988115936518\n",
      "[step: 2120] loss: 66.15680694580078\n",
      "[step: 2120] loss: 0.006633761338889599\n",
      "[step: 2121] loss: 66.13951110839844\n",
      "[step: 2121] loss: 0.0066335201263427734\n",
      "[step: 2122] loss: 66.11080169677734\n",
      "[step: 2122] loss: 0.0066332584246993065\n",
      "[step: 2123] loss: 66.0760498046875\n",
      "[step: 2123] loss: 0.006632977165281773\n",
      "[step: 2124] loss: 66.04719543457031\n",
      "[step: 2124] loss: 0.006632714532315731\n",
      "[step: 2125] loss: 66.02664947509766\n",
      "[step: 2125] loss: 0.006632476579397917\n",
      "[step: 2126] loss: 66.00625610351562\n",
      "[step: 2126] loss: 0.0066322265192866325\n",
      "[step: 2127] loss: 65.97940063476562\n",
      "[step: 2127] loss: 0.00663199694827199\n",
      "[step: 2128] loss: 65.94866180419922\n",
      "[step: 2128] loss: 0.006631749216467142\n",
      "[step: 2129] loss: 65.92060852050781\n",
      "[step: 2129] loss: 0.006631486117839813\n",
      "[step: 2130] loss: 65.89752197265625\n",
      "[step: 2130] loss: 0.006631226744502783\n",
      "[step: 2131] loss: 65.87564086914062\n",
      "[step: 2131] loss: 0.0066309706307947636\n",
      "[step: 2132] loss: 65.8508071899414\n",
      "[step: 2132] loss: 0.006630721036344767\n",
      "[step: 2133] loss: 65.8228759765625\n",
      "[step: 2133] loss: 0.006630477029830217\n",
      "[step: 2134] loss: 65.79522705078125\n",
      "[step: 2134] loss: 0.006630238611251116\n",
      "[step: 2135] loss: 65.77008056640625\n",
      "[step: 2135] loss: 0.00662999227643013\n",
      "[step: 2136] loss: 65.74700927734375\n",
      "[step: 2136] loss: 0.0066297417506575584\n",
      "[step: 2137] loss: 65.72331237792969\n",
      "[step: 2137] loss: 0.006629489362239838\n",
      "[step: 2138] loss: 65.69763946533203\n",
      "[step: 2138] loss: 0.00662923464551568\n",
      "[step: 2139] loss: 65.67095947265625\n",
      "[step: 2139] loss: 0.00662898737937212\n",
      "[step: 2140] loss: 65.64486694335938\n",
      "[step: 2140] loss: 0.006628736853599548\n",
      "[step: 2141] loss: 65.62028503417969\n",
      "[step: 2141] loss: 0.006628493312746286\n",
      "[step: 2142] loss: 65.596435546875\n",
      "[step: 2142] loss: 0.006628247443586588\n",
      "[step: 2143] loss: 65.5722427368164\n",
      "[step: 2143] loss: 0.006628001108765602\n",
      "[step: 2144] loss: 65.54698181152344\n",
      "[step: 2144] loss: 0.006627757102251053\n",
      "[step: 2145] loss: 65.52123260498047\n",
      "[step: 2145] loss: 0.006627506110817194\n",
      "[step: 2146] loss: 65.49578094482422\n",
      "[step: 2146] loss: 0.006627259775996208\n",
      "[step: 2147] loss: 65.47100830078125\n",
      "[step: 2147] loss: 0.006627001333981752\n",
      "[step: 2148] loss: 65.44680786132812\n",
      "[step: 2148] loss: 0.0066267647780478\n",
      "[step: 2149] loss: 65.4225082397461\n",
      "[step: 2149] loss: 0.006626512389630079\n",
      "[step: 2150] loss: 65.3978500366211\n",
      "[step: 2150] loss: 0.006626262329518795\n",
      "[step: 2151] loss: 65.37281799316406\n",
      "[step: 2151] loss: 0.006626018788665533\n",
      "[step: 2152] loss: 65.34771728515625\n",
      "[step: 2152] loss: 0.006625778507441282\n",
      "[step: 2153] loss: 65.32274627685547\n",
      "[step: 2153] loss: 0.006625523790717125\n",
      "[step: 2154] loss: 65.29817199707031\n",
      "[step: 2154] loss: 0.006625282112509012\n",
      "[step: 2155] loss: 65.27388000488281\n",
      "[step: 2155] loss: 0.006625030655413866\n",
      "[step: 2156] loss: 65.24945068359375\n",
      "[step: 2156] loss: 0.0066247861832380295\n",
      "[step: 2157] loss: 65.22496032714844\n",
      "[step: 2157] loss: 0.006624537520110607\n",
      "[step: 2158] loss: 65.20030975341797\n",
      "[step: 2158] loss: 0.006624290253967047\n",
      "[step: 2159] loss: 65.17552185058594\n",
      "[step: 2159] loss: 0.006624041590839624\n",
      "[step: 2160] loss: 65.15078735351562\n",
      "[step: 2160] loss: 0.006623797584325075\n",
      "[step: 2161] loss: 65.12625885009766\n",
      "[step: 2161] loss: 0.006623550318181515\n",
      "[step: 2162] loss: 65.10189819335938\n",
      "[step: 2162] loss: 0.00662330724298954\n",
      "[step: 2163] loss: 65.07755279541016\n",
      "[step: 2163] loss: 0.006623055785894394\n",
      "[step: 2164] loss: 65.05319213867188\n",
      "[step: 2164] loss: 0.006622813176363707\n",
      "[step: 2165] loss: 65.02880859375\n",
      "[step: 2165] loss: 0.006622570101171732\n",
      "[step: 2166] loss: 65.00437927246094\n",
      "[step: 2166] loss: 0.006622323766350746\n",
      "[step: 2167] loss: 64.9798812866211\n",
      "[step: 2167] loss: 0.0066220746375620365\n",
      "[step: 2168] loss: 64.9554214477539\n",
      "[step: 2168] loss: 0.006621837615966797\n",
      "[step: 2169] loss: 64.9310302734375\n",
      "[step: 2169] loss: 0.006621585227549076\n",
      "[step: 2170] loss: 64.90663146972656\n",
      "[step: 2170] loss: 0.00662133377045393\n",
      "[step: 2171] loss: 64.88236236572266\n",
      "[step: 2171] loss: 0.00662109674885869\n",
      "[step: 2172] loss: 64.8581314086914\n",
      "[step: 2172] loss: 0.0066208504140377045\n",
      "[step: 2173] loss: 64.83389282226562\n",
      "[step: 2173] loss: 0.006620605010539293\n",
      "[step: 2174] loss: 64.80965423583984\n",
      "[step: 2174] loss: 0.006620359607040882\n",
      "[step: 2175] loss: 64.78540802001953\n",
      "[step: 2175] loss: 0.006620115600526333\n",
      "[step: 2176] loss: 64.76116180419922\n",
      "[step: 2176] loss: 0.006619870662689209\n",
      "[step: 2177] loss: 64.7369384765625\n",
      "[step: 2177] loss: 0.006619623396545649\n",
      "[step: 2178] loss: 64.71266174316406\n",
      "[step: 2178] loss: 0.006619376130402088\n",
      "[step: 2179] loss: 64.68850708007812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2179] loss: 0.006619131658226252\n",
      "[step: 2180] loss: 64.66429901123047\n",
      "[step: 2180] loss: 0.006618889980018139\n",
      "[step: 2181] loss: 64.64017486572266\n",
      "[step: 2181] loss: 0.006618644576519728\n",
      "[step: 2182] loss: 64.61601257324219\n",
      "[step: 2182] loss: 0.006618397310376167\n",
      "[step: 2183] loss: 64.59183502197266\n",
      "[step: 2183] loss: 0.00661815470084548\n",
      "[step: 2184] loss: 64.56779479980469\n",
      "[step: 2184] loss: 0.006617906969040632\n",
      "[step: 2185] loss: 64.54366302490234\n",
      "[step: 2185] loss: 0.006617662962526083\n",
      "[step: 2186] loss: 64.51959991455078\n",
      "[step: 2186] loss: 0.006617413368076086\n",
      "[step: 2187] loss: 64.49549865722656\n",
      "[step: 2187] loss: 0.006617175880819559\n",
      "[step: 2188] loss: 64.47157287597656\n",
      "[step: 2188] loss: 0.006616928149014711\n",
      "[step: 2189] loss: 64.447509765625\n",
      "[step: 2189] loss: 0.00661669485270977\n",
      "[step: 2190] loss: 64.42350006103516\n",
      "[step: 2190] loss: 0.006616442464292049\n",
      "[step: 2191] loss: 64.39949798583984\n",
      "[step: 2191] loss: 0.0066162049770355225\n",
      "[step: 2192] loss: 64.37554168701172\n",
      "[step: 2192] loss: 0.006615955848246813\n",
      "[step: 2193] loss: 64.35160827636719\n",
      "[step: 2193] loss: 0.006615718360990286\n",
      "[step: 2194] loss: 64.32769012451172\n",
      "[step: 2194] loss: 0.006615476682782173\n",
      "[step: 2195] loss: 64.30384826660156\n",
      "[step: 2195] loss: 0.006615242455154657\n",
      "[step: 2196] loss: 64.27997589111328\n",
      "[step: 2196] loss: 0.006615008227527142\n",
      "[step: 2197] loss: 64.25621795654297\n",
      "[step: 2197] loss: 0.006614777725189924\n",
      "[step: 2198] loss: 64.2325439453125\n",
      "[step: 2198] loss: 0.006614564452320337\n",
      "[step: 2199] loss: 64.20892333984375\n",
      "[step: 2199] loss: 0.006614362820982933\n",
      "[step: 2200] loss: 64.18563842773438\n",
      "[step: 2200] loss: 0.006614194251596928\n",
      "[step: 2201] loss: 64.16262817382812\n",
      "[step: 2201] loss: 0.006614090409129858\n",
      "[step: 2202] loss: 64.14033508300781\n",
      "[step: 2202] loss: 0.006614075507968664\n",
      "[step: 2203] loss: 64.11898803710938\n",
      "[step: 2203] loss: 0.00661424407735467\n",
      "[step: 2204] loss: 64.09954833984375\n",
      "[step: 2204] loss: 0.006614724639803171\n",
      "[step: 2205] loss: 64.08352661132812\n",
      "[step: 2205] loss: 0.006615742109715939\n",
      "[step: 2206] loss: 64.07344055175781\n",
      "[step: 2206] loss: 0.006617793347686529\n",
      "[step: 2207] loss: 64.07504272460938\n",
      "[step: 2207] loss: 0.006621429231017828\n",
      "[step: 2208] loss: 64.09730529785156\n",
      "[step: 2208] loss: 0.006628117989748716\n",
      "[step: 2209] loss: 64.1614990234375\n",
      "[step: 2209] loss: 0.006638177204877138\n",
      "[step: 2210] loss: 64.29984283447266\n",
      "[step: 2210] loss: 0.006654058117419481\n",
      "[step: 2211] loss: 64.59777069091797\n",
      "[step: 2211] loss: 0.006668487098067999\n",
      "[step: 2212] loss: 65.15989685058594\n",
      "[step: 2212] loss: 0.00667882664129138\n",
      "[step: 2213] loss: 66.33096313476562\n",
      "[step: 2213] loss: 0.006664001848548651\n",
      "[step: 2214] loss: 68.30724334716797\n",
      "[step: 2214] loss: 0.006636143662035465\n",
      "[step: 2215] loss: 72.24539947509766\n",
      "[step: 2215] loss: 0.006613186094909906\n",
      "[step: 2216] loss: 76.86749267578125\n",
      "[step: 2216] loss: 0.006615204270929098\n",
      "[step: 2217] loss: 83.64747619628906\n",
      "[step: 2217] loss: 0.006632703822106123\n",
      "[step: 2218] loss: 82.10789489746094\n",
      "[step: 2218] loss: 0.006639829836785793\n",
      "[step: 2219] loss: 75.77886962890625\n",
      "[step: 2219] loss: 0.006628772709518671\n",
      "[step: 2220] loss: 65.721923828125\n",
      "[step: 2220] loss: 0.006612520199269056\n",
      "[step: 2221] loss: 65.26097106933594\n",
      "[step: 2221] loss: 0.006611551623791456\n",
      "[step: 2222] loss: 71.94514465332031\n",
      "[step: 2222] loss: 0.006622541695833206\n",
      "[step: 2223] loss: 72.69817352294922\n",
      "[step: 2223] loss: 0.006627092137932777\n",
      "[step: 2224] loss: 67.34004974365234\n",
      "[step: 2224] loss: 0.006619648542255163\n",
      "[step: 2225] loss: 63.894859313964844\n",
      "[step: 2225] loss: 0.006609858945012093\n",
      "[step: 2226] loss: 67.35626983642578\n",
      "[step: 2226] loss: 0.006609760690480471\n",
      "[step: 2227] loss: 69.71931457519531\n",
      "[step: 2227] loss: 0.006616554223001003\n",
      "[step: 2228] loss: 65.7188720703125\n",
      "[step: 2228] loss: 0.006619031075388193\n",
      "[step: 2229] loss: 63.88376235961914\n",
      "[step: 2229] loss: 0.006614112760871649\n",
      "[step: 2230] loss: 66.60064697265625\n",
      "[step: 2230] loss: 0.006608077324926853\n",
      "[step: 2231] loss: 66.96627807617188\n",
      "[step: 2231] loss: 0.006607943680137396\n",
      "[step: 2232] loss: 64.39653015136719\n",
      "[step: 2232] loss: 0.006611871067434549\n",
      "[step: 2233] loss: 63.924041748046875\n",
      "[step: 2233] loss: 0.006613351870328188\n",
      "[step: 2234] loss: 65.74697875976562\n",
      "[step: 2234] loss: 0.006610419601202011\n",
      "[step: 2235] loss: 65.5587158203125\n",
      "[step: 2235] loss: 0.0066066645085811615\n",
      "[step: 2236] loss: 63.73590850830078\n",
      "[step: 2236] loss: 0.0066063194535672665\n",
      "[step: 2237] loss: 64.09373474121094\n",
      "[step: 2237] loss: 0.006608541123569012\n",
      "[step: 2238] loss: 65.23175811767578\n",
      "[step: 2238] loss: 0.006609442178159952\n",
      "[step: 2239] loss: 64.358154296875\n",
      "[step: 2239] loss: 0.006607727147638798\n",
      "[step: 2240] loss: 63.46226501464844\n",
      "[step: 2240] loss: 0.006605382543057203\n",
      "[step: 2241] loss: 64.1474380493164\n",
      "[step: 2241] loss: 0.006604933179914951\n",
      "[step: 2242] loss: 64.47540283203125\n",
      "[step: 2242] loss: 0.006606099661439657\n",
      "[step: 2243] loss: 63.68221664428711\n",
      "[step: 2243] loss: 0.00660667521879077\n",
      "[step: 2244] loss: 63.43653869628906\n",
      "[step: 2244] loss: 0.006605674047023058\n",
      "[step: 2245] loss: 64.0004653930664\n",
      "[step: 2245] loss: 0.006604171358048916\n",
      "[step: 2246] loss: 63.92860794067383\n",
      "[step: 2246] loss: 0.006603682413697243\n",
      "[step: 2247] loss: 63.3471794128418\n",
      "[step: 2247] loss: 0.006604206748306751\n",
      "[step: 2248] loss: 63.440093994140625\n",
      "[step: 2248] loss: 0.006604576483368874\n",
      "[step: 2249] loss: 63.780757904052734\n",
      "[step: 2249] loss: 0.006604028400033712\n",
      "[step: 2250] loss: 63.512489318847656\n",
      "[step: 2250] loss: 0.006603027693927288\n",
      "[step: 2251] loss: 63.19339370727539\n",
      "[step: 2251] loss: 0.0066024973057210445\n",
      "[step: 2252] loss: 63.366737365722656\n",
      "[step: 2252] loss: 0.006602634210139513\n",
      "[step: 2253] loss: 63.504371643066406\n",
      "[step: 2253] loss: 0.00660284049808979\n",
      "[step: 2254] loss: 63.26448440551758\n",
      "[step: 2254] loss: 0.006602573208510876\n",
      "[step: 2255] loss: 63.09746551513672\n",
      "[step: 2255] loss: 0.0066019101068377495\n",
      "[step: 2256] loss: 63.23678970336914\n",
      "[step: 2256] loss: 0.006601380649954081\n",
      "[step: 2257] loss: 63.292152404785156\n",
      "[step: 2257] loss: 0.006601252127438784\n",
      "[step: 2258] loss: 63.10193634033203\n",
      "[step: 2258] loss: 0.006601317785680294\n",
      "[step: 2259] loss: 63.0040283203125\n",
      "[step: 2259] loss: 0.00660119391977787\n",
      "[step: 2260] loss: 63.09848403930664\n",
      "[step: 2260] loss: 0.006600795313715935\n",
      "[step: 2261] loss: 63.11381530761719\n",
      "[step: 2261] loss: 0.006600326392799616\n",
      "[step: 2262] loss: 62.980926513671875\n",
      "[step: 2262] loss: 0.006600035820156336\n",
      "[step: 2263] loss: 62.90913391113281\n",
      "[step: 2263] loss: 0.006599943153560162\n",
      "[step: 2264] loss: 62.96133041381836\n",
      "[step: 2264] loss: 0.006599858868867159\n",
      "[step: 2265] loss: 62.972145080566406\n",
      "[step: 2265] loss: 0.006599620915949345\n",
      "[step: 2266] loss: 62.879112243652344\n",
      "[step: 2266] loss: 0.006599272135645151\n",
      "[step: 2267] loss: 62.81498718261719\n",
      "[step: 2267] loss: 0.006598930340260267\n",
      "[step: 2268] loss: 62.83684158325195\n",
      "[step: 2268] loss: 0.006598711013793945\n",
      "[step: 2269] loss: 62.84503936767578\n",
      "[step: 2269] loss: 0.006598569918423891\n",
      "[step: 2270] loss: 62.784751892089844\n",
      "[step: 2270] loss: 0.006598409730941057\n",
      "[step: 2271] loss: 62.724639892578125\n",
      "[step: 2271] loss: 0.006598164793103933\n",
      "[step: 2272] loss: 62.720672607421875\n",
      "[step: 2272] loss: 0.006597866304218769\n",
      "[step: 2273] loss: 62.72829818725586\n",
      "[step: 2273] loss: 0.006597586441785097\n",
      "[step: 2274] loss: 62.69279479980469\n",
      "[step: 2274] loss: 0.0065973661839962006\n",
      "[step: 2275] loss: 62.63948059082031\n",
      "[step: 2275] loss: 0.0065971799194812775\n",
      "[step: 2276] loss: 62.61621856689453\n",
      "[step: 2276] loss: 0.006597001105546951\n",
      "[step: 2277] loss: 62.61573028564453\n",
      "[step: 2277] loss: 0.006596766412258148\n",
      "[step: 2278] loss: 62.59851837158203\n",
      "[step: 2278] loss: 0.006596504244953394\n",
      "[step: 2279] loss: 62.557701110839844\n",
      "[step: 2279] loss: 0.006596251856535673\n",
      "[step: 2280] loss: 62.522987365722656\n",
      "[step: 2280] loss: 0.006596025545150042\n",
      "[step: 2281] loss: 62.50947952270508\n",
      "[step: 2281] loss: 0.006595818791538477\n",
      "[step: 2282] loss: 62.49906539916992\n",
      "[step: 2282] loss: 0.006595620419830084\n",
      "[step: 2283] loss: 62.47322082519531\n",
      "[step: 2283] loss: 0.0065954034216701984\n",
      "[step: 2284] loss: 62.438655853271484\n",
      "[step: 2284] loss: 0.00659516453742981\n",
      "[step: 2285] loss: 62.41307067871094\n",
      "[step: 2285] loss: 0.0065949284471571445\n",
      "[step: 2286] loss: 62.39861297607422\n",
      "[step: 2286] loss: 0.006594689097255468\n",
      "[step: 2287] loss: 62.38208770751953\n",
      "[step: 2287] loss: 0.006594470236450434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2288] loss: 62.35601806640625\n",
      "[step: 2288] loss: 0.006594268139451742\n",
      "[step: 2289] loss: 62.326805114746094\n",
      "[step: 2289] loss: 0.006594054400920868\n",
      "[step: 2290] loss: 62.30377197265625\n",
      "[step: 2290] loss: 0.006593827158212662\n",
      "[step: 2291] loss: 62.28664016723633\n",
      "[step: 2291] loss: 0.006593597121536732\n",
      "[step: 2292] loss: 62.26793670654297\n",
      "[step: 2292] loss: 0.0065933638252317905\n",
      "[step: 2293] loss: 62.24363327026367\n",
      "[step: 2293] loss: 0.006593140307813883\n",
      "[step: 2294] loss: 62.21757507324219\n",
      "[step: 2294] loss: 0.0065929158590734005\n",
      "[step: 2295] loss: 62.19469451904297\n",
      "[step: 2295] loss: 0.006592698395252228\n",
      "[step: 2296] loss: 62.17551803588867\n",
      "[step: 2296] loss: 0.006592483259737492\n",
      "[step: 2297] loss: 62.15611267089844\n",
      "[step: 2297] loss: 0.006592270452529192\n",
      "[step: 2298] loss: 62.13374710083008\n",
      "[step: 2298] loss: 0.0065920413471758366\n",
      "[step: 2299] loss: 62.10954284667969\n",
      "[step: 2299] loss: 0.00659181410446763\n",
      "[step: 2300] loss: 62.08649444580078\n",
      "[step: 2300] loss: 0.0065915887244045734\n",
      "[step: 2301] loss: 62.06564712524414\n",
      "[step: 2301] loss: 0.006591367535293102\n",
      "[step: 2302] loss: 62.045745849609375\n",
      "[step: 2302] loss: 0.006591158919036388\n",
      "[step: 2303] loss: 62.02473449707031\n",
      "[step: 2303] loss: 0.0065909321419894695\n",
      "[step: 2304] loss: 62.0020866394043\n",
      "[step: 2304] loss: 0.006590708624571562\n",
      "[step: 2305] loss: 61.979248046875\n",
      "[step: 2305] loss: 0.006590493023395538\n",
      "[step: 2306] loss: 61.95732498168945\n",
      "[step: 2306] loss: 0.006590267643332481\n",
      "[step: 2307] loss: 61.93657684326172\n",
      "[step: 2307] loss: 0.0065900469198822975\n",
      "[step: 2308] loss: 61.91600036621094\n",
      "[step: 2308] loss: 0.006589823868125677\n",
      "[step: 2309] loss: 61.894691467285156\n",
      "[step: 2309] loss: 0.006589607801288366\n",
      "[step: 2310] loss: 61.872589111328125\n",
      "[step: 2310] loss: 0.006589389871805906\n",
      "[step: 2311] loss: 61.85051727294922\n",
      "[step: 2311] loss: 0.006589164026081562\n",
      "[step: 2312] loss: 61.82886505126953\n",
      "[step: 2312] loss: 0.006588946096599102\n",
      "[step: 2313] loss: 61.80784606933594\n",
      "[step: 2313] loss: 0.00658872677013278\n",
      "[step: 2314] loss: 61.786956787109375\n",
      "[step: 2314] loss: 0.006588505115360022\n",
      "[step: 2315] loss: 61.76580047607422\n",
      "[step: 2315] loss: 0.00658828392624855\n",
      "[step: 2316] loss: 61.74427795410156\n",
      "[step: 2316] loss: 0.006588061805814505\n",
      "[step: 2317] loss: 61.72251892089844\n",
      "[step: 2317] loss: 0.006587840151041746\n",
      "[step: 2318] loss: 61.700965881347656\n",
      "[step: 2318] loss: 0.006587617564946413\n",
      "[step: 2319] loss: 61.679718017578125\n",
      "[step: 2319] loss: 0.006587400566786528\n",
      "[step: 2320] loss: 61.65864944458008\n",
      "[step: 2320] loss: 0.006587174255400896\n",
      "[step: 2321] loss: 61.63761901855469\n",
      "[step: 2321] loss: 0.006586962845176458\n",
      "[step: 2322] loss: 61.61650085449219\n",
      "[step: 2322] loss: 0.006586738396435976\n",
      "[step: 2323] loss: 61.59514617919922\n",
      "[step: 2323] loss: 0.0065865167416632175\n",
      "[step: 2324] loss: 61.57373046875\n",
      "[step: 2324] loss: 0.00658629834651947\n",
      "[step: 2325] loss: 61.55241012573242\n",
      "[step: 2325] loss: 0.006586079485714436\n",
      "[step: 2326] loss: 61.5311279296875\n",
      "[step: 2326] loss: 0.006585860624909401\n",
      "[step: 2327] loss: 61.510009765625\n",
      "[step: 2327] loss: 0.006585641298443079\n",
      "[step: 2328] loss: 61.48897933959961\n",
      "[step: 2328] loss: 0.006585422903299332\n",
      "[step: 2329] loss: 61.467918395996094\n",
      "[step: 2329] loss: 0.0065852003172039986\n",
      "[step: 2330] loss: 61.44679260253906\n",
      "[step: 2330] loss: 0.006584979593753815\n",
      "[step: 2331] loss: 61.425575256347656\n",
      "[step: 2331] loss: 0.006584762595593929\n",
      "[step: 2332] loss: 61.40435028076172\n",
      "[step: 2332] loss: 0.006584544666111469\n",
      "[step: 2333] loss: 61.383174896240234\n",
      "[step: 2333] loss: 0.006584321614354849\n",
      "[step: 2334] loss: 61.36204528808594\n",
      "[step: 2334] loss: 0.00658409995958209\n",
      "[step: 2335] loss: 61.34101867675781\n",
      "[step: 2335] loss: 0.006583885755389929\n",
      "[step: 2336] loss: 61.3199462890625\n",
      "[step: 2336] loss: 0.006583657115697861\n",
      "[step: 2337] loss: 61.29889678955078\n",
      "[step: 2337] loss: 0.006583448965102434\n",
      "[step: 2338] loss: 61.27793884277344\n",
      "[step: 2338] loss: 0.006583221722394228\n",
      "[step: 2339] loss: 61.25691223144531\n",
      "[step: 2339] loss: 0.006583002395927906\n",
      "[step: 2340] loss: 61.23578643798828\n",
      "[step: 2340] loss: 0.006582789123058319\n",
      "[step: 2341] loss: 61.21479415893555\n",
      "[step: 2341] loss: 0.0065825688652694225\n",
      "[step: 2342] loss: 61.19376754760742\n",
      "[step: 2342] loss: 0.0065823509357869625\n",
      "[step: 2343] loss: 61.1727409362793\n",
      "[step: 2343] loss: 0.006582132540643215\n",
      "[step: 2344] loss: 61.15168762207031\n",
      "[step: 2344] loss: 0.006581910885870457\n",
      "[step: 2345] loss: 61.13066864013672\n",
      "[step: 2345] loss: 0.006581686437129974\n",
      "[step: 2346] loss: 61.10969924926758\n",
      "[step: 2346] loss: 0.0065814726985991\n",
      "[step: 2347] loss: 61.08875274658203\n",
      "[step: 2347] loss: 0.006581251043826342\n",
      "[step: 2348] loss: 61.067806243896484\n",
      "[step: 2348] loss: 0.006581031251698732\n",
      "[step: 2349] loss: 61.04685974121094\n",
      "[step: 2349] loss: 0.006580808665603399\n",
      "[step: 2350] loss: 61.02587127685547\n",
      "[step: 2350] loss: 0.0065805912017822266\n",
      "[step: 2351] loss: 61.004974365234375\n",
      "[step: 2351] loss: 0.00658037094399333\n",
      "[step: 2352] loss: 60.98408508300781\n",
      "[step: 2352] loss: 0.006580159533768892\n",
      "[step: 2353] loss: 60.963157653808594\n",
      "[step: 2353] loss: 0.006579941138625145\n",
      "[step: 2354] loss: 60.942298889160156\n",
      "[step: 2354] loss: 0.006579719949513674\n",
      "[step: 2355] loss: 60.92137145996094\n",
      "[step: 2355] loss: 0.006579500623047352\n",
      "[step: 2356] loss: 60.900508880615234\n",
      "[step: 2356] loss: 0.00657928129658103\n",
      "[step: 2357] loss: 60.8796501159668\n",
      "[step: 2357] loss: 0.00657906336709857\n",
      "[step: 2358] loss: 60.85881805419922\n",
      "[step: 2358] loss: 0.006578843574970961\n",
      "[step: 2359] loss: 60.83797836303711\n",
      "[step: 2359] loss: 0.0065786270424723625\n",
      "[step: 2360] loss: 60.81712341308594\n",
      "[step: 2360] loss: 0.006578404922038317\n",
      "[step: 2361] loss: 60.79633331298828\n",
      "[step: 2361] loss: 0.0065781911835074425\n",
      "[step: 2362] loss: 60.7756233215332\n",
      "[step: 2362] loss: 0.006577974651008844\n",
      "[step: 2363] loss: 60.75489807128906\n",
      "[step: 2363] loss: 0.006577750667929649\n",
      "[step: 2364] loss: 60.73428726196289\n",
      "[step: 2364] loss: 0.006577533204108477\n",
      "[step: 2365] loss: 60.71373748779297\n",
      "[step: 2365] loss: 0.006577318534255028\n",
      "[step: 2366] loss: 60.69351577758789\n",
      "[step: 2366] loss: 0.006577101070433855\n",
      "[step: 2367] loss: 60.67359161376953\n",
      "[step: 2367] loss: 0.006576886400580406\n",
      "[step: 2368] loss: 60.654273986816406\n",
      "[step: 2368] loss: 0.0065766689367592335\n",
      "[step: 2369] loss: 60.63584518432617\n",
      "[step: 2369] loss: 0.006576454266905785\n",
      "[step: 2370] loss: 60.619163513183594\n",
      "[step: 2370] loss: 0.00657624052837491\n",
      "[step: 2371] loss: 60.605655670166016\n",
      "[step: 2371] loss: 0.006576038431376219\n",
      "[step: 2372] loss: 60.5975456237793\n",
      "[step: 2372] loss: 0.006575834937393665\n",
      "[step: 2373] loss: 60.599891662597656\n",
      "[step: 2373] loss: 0.006575649604201317\n",
      "[step: 2374] loss: 60.620574951171875\n",
      "[step: 2374] loss: 0.006575488019734621\n",
      "[step: 2375] loss: 60.678367614746094\n",
      "[step: 2375] loss: 0.006575367879122496\n",
      "[step: 2376] loss: 60.801246643066406\n",
      "[step: 2376] loss: 0.006575328763574362\n",
      "[step: 2377] loss: 61.061981201171875\n",
      "[step: 2377] loss: 0.006575435865670443\n",
      "[step: 2378] loss: 61.55424499511719\n",
      "[step: 2378] loss: 0.006575791165232658\n",
      "[step: 2379] loss: 62.566253662109375\n",
      "[step: 2379] loss: 0.006576641462743282\n",
      "[step: 2380] loss: 64.30497741699219\n",
      "[step: 2380] loss: 0.006578333675861359\n",
      "[step: 2381] loss: 67.7471923828125\n",
      "[step: 2381] loss: 0.006581681314855814\n",
      "[step: 2382] loss: 72.1410140991211\n",
      "[step: 2382] loss: 0.006587543990463018\n",
      "[step: 2383] loss: 78.85035705566406\n",
      "[step: 2383] loss: 0.006598268169909716\n",
      "[step: 2384] loss: 79.208251953125\n",
      "[step: 2384] loss: 0.006613569334149361\n",
      "[step: 2385] loss: 74.87185668945312\n",
      "[step: 2385] loss: 0.006635778117924929\n",
      "[step: 2386] loss: 63.994354248046875\n",
      "[step: 2386] loss: 0.0066488636657595634\n",
      "[step: 2387] loss: 60.9449348449707\n",
      "[step: 2387] loss: 0.006647519301623106\n",
      "[step: 2388] loss: 66.7682113647461\n",
      "[step: 2388] loss: 0.006613621488213539\n",
      "[step: 2389] loss: 69.75074768066406\n",
      "[step: 2389] loss: 0.006580186542123556\n",
      "[step: 2390] loss: 65.85873413085938\n",
      "[step: 2390] loss: 0.006574762985110283\n",
      "[step: 2391] loss: 60.66802215576172\n",
      "[step: 2391] loss: 0.006594745442271233\n",
      "[step: 2392] loss: 62.6793212890625\n",
      "[step: 2392] loss: 0.006609795615077019\n",
      "[step: 2393] loss: 66.33175659179688\n",
      "[step: 2393] loss: 0.00659668305888772\n",
      "[step: 2394] loss: 63.54886245727539\n",
      "[step: 2394] loss: 0.006575974635779858\n",
      "[step: 2395] loss: 60.39614486694336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2395] loss: 0.0065738060511648655\n",
      "[step: 2396] loss: 62.121498107910156\n",
      "[step: 2396] loss: 0.006587706506252289\n",
      "[step: 2397] loss: 63.85988235473633\n",
      "[step: 2397] loss: 0.006594101898372173\n",
      "[step: 2398] loss: 62.006980895996094\n",
      "[step: 2398] loss: 0.006582587491720915\n",
      "[step: 2399] loss: 60.24860382080078\n",
      "[step: 2399] loss: 0.006571310106664896\n",
      "[step: 2400] loss: 61.69316864013672\n",
      "[step: 2400] loss: 0.0065737622790038586\n",
      "[step: 2401] loss: 62.655242919921875\n",
      "[step: 2401] loss: 0.006582464091479778\n",
      "[step: 2402] loss: 60.93132781982422\n",
      "[step: 2402] loss: 0.00658335629850626\n",
      "[step: 2403] loss: 60.247928619384766\n",
      "[step: 2403] loss: 0.006574844941496849\n",
      "[step: 2404] loss: 61.4763298034668\n",
      "[step: 2404] loss: 0.006569405551999807\n",
      "[step: 2405] loss: 61.477867126464844\n",
      "[step: 2405] loss: 0.006572566460818052\n",
      "[step: 2406] loss: 60.2924690246582\n",
      "[step: 2406] loss: 0.006577223539352417\n",
      "[step: 2407] loss: 60.29621124267578\n",
      "[step: 2407] loss: 0.006576097570359707\n",
      "[step: 2408] loss: 61.07449722290039\n",
      "[step: 2408] loss: 0.006570698693394661\n",
      "[step: 2409] loss: 60.763370513916016\n",
      "[step: 2409] loss: 0.006568378768861294\n",
      "[step: 2410] loss: 60.026031494140625\n",
      "[step: 2410] loss: 0.006570905912667513\n",
      "[step: 2411] loss: 60.315521240234375\n",
      "[step: 2411] loss: 0.006573158781975508\n",
      "[step: 2412] loss: 60.72208786010742\n",
      "[step: 2412] loss: 0.006571568548679352\n",
      "[step: 2413] loss: 60.257720947265625\n",
      "[step: 2413] loss: 0.006568307057023048\n",
      "[step: 2414] loss: 59.914939880371094\n",
      "[step: 2414] loss: 0.006567513104528189\n",
      "[step: 2415] loss: 60.23976516723633\n",
      "[step: 2415] loss: 0.006569223944097757\n",
      "[step: 2416] loss: 60.35255813598633\n",
      "[step: 2416] loss: 0.006570110097527504\n",
      "[step: 2417] loss: 59.98368453979492\n",
      "[step: 2417] loss: 0.006568684242665768\n",
      "[step: 2418] loss: 59.843421936035156\n",
      "[step: 2418] loss: 0.00656674662604928\n",
      "[step: 2419] loss: 60.0789680480957\n",
      "[step: 2419] loss: 0.006566562224179506\n",
      "[step: 2420] loss: 60.098602294921875\n",
      "[step: 2420] loss: 0.006567612290382385\n",
      "[step: 2421] loss: 59.82892608642578\n",
      "[step: 2421] loss: 0.006567840464413166\n",
      "[step: 2422] loss: 59.762786865234375\n",
      "[step: 2422] loss: 0.006566736381500959\n",
      "[step: 2423] loss: 59.918663024902344\n",
      "[step: 2423] loss: 0.006565580144524574\n",
      "[step: 2424] loss: 59.89979553222656\n",
      "[step: 2424] loss: 0.006565513554960489\n",
      "[step: 2425] loss: 59.7148323059082\n",
      "[step: 2425] loss: 0.006566074676811695\n",
      "[step: 2426] loss: 59.66962432861328\n",
      "[step: 2426] loss: 0.006566063966602087\n",
      "[step: 2427] loss: 59.76519012451172\n",
      "[step: 2427] loss: 0.0065652839839458466\n",
      "[step: 2428] loss: 59.7551383972168\n",
      "[step: 2428] loss: 0.006564526818692684\n",
      "[step: 2429] loss: 59.62608337402344\n",
      "[step: 2429] loss: 0.006564419716596603\n",
      "[step: 2430] loss: 59.57673645019531\n",
      "[step: 2430] loss: 0.006564660929143429\n",
      "[step: 2431] loss: 59.630863189697266\n",
      "[step: 2431] loss: 0.006564597133547068\n",
      "[step: 2432] loss: 59.63057327270508\n",
      "[step: 2432] loss: 0.006564067676663399\n",
      "[step: 2433] loss: 59.54370880126953\n",
      "[step: 2433] loss: 0.006563508417457342\n",
      "[step: 2434] loss: 59.48927307128906\n",
      "[step: 2434] loss: 0.006563331466168165\n",
      "[step: 2435] loss: 59.510826110839844\n",
      "[step: 2435] loss: 0.006563391070812941\n",
      "[step: 2436] loss: 59.51983642578125\n",
      "[step: 2436] loss: 0.006563312374055386\n",
      "[step: 2437] loss: 59.4656982421875\n",
      "[step: 2437] loss: 0.006562951486557722\n",
      "[step: 2438] loss: 59.409217834472656\n",
      "[step: 2438] loss: 0.006562524940818548\n",
      "[step: 2439] loss: 59.40302276611328\n",
      "[step: 2439] loss: 0.006562276277691126\n",
      "[step: 2440] loss: 59.41218948364258\n",
      "[step: 2440] loss: 0.006562215741723776\n",
      "[step: 2441] loss: 59.3857421875\n",
      "[step: 2441] loss: 0.006562122143805027\n",
      "[step: 2442] loss: 59.33684539794922\n",
      "[step: 2442] loss: 0.006561874412000179\n",
      "[step: 2443] loss: 59.31015396118164\n",
      "[step: 2443] loss: 0.0065615479834377766\n",
      "[step: 2444] loss: 59.30889892578125\n",
      "[step: 2444] loss: 0.006561270449310541\n",
      "[step: 2445] loss: 59.29901123046875\n",
      "[step: 2445] loss: 0.006561114452779293\n",
      "[step: 2446] loss: 59.266082763671875\n",
      "[step: 2446] loss: 0.006560996174812317\n",
      "[step: 2447] loss: 59.230873107910156\n",
      "[step: 2447] loss: 0.006560814566910267\n",
      "[step: 2448] loss: 59.21379852294922\n",
      "[step: 2448] loss: 0.006560550071299076\n",
      "[step: 2449] loss: 59.206172943115234\n",
      "[step: 2449] loss: 0.006560284178704023\n",
      "[step: 2450] loss: 59.18870162963867\n",
      "[step: 2450] loss: 0.006560065317898989\n",
      "[step: 2451] loss: 59.15911102294922\n",
      "[step: 2451] loss: 0.006559919100254774\n",
      "[step: 2452] loss: 59.131866455078125\n",
      "[step: 2452] loss: 0.006559746339917183\n",
      "[step: 2453] loss: 59.11537170410156\n",
      "[step: 2453] loss: 0.006559543777257204\n",
      "[step: 2454] loss: 59.10295867919922\n",
      "[step: 2454] loss: 0.006559300702065229\n",
      "[step: 2455] loss: 59.08415985107422\n",
      "[step: 2455] loss: 0.00655906880274415\n",
      "[step: 2456] loss: 59.058712005615234\n",
      "[step: 2456] loss: 0.006558872293680906\n",
      "[step: 2457] loss: 59.034523010253906\n",
      "[step: 2457] loss: 0.0065586986020207405\n",
      "[step: 2458] loss: 59.016510009765625\n",
      "[step: 2458] loss: 0.0065585169941186905\n",
      "[step: 2459] loss: 59.00139236450195\n",
      "[step: 2459] loss: 0.006558307912200689\n",
      "[step: 2460] loss: 58.98328399658203\n",
      "[step: 2460] loss: 0.00655808299779892\n",
      "[step: 2461] loss: 58.96087646484375\n",
      "[step: 2461] loss: 0.006557869259268045\n",
      "[step: 2462] loss: 58.93817138671875\n",
      "[step: 2462] loss: 0.006557674612849951\n",
      "[step: 2463] loss: 58.91853332519531\n",
      "[step: 2463] loss: 0.006557484623044729\n",
      "[step: 2464] loss: 58.90149688720703\n",
      "[step: 2464] loss: 0.006557298358529806\n",
      "[step: 2465] loss: 58.883949279785156\n",
      "[step: 2465] loss: 0.006557086016982794\n",
      "[step: 2466] loss: 58.86393737792969\n",
      "[step: 2466] loss: 0.006556876935064793\n",
      "[step: 2467] loss: 58.84252166748047\n",
      "[step: 2467] loss: 0.006556672044098377\n",
      "[step: 2468] loss: 58.822021484375\n",
      "[step: 2468] loss: 0.006556479725986719\n",
      "[step: 2469] loss: 58.80332946777344\n",
      "[step: 2469] loss: 0.006556279491633177\n",
      "[step: 2470] loss: 58.78546142578125\n",
      "[step: 2470] loss: 0.006556086242198944\n",
      "[step: 2471] loss: 58.76690673828125\n",
      "[step: 2471] loss: 0.006555886939167976\n",
      "[step: 2472] loss: 58.74708557128906\n",
      "[step: 2472] loss: 0.006555676925927401\n",
      "[step: 2473] loss: 58.72678756713867\n",
      "[step: 2473] loss: 0.006555475294589996\n",
      "[step: 2474] loss: 58.7069206237793\n",
      "[step: 2474] loss: 0.006555280182510614\n",
      "[step: 2475] loss: 58.68805694580078\n",
      "[step: 2475] loss: 0.006555076688528061\n",
      "[step: 2476] loss: 58.66968536376953\n",
      "[step: 2476] loss: 0.0065548839047551155\n",
      "[step: 2477] loss: 58.65098571777344\n",
      "[step: 2477] loss: 0.0065546841360628605\n",
      "[step: 2478] loss: 58.63170623779297\n",
      "[step: 2478] loss: 0.006554486695677042\n",
      "[step: 2479] loss: 58.61198043823242\n",
      "[step: 2479] loss: 0.006554282736033201\n",
      "[step: 2480] loss: 58.592437744140625\n",
      "[step: 2480] loss: 0.006554082501679659\n",
      "[step: 2481] loss: 58.573299407958984\n",
      "[step: 2481] loss: 0.006553887855261564\n",
      "[step: 2482] loss: 58.554473876953125\n",
      "[step: 2482] loss: 0.006553684826940298\n",
      "[step: 2483] loss: 58.53575897216797\n",
      "[step: 2483] loss: 0.006553485989570618\n",
      "[step: 2484] loss: 58.516849517822266\n",
      "[step: 2484] loss: 0.006553289946168661\n",
      "[step: 2485] loss: 58.49773406982422\n",
      "[step: 2485] loss: 0.006553088314831257\n",
      "[step: 2486] loss: 58.478370666503906\n",
      "[step: 2486] loss: 0.006552886683493853\n",
      "[step: 2487] loss: 58.459129333496094\n",
      "[step: 2487] loss: 0.0065526897087693214\n",
      "[step: 2488] loss: 58.440032958984375\n",
      "[step: 2488] loss: 0.006552489940077066\n",
      "[step: 2489] loss: 58.42113494873047\n",
      "[step: 2489] loss: 0.006552290637046099\n",
      "[step: 2490] loss: 58.402252197265625\n",
      "[step: 2490] loss: 0.006552096456289291\n",
      "[step: 2491] loss: 58.38345718383789\n",
      "[step: 2491] loss: 0.006551893427968025\n",
      "[step: 2492] loss: 58.36445617675781\n",
      "[step: 2492] loss: 0.006551698315888643\n",
      "[step: 2493] loss: 58.345428466796875\n",
      "[step: 2493] loss: 0.006551493890583515\n",
      "[step: 2494] loss: 58.326332092285156\n",
      "[step: 2494] loss: 0.00655130110681057\n",
      "[step: 2495] loss: 58.307281494140625\n",
      "[step: 2495] loss: 0.006551095750182867\n",
      "[step: 2496] loss: 58.28825378417969\n",
      "[step: 2496] loss: 0.006550896912813187\n",
      "[step: 2497] loss: 58.269351959228516\n",
      "[step: 2497] loss: 0.0065506985411047935\n",
      "[step: 2498] loss: 58.25046157836914\n",
      "[step: 2498] loss: 0.0065505001693964005\n",
      "[step: 2499] loss: 58.23162841796875\n",
      "[step: 2499] loss: 0.006550302263349295\n",
      "[step: 2500] loss: 58.21272277832031\n",
      "[step: 2500] loss: 0.006550106685608625\n",
      "[step: 2501] loss: 58.193824768066406\n",
      "[step: 2501] loss: 0.006549907382577658\n",
      "[step: 2502] loss: 58.17491149902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2502] loss: 0.006549707148224115\n",
      "[step: 2503] loss: 58.15596008300781\n",
      "[step: 2503] loss: 0.006549512036144733\n",
      "[step: 2504] loss: 58.136993408203125\n",
      "[step: 2504] loss: 0.006549309007823467\n",
      "[step: 2505] loss: 58.118064880371094\n",
      "[step: 2505] loss: 0.006549110636115074\n",
      "[step: 2506] loss: 58.09916687011719\n",
      "[step: 2506] loss: 0.006548912730067968\n",
      "[step: 2507] loss: 58.08025360107422\n",
      "[step: 2507] loss: 0.006548713427037001\n",
      "[step: 2508] loss: 58.06138229370117\n",
      "[step: 2508] loss: 0.006548518314957619\n",
      "[step: 2509] loss: 58.042518615722656\n",
      "[step: 2509] loss: 0.006548311095684767\n",
      "[step: 2510] loss: 58.02360534667969\n",
      "[step: 2510] loss: 0.0065481215715408325\n",
      "[step: 2511] loss: 58.00477600097656\n",
      "[step: 2511] loss: 0.0065479218028485775\n",
      "[step: 2512] loss: 57.98594665527344\n",
      "[step: 2512] loss: 0.006547719240188599\n",
      "[step: 2513] loss: 57.96711349487305\n",
      "[step: 2513] loss: 0.006547520402818918\n",
      "[step: 2514] loss: 57.948299407958984\n",
      "[step: 2514] loss: 0.006547325756400824\n",
      "[step: 2515] loss: 57.92948913574219\n",
      "[step: 2515] loss: 0.00654712924733758\n",
      "[step: 2516] loss: 57.910667419433594\n",
      "[step: 2516] loss: 0.0065469276160001755\n",
      "[step: 2517] loss: 57.89185333251953\n",
      "[step: 2517] loss: 0.006546735763549805\n",
      "[step: 2518] loss: 57.87299346923828\n",
      "[step: 2518] loss: 0.006546533666551113\n",
      "[step: 2519] loss: 57.854209899902344\n",
      "[step: 2519] loss: 0.00654633529484272\n",
      "[step: 2520] loss: 57.83539581298828\n",
      "[step: 2520] loss: 0.006546133663505316\n",
      "[step: 2521] loss: 57.816627502441406\n",
      "[step: 2521] loss: 0.006545942276716232\n",
      "[step: 2522] loss: 57.7978401184082\n",
      "[step: 2522] loss: 0.006545733660459518\n",
      "[step: 2523] loss: 57.779117584228516\n",
      "[step: 2523] loss: 0.006545542739331722\n",
      "[step: 2524] loss: 57.76042556762695\n",
      "[step: 2524] loss: 0.006545344367623329\n",
      "[step: 2525] loss: 57.74180221557617\n",
      "[step: 2525] loss: 0.006545140407979488\n",
      "[step: 2526] loss: 57.723209381103516\n",
      "[step: 2526] loss: 0.006544944830238819\n",
      "[step: 2527] loss: 57.704734802246094\n",
      "[step: 2527] loss: 0.006544747855514288\n",
      "[step: 2528] loss: 57.686588287353516\n",
      "[step: 2528] loss: 0.00654454855248332\n",
      "[step: 2529] loss: 57.66874694824219\n",
      "[step: 2529] loss: 0.006544345524162054\n",
      "[step: 2530] loss: 57.65147399902344\n",
      "[step: 2530] loss: 0.006544146221131086\n",
      "[step: 2531] loss: 57.635231018066406\n",
      "[step: 2531] loss: 0.006543956231325865\n",
      "[step: 2532] loss: 57.62078857421875\n",
      "[step: 2532] loss: 0.0065437559969723225\n",
      "[step: 2533] loss: 57.60939025878906\n",
      "[step: 2533] loss: 0.00654355576261878\n",
      "[step: 2534] loss: 57.60380554199219\n",
      "[step: 2534] loss: 0.006543352734297514\n",
      "[step: 2535] loss: 57.60826110839844\n",
      "[step: 2535] loss: 0.006543159484863281\n",
      "[step: 2536] loss: 57.632205963134766\n",
      "[step: 2536] loss: 0.00654296251013875\n",
      "[step: 2537] loss: 57.69062042236328\n",
      "[step: 2537] loss: 0.006542766001075506\n",
      "[step: 2538] loss: 57.81898498535156\n",
      "[step: 2538] loss: 0.006542564835399389\n",
      "[step: 2539] loss: 58.06904602050781\n",
      "[step: 2539] loss: 0.0065423729829490185\n",
      "[step: 2540] loss: 58.579708099365234\n",
      "[step: 2540] loss: 0.006542168091982603\n",
      "[step: 2541] loss: 59.505496978759766\n",
      "[step: 2541] loss: 0.0065419673919677734\n",
      "[step: 2542] loss: 61.36253356933594\n",
      "[step: 2542] loss: 0.006541773211210966\n",
      "[step: 2543] loss: 64.27704620361328\n",
      "[step: 2543] loss: 0.006541572976857424\n",
      "[step: 2544] loss: 69.52902221679688\n",
      "[step: 2544] loss: 0.0065413848496973515\n",
      "[step: 2545] loss: 74.04865264892578\n",
      "[step: 2545] loss: 0.0065411850810050964\n",
      "[step: 2546] loss: 77.91922760009766\n",
      "[step: 2546] loss: 0.006540985312312841\n",
      "[step: 2547] loss: 70.92745208740234\n",
      "[step: 2547] loss: 0.006540779490023851\n",
      "[step: 2548] loss: 61.47650909423828\n",
      "[step: 2548] loss: 0.0065405950881540775\n",
      "[step: 2549] loss: 57.793922424316406\n",
      "[step: 2549] loss: 0.006540392991155386\n",
      "[step: 2550] loss: 62.957244873046875\n",
      "[step: 2550] loss: 0.006540200207382441\n",
      "[step: 2551] loss: 67.6578369140625\n",
      "[step: 2551] loss: 0.0065399962477386\n",
      "[step: 2552] loss: 63.16318893432617\n",
      "[step: 2552] loss: 0.006539797410368919\n",
      "[step: 2553] loss: 57.878196716308594\n",
      "[step: 2553] loss: 0.006539599038660526\n",
      "[step: 2554] loss: 59.19915008544922\n",
      "[step: 2554] loss: 0.006539406720548868\n",
      "[step: 2555] loss: 62.65964126586914\n",
      "[step: 2555] loss: 0.006539206486195326\n",
      "[step: 2556] loss: 61.2950553894043\n",
      "[step: 2556] loss: 0.006539005786180496\n",
      "[step: 2557] loss: 57.66283416748047\n",
      "[step: 2557] loss: 0.006538810674101114\n",
      "[step: 2558] loss: 58.621116638183594\n",
      "[step: 2558] loss: 0.0065386234782636166\n",
      "[step: 2559] loss: 61.10448455810547\n",
      "[step: 2559] loss: 0.0065384223125875\n",
      "[step: 2560] loss: 59.60064697265625\n",
      "[step: 2560] loss: 0.006538236979395151\n",
      "[step: 2561] loss: 57.38756561279297\n",
      "[step: 2561] loss: 0.006538050249218941\n",
      "[step: 2562] loss: 58.23466491699219\n",
      "[step: 2562] loss: 0.0065378788858652115\n",
      "[step: 2563] loss: 59.562801361083984\n",
      "[step: 2563] loss: 0.006537716835737228\n",
      "[step: 2564] loss: 58.510650634765625\n",
      "[step: 2564] loss: 0.006537579465657473\n",
      "[step: 2565] loss: 57.22557830810547\n",
      "[step: 2565] loss: 0.006537500768899918\n",
      "[step: 2566] loss: 58.05169677734375\n",
      "[step: 2566] loss: 0.006537482142448425\n",
      "[step: 2567] loss: 58.774200439453125\n",
      "[step: 2567] loss: 0.006537622306495905\n",
      "[step: 2568] loss: 57.73191833496094\n",
      "[step: 2568] loss: 0.00653802091255784\n",
      "[step: 2569] loss: 57.160301208496094\n",
      "[step: 2569] loss: 0.006538846995681524\n",
      "[step: 2570] loss: 57.90296173095703\n",
      "[step: 2570] loss: 0.006540536880493164\n",
      "[step: 2571] loss: 58.0528564453125\n",
      "[step: 2571] loss: 0.006543558090925217\n",
      "[step: 2572] loss: 57.29143524169922\n",
      "[step: 2572] loss: 0.0065491837449371815\n",
      "[step: 2573] loss: 57.135498046875\n",
      "[step: 2573] loss: 0.006558125372976065\n",
      "[step: 2574] loss: 57.652488708496094\n",
      "[step: 2574] loss: 0.006573102902621031\n",
      "[step: 2575] loss: 57.59272003173828\n",
      "[step: 2575] loss: 0.006590219214558601\n",
      "[step: 2576] loss: 57.055992126464844\n",
      "[step: 2576] loss: 0.006608846131712198\n",
      "[step: 2577] loss: 57.08681869506836\n",
      "[step: 2577] loss: 0.006606363691389561\n",
      "[step: 2578] loss: 57.4261474609375\n",
      "[step: 2578] loss: 0.006584846414625645\n",
      "[step: 2579] loss: 57.27354431152344\n",
      "[step: 2579] loss: 0.006549903657287359\n",
      "[step: 2580] loss: 56.932552337646484\n",
      "[step: 2580] loss: 0.006535026244819164\n",
      "[step: 2581] loss: 56.98888397216797\n",
      "[step: 2581] loss: 0.00654775183647871\n",
      "[step: 2582] loss: 57.19057846069336\n",
      "[step: 2582] loss: 0.0065655894577503204\n",
      "[step: 2583] loss: 57.077667236328125\n",
      "[step: 2583] loss: 0.006565827410668135\n",
      "[step: 2584] loss: 56.845455169677734\n",
      "[step: 2584] loss: 0.006546461023390293\n",
      "[step: 2585] loss: 56.87930679321289\n",
      "[step: 2585] loss: 0.006534119602292776\n",
      "[step: 2586] loss: 57.01066207885742\n",
      "[step: 2586] loss: 0.006540966220200062\n",
      "[step: 2587] loss: 56.9286003112793\n",
      "[step: 2587] loss: 0.006552642676979303\n",
      "[step: 2588] loss: 56.76820755004883\n",
      "[step: 2588] loss: 0.006552408449351788\n",
      "[step: 2589] loss: 56.76957702636719\n",
      "[step: 2589] loss: 0.006540271453559399\n",
      "[step: 2590] loss: 56.85231018066406\n",
      "[step: 2590] loss: 0.00653289956972003\n",
      "[step: 2591] loss: 56.81536102294922\n",
      "[step: 2591] loss: 0.006537336390465498\n",
      "[step: 2592] loss: 56.69794464111328\n",
      "[step: 2592] loss: 0.006544575095176697\n",
      "[step: 2593] loss: 56.671817779541016\n",
      "[step: 2593] loss: 0.006544188596308231\n",
      "[step: 2594] loss: 56.72235870361328\n",
      "[step: 2594] loss: 0.0065366243943572044\n",
      "[step: 2595] loss: 56.71149444580078\n",
      "[step: 2595] loss: 0.006531908642500639\n",
      "[step: 2596] loss: 56.6317138671875\n",
      "[step: 2596] loss: 0.006534258369356394\n",
      "[step: 2597] loss: 56.58573913574219\n",
      "[step: 2597] loss: 0.006538615562021732\n",
      "[step: 2598] loss: 56.604454040527344\n",
      "[step: 2598] loss: 0.006538589484989643\n",
      "[step: 2599] loss: 56.612640380859375\n",
      "[step: 2599] loss: 0.006534040439873934\n",
      "[step: 2600] loss: 56.565391540527344\n",
      "[step: 2600] loss: 0.006530942860990763\n",
      "[step: 2601] loss: 56.51335144042969\n",
      "[step: 2601] loss: 0.006532084196805954\n",
      "[step: 2602] loss: 56.50359344482422\n",
      "[step: 2602] loss: 0.006534612737596035\n",
      "[step: 2603] loss: 56.511924743652344\n",
      "[step: 2603] loss: 0.006534700281918049\n",
      "[step: 2604] loss: 56.493309020996094\n",
      "[step: 2604] loss: 0.006532020401209593\n",
      "[step: 2605] loss: 56.450279235839844\n",
      "[step: 2605] loss: 0.006529975216835737\n",
      "[step: 2606] loss: 56.420127868652344\n",
      "[step: 2606] loss: 0.006530461832880974\n",
      "[step: 2607] loss: 56.41493225097656\n",
      "[step: 2607] loss: 0.006531906314194202\n",
      "[step: 2608] loss: 56.409820556640625\n",
      "[step: 2608] loss: 0.006531971041113138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2609] loss: 56.385284423828125\n",
      "[step: 2609] loss: 0.006530402693897486\n",
      "[step: 2610] loss: 56.35187530517578\n",
      "[step: 2610] loss: 0.006529056932777166\n",
      "[step: 2611] loss: 56.330230712890625\n",
      "[step: 2611] loss: 0.006529147271066904\n",
      "[step: 2612] loss: 56.321632385253906\n",
      "[step: 2612] loss: 0.006529937032610178\n",
      "[step: 2613] loss: 56.310203552246094\n",
      "[step: 2613] loss: 0.00653001619502902\n",
      "[step: 2614] loss: 56.28728485107422\n",
      "[step: 2614] loss: 0.006529102101922035\n",
      "[step: 2615] loss: 56.260154724121094\n",
      "[step: 2615] loss: 0.006528159603476524\n",
      "[step: 2616] loss: 56.24030303955078\n",
      "[step: 2616] loss: 0.006527991034090519\n",
      "[step: 2617] loss: 56.227783203125\n",
      "[step: 2617] loss: 0.006528369151055813\n",
      "[step: 2618] loss: 56.214195251464844\n",
      "[step: 2618] loss: 0.0065284669399261475\n",
      "[step: 2619] loss: 56.19418716430664\n",
      "[step: 2619] loss: 0.006527960766106844\n",
      "[step: 2620] loss: 56.17082977294922\n",
      "[step: 2620] loss: 0.0065272836945950985\n",
      "[step: 2621] loss: 56.15068054199219\n",
      "[step: 2621] loss: 0.006526967044919729\n",
      "[step: 2622] loss: 56.135257720947266\n",
      "[step: 2622] loss: 0.0065270536579191685\n",
      "[step: 2623] loss: 56.12071228027344\n",
      "[step: 2623] loss: 0.006527136545628309\n",
      "[step: 2624] loss: 56.10309982299805\n",
      "[step: 2624] loss: 0.006526883691549301\n",
      "[step: 2625] loss: 56.08252716064453\n",
      "[step: 2625] loss: 0.006526411045342684\n",
      "[step: 2626] loss: 56.06220245361328\n",
      "[step: 2626] loss: 0.006526040844619274\n",
      "[step: 2627] loss: 56.04448699951172\n",
      "[step: 2627] loss: 0.0065259248949587345\n",
      "[step: 2628] loss: 56.02869415283203\n",
      "[step: 2628] loss: 0.006525920704007149\n",
      "[step: 2629] loss: 56.012298583984375\n",
      "[step: 2629] loss: 0.006525805220007896\n",
      "[step: 2630] loss: 55.99403381347656\n",
      "[step: 2630] loss: 0.006525513716042042\n",
      "[step: 2631] loss: 55.97471237182617\n",
      "[step: 2631] loss: 0.006525173783302307\n",
      "[step: 2632] loss: 55.955841064453125\n",
      "[step: 2632] loss: 0.006524934899061918\n",
      "[step: 2633] loss: 55.93833923339844\n",
      "[step: 2633] loss: 0.00652482220903039\n",
      "[step: 2634] loss: 55.92167663574219\n",
      "[step: 2634] loss: 0.0065247355960309505\n",
      "[step: 2635] loss: 55.90479278564453\n",
      "[step: 2635] loss: 0.006524556316435337\n",
      "[step: 2636] loss: 55.88695526123047\n",
      "[step: 2636] loss: 0.006524298340082169\n",
      "[step: 2637] loss: 55.86851119995117\n",
      "[step: 2637] loss: 0.006524033844470978\n",
      "[step: 2638] loss: 55.850154876708984\n",
      "[step: 2638] loss: 0.006523836404085159\n",
      "[step: 2639] loss: 55.83234405517578\n",
      "[step: 2639] loss: 0.006523701827973127\n",
      "[step: 2640] loss: 55.81513214111328\n",
      "[step: 2640] loss: 0.006523566786199808\n",
      "[step: 2641] loss: 55.79808807373047\n",
      "[step: 2641] loss: 0.006523383781313896\n",
      "[step: 2642] loss: 55.78068542480469\n",
      "[step: 2642] loss: 0.0065231542102992535\n",
      "[step: 2643] loss: 55.762939453125\n",
      "[step: 2643] loss: 0.006522920913994312\n",
      "[step: 2644] loss: 55.744895935058594\n",
      "[step: 2644] loss: 0.006522734649479389\n",
      "[step: 2645] loss: 55.72697448730469\n",
      "[step: 2645] loss: 0.006522570736706257\n",
      "[step: 2646] loss: 55.709320068359375\n",
      "[step: 2646] loss: 0.006522413808852434\n",
      "[step: 2647] loss: 55.69196701049805\n",
      "[step: 2647] loss: 0.006522241979837418\n",
      "[step: 2648] loss: 55.67466735839844\n",
      "[step: 2648] loss: 0.006522038951516151\n",
      "[step: 2649] loss: 55.65729522705078\n",
      "[step: 2649] loss: 0.006521834526211023\n",
      "[step: 2650] loss: 55.6397590637207\n",
      "[step: 2650] loss: 0.006521637551486492\n",
      "[step: 2651] loss: 55.62206268310547\n",
      "[step: 2651] loss: 0.006521458271890879\n",
      "[step: 2652] loss: 55.60432434082031\n",
      "[step: 2652] loss: 0.0065212976187467575\n",
      "[step: 2653] loss: 55.5866584777832\n",
      "[step: 2653] loss: 0.006521120201796293\n",
      "[step: 2654] loss: 55.569114685058594\n",
      "[step: 2654] loss: 0.006520929280668497\n",
      "[step: 2655] loss: 55.55170440673828\n",
      "[step: 2655] loss: 0.006520729977637529\n",
      "[step: 2656] loss: 55.53424072265625\n",
      "[step: 2656] loss: 0.006520539987832308\n",
      "[step: 2657] loss: 55.51686096191406\n",
      "[step: 2657] loss: 0.0065203579142689705\n",
      "[step: 2658] loss: 55.49943923950195\n",
      "[step: 2658] loss: 0.006520180497318506\n",
      "[step: 2659] loss: 55.481910705566406\n",
      "[step: 2659] loss: 0.0065200007520616055\n",
      "[step: 2660] loss: 55.46435546875\n",
      "[step: 2660] loss: 0.0065198191441595554\n",
      "[step: 2661] loss: 55.446815490722656\n",
      "[step: 2661] loss: 0.006519634742289782\n",
      "[step: 2662] loss: 55.42932891845703\n",
      "[step: 2662] loss: 0.006519445218145847\n",
      "[step: 2663] loss: 55.411773681640625\n",
      "[step: 2663] loss: 0.0065192608162760735\n",
      "[step: 2664] loss: 55.394264221191406\n",
      "[step: 2664] loss: 0.006519075017422438\n",
      "[step: 2665] loss: 55.37682342529297\n",
      "[step: 2665] loss: 0.006518894340842962\n",
      "[step: 2666] loss: 55.35940933227539\n",
      "[step: 2666] loss: 0.0065187192521989346\n",
      "[step: 2667] loss: 55.34190368652344\n",
      "[step: 2667] loss: 0.006518532522022724\n",
      "[step: 2668] loss: 55.324501037597656\n",
      "[step: 2668] loss: 0.0065183513797819614\n",
      "[step: 2669] loss: 55.30710220336914\n",
      "[step: 2669] loss: 0.006518164649605751\n",
      "[step: 2670] loss: 55.28962707519531\n",
      "[step: 2670] loss: 0.006517977919429541\n",
      "[step: 2671] loss: 55.272212982177734\n",
      "[step: 2671] loss: 0.0065177977085113525\n",
      "[step: 2672] loss: 55.254756927490234\n",
      "[step: 2672] loss: 0.006517615634948015\n",
      "[step: 2673] loss: 55.23731994628906\n",
      "[step: 2673] loss: 0.006517433561384678\n",
      "[step: 2674] loss: 55.219810485839844\n",
      "[step: 2674] loss: 0.006517250556498766\n",
      "[step: 2675] loss: 55.202430725097656\n",
      "[step: 2675] loss: 0.006517068482935429\n",
      "[step: 2676] loss: 55.184932708740234\n",
      "[step: 2676] loss: 0.006516882684081793\n",
      "[step: 2677] loss: 55.16748809814453\n",
      "[step: 2677] loss: 0.006516702938824892\n",
      "[step: 2678] loss: 55.150054931640625\n",
      "[step: 2678] loss: 0.006516522262245417\n",
      "[step: 2679] loss: 55.13262176513672\n",
      "[step: 2679] loss: 0.006516333669424057\n",
      "[step: 2680] loss: 55.115234375\n",
      "[step: 2680] loss: 0.00651615671813488\n",
      "[step: 2681] loss: 55.097782135009766\n",
      "[step: 2681] loss: 0.0065159727819263935\n",
      "[step: 2682] loss: 55.08037185668945\n",
      "[step: 2682] loss: 0.00651578651741147\n",
      "[step: 2683] loss: 55.06298828125\n",
      "[step: 2683] loss: 0.0065156095661222935\n",
      "[step: 2684] loss: 55.04558563232422\n",
      "[step: 2684] loss: 0.0065154205076396465\n",
      "[step: 2685] loss: 55.02829360961914\n",
      "[step: 2685] loss: 0.006515242625027895\n",
      "[step: 2686] loss: 55.01093673706055\n",
      "[step: 2686] loss: 0.006515063811093569\n",
      "[step: 2687] loss: 54.99364471435547\n",
      "[step: 2687] loss: 0.006514871027320623\n",
      "[step: 2688] loss: 54.976463317871094\n",
      "[step: 2688] loss: 0.0065146926790475845\n",
      "[step: 2689] loss: 54.95945739746094\n",
      "[step: 2689] loss: 0.006514513865113258\n",
      "[step: 2690] loss: 54.942726135253906\n",
      "[step: 2690] loss: 0.006514327134937048\n",
      "[step: 2691] loss: 54.926422119140625\n",
      "[step: 2691] loss: 0.006514145992696285\n",
      "[step: 2692] loss: 54.91087341308594\n",
      "[step: 2692] loss: 0.00651396531611681\n",
      "[step: 2693] loss: 54.89675521850586\n",
      "[step: 2693] loss: 0.006513782776892185\n",
      "[step: 2694] loss: 54.885093688964844\n",
      "[step: 2694] loss: 0.006513597909361124\n",
      "[step: 2695] loss: 54.87791442871094\n",
      "[step: 2695] loss: 0.006513418164104223\n",
      "[step: 2696] loss: 54.87931823730469\n",
      "[step: 2696] loss: 0.006513233296573162\n",
      "[step: 2697] loss: 54.896240234375\n",
      "[step: 2697] loss: 0.006513047497719526\n",
      "[step: 2698] loss: 54.944129943847656\n",
      "[step: 2698] loss: 0.006512869149446487\n",
      "[step: 2699] loss: 55.04849624633789\n",
      "[step: 2699] loss: 0.006512688938528299\n",
      "[step: 2700] loss: 55.270591735839844\n",
      "[step: 2700] loss: 0.006512500811368227\n",
      "[step: 2701] loss: 55.70148468017578\n",
      "[step: 2701] loss: 0.006512323394417763\n",
      "[step: 2702] loss: 56.58815002441406\n",
      "[step: 2702] loss: 0.006512138061225414\n",
      "[step: 2703] loss: 58.18486785888672\n",
      "[step: 2703] loss: 0.0065119569189846516\n",
      "[step: 2704] loss: 61.35855484008789\n",
      "[step: 2704] loss: 0.006511774845421314\n",
      "[step: 2705] loss: 65.88760375976562\n",
      "[step: 2705] loss: 0.006511594168841839\n",
      "[step: 2706] loss: 73.022705078125\n",
      "[step: 2706] loss: 0.006511414889246225\n",
      "[step: 2707] loss: 75.37906646728516\n",
      "[step: 2707] loss: 0.006511228624731302\n",
      "[step: 2708] loss: 72.65045166015625\n",
      "[step: 2708] loss: 0.00651104049757123\n",
      "[step: 2709] loss: 60.509307861328125\n",
      "[step: 2709] loss: 0.0065108598209917545\n",
      "[step: 2710] loss: 55.09447479248047\n",
      "[step: 2710] loss: 0.006510679144412279\n",
      "[step: 2711] loss: 60.57843017578125\n",
      "[step: 2711] loss: 0.006510498002171516\n",
      "[step: 2712] loss: 64.97895812988281\n",
      "[step: 2712] loss: 0.0065103149972856045\n",
      "[step: 2713] loss: 61.41749954223633\n",
      "[step: 2713] loss: 0.006510132923722267\n",
      "[step: 2714] loss: 55.2718505859375\n",
      "[step: 2714] loss: 0.006509953644126654\n",
      "[step: 2715] loss: 57.092994689941406\n",
      "[step: 2715] loss: 0.006509771570563316\n",
      "[step: 2716] loss: 61.266929626464844\n",
      "[step: 2716] loss: 0.006509588565677404\n",
      "[step: 2717] loss: 58.359031677246094\n",
      "[step: 2717] loss: 0.006509409751743078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2718] loss: 54.801048278808594\n",
      "[step: 2718] loss: 0.00650921743363142\n",
      "[step: 2719] loss: 56.72908020019531\n",
      "[step: 2719] loss: 0.006509044673293829\n",
      "[step: 2720] loss: 58.66192626953125\n",
      "[step: 2720] loss: 0.006508862599730492\n",
      "[step: 2721] loss: 56.53228759765625\n",
      "[step: 2721] loss: 0.006508677266538143\n",
      "[step: 2722] loss: 54.65519714355469\n",
      "[step: 2722] loss: 0.006508492399007082\n",
      "[step: 2723] loss: 56.323116302490234\n",
      "[step: 2723] loss: 0.00650830939412117\n",
      "[step: 2724] loss: 57.318382263183594\n",
      "[step: 2724] loss: 0.0065081361681222916\n",
      "[step: 2725] loss: 55.362117767333984\n",
      "[step: 2725] loss: 0.006507938727736473\n",
      "[step: 2726] loss: 54.69036865234375\n",
      "[step: 2726] loss: 0.006507762707769871\n",
      "[step: 2727] loss: 56.08216857910156\n",
      "[step: 2727] loss: 0.006507575511932373\n",
      "[step: 2728] loss: 55.98857879638672\n",
      "[step: 2728] loss: 0.006507394835352898\n",
      "[step: 2729] loss: 54.667659759521484\n",
      "[step: 2729] loss: 0.006507218815386295\n",
      "[step: 2730] loss: 54.805091857910156\n",
      "[step: 2730] loss: 0.0065070330165326595\n",
      "[step: 2731] loss: 55.63569641113281\n",
      "[step: 2731] loss: 0.006506850477308035\n",
      "[step: 2732] loss: 55.163246154785156\n",
      "[step: 2732] loss: 0.006506671197712421\n",
      "[step: 2733] loss: 54.425819396972656\n",
      "[step: 2733] loss: 0.00650648633018136\n",
      "[step: 2734] loss: 54.86312484741211\n",
      "[step: 2734] loss: 0.0065063065849244595\n",
      "[step: 2735] loss: 55.221065521240234\n",
      "[step: 2735] loss: 0.0065061235800385475\n",
      "[step: 2736] loss: 54.62477111816406\n",
      "[step: 2736] loss: 0.006505947560071945\n",
      "[step: 2737] loss: 54.36878967285156\n",
      "[step: 2737] loss: 0.006505761761218309\n",
      "[step: 2738] loss: 54.79182434082031\n",
      "[step: 2738] loss: 0.006505579221993685\n",
      "[step: 2739] loss: 54.79833984375\n",
      "[step: 2739] loss: 0.006505395285785198\n",
      "[step: 2740] loss: 54.36137008666992\n",
      "[step: 2740] loss: 0.006505217868834734\n",
      "[step: 2741] loss: 54.332801818847656\n",
      "[step: 2741] loss: 0.0065050325356423855\n",
      "[step: 2742] loss: 54.60712432861328\n",
      "[step: 2742] loss: 0.00650485884398222\n",
      "[step: 2743] loss: 54.52090835571289\n",
      "[step: 2743] loss: 0.006504679564386606\n",
      "[step: 2744] loss: 54.232948303222656\n",
      "[step: 2744] loss: 0.006504503078758717\n",
      "[step: 2745] loss: 54.269744873046875\n",
      "[step: 2745] loss: 0.006504336837679148\n",
      "[step: 2746] loss: 54.436424255371094\n",
      "[step: 2746] loss: 0.0065041654743254185\n",
      "[step: 2747] loss: 54.324302673339844\n",
      "[step: 2747] loss: 0.006504025775939226\n",
      "[step: 2748] loss: 54.13990783691406\n",
      "[step: 2748] loss: 0.0065039037726819515\n",
      "[step: 2749] loss: 54.18231964111328\n",
      "[step: 2749] loss: 0.006503824144601822\n",
      "[step: 2750] loss: 54.27641296386719\n",
      "[step: 2750] loss: 0.006503823213279247\n",
      "[step: 2751] loss: 54.191070556640625\n",
      "[step: 2751] loss: 0.0065039703622460365\n",
      "[step: 2752] loss: 54.065547943115234\n",
      "[step: 2752] loss: 0.006504346616566181\n",
      "[step: 2753] loss: 54.0892448425293\n",
      "[step: 2753] loss: 0.006505195517092943\n",
      "[step: 2754] loss: 54.147735595703125\n",
      "[step: 2754] loss: 0.006506822537630796\n",
      "[step: 2755] loss: 54.08674621582031\n",
      "[step: 2755] loss: 0.006509959697723389\n",
      "[step: 2756] loss: 53.99526596069336\n",
      "[step: 2756] loss: 0.006515374407172203\n",
      "[step: 2757] loss: 53.99851989746094\n",
      "[step: 2757] loss: 0.006525173783302307\n",
      "[step: 2758] loss: 54.03413009643555\n",
      "[step: 2758] loss: 0.006539465393871069\n",
      "[step: 2759] loss: 53.9989128112793\n",
      "[step: 2759] loss: 0.006560981273651123\n",
      "[step: 2760] loss: 53.929039001464844\n",
      "[step: 2760] loss: 0.006577624008059502\n",
      "[step: 2761] loss: 53.91253662109375\n",
      "[step: 2761] loss: 0.00658445293083787\n",
      "[step: 2762] loss: 53.93318176269531\n",
      "[step: 2762] loss: 0.006557871121913195\n",
      "[step: 2763] loss: 53.91720962524414\n",
      "[step: 2763] loss: 0.006520962342619896\n",
      "[step: 2764] loss: 53.86564636230469\n",
      "[step: 2764] loss: 0.00650176964700222\n",
      "[step: 2765] loss: 53.83576583862305\n",
      "[step: 2765] loss: 0.00651416415348649\n",
      "[step: 2766] loss: 53.84015655517578\n",
      "[step: 2766] loss: 0.006535746622830629\n",
      "[step: 2767] loss: 53.83625793457031\n",
      "[step: 2767] loss: 0.006534883286803961\n",
      "[step: 2768] loss: 53.80250549316406\n",
      "[step: 2768] loss: 0.006514879409223795\n",
      "[step: 2769] loss: 53.767391204833984\n",
      "[step: 2769] loss: 0.006500883959233761\n",
      "[step: 2770] loss: 53.75581359863281\n",
      "[step: 2770] loss: 0.006508565507829189\n",
      "[step: 2771] loss: 53.75330352783203\n",
      "[step: 2771] loss: 0.006522095296531916\n",
      "[step: 2772] loss: 53.735294342041016\n",
      "[step: 2772] loss: 0.006520145572721958\n",
      "[step: 2773] loss: 53.704673767089844\n",
      "[step: 2773] loss: 0.006507033482193947\n",
      "[step: 2774] loss: 53.68207931518555\n",
      "[step: 2774] loss: 0.006499633193016052\n",
      "[step: 2775] loss: 53.672698974609375\n",
      "[step: 2775] loss: 0.006505442317575216\n",
      "[step: 2776] loss: 53.662635803222656\n",
      "[step: 2776] loss: 0.006513416301459074\n",
      "[step: 2777] loss: 53.64175033569336\n",
      "[step: 2777] loss: 0.006511221639811993\n",
      "[step: 2778] loss: 53.61671447753906\n",
      "[step: 2778] loss: 0.0065028248354792595\n",
      "[step: 2779] loss: 53.598541259765625\n",
      "[step: 2779] loss: 0.0064986953511834145\n",
      "[step: 2780] loss: 53.587066650390625\n",
      "[step: 2780] loss: 0.006502379663288593\n",
      "[step: 2781] loss: 53.57350540161133\n",
      "[step: 2781] loss: 0.006506977137178183\n",
      "[step: 2782] loss: 53.55365753173828\n",
      "[step: 2782] loss: 0.006505357101559639\n",
      "[step: 2783] loss: 53.532135009765625\n",
      "[step: 2783] loss: 0.006500168703496456\n",
      "[step: 2784] loss: 53.51481628417969\n",
      "[step: 2784] loss: 0.0064978101290762424\n",
      "[step: 2785] loss: 53.501243591308594\n",
      "[step: 2785] loss: 0.006500031799077988\n",
      "[step: 2786] loss: 53.486724853515625\n",
      "[step: 2786] loss: 0.0065025826916098595\n",
      "[step: 2787] loss: 53.46855926513672\n",
      "[step: 2787] loss: 0.006501475349068642\n",
      "[step: 2788] loss: 53.448951721191406\n",
      "[step: 2788] loss: 0.006498289294540882\n",
      "[step: 2789] loss: 53.43134307861328\n",
      "[step: 2789] loss: 0.006496906280517578\n",
      "[step: 2790] loss: 53.41624450683594\n",
      "[step: 2790] loss: 0.006498225964605808\n",
      "[step: 2791] loss: 53.401275634765625\n",
      "[step: 2791] loss: 0.006499606650322676\n",
      "[step: 2792] loss: 53.38453674316406\n",
      "[step: 2792] loss: 0.0064988406375050545\n",
      "[step: 2793] loss: 53.36632537841797\n",
      "[step: 2793] loss: 0.0064969053491950035\n",
      "[step: 2794] loss: 53.34852981567383\n",
      "[step: 2794] loss: 0.0064960322342813015\n",
      "[step: 2795] loss: 53.332252502441406\n",
      "[step: 2795] loss: 0.0064967479556798935\n",
      "[step: 2796] loss: 53.31678009033203\n",
      "[step: 2796] loss: 0.006497499532997608\n",
      "[step: 2797] loss: 53.30085754394531\n",
      "[step: 2797] loss: 0.006497007794678211\n",
      "[step: 2798] loss: 53.28382110595703\n",
      "[step: 2798] loss: 0.006495809648185968\n",
      "[step: 2799] loss: 53.26637268066406\n",
      "[step: 2799] loss: 0.0064951637759804726\n",
      "[step: 2800] loss: 53.249366760253906\n",
      "[step: 2800] loss: 0.00649545481428504\n",
      "[step: 2801] loss: 53.23311996459961\n",
      "[step: 2801] loss: 0.006495874375104904\n",
      "[step: 2802] loss: 53.21723937988281\n",
      "[step: 2802] loss: 0.006495606619864702\n",
      "[step: 2803] loss: 53.2010612487793\n",
      "[step: 2803] loss: 0.0064948489889502525\n",
      "[step: 2804] loss: 53.184349060058594\n",
      "[step: 2804] loss: 0.006494323723018169\n",
      "[step: 2805] loss: 53.167335510253906\n",
      "[step: 2805] loss: 0.006494342815130949\n",
      "[step: 2806] loss: 53.150634765625\n",
      "[step: 2806] loss: 0.006494543049484491\n",
      "[step: 2807] loss: 53.13416290283203\n",
      "[step: 2807] loss: 0.006494422908872366\n",
      "[step: 2808] loss: 53.11801528930664\n",
      "[step: 2808] loss: 0.006493952590972185\n",
      "[step: 2809] loss: 53.101806640625\n",
      "[step: 2809] loss: 0.006493498105555773\n",
      "[step: 2810] loss: 53.08538818359375\n",
      "[step: 2810] loss: 0.006493339315056801\n",
      "[step: 2811] loss: 53.06874084472656\n",
      "[step: 2811] loss: 0.006493383552879095\n",
      "[step: 2812] loss: 53.05208969116211\n",
      "[step: 2812] loss: 0.006493333261460066\n",
      "[step: 2813] loss: 53.035457611083984\n",
      "[step: 2813] loss: 0.0064930603839457035\n",
      "[step: 2814] loss: 53.019168853759766\n",
      "[step: 2814] loss: 0.006492693442851305\n",
      "[step: 2815] loss: 53.002830505371094\n",
      "[step: 2815] loss: 0.006492444779723883\n",
      "[step: 2816] loss: 52.986576080322266\n",
      "[step: 2816] loss: 0.006492359098047018\n",
      "[step: 2817] loss: 52.970149993896484\n",
      "[step: 2817] loss: 0.006492306012660265\n",
      "[step: 2818] loss: 52.95375061035156\n",
      "[step: 2818] loss: 0.006492149084806442\n",
      "[step: 2819] loss: 52.93719482421875\n",
      "[step: 2819] loss: 0.006491879466921091\n",
      "[step: 2820] loss: 52.92072296142578\n",
      "[step: 2820] loss: 0.006491615902632475\n",
      "[step: 2821] loss: 52.90435028076172\n",
      "[step: 2821] loss: 0.006491435691714287\n",
      "[step: 2822] loss: 52.88791275024414\n",
      "[step: 2822] loss: 0.006491332780569792\n",
      "[step: 2823] loss: 52.87162780761719\n",
      "[step: 2823] loss: 0.0064912158995866776\n",
      "[step: 2824] loss: 52.855316162109375\n",
      "[step: 2824] loss: 0.006491031497716904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2825] loss: 52.83891296386719\n",
      "[step: 2825] loss: 0.0064908042550086975\n",
      "[step: 2826] loss: 52.8225212097168\n",
      "[step: 2826] loss: 0.006490590516477823\n",
      "[step: 2827] loss: 52.80609130859375\n",
      "[step: 2827] loss: 0.006490429863333702\n",
      "[step: 2828] loss: 52.78973388671875\n",
      "[step: 2828] loss: 0.006490292027592659\n",
      "[step: 2829] loss: 52.77325439453125\n",
      "[step: 2829] loss: 0.006490146741271019\n",
      "[step: 2830] loss: 52.75690841674805\n",
      "[step: 2830] loss: 0.006489969789981842\n",
      "[step: 2831] loss: 52.74058532714844\n",
      "[step: 2831] loss: 0.006489765364676714\n",
      "[step: 2832] loss: 52.724205017089844\n",
      "[step: 2832] loss: 0.00648957584053278\n",
      "[step: 2833] loss: 52.707862854003906\n",
      "[step: 2833] loss: 0.006489408668130636\n",
      "[step: 2834] loss: 52.69152069091797\n",
      "[step: 2834] loss: 0.006489259656518698\n",
      "[step: 2835] loss: 52.67512512207031\n",
      "[step: 2835] loss: 0.006489102263003588\n",
      "[step: 2836] loss: 52.65879440307617\n",
      "[step: 2836] loss: 0.006488934624940157\n",
      "[step: 2837] loss: 52.642486572265625\n",
      "[step: 2837] loss: 0.00648875255137682\n",
      "[step: 2838] loss: 52.62611389160156\n",
      "[step: 2838] loss: 0.006488565821200609\n",
      "[step: 2839] loss: 52.60973358154297\n",
      "[step: 2839] loss: 0.006488391198217869\n",
      "[step: 2840] loss: 52.593345642089844\n",
      "[step: 2840] loss: 0.006488229148089886\n",
      "[step: 2841] loss: 52.57698059082031\n",
      "[step: 2841] loss: 0.006488070823252201\n",
      "[step: 2842] loss: 52.560707092285156\n",
      "[step: 2842] loss: 0.0064879003912210464\n",
      "[step: 2843] loss: 52.544288635253906\n",
      "[step: 2843] loss: 0.006487734615802765\n",
      "[step: 2844] loss: 52.52793884277344\n",
      "[step: 2844] loss: 0.006487553007900715\n",
      "[step: 2845] loss: 52.51158905029297\n",
      "[step: 2845] loss: 0.0064873783849179745\n",
      "[step: 2846] loss: 52.49528503417969\n",
      "[step: 2846] loss: 0.006487210746854544\n",
      "[step: 2847] loss: 52.47893142700195\n",
      "[step: 2847] loss: 0.0064870454370975494\n",
      "[step: 2848] loss: 52.46261215209961\n",
      "[step: 2848] loss: 0.0064868805930018425\n",
      "[step: 2849] loss: 52.446258544921875\n",
      "[step: 2849] loss: 0.00648671155795455\n",
      "[step: 2850] loss: 52.429893493652344\n",
      "[step: 2850] loss: 0.006486544385552406\n",
      "[step: 2851] loss: 52.4135627746582\n",
      "[step: 2851] loss: 0.006486369762569666\n",
      "[step: 2852] loss: 52.39722442626953\n",
      "[step: 2852] loss: 0.006486199330538511\n",
      "[step: 2853] loss: 52.380882263183594\n",
      "[step: 2853] loss: 0.006486032623797655\n",
      "[step: 2854] loss: 52.364540100097656\n",
      "[step: 2854] loss: 0.006485862657427788\n",
      "[step: 2855] loss: 52.348228454589844\n",
      "[step: 2855] loss: 0.006485695950686932\n",
      "[step: 2856] loss: 52.33198165893555\n",
      "[step: 2856] loss: 0.006485530640929937\n",
      "[step: 2857] loss: 52.315673828125\n",
      "[step: 2857] loss: 0.00648536579683423\n",
      "[step: 2858] loss: 52.29944610595703\n",
      "[step: 2858] loss: 0.006485190708190203\n",
      "[step: 2859] loss: 52.28319549560547\n",
      "[step: 2859] loss: 0.006485019810497761\n",
      "[step: 2860] loss: 52.26709747314453\n",
      "[step: 2860] loss: 0.006484850775450468\n",
      "[step: 2861] loss: 52.251094818115234\n",
      "[step: 2861] loss: 0.006484684534370899\n",
      "[step: 2862] loss: 52.23529052734375\n",
      "[step: 2862] loss: 0.006484511308372021\n",
      "[step: 2863] loss: 52.219993591308594\n",
      "[step: 2863] loss: 0.006484347395598888\n",
      "[step: 2864] loss: 52.20528030395508\n",
      "[step: 2864] loss: 0.0064841764979064465\n",
      "[step: 2865] loss: 52.19188690185547\n",
      "[step: 2865] loss: 0.006484008394181728\n",
      "[step: 2866] loss: 52.18064880371094\n",
      "[step: 2866] loss: 0.006483841221779585\n",
      "[step: 2867] loss: 52.173805236816406\n",
      "[step: 2867] loss: 0.006483670324087143\n",
      "[step: 2868] loss: 52.17475128173828\n",
      "[step: 2868] loss: 0.00648350128903985\n",
      "[step: 2869] loss: 52.19103240966797\n",
      "[step: 2869] loss: 0.006483332719653845\n",
      "[step: 2870] loss: 52.23584747314453\n",
      "[step: 2870] loss: 0.006483163218945265\n",
      "[step: 2871] loss: 52.3388557434082\n",
      "[step: 2871] loss: 0.006482994649559259\n",
      "[step: 2872] loss: 52.5491943359375\n",
      "[step: 2872] loss: 0.006482827477157116\n",
      "[step: 2873] loss: 52.988914489746094\n",
      "[step: 2873] loss: 0.006482663564383984\n",
      "[step: 2874] loss: 53.827789306640625\n",
      "[step: 2874] loss: 0.006482488941401243\n",
      "[step: 2875] loss: 55.54957962036133\n",
      "[step: 2875] loss: 0.0064823217689991\n",
      "[step: 2876] loss: 58.4833869934082\n",
      "[step: 2876] loss: 0.006482151336967945\n",
      "[step: 2877] loss: 63.977882385253906\n",
      "[step: 2877] loss: 0.0064819855615496635\n",
      "[step: 2878] loss: 69.83026123046875\n",
      "[step: 2878] loss: 0.006481816526502371\n",
      "[step: 2879] loss: 75.84062957763672\n",
      "[step: 2879] loss: 0.006481651216745377\n",
      "[step: 2880] loss: 69.728759765625\n",
      "[step: 2880] loss: 0.006481475662440062\n",
      "[step: 2881] loss: 58.58811950683594\n",
      "[step: 2881] loss: 0.006481307093054056\n",
      "[step: 2882] loss: 52.426490783691406\n",
      "[step: 2882] loss: 0.0064811403863132\n",
      "[step: 2883] loss: 58.137786865234375\n",
      "[step: 2883] loss: 0.006480972282588482\n",
      "[step: 2884] loss: 63.97377014160156\n",
      "[step: 2884] loss: 0.0064808065071702\n",
      "[step: 2885] loss: 58.28935241699219\n",
      "[step: 2885] loss: 0.0064806342124938965\n",
      "[step: 2886] loss: 52.416316986083984\n",
      "[step: 2886] loss: 0.0064804647117853165\n",
      "[step: 2887] loss: 55.13439178466797\n",
      "[step: 2887] loss: 0.006480302661657333\n",
      "[step: 2888] loss: 58.465171813964844\n",
      "[step: 2888] loss: 0.006480127107352018\n",
      "[step: 2889] loss: 55.355018615722656\n",
      "[step: 2889] loss: 0.006479962728917599\n",
      "[step: 2890] loss: 52.099334716796875\n",
      "[step: 2890] loss: 0.0064797913655638695\n",
      "[step: 2891] loss: 54.72697067260742\n",
      "[step: 2891] loss: 0.006479628384113312\n",
      "[step: 2892] loss: 56.49701690673828\n",
      "[step: 2892] loss: 0.00647945748642087\n",
      "[step: 2893] loss: 53.37361526489258\n",
      "[step: 2893] loss: 0.006479283329099417\n",
      "[step: 2894] loss: 52.13972473144531\n",
      "[step: 2894] loss: 0.006479112897068262\n",
      "[step: 2895] loss: 54.36658477783203\n",
      "[step: 2895] loss: 0.006478952243924141\n",
      "[step: 2896] loss: 54.4426383972168\n",
      "[step: 2896] loss: 0.006478778552263975\n",
      "[step: 2897] loss: 52.32367706298828\n",
      "[step: 2897] loss: 0.006478612776845694\n",
      "[step: 2898] loss: 52.298072814941406\n",
      "[step: 2898] loss: 0.006478442344814539\n",
      "[step: 2899] loss: 53.70331573486328\n",
      "[step: 2899] loss: 0.006478276569396257\n",
      "[step: 2900] loss: 53.1446418762207\n",
      "[step: 2900] loss: 0.006478108465671539\n",
      "[step: 2901] loss: 51.86390686035156\n",
      "[step: 2901] loss: 0.006477936636656523\n",
      "[step: 2902] loss: 52.486968994140625\n",
      "[step: 2902] loss: 0.006477765738964081\n",
      "[step: 2903] loss: 53.159690856933594\n",
      "[step: 2903] loss: 0.0064776004292070866\n",
      "[step: 2904] loss: 52.234657287597656\n",
      "[step: 2904] loss: 0.006477427203208208\n",
      "[step: 2905] loss: 51.81254577636719\n",
      "[step: 2905] loss: 0.006477258633822203\n",
      "[step: 2906] loss: 52.51194381713867\n",
      "[step: 2906] loss: 0.006477096118032932\n",
      "[step: 2907] loss: 52.46619415283203\n",
      "[step: 2907] loss: 0.006476923357695341\n",
      "[step: 2908] loss: 51.780609130859375\n",
      "[step: 2908] loss: 0.006476756185293198\n",
      "[step: 2909] loss: 51.874473571777344\n",
      "[step: 2909] loss: 0.006476588547229767\n",
      "[step: 2910] loss: 52.283607482910156\n",
      "[step: 2910] loss: 0.006476414389908314\n",
      "[step: 2911] loss: 52.00126266479492\n",
      "[step: 2911] loss: 0.006476245354861021\n",
      "[step: 2912] loss: 51.63009262084961\n",
      "[step: 2912] loss: 0.006476076319813728\n",
      "[step: 2913] loss: 51.856544494628906\n",
      "[step: 2913] loss: 0.006475911475718021\n",
      "[step: 2914] loss: 52.02877426147461\n",
      "[step: 2914] loss: 0.0064757466316223145\n",
      "[step: 2915] loss: 51.72455978393555\n",
      "[step: 2915] loss: 0.006475578527897596\n",
      "[step: 2916] loss: 51.57135009765625\n",
      "[step: 2916] loss: 0.006475408561527729\n",
      "[step: 2917] loss: 51.77288818359375\n",
      "[step: 2917] loss: 0.006475240923464298\n",
      "[step: 2918] loss: 51.79344940185547\n",
      "[step: 2918] loss: 0.006475068628787994\n",
      "[step: 2919] loss: 51.56492614746094\n",
      "[step: 2919] loss: 0.006474899128079414\n",
      "[step: 2920] loss: 51.51373291015625\n",
      "[step: 2920] loss: 0.006474729627370834\n",
      "[step: 2921] loss: 51.647857666015625\n",
      "[step: 2921] loss: 0.006474565248936415\n",
      "[step: 2922] loss: 51.62440490722656\n",
      "[step: 2922] loss: 0.0064743924885988235\n",
      "[step: 2923] loss: 51.464210510253906\n",
      "[step: 2923] loss: 0.006474219262599945\n",
      "[step: 2924] loss: 51.44742965698242\n",
      "[step: 2924] loss: 0.006474057212471962\n",
      "[step: 2925] loss: 51.53217315673828\n",
      "[step: 2925] loss: 0.006473886780440807\n",
      "[step: 2926] loss: 51.49531555175781\n",
      "[step: 2926] loss: 0.006473721470683813\n",
      "[step: 2927] loss: 51.385440826416016\n",
      "[step: 2927] loss: 0.0064735496416687965\n",
      "[step: 2928] loss: 51.37534713745117\n",
      "[step: 2928] loss: 0.006473384331911802\n",
      "[step: 2929] loss: 51.424476623535156\n",
      "[step: 2929] loss: 0.006473221350461245\n",
      "[step: 2930] loss: 51.39564514160156\n",
      "[step: 2930] loss: 0.006473048124462366\n",
      "[step: 2931] loss: 51.31650161743164\n",
      "[step: 2931] loss: 0.006472882814705372\n",
      "[step: 2932] loss: 51.29961013793945\n",
      "[step: 2932] loss: 0.006472719833254814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2933] loss: 51.32781982421875\n",
      "[step: 2933] loss: 0.006472558248788118\n",
      "[step: 2934] loss: 51.309688568115234\n",
      "[step: 2934] loss: 0.006472402717918158\n",
      "[step: 2935] loss: 51.25214385986328\n",
      "[step: 2935] loss: 0.0064722513779997826\n",
      "[step: 2936] loss: 51.227237701416016\n",
      "[step: 2936] loss: 0.006472120992839336\n",
      "[step: 2937] loss: 51.23923873901367\n",
      "[step: 2937] loss: 0.006472012028098106\n",
      "[step: 2938] loss: 51.23158264160156\n",
      "[step: 2938] loss: 0.006471938919275999\n",
      "[step: 2939] loss: 51.19063949584961\n",
      "[step: 2939] loss: 0.0064719440415501595\n",
      "[step: 2940] loss: 51.159584045410156\n",
      "[step: 2940] loss: 0.006472066510468721\n",
      "[step: 2941] loss: 51.15764236450195\n",
      "[step: 2941] loss: 0.006472406443208456\n",
      "[step: 2942] loss: 51.1544303894043\n",
      "[step: 2942] loss: 0.006473098881542683\n",
      "[step: 2943] loss: 51.128082275390625\n",
      "[step: 2943] loss: 0.0064744786359369755\n",
      "[step: 2944] loss: 51.096923828125\n",
      "[step: 2944] loss: 0.00647692009806633\n",
      "[step: 2945] loss: 51.083316802978516\n",
      "[step: 2945] loss: 0.006481412332504988\n",
      "[step: 2946] loss: 51.07895278930664\n",
      "[step: 2946] loss: 0.006488649640232325\n",
      "[step: 2947] loss: 51.063568115234375\n",
      "[step: 2947] loss: 0.0065009696409106255\n",
      "[step: 2948] loss: 51.03728485107422\n",
      "[step: 2948] loss: 0.006516754627227783\n",
      "[step: 2949] loss: 51.01631546020508\n",
      "[step: 2949] loss: 0.006537184119224548\n",
      "[step: 2950] loss: 51.00600814819336\n",
      "[step: 2950] loss: 0.006545597221702337\n",
      "[step: 2951] loss: 50.99568176269531\n",
      "[step: 2951] loss: 0.006538668647408485\n",
      "[step: 2952] loss: 50.976844787597656\n",
      "[step: 2952] loss: 0.006505901925265789\n",
      "[step: 2953] loss: 50.954689025878906\n",
      "[step: 2953] loss: 0.006476292852312326\n",
      "[step: 2954] loss: 50.937965393066406\n",
      "[step: 2954] loss: 0.006471524015069008\n",
      "[step: 2955] loss: 50.92643737792969\n",
      "[step: 2955] loss: 0.006488804239779711\n",
      "[step: 2956] loss: 50.91304016113281\n",
      "[step: 2956] loss: 0.006503878626972437\n",
      "[step: 2957] loss: 50.89472579956055\n",
      "[step: 2957] loss: 0.0064960066229105\n",
      "[step: 2958] loss: 50.87538146972656\n",
      "[step: 2958] loss: 0.006477080285549164\n",
      "[step: 2959] loss: 50.85957336425781\n",
      "[step: 2959] loss: 0.006468933075666428\n",
      "[step: 2960] loss: 50.84656524658203\n",
      "[step: 2960] loss: 0.006477914750576019\n",
      "[step: 2961] loss: 50.83218765258789\n",
      "[step: 2961] loss: 0.006488634739071131\n",
      "[step: 2962] loss: 50.815067291259766\n",
      "[step: 2962] loss: 0.006485790945589542\n",
      "[step: 2963] loss: 50.79722595214844\n",
      "[step: 2963] loss: 0.006474328693002462\n",
      "[step: 2964] loss: 50.781341552734375\n",
      "[step: 2964] loss: 0.006467800121754408\n",
      "[step: 2965] loss: 50.76713562011719\n",
      "[step: 2965] loss: 0.006472290027886629\n",
      "[step: 2966] loss: 50.752647399902344\n",
      "[step: 2966] loss: 0.006479417905211449\n",
      "[step: 2967] loss: 50.73649978637695\n",
      "[step: 2967] loss: 0.006478987634181976\n",
      "[step: 2968] loss: 50.71962356567383\n",
      "[step: 2968] loss: 0.006472295615822077\n",
      "[step: 2969] loss: 50.7034797668457\n",
      "[step: 2969] loss: 0.006467122118920088\n",
      "[step: 2970] loss: 50.68854522705078\n",
      "[step: 2970] loss: 0.0064686741679906845\n",
      "[step: 2971] loss: 50.67387390136719\n",
      "[step: 2971] loss: 0.0064731682650744915\n",
      "[step: 2972] loss: 50.65846252441406\n",
      "[step: 2972] loss: 0.006473923567682505\n",
      "[step: 2973] loss: 50.642295837402344\n",
      "[step: 2973] loss: 0.006470331456512213\n",
      "[step: 2974] loss: 50.62614822387695\n",
      "[step: 2974] loss: 0.006466554943472147\n",
      "[step: 2975] loss: 50.61064910888672\n",
      "[step: 2975] loss: 0.00646657170727849\n",
      "[step: 2976] loss: 50.59558868408203\n",
      "[step: 2976] loss: 0.006469153333455324\n",
      "[step: 2977] loss: 50.580528259277344\n",
      "[step: 2977] loss: 0.0064702024683356285\n",
      "[step: 2978] loss: 50.56499481201172\n",
      "[step: 2978] loss: 0.0064684487879276276\n",
      "[step: 2979] loss: 50.54924011230469\n",
      "[step: 2979] loss: 0.006465905345976353\n",
      "[step: 2980] loss: 50.53342819213867\n",
      "[step: 2980] loss: 0.0064652482978999615\n",
      "[step: 2981] loss: 50.517967224121094\n",
      "[step: 2981] loss: 0.006466548424214125\n",
      "[step: 2982] loss: 50.50275421142578\n",
      "[step: 2982] loss: 0.0064675151370465755\n",
      "[step: 2983] loss: 50.487525939941406\n",
      "[step: 2983] loss: 0.006466798018664122\n",
      "[step: 2984] loss: 50.472190856933594\n",
      "[step: 2984] loss: 0.006465185433626175\n",
      "[step: 2985] loss: 50.45665740966797\n",
      "[step: 2985] loss: 0.006464328616857529\n",
      "[step: 2986] loss: 50.4410400390625\n",
      "[step: 2986] loss: 0.006464764941483736\n",
      "[step: 2987] loss: 50.425537109375\n",
      "[step: 2987] loss: 0.006465476471930742\n",
      "[step: 2988] loss: 50.41020965576172\n",
      "[step: 2988] loss: 0.006465341430157423\n",
      "[step: 2989] loss: 50.39494705200195\n",
      "[step: 2989] loss: 0.006464408710598946\n",
      "[step: 2990] loss: 50.379722595214844\n",
      "[step: 2990] loss: 0.00646359333768487\n",
      "[step: 2991] loss: 50.36429214477539\n",
      "[step: 2991] loss: 0.006463504396378994\n",
      "[step: 2992] loss: 50.3487434387207\n",
      "[step: 2992] loss: 0.006463884375989437\n",
      "[step: 2993] loss: 50.33335876464844\n",
      "[step: 2993] loss: 0.006463993340730667\n",
      "[step: 2994] loss: 50.31793212890625\n",
      "[step: 2994] loss: 0.006463558878749609\n",
      "[step: 2995] loss: 50.3026123046875\n",
      "[step: 2995] loss: 0.006462935823947191\n",
      "[step: 2996] loss: 50.28730010986328\n",
      "[step: 2996] loss: 0.006462568882852793\n",
      "[step: 2997] loss: 50.27201843261719\n",
      "[step: 2997] loss: 0.006462614051997662\n",
      "[step: 2998] loss: 50.25670623779297\n",
      "[step: 2998] loss: 0.0064627439714968204\n",
      "[step: 2999] loss: 50.24129104614258\n",
      "[step: 2999] loss: 0.006462617311626673\n",
      "[step: 3000] loss: 50.225975036621094\n",
      "[step: 3000] loss: 0.006462242919951677\n",
      "[step: 3001] loss: 50.21060562133789\n",
      "[step: 3001] loss: 0.006461836397647858\n",
      "[step: 3002] loss: 50.19526290893555\n",
      "[step: 3002] loss: 0.006461631506681442\n",
      "[step: 3003] loss: 50.17991638183594\n",
      "[step: 3003] loss: 0.006461618468165398\n",
      "[step: 3004] loss: 50.164588928222656\n",
      "[step: 3004] loss: 0.006461591925472021\n",
      "[step: 3005] loss: 50.14932632446289\n",
      "[step: 3005] loss: 0.0064614275470376015\n",
      "[step: 3006] loss: 50.134029388427734\n",
      "[step: 3006] loss: 0.006461138371378183\n",
      "[step: 3007] loss: 50.118717193603516\n",
      "[step: 3007] loss: 0.006460851524025202\n",
      "[step: 3008] loss: 50.10337829589844\n",
      "[step: 3008] loss: 0.006460679229348898\n",
      "[step: 3009] loss: 50.08808898925781\n",
      "[step: 3009] loss: 0.006460594478994608\n",
      "[step: 3010] loss: 50.072750091552734\n",
      "[step: 3010] loss: 0.0064605106599628925\n",
      "[step: 3011] loss: 50.057437896728516\n",
      "[step: 3011] loss: 0.006460357923060656\n",
      "[step: 3012] loss: 50.04216003417969\n",
      "[step: 3012] loss: 0.006460129749029875\n",
      "[step: 3013] loss: 50.02686309814453\n",
      "[step: 3013] loss: 0.006459898315370083\n",
      "[step: 3014] loss: 50.01153564453125\n",
      "[step: 3014] loss: 0.006459719967097044\n",
      "[step: 3015] loss: 49.996307373046875\n",
      "[step: 3015] loss: 0.006459597032517195\n",
      "[step: 3016] loss: 49.98099136352539\n",
      "[step: 3016] loss: 0.006459477823227644\n",
      "[step: 3017] loss: 49.965763092041016\n",
      "[step: 3017] loss: 0.006459335796535015\n",
      "[step: 3018] loss: 49.95051574707031\n",
      "[step: 3018] loss: 0.006459146272391081\n",
      "[step: 3019] loss: 49.93522262573242\n",
      "[step: 3019] loss: 0.006458950228989124\n",
      "[step: 3020] loss: 49.919921875\n",
      "[step: 3020] loss: 0.006458773743361235\n",
      "[step: 3021] loss: 49.904579162597656\n",
      "[step: 3021] loss: 0.006458621472120285\n",
      "[step: 3022] loss: 49.889366149902344\n",
      "[step: 3022] loss: 0.006458486430346966\n",
      "[step: 3023] loss: 49.87413024902344\n",
      "[step: 3023] loss: 0.006458341609686613\n",
      "[step: 3024] loss: 49.85882568359375\n",
      "[step: 3024] loss: 0.006458182819187641\n",
      "[step: 3025] loss: 49.84355163574219\n",
      "[step: 3025] loss: 0.0064580123871564865\n",
      "[step: 3026] loss: 49.8282356262207\n",
      "[step: 3026] loss: 0.006457827053964138\n",
      "[step: 3027] loss: 49.81297302246094\n",
      "[step: 3027] loss: 0.006457660812884569\n",
      "[step: 3028] loss: 49.797760009765625\n",
      "[step: 3028] loss: 0.006457506213337183\n",
      "[step: 3029] loss: 49.78246307373047\n",
      "[step: 3029] loss: 0.006457362323999405\n",
      "[step: 3030] loss: 49.76722717285156\n",
      "[step: 3030] loss: 0.006457212846726179\n",
      "[step: 3031] loss: 49.75196838378906\n",
      "[step: 3031] loss: 0.006457051262259483\n",
      "[step: 3032] loss: 49.73674011230469\n",
      "[step: 3032] loss: 0.0064568850211799145\n",
      "[step: 3033] loss: 49.7215461730957\n",
      "[step: 3033] loss: 0.006456725299358368\n",
      "[step: 3034] loss: 49.70625305175781\n",
      "[step: 3034] loss: 0.006456559058278799\n",
      "[step: 3035] loss: 49.69105911254883\n",
      "[step: 3035] loss: 0.006456397473812103\n",
      "[step: 3036] loss: 49.675750732421875\n",
      "[step: 3036] loss: 0.00645624753087759\n",
      "[step: 3037] loss: 49.66059112548828\n",
      "[step: 3037] loss: 0.0064560892060399055\n",
      "[step: 3038] loss: 49.64537811279297\n",
      "[step: 3038] loss: 0.006455938797444105\n",
      "[step: 3039] loss: 49.63018035888672\n",
      "[step: 3039] loss: 0.0064557683654129505\n",
      "[step: 3040] loss: 49.61500549316406\n",
      "[step: 3040] loss: 0.006455615628510714\n",
      "[step: 3041] loss: 49.59993362426758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3041] loss: 0.006455449387431145\n",
      "[step: 3042] loss: 49.58488082885742\n",
      "[step: 3042] loss: 0.0064552947878837585\n",
      "[step: 3043] loss: 49.57001495361328\n",
      "[step: 3043] loss: 0.006455135531723499\n",
      "[step: 3044] loss: 49.55534362792969\n",
      "[step: 3044] loss: 0.006454979069530964\n",
      "[step: 3045] loss: 49.541282653808594\n",
      "[step: 3045] loss: 0.006454817019402981\n",
      "[step: 3046] loss: 49.52793884277344\n",
      "[step: 3046] loss: 0.006454661022871733\n",
      "[step: 3047] loss: 49.51626205444336\n",
      "[step: 3047] loss: 0.006454506888985634\n",
      "[step: 3048] loss: 49.507659912109375\n",
      "[step: 3048] loss: 0.006454345304518938\n",
      "[step: 3049] loss: 49.504966735839844\n",
      "[step: 3049] loss: 0.0064541855826973915\n",
      "[step: 3050] loss: 49.51343536376953\n",
      "[step: 3050] loss: 0.006454025395214558\n",
      "[step: 3051] loss: 49.54464340209961\n",
      "[step: 3051] loss: 0.0064538693986833096\n",
      "[step: 3052] loss: 49.619319915771484\n",
      "[step: 3052] loss: 0.006453710608184338\n",
      "[step: 3053] loss: 49.78575897216797\n",
      "[step: 3053] loss: 0.006453552283346653\n",
      "[step: 3054] loss: 50.125640869140625\n",
      "[step: 3054] loss: 0.0064533911645412445\n",
      "[step: 3055] loss: 50.846553802490234\n",
      "[step: 3055] loss: 0.006453234702348709\n",
      "[step: 3056] loss: 52.229400634765625\n",
      "[step: 3056] loss: 0.006453078705817461\n",
      "[step: 3057] loss: 55.09580993652344\n",
      "[step: 3057] loss: 0.006452921312302351\n",
      "[step: 3058] loss: 59.73632049560547\n",
      "[step: 3058] loss: 0.006452762521803379\n",
      "[step: 3059] loss: 67.8548583984375\n",
      "[step: 3059] loss: 0.006452601868659258\n",
      "[step: 3060] loss: 73.35179901123047\n",
      "[step: 3060] loss: 0.006452448200434446\n",
      "[step: 3061] loss: 74.47177124023438\n",
      "[step: 3061] loss: 0.006452285684645176\n",
      "[step: 3062] loss: 60.383087158203125\n",
      "[step: 3062] loss: 0.006452119909226894\n",
      "[step: 3063] loss: 50.13223648071289\n",
      "[step: 3063] loss: 0.00645196670666337\n",
      "[step: 3064] loss: 54.487937927246094\n",
      "[step: 3064] loss: 0.006451813038438559\n",
      "[step: 3065] loss: 61.38856506347656\n",
      "[step: 3065] loss: 0.006451648194342852\n",
      "[step: 3066] loss: 58.03919219970703\n",
      "[step: 3066] loss: 0.006451496854424477\n",
      "[step: 3067] loss: 50.194217681884766\n",
      "[step: 3067] loss: 0.006451335735619068\n",
      "[step: 3068] loss: 52.847904205322266\n",
      "[step: 3068] loss: 0.006451184395700693\n",
      "[step: 3069] loss: 57.66455078125\n",
      "[step: 3069] loss: 0.006451014429330826\n",
      "[step: 3070] loss: 52.815616607666016\n",
      "[step: 3070] loss: 0.006450863555073738\n",
      "[step: 3071] loss: 49.6343994140625\n",
      "[step: 3071] loss: 0.0064506977796554565\n",
      "[step: 3072] loss: 53.414703369140625\n",
      "[step: 3072] loss: 0.0064505464397370815\n",
      "[step: 3073] loss: 53.73178482055664\n",
      "[step: 3073] loss: 0.006450382526963949\n",
      "[step: 3074] loss: 50.06523895263672\n",
      "[step: 3074] loss: 0.006450227461755276\n",
      "[step: 3075] loss: 50.16950225830078\n",
      "[step: 3075] loss: 0.006450065411627293\n",
      "[step: 3076] loss: 52.625587463378906\n",
      "[step: 3076] loss: 0.0064499154686927795\n",
      "[step: 3077] loss: 51.49308776855469\n",
      "[step: 3077] loss: 0.006449747830629349\n",
      "[step: 3078] loss: 49.351898193359375\n",
      "[step: 3078] loss: 0.006449596956372261\n",
      "[step: 3079] loss: 50.69055938720703\n",
      "[step: 3079] loss: 0.006449435837566853\n",
      "[step: 3080] loss: 51.63630294799805\n",
      "[step: 3080] loss: 0.006449274253100157\n",
      "[step: 3081] loss: 49.80013656616211\n",
      "[step: 3081] loss: 0.006449117325246334\n",
      "[step: 3082] loss: 49.489662170410156\n",
      "[step: 3082] loss: 0.006448958069086075\n",
      "[step: 3083] loss: 50.79861068725586\n",
      "[step: 3083] loss: 0.006448802072554827\n",
      "[step: 3084] loss: 50.215065002441406\n",
      "[step: 3084] loss: 0.006448641885071993\n",
      "[step: 3085] loss: 49.191566467285156\n",
      "[step: 3085] loss: 0.006448482163250446\n",
      "[step: 3086] loss: 49.85595703125\n",
      "[step: 3086] loss: 0.0064483224414289\n",
      "[step: 3087] loss: 50.18305206298828\n",
      "[step: 3087] loss: 0.006448166910558939\n",
      "[step: 3088] loss: 49.345123291015625\n",
      "[step: 3088] loss: 0.006448010448366404\n",
      "[step: 3089] loss: 49.278221130371094\n",
      "[step: 3089] loss: 0.00644785026088357\n",
      "[step: 3090] loss: 49.839508056640625\n",
      "[step: 3090] loss: 0.0064476956613361835\n",
      "[step: 3091] loss: 49.52220153808594\n",
      "[step: 3091] loss: 0.006447531282901764\n",
      "[step: 3092] loss: 49.06416320800781\n",
      "[step: 3092] loss: 0.0064473748207092285\n",
      "[step: 3093] loss: 49.40409851074219\n",
      "[step: 3093] loss: 0.006447214633226395\n",
      "[step: 3094] loss: 49.541015625\n",
      "[step: 3094] loss: 0.006447054445743561\n",
      "[step: 3095] loss: 49.10383224487305\n",
      "[step: 3095] loss: 0.0064468965865671635\n",
      "[step: 3096] loss: 49.069007873535156\n",
      "[step: 3096] loss: 0.006446740124374628\n",
      "[step: 3097] loss: 49.35151672363281\n",
      "[step: 3097] loss: 0.006446583662182093\n",
      "[step: 3098] loss: 49.19920349121094\n",
      "[step: 3098] loss: 0.006446425803005695\n",
      "[step: 3099] loss: 48.94674301147461\n",
      "[step: 3099] loss: 0.006446263752877712\n",
      "[step: 3100] loss: 49.073204040527344\n",
      "[step: 3100] loss: 0.006446104031056166\n",
      "[step: 3101] loss: 49.17521667480469\n",
      "[step: 3101] loss: 0.006445944309234619\n",
      "[step: 3102] loss: 48.98374938964844\n",
      "[step: 3102] loss: 0.006445791106671095\n",
      "[step: 3103] loss: 48.887489318847656\n",
      "[step: 3103] loss: 0.006445628125220537\n",
      "[step: 3104] loss: 49.01111602783203\n",
      "[step: 3104] loss: 0.006445472594350576\n",
      "[step: 3105] loss: 49.005760192871094\n",
      "[step: 3105] loss: 0.006445314269512892\n",
      "[step: 3106] loss: 48.85337829589844\n",
      "[step: 3106] loss: 0.006445147097110748\n",
      "[step: 3107] loss: 48.843666076660156\n",
      "[step: 3107] loss: 0.006444996688514948\n",
      "[step: 3108] loss: 48.92266082763672\n",
      "[step: 3108] loss: 0.006444833241403103\n",
      "[step: 3109] loss: 48.87035369873047\n",
      "[step: 3109] loss: 0.006444677710533142\n",
      "[step: 3110] loss: 48.77284240722656\n",
      "[step: 3110] loss: 0.006444520782679319\n",
      "[step: 3111] loss: 48.78791046142578\n",
      "[step: 3111] loss: 0.006444354075938463\n",
      "[step: 3112] loss: 48.82447814941406\n",
      "[step: 3112] loss: 0.006444201804697514\n",
      "[step: 3113] loss: 48.76886749267578\n",
      "[step: 3113] loss: 0.00644404161721468\n",
      "[step: 3114] loss: 48.70726013183594\n",
      "[step: 3114] loss: 0.006443886552006006\n",
      "[step: 3115] loss: 48.722320556640625\n",
      "[step: 3115] loss: 0.006443732883781195\n",
      "[step: 3116] loss: 48.73612594604492\n",
      "[step: 3116] loss: 0.006443568971008062\n",
      "[step: 3117] loss: 48.688697814941406\n",
      "[step: 3117] loss: 0.00644341204315424\n",
      "[step: 3118] loss: 48.6462287902832\n",
      "[step: 3118] loss: 0.0064432513900101185\n",
      "[step: 3119] loss: 48.65283966064453\n",
      "[step: 3119] loss: 0.006443105638027191\n",
      "[step: 3120] loss: 48.65597915649414\n",
      "[step: 3120] loss: 0.00644295709207654\n",
      "[step: 3121] loss: 48.62006378173828\n",
      "[step: 3121] loss: 0.0064428276382386684\n",
      "[step: 3122] loss: 48.58614730834961\n",
      "[step: 3122] loss: 0.006442708894610405\n",
      "[step: 3123] loss: 48.58430862426758\n",
      "[step: 3123] loss: 0.006442621350288391\n",
      "[step: 3124] loss: 48.583003997802734\n",
      "[step: 3124] loss: 0.006442587822675705\n",
      "[step: 3125] loss: 48.5560302734375\n",
      "[step: 3125] loss: 0.00644265953451395\n",
      "[step: 3126] loss: 48.526878356933594\n",
      "[step: 3126] loss: 0.006442921236157417\n",
      "[step: 3127] loss: 48.51824951171875\n",
      "[step: 3127] loss: 0.006443512160331011\n",
      "[step: 3128] loss: 48.51428985595703\n",
      "[step: 3128] loss: 0.006444770842790604\n",
      "[step: 3129] loss: 48.49472427368164\n",
      "[step: 3129] loss: 0.006447131279855967\n",
      "[step: 3130] loss: 48.46903991699219\n",
      "[step: 3130] loss: 0.006451692897826433\n",
      "[step: 3131] loss: 48.45530319213867\n",
      "[step: 3131] loss: 0.006459592375904322\n",
      "[step: 3132] loss: 48.448665618896484\n",
      "[step: 3132] loss: 0.006473890505731106\n",
      "[step: 3133] loss: 48.433990478515625\n",
      "[step: 3133] loss: 0.006494033616036177\n",
      "[step: 3134] loss: 48.41209030151367\n",
      "[step: 3134] loss: 0.0065224976278841496\n",
      "[step: 3135] loss: 48.39502716064453\n",
      "[step: 3135] loss: 0.00653802277520299\n",
      "[step: 3136] loss: 48.38511657714844\n",
      "[step: 3136] loss: 0.0065328278578817844\n",
      "[step: 3137] loss: 48.37327575683594\n",
      "[step: 3137] loss: 0.006488309241831303\n",
      "[step: 3138] loss: 48.35527038574219\n",
      "[step: 3138] loss: 0.006447817664593458\n",
      "[step: 3139] loss: 48.337074279785156\n",
      "[step: 3139] loss: 0.006445444654673338\n",
      "[step: 3140] loss: 48.32379913330078\n",
      "[step: 3140] loss: 0.0064721181988716125\n",
      "[step: 3141] loss: 48.31245422363281\n",
      "[step: 3141] loss: 0.006487392820417881\n",
      "[step: 3142] loss: 48.29790115356445\n",
      "[step: 3142] loss: 0.006467126309871674\n",
      "[step: 3143] loss: 48.280670166015625\n",
      "[step: 3143] loss: 0.006442924030125141\n",
      "[step: 3144] loss: 48.26496887207031\n",
      "[step: 3144] loss: 0.006445383653044701\n",
      "[step: 3145] loss: 48.252197265625\n",
      "[step: 3145] loss: 0.0064634098671376705\n",
      "[step: 3146] loss: 48.239410400390625\n",
      "[step: 3146] loss: 0.006467188708484173\n",
      "[step: 3147] loss: 48.22431182861328\n",
      "[step: 3147] loss: 0.006450689397752285\n",
      "[step: 3148] loss: 48.208282470703125\n",
      "[step: 3148] loss: 0.006439375225454569\n",
      "[step: 3149] loss: 48.19359588623047\n",
      "[step: 3149] loss: 0.006446149665862322\n",
      "[step: 3150] loss: 48.180442810058594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3150] loss: 0.00645630294457078\n",
      "[step: 3151] loss: 48.16700744628906\n",
      "[step: 3151] loss: 0.0064537362195551395\n",
      "[step: 3152] loss: 48.15214538574219\n",
      "[step: 3152] loss: 0.006442418321967125\n",
      "[step: 3153] loss: 48.13685607910156\n",
      "[step: 3153] loss: 0.0064386469312012196\n",
      "[step: 3154] loss: 48.12248992919922\n",
      "[step: 3154] loss: 0.0064449249766767025\n",
      "[step: 3155] loss: 48.10895538330078\n",
      "[step: 3155] loss: 0.006449306849390268\n",
      "[step: 3156] loss: 48.0952262878418\n",
      "[step: 3156] loss: 0.006445149425417185\n",
      "[step: 3157] loss: 48.08077621459961\n",
      "[step: 3157] loss: 0.006438658572733402\n",
      "[step: 3158] loss: 48.066017150878906\n",
      "[step: 3158] loss: 0.006438460201025009\n",
      "[step: 3159] loss: 48.05164337158203\n",
      "[step: 3159] loss: 0.006442763842642307\n",
      "[step: 3160] loss: 48.037803649902344\n",
      "[step: 3160] loss: 0.006443927995860577\n",
      "[step: 3161] loss: 48.0240478515625\n",
      "[step: 3161] loss: 0.006440267898142338\n",
      "[step: 3162] loss: 48.00994110107422\n",
      "[step: 3162] loss: 0.006436976138502359\n",
      "[step: 3163] loss: 47.995506286621094\n",
      "[step: 3163] loss: 0.006437927484512329\n",
      "[step: 3164] loss: 47.981117248535156\n",
      "[step: 3164] loss: 0.00644040247425437\n",
      "[step: 3165] loss: 47.96714401245117\n",
      "[step: 3165] loss: 0.006440178491175175\n",
      "[step: 3166] loss: 47.953277587890625\n",
      "[step: 3166] loss: 0.0064375982619822025\n",
      "[step: 3167] loss: 47.93937683105469\n",
      "[step: 3167] loss: 0.006436087656766176\n",
      "[step: 3168] loss: 47.92519760131836\n",
      "[step: 3168] loss: 0.006437066476792097\n",
      "[step: 3169] loss: 47.91099166870117\n",
      "[step: 3169] loss: 0.006438324227929115\n",
      "[step: 3170] loss: 47.89682388305664\n",
      "[step: 3170] loss: 0.0064377132803201675\n",
      "[step: 3171] loss: 47.88286590576172\n",
      "[step: 3171] loss: 0.006436052266508341\n",
      "[step: 3172] loss: 47.868988037109375\n",
      "[step: 3172] loss: 0.006435363553464413\n",
      "[step: 3173] loss: 47.85505294799805\n",
      "[step: 3173] loss: 0.006436046212911606\n",
      "[step: 3174] loss: 47.840980529785156\n",
      "[step: 3174] loss: 0.006436612922698259\n",
      "[step: 3175] loss: 47.826927185058594\n",
      "[step: 3175] loss: 0.006436030846089125\n",
      "[step: 3176] loss: 47.81281280517578\n",
      "[step: 3176] loss: 0.006434989627450705\n",
      "[step: 3177] loss: 47.798866271972656\n",
      "[step: 3177] loss: 0.00643462548032403\n",
      "[step: 3178] loss: 47.78497314453125\n",
      "[step: 3178] loss: 0.006435013376176357\n",
      "[step: 3179] loss: 47.77105712890625\n",
      "[step: 3179] loss: 0.006435252260416746\n",
      "[step: 3180] loss: 47.757118225097656\n",
      "[step: 3180] loss: 0.00643481221050024\n",
      "[step: 3181] loss: 47.743133544921875\n",
      "[step: 3181] loss: 0.006434136535972357\n",
      "[step: 3182] loss: 47.72911071777344\n",
      "[step: 3182] loss: 0.006433865055441856\n",
      "[step: 3183] loss: 47.71516418457031\n",
      "[step: 3183] loss: 0.006434031296521425\n",
      "[step: 3184] loss: 47.70125198364258\n",
      "[step: 3184] loss: 0.006434121169149876\n",
      "[step: 3185] loss: 47.6873893737793\n",
      "[step: 3185] loss: 0.006433826871216297\n",
      "[step: 3186] loss: 47.67344665527344\n",
      "[step: 3186] loss: 0.006433362141251564\n",
      "[step: 3187] loss: 47.659568786621094\n",
      "[step: 3187] loss: 0.006433104630559683\n",
      "[step: 3188] loss: 47.64564514160156\n",
      "[step: 3188] loss: 0.006433109752833843\n",
      "[step: 3189] loss: 47.63169860839844\n",
      "[step: 3189] loss: 0.006433134898543358\n",
      "[step: 3190] loss: 47.61778259277344\n",
      "[step: 3190] loss: 0.006432945374399424\n",
      "[step: 3191] loss: 47.60395431518555\n",
      "[step: 3191] loss: 0.006432611495256424\n",
      "[step: 3192] loss: 47.5900764465332\n",
      "[step: 3192] loss: 0.006432362366467714\n",
      "[step: 3193] loss: 47.576210021972656\n",
      "[step: 3193] loss: 0.0064322748221457005\n",
      "[step: 3194] loss: 47.56230163574219\n",
      "[step: 3194] loss: 0.006432233843952417\n",
      "[step: 3195] loss: 47.54847717285156\n",
      "[step: 3195] loss: 0.006432108581066132\n",
      "[step: 3196] loss: 47.534629821777344\n",
      "[step: 3196] loss: 0.006431868765503168\n",
      "[step: 3197] loss: 47.52074432373047\n",
      "[step: 3197] loss: 0.006431630812585354\n",
      "[step: 3198] loss: 47.506874084472656\n",
      "[step: 3198] loss: 0.006431492045521736\n",
      "[step: 3199] loss: 47.49306869506836\n",
      "[step: 3199] loss: 0.006431395187973976\n",
      "[step: 3200] loss: 47.47923278808594\n",
      "[step: 3200] loss: 0.00643129413947463\n",
      "[step: 3201] loss: 47.465423583984375\n",
      "[step: 3201] loss: 0.006431113928556442\n",
      "[step: 3202] loss: 47.451595306396484\n",
      "[step: 3202] loss: 0.006430916488170624\n",
      "[step: 3203] loss: 47.43778991699219\n",
      "[step: 3203] loss: 0.006430735811591148\n",
      "[step: 3204] loss: 47.42395782470703\n",
      "[step: 3204] loss: 0.006430605426430702\n",
      "[step: 3205] loss: 47.41020202636719\n",
      "[step: 3205] loss: 0.006430499255657196\n",
      "[step: 3206] loss: 47.396366119384766\n",
      "[step: 3206] loss: 0.006430358625948429\n",
      "[step: 3207] loss: 47.38255310058594\n",
      "[step: 3207] loss: 0.006430181208997965\n",
      "[step: 3208] loss: 47.368797302246094\n",
      "[step: 3208] loss: 0.006430007517337799\n",
      "[step: 3209] loss: 47.35496520996094\n",
      "[step: 3209] loss: 0.006429850589483976\n",
      "[step: 3210] loss: 47.341224670410156\n",
      "[step: 3210] loss: 0.006429722066968679\n",
      "[step: 3211] loss: 47.32740783691406\n",
      "[step: 3211] loss: 0.006429588422179222\n",
      "[step: 3212] loss: 47.31365966796875\n",
      "[step: 3212] loss: 0.006429444998502731\n",
      "[step: 3213] loss: 47.29987716674805\n",
      "[step: 3213] loss: 0.006429277826100588\n",
      "[step: 3214] loss: 47.2861328125\n",
      "[step: 3214] loss: 0.0064291139133274555\n",
      "[step: 3215] loss: 47.272369384765625\n",
      "[step: 3215] loss: 0.006428966764360666\n",
      "[step: 3216] loss: 47.25864791870117\n",
      "[step: 3216] loss: 0.0064288233406841755\n",
      "[step: 3217] loss: 47.244869232177734\n",
      "[step: 3217] loss: 0.006428685504943132\n",
      "[step: 3218] loss: 47.23113250732422\n",
      "[step: 3218] loss: 0.006428541615605354\n",
      "[step: 3219] loss: 47.217411041259766\n",
      "[step: 3219] loss: 0.006428384222090244\n",
      "[step: 3220] loss: 47.203670501708984\n",
      "[step: 3220] loss: 0.0064282226376235485\n",
      "[step: 3221] loss: 47.18995666503906\n",
      "[step: 3221] loss: 0.006428081542253494\n",
      "[step: 3222] loss: 47.17617416381836\n",
      "[step: 3222] loss: 0.006427939981222153\n",
      "[step: 3223] loss: 47.16255187988281\n",
      "[step: 3223] loss: 0.006427787709981203\n",
      "[step: 3224] loss: 47.1488037109375\n",
      "[step: 3224] loss: 0.006427649408578873\n",
      "[step: 3225] loss: 47.13512420654297\n",
      "[step: 3225] loss: 0.006427501328289509\n",
      "[step: 3226] loss: 47.12141418457031\n",
      "[step: 3226] loss: 0.006427347660064697\n",
      "[step: 3227] loss: 47.10771942138672\n",
      "[step: 3227] loss: 0.0064271898008883\n",
      "[step: 3228] loss: 47.09404754638672\n",
      "[step: 3228] loss: 0.006427048705518246\n",
      "[step: 3229] loss: 47.08036804199219\n",
      "[step: 3229] loss: 0.006426902487874031\n",
      "[step: 3230] loss: 47.06669998168945\n",
      "[step: 3230] loss: 0.006426757667213678\n",
      "[step: 3231] loss: 47.05297088623047\n",
      "[step: 3231] loss: 0.006426607258617878\n",
      "[step: 3232] loss: 47.03932189941406\n",
      "[step: 3232] loss: 0.0064264643006026745\n",
      "[step: 3233] loss: 47.025718688964844\n",
      "[step: 3233] loss: 0.006426312495023012\n",
      "[step: 3234] loss: 47.01211929321289\n",
      "[step: 3234] loss: 0.006426164880394936\n",
      "[step: 3235] loss: 46.99854278564453\n",
      "[step: 3235] loss: 0.006426018197089434\n",
      "[step: 3236] loss: 46.985069274902344\n",
      "[step: 3236] loss: 0.006425866391509771\n",
      "[step: 3237] loss: 46.97166061401367\n",
      "[step: 3237] loss: 0.006425723899155855\n",
      "[step: 3238] loss: 46.95849609375\n",
      "[step: 3238] loss: 0.006425575353205204\n",
      "[step: 3239] loss: 46.9456787109375\n",
      "[step: 3239] loss: 0.006425425875931978\n",
      "[step: 3240] loss: 46.93346405029297\n",
      "[step: 3240] loss: 0.006425279658287764\n",
      "[step: 3241] loss: 46.92245101928711\n",
      "[step: 3241] loss: 0.006425132043659687\n",
      "[step: 3242] loss: 46.913482666015625\n",
      "[step: 3242] loss: 0.006424983497709036\n",
      "[step: 3243] loss: 46.90863800048828\n",
      "[step: 3243] loss: 0.006424835417419672\n",
      "[step: 3244] loss: 46.9112548828125\n",
      "[step: 3244] loss: 0.006424687337130308\n",
      "[step: 3245] loss: 46.928810119628906\n",
      "[step: 3245] loss: 0.006424538791179657\n",
      "[step: 3246] loss: 46.97446823120117\n",
      "[step: 3246] loss: 0.006424396298825741\n",
      "[step: 3247] loss: 47.07811737060547\n",
      "[step: 3247] loss: 0.006424244958907366\n",
      "[step: 3248] loss: 47.29076385498047\n",
      "[step: 3248] loss: 0.006424095015972853\n",
      "[step: 3249] loss: 47.737403869628906\n",
      "[step: 3249] loss: 0.006423946935683489\n",
      "[step: 3250] loss: 48.602325439453125\n",
      "[step: 3250] loss: 0.0064238025806844234\n",
      "[step: 3251] loss: 50.3950309753418\n",
      "[step: 3251] loss: 0.006423651706427336\n",
      "[step: 3252] loss: 53.52105712890625\n",
      "[step: 3252] loss: 0.006423502694815397\n",
      "[step: 3253] loss: 59.46841812133789\n",
      "[step: 3253] loss: 0.006423359736800194\n",
      "[step: 3254] loss: 66.13294982910156\n",
      "[step: 3254] loss: 0.006423212122172117\n",
      "[step: 3255] loss: 73.29719543457031\n",
      "[step: 3255] loss: 0.006423065904527903\n",
      "[step: 3256] loss: 67.07252502441406\n",
      "[step: 3256] loss: 0.006422914564609528\n",
      "[step: 3257] loss: 54.59040069580078\n",
      "[step: 3257] loss: 0.006422769278287888\n",
      "[step: 3258] loss: 47.25390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3258] loss: 0.006422622129321098\n",
      "[step: 3259] loss: 53.70415496826172\n",
      "[step: 3259] loss: 0.006422474514693022\n",
      "[step: 3260] loss: 59.86358642578125\n",
      "[step: 3260] loss: 0.006422323640435934\n",
      "[step: 3261] loss: 52.793636322021484\n",
      "[step: 3261] loss: 0.006422176957130432\n",
      "[step: 3262] loss: 47.18026351928711\n",
      "[step: 3262] loss: 0.006422027014195919\n",
      "[step: 3263] loss: 51.471229553222656\n",
      "[step: 3263] loss: 0.006421885918825865\n",
      "[step: 3264] loss: 53.69709777832031\n",
      "[step: 3264] loss: 0.006421735044568777\n",
      "[step: 3265] loss: 49.02928924560547\n",
      "[step: 3265] loss: 0.006421588361263275\n",
      "[step: 3266] loss: 47.28984832763672\n",
      "[step: 3266] loss: 0.006421439349651337\n",
      "[step: 3267] loss: 50.9489860534668\n",
      "[step: 3267] loss: 0.006421290338039398\n",
      "[step: 3268] loss: 50.92361068725586\n",
      "[step: 3268] loss: 0.006421139929443598\n",
      "[step: 3269] loss: 47.208740234375\n",
      "[step: 3269] loss: 0.006420993711799383\n",
      "[step: 3270] loss: 47.921600341796875\n",
      "[step: 3270] loss: 0.006420847959816456\n",
      "[step: 3271] loss: 50.30518341064453\n",
      "[step: 3271] loss: 0.006420698016881943\n",
      "[step: 3272] loss: 48.489540100097656\n",
      "[step: 3272] loss: 0.006420549470931292\n",
      "[step: 3273] loss: 46.78185272216797\n",
      "[step: 3273] loss: 0.0064204055815935135\n",
      "[step: 3274] loss: 48.299713134765625\n",
      "[step: 3274] loss: 0.006420255638659\n",
      "[step: 3275] loss: 48.764854431152344\n",
      "[step: 3275] loss: 0.0064201075583696365\n",
      "[step: 3276] loss: 47.10535430908203\n",
      "[step: 3276] loss: 0.006419959478080273\n",
      "[step: 3277] loss: 46.95428466796875\n",
      "[step: 3277] loss: 0.006419810466468334\n",
      "[step: 3278] loss: 48.10699462890625\n",
      "[step: 3278] loss: 0.006419661920517683\n",
      "[step: 3279] loss: 47.61565399169922\n",
      "[step: 3279] loss: 0.006419516634196043\n",
      "[step: 3280] loss: 46.6376953125\n",
      "[step: 3280] loss: 0.006419372744858265\n",
      "[step: 3281] loss: 47.259254455566406\n",
      "[step: 3281] loss: 0.006419217213988304\n",
      "[step: 3282] loss: 47.640968322753906\n",
      "[step: 3282] loss: 0.0064190770499408245\n",
      "[step: 3283] loss: 46.80553436279297\n",
      "[step: 3283] loss: 0.006418927107006311\n",
      "[step: 3284] loss: 46.70830535888672\n",
      "[step: 3284] loss: 0.006418776232749224\n",
      "[step: 3285] loss: 47.28065490722656\n",
      "[step: 3285] loss: 0.006418632343411446\n",
      "[step: 3286] loss: 46.98744201660156\n",
      "[step: 3286] loss: 0.0064184824004769325\n",
      "[step: 3287] loss: 46.5205078125\n",
      "[step: 3287] loss: 0.006418333854526281\n",
      "[step: 3288] loss: 46.8114013671875\n",
      "[step: 3288] loss: 0.006418190896511078\n",
      "[step: 3289] loss: 46.974220275878906\n",
      "[step: 3289] loss: 0.006418040953576565\n",
      "[step: 3290] loss: 46.590782165527344\n",
      "[step: 3290] loss: 0.006417896132916212\n",
      "[step: 3291] loss: 46.50154495239258\n",
      "[step: 3291] loss: 0.006417747121304274\n",
      "[step: 3292] loss: 46.768306732177734\n",
      "[step: 3292] loss: 0.006417595781385899\n",
      "[step: 3293] loss: 46.69172668457031\n",
      "[step: 3293] loss: 0.006417447701096535\n",
      "[step: 3294] loss: 46.428253173828125\n",
      "[step: 3294] loss: 0.006417299620807171\n",
      "[step: 3295] loss: 46.502235412597656\n",
      "[step: 3295] loss: 0.006417146418243647\n",
      "[step: 3296] loss: 46.63902282714844\n",
      "[step: 3296] loss: 0.0064170025289058685\n",
      "[step: 3297] loss: 46.485748291015625\n",
      "[step: 3297] loss: 0.006416855845600367\n",
      "[step: 3298] loss: 46.35707092285156\n",
      "[step: 3298] loss: 0.00641670823097229\n",
      "[step: 3299] loss: 46.45759582519531\n",
      "[step: 3299] loss: 0.0064165592193603516\n",
      "[step: 3300] loss: 46.488834381103516\n",
      "[step: 3300] loss: 0.006416411604732275\n",
      "[step: 3301] loss: 46.353919982910156\n",
      "[step: 3301] loss: 0.0064162625931203365\n",
      "[step: 3302] loss: 46.308624267578125\n",
      "[step: 3302] loss: 0.006416110787540674\n",
      "[step: 3303] loss: 46.38426971435547\n",
      "[step: 3303] loss: 0.006415963638573885\n",
      "[step: 3304] loss: 46.36973571777344\n",
      "[step: 3304] loss: 0.006415818352252245\n",
      "[step: 3305] loss: 46.27123260498047\n",
      "[step: 3305] loss: 0.006415668409317732\n",
      "[step: 3306] loss: 46.25916290283203\n",
      "[step: 3306] loss: 0.006415513809770346\n",
      "[step: 3307] loss: 46.30486297607422\n",
      "[step: 3307] loss: 0.006415370386093855\n",
      "[step: 3308] loss: 46.27459716796875\n",
      "[step: 3308] loss: 0.006415223702788353\n",
      "[step: 3309] loss: 46.20653533935547\n",
      "[step: 3309] loss: 0.00641507375985384\n",
      "[step: 3310] loss: 46.2027587890625\n",
      "[step: 3310] loss: 0.006414921022951603\n",
      "[step: 3311] loss: 46.22687530517578\n",
      "[step: 3311] loss: 0.006414774339646101\n",
      "[step: 3312] loss: 46.19905471801758\n",
      "[step: 3312] loss: 0.006414628587663174\n",
      "[step: 3313] loss: 46.149932861328125\n",
      "[step: 3313] loss: 0.006414481904357672\n",
      "[step: 3314] loss: 46.14358901977539\n",
      "[step: 3314] loss: 0.006414335686713457\n",
      "[step: 3315] loss: 46.15537643432617\n",
      "[step: 3315] loss: 0.006414184346795082\n",
      "[step: 3316] loss: 46.13383483886719\n",
      "[step: 3316] loss: 0.006414039060473442\n",
      "[step: 3317] loss: 46.09649658203125\n",
      "[step: 3317] loss: 0.006413890048861504\n",
      "[step: 3318] loss: 46.085105895996094\n",
      "[step: 3318] loss: 0.006413741502910852\n",
      "[step: 3319] loss: 46.089111328125\n",
      "[step: 3319] loss: 0.006413591559976339\n",
      "[step: 3320] loss: 46.073936462402344\n",
      "[step: 3320] loss: 0.006413446739315987\n",
      "[step: 3321] loss: 46.0444221496582\n",
      "[step: 3321] loss: 0.006413300056010485\n",
      "[step: 3322] loss: 46.02885437011719\n",
      "[step: 3322] loss: 0.006413150578737259\n",
      "[step: 3323] loss: 46.02692794799805\n",
      "[step: 3323] loss: 0.0064130062237381935\n",
      "[step: 3324] loss: 46.01624298095703\n",
      "[step: 3324] loss: 0.006412857212126255\n",
      "[step: 3325] loss: 45.99329376220703\n",
      "[step: 3325] loss: 0.006412716116756201\n",
      "[step: 3326] loss: 45.97523498535156\n",
      "[step: 3326] loss: 0.006412580143660307\n",
      "[step: 3327] loss: 45.968292236328125\n",
      "[step: 3327] loss: 0.0064124432392418385\n",
      "[step: 3328] loss: 45.9599723815918\n",
      "[step: 3328] loss: 0.006412323098629713\n",
      "[step: 3329] loss: 45.942447662353516\n",
      "[step: 3329] loss: 0.00641222158446908\n",
      "[step: 3330] loss: 45.923927307128906\n",
      "[step: 3330] loss: 0.0064121438190341\n",
      "[step: 3331] loss: 45.91252136230469\n",
      "[step: 3331] loss: 0.006412125658243895\n",
      "[step: 3332] loss: 45.904075622558594\n",
      "[step: 3332] loss: 0.006412195507436991\n",
      "[step: 3333] loss: 45.89094161987305\n",
      "[step: 3333] loss: 0.006412453018128872\n",
      "[step: 3334] loss: 45.87397766113281\n",
      "[step: 3334] loss: 0.006413012742996216\n",
      "[step: 3335] loss: 45.85957336425781\n",
      "[step: 3335] loss: 0.0064141517505049706\n",
      "[step: 3336] loss: 45.849281311035156\n",
      "[step: 3336] loss: 0.006416230462491512\n",
      "[step: 3337] loss: 45.83844757080078\n",
      "[step: 3337] loss: 0.00642017088830471\n",
      "[step: 3338] loss: 45.824249267578125\n",
      "[step: 3338] loss: 0.006426836363971233\n",
      "[step: 3339] loss: 45.80919647216797\n",
      "[step: 3339] loss: 0.006438779179006815\n",
      "[step: 3340] loss: 45.79645538330078\n",
      "[step: 3340] loss: 0.006455740425735712\n",
      "[step: 3341] loss: 45.78544998168945\n",
      "[step: 3341] loss: 0.006480726413428783\n",
      "[step: 3342] loss: 45.77357864379883\n",
      "[step: 3342] loss: 0.006498484872281551\n",
      "[step: 3343] loss: 45.759769439697266\n",
      "[step: 3343] loss: 0.006503396667540073\n",
      "[step: 3344] loss: 45.7458381652832\n",
      "[step: 3344] loss: 0.0064710211008787155\n",
      "[step: 3345] loss: 45.73336410522461\n",
      "[step: 3345] loss: 0.0064293742179870605\n",
      "[step: 3346] loss: 45.72187042236328\n",
      "[step: 3346] loss: 0.0064106285572052\n",
      "[step: 3347] loss: 45.7097282409668\n",
      "[step: 3347] loss: 0.006426535546779633\n",
      "[step: 3348] loss: 45.69643020629883\n",
      "[step: 3348] loss: 0.006449865642935038\n",
      "[step: 3349] loss: 45.68312072753906\n",
      "[step: 3349] loss: 0.006446628365665674\n",
      "[step: 3350] loss: 45.67066955566406\n",
      "[step: 3350] loss: 0.006423586048185825\n",
      "[step: 3351] loss: 45.65882873535156\n",
      "[step: 3351] loss: 0.006409760098904371\n",
      "[step: 3352] loss: 45.64662551879883\n",
      "[step: 3352] loss: 0.006419602315872908\n",
      "[step: 3353] loss: 45.63383483886719\n",
      "[step: 3353] loss: 0.0064340452663600445\n",
      "[step: 3354] loss: 45.620887756347656\n",
      "[step: 3354] loss: 0.006430666893720627\n",
      "[step: 3355] loss: 45.60834503173828\n",
      "[step: 3355] loss: 0.006415854208171368\n",
      "[step: 3356] loss: 45.596214294433594\n",
      "[step: 3356] loss: 0.006408842280507088\n",
      "[step: 3357] loss: 45.58409881591797\n",
      "[step: 3357] loss: 0.00641617551445961\n",
      "[step: 3358] loss: 45.57167053222656\n",
      "[step: 3358] loss: 0.006424351595342159\n",
      "[step: 3359] loss: 45.55897521972656\n",
      "[step: 3359] loss: 0.006420803256332874\n",
      "[step: 3360] loss: 45.54639434814453\n",
      "[step: 3360] loss: 0.006411488633602858\n",
      "[step: 3361] loss: 45.53412628173828\n",
      "[step: 3361] loss: 0.006408112123608589\n",
      "[step: 3362] loss: 45.521934509277344\n",
      "[step: 3362] loss: 0.006413088645786047\n",
      "[step: 3363] loss: 45.509742736816406\n",
      "[step: 3363] loss: 0.006417572498321533\n",
      "[step: 3364] loss: 45.49732208251953\n",
      "[step: 3364] loss: 0.00641474686563015\n",
      "[step: 3365] loss: 45.484840393066406\n",
      "[step: 3365] loss: 0.006409044843167067\n",
      "[step: 3366] loss: 45.472469329833984\n",
      "[step: 3366] loss: 0.006407380569726229\n",
      "[step: 3367] loss: 45.460227966308594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3367] loss: 0.006410528440028429\n",
      "[step: 3368] loss: 45.448055267333984\n",
      "[step: 3368] loss: 0.006412955932319164\n",
      "[step: 3369] loss: 45.435829162597656\n",
      "[step: 3369] loss: 0.006411015521734953\n",
      "[step: 3370] loss: 45.423500061035156\n",
      "[step: 3370] loss: 0.0064075603149831295\n",
      "[step: 3371] loss: 45.411216735839844\n",
      "[step: 3371] loss: 0.006406567059457302\n",
      "[step: 3372] loss: 45.39886474609375\n",
      "[step: 3372] loss: 0.006408385466784239\n",
      "[step: 3373] loss: 45.38664245605469\n",
      "[step: 3373] loss: 0.006409809924662113\n",
      "[step: 3374] loss: 45.37446594238281\n",
      "[step: 3374] loss: 0.006408666260540485\n",
      "[step: 3375] loss: 45.36231231689453\n",
      "[step: 3375] loss: 0.006406530272215605\n",
      "[step: 3376] loss: 45.35009002685547\n",
      "[step: 3376] loss: 0.0064057717099785805\n",
      "[step: 3377] loss: 45.337894439697266\n",
      "[step: 3377] loss: 0.006406712345778942\n",
      "[step: 3378] loss: 45.325660705566406\n",
      "[step: 3378] loss: 0.006407595239579678\n",
      "[step: 3379] loss: 45.31340026855469\n",
      "[step: 3379] loss: 0.0064069963991642\n",
      "[step: 3380] loss: 45.30120849609375\n",
      "[step: 3380] loss: 0.006405674386769533\n",
      "[step: 3381] loss: 45.28911590576172\n",
      "[step: 3381] loss: 0.00640502804890275\n",
      "[step: 3382] loss: 45.27696990966797\n",
      "[step: 3382] loss: 0.006405425723642111\n",
      "[step: 3383] loss: 45.264793395996094\n",
      "[step: 3383] loss: 0.006405971013009548\n",
      "[step: 3384] loss: 45.252655029296875\n",
      "[step: 3384] loss: 0.006405723746865988\n",
      "[step: 3385] loss: 45.240516662597656\n",
      "[step: 3385] loss: 0.006404899526387453\n",
      "[step: 3386] loss: 45.22840118408203\n",
      "[step: 3386] loss: 0.006404325366020203\n",
      "[step: 3387] loss: 45.21626663208008\n",
      "[step: 3387] loss: 0.006404385901987553\n",
      "[step: 3388] loss: 45.20411682128906\n",
      "[step: 3388] loss: 0.0064046867191791534\n",
      "[step: 3389] loss: 45.192047119140625\n",
      "[step: 3389] loss: 0.006404639221727848\n",
      "[step: 3390] loss: 45.179969787597656\n",
      "[step: 3390] loss: 0.006404165644198656\n",
      "[step: 3391] loss: 45.167877197265625\n",
      "[step: 3391] loss: 0.00640367204323411\n",
      "[step: 3392] loss: 45.15580749511719\n",
      "[step: 3392] loss: 0.006403516512364149\n",
      "[step: 3393] loss: 45.14370346069336\n",
      "[step: 3393] loss: 0.006403619423508644\n",
      "[step: 3394] loss: 45.131622314453125\n",
      "[step: 3394] loss: 0.006403647363185883\n",
      "[step: 3395] loss: 45.119529724121094\n",
      "[step: 3395] loss: 0.006403409410268068\n",
      "[step: 3396] loss: 45.1075325012207\n",
      "[step: 3396] loss: 0.00640304246917367\n",
      "[step: 3397] loss: 45.095489501953125\n",
      "[step: 3397] loss: 0.006402784958481789\n",
      "[step: 3398] loss: 45.08344268798828\n",
      "[step: 3398] loss: 0.006402714643627405\n",
      "[step: 3399] loss: 45.07136535644531\n",
      "[step: 3399] loss: 0.006402716971933842\n",
      "[step: 3400] loss: 45.05939865112305\n",
      "[step: 3400] loss: 0.006402619648724794\n",
      "[step: 3401] loss: 45.04737091064453\n",
      "[step: 3401] loss: 0.006402384024113417\n",
      "[step: 3402] loss: 45.035316467285156\n",
      "[step: 3402] loss: 0.006402119062840939\n",
      "[step: 3403] loss: 45.02332305908203\n",
      "[step: 3403] loss: 0.006401950027793646\n",
      "[step: 3404] loss: 45.01136016845703\n",
      "[step: 3404] loss: 0.006401872728019953\n",
      "[step: 3405] loss: 44.999366760253906\n",
      "[step: 3405] loss: 0.0064017982222139835\n",
      "[step: 3406] loss: 44.98735809326172\n",
      "[step: 3406] loss: 0.006401673890650272\n",
      "[step: 3407] loss: 44.975372314453125\n",
      "[step: 3407] loss: 0.00640146853402257\n",
      "[step: 3408] loss: 44.96337127685547\n",
      "[step: 3408] loss: 0.006401268299669027\n",
      "[step: 3409] loss: 44.951416015625\n",
      "[step: 3409] loss: 0.006401121616363525\n",
      "[step: 3410] loss: 44.93946838378906\n",
      "[step: 3410] loss: 0.006401012651622295\n",
      "[step: 3411] loss: 44.927520751953125\n",
      "[step: 3411] loss: 0.006400912068784237\n",
      "[step: 3412] loss: 44.91558074951172\n",
      "[step: 3412] loss: 0.006400777027010918\n",
      "[step: 3413] loss: 44.90361022949219\n",
      "[step: 3413] loss: 0.006400613114237785\n",
      "[step: 3414] loss: 44.89171600341797\n",
      "[step: 3414] loss: 0.006400431971997023\n",
      "[step: 3415] loss: 44.879764556884766\n",
      "[step: 3415] loss: 0.006400282960385084\n",
      "[step: 3416] loss: 44.867897033691406\n",
      "[step: 3416] loss: 0.0064001670107245445\n",
      "[step: 3417] loss: 44.855934143066406\n",
      "[step: 3417] loss: 0.0064000459387898445\n",
      "[step: 3418] loss: 44.84403610229492\n",
      "[step: 3418] loss: 0.006399916019290686\n",
      "[step: 3419] loss: 44.83214569091797\n",
      "[step: 3419] loss: 0.006399771198630333\n",
      "[step: 3420] loss: 44.820228576660156\n",
      "[step: 3420] loss: 0.006399612408131361\n",
      "[step: 3421] loss: 44.808353424072266\n",
      "[step: 3421] loss: 0.006399458274245262\n",
      "[step: 3422] loss: 44.79644012451172\n",
      "[step: 3422] loss: 0.006399320904165506\n",
      "[step: 3423] loss: 44.78463363647461\n",
      "[step: 3423] loss: 0.006399192847311497\n",
      "[step: 3424] loss: 44.772727966308594\n",
      "[step: 3424] loss: 0.0063990638591349125\n",
      "[step: 3425] loss: 44.760894775390625\n",
      "[step: 3425] loss: 0.0063989232294261456\n",
      "[step: 3426] loss: 44.74900817871094\n",
      "[step: 3426] loss: 0.006398780271410942\n",
      "[step: 3427] loss: 44.73719024658203\n",
      "[step: 3427] loss: 0.00639863358810544\n",
      "[step: 3428] loss: 44.725379943847656\n",
      "[step: 3428] loss: 0.006398489233106375\n",
      "[step: 3429] loss: 44.71356201171875\n",
      "[step: 3429] loss: 0.006398356519639492\n",
      "[step: 3430] loss: 44.701759338378906\n",
      "[step: 3430] loss: 0.006398222874850035\n",
      "[step: 3431] loss: 44.690067291259766\n",
      "[step: 3431] loss: 0.006398089695721865\n",
      "[step: 3432] loss: 44.67842102050781\n",
      "[step: 3432] loss: 0.00639794347807765\n",
      "[step: 3433] loss: 44.66688537597656\n",
      "[step: 3433] loss: 0.006397804711014032\n",
      "[step: 3434] loss: 44.65559387207031\n",
      "[step: 3434] loss: 0.006397664546966553\n",
      "[step: 3435] loss: 44.64457702636719\n",
      "[step: 3435] loss: 0.0063975234515964985\n",
      "[step: 3436] loss: 44.63420867919922\n",
      "[step: 3436] loss: 0.006397380959242582\n",
      "[step: 3437] loss: 44.624996185302734\n",
      "[step: 3437] loss: 0.006397251971065998\n",
      "[step: 3438] loss: 44.617820739746094\n",
      "[step: 3438] loss: 0.006397109478712082\n",
      "[step: 3439] loss: 44.61446762084961\n",
      "[step: 3439] loss: 0.006396974436938763\n",
      "[step: 3440] loss: 44.61824035644531\n",
      "[step: 3440] loss: 0.006396836135536432\n",
      "[step: 3441] loss: 44.635643005371094\n",
      "[step: 3441] loss: 0.006396695040166378\n",
      "[step: 3442] loss: 44.67877197265625\n",
      "[step: 3442] loss: 0.0063965581357479095\n",
      "[step: 3443] loss: 44.773773193359375\n",
      "[step: 3443] loss: 0.006396410521119833\n",
      "[step: 3444] loss: 44.96526336669922\n",
      "[step: 3444] loss: 0.006396282464265823\n",
      "[step: 3445] loss: 45.3602294921875\n",
      "[step: 3445] loss: 0.006396141834557056\n",
      "[step: 3446] loss: 46.11539840698242\n",
      "[step: 3446] loss: 0.006396011915057898\n",
      "[step: 3447] loss: 47.65735626220703\n",
      "[step: 3447] loss: 0.0063958680257201195\n",
      "[step: 3448] loss: 50.349266052246094\n",
      "[step: 3448] loss: 0.006395731586962938\n",
      "[step: 3449] loss: 55.473575592041016\n",
      "[step: 3449] loss: 0.00639558769762516\n",
      "[step: 3450] loss: 61.67198944091797\n",
      "[step: 3450] loss: 0.006395450793206692\n",
      "[step: 3451] loss: 69.2037353515625\n",
      "[step: 3451] loss: 0.006395311560481787\n",
      "[step: 3452] loss: 66.0726318359375\n",
      "[step: 3452] loss: 0.006395173724740744\n",
      "[step: 3453] loss: 55.76458740234375\n",
      "[step: 3453] loss: 0.00639503775164485\n",
      "[step: 3454] loss: 45.454994201660156\n",
      "[step: 3454] loss: 0.006394900381565094\n",
      "[step: 3455] loss: 48.560157775878906\n",
      "[step: 3455] loss: 0.006394766736775637\n",
      "[step: 3456] loss: 56.323387145996094\n",
      "[step: 3456] loss: 0.006394621450453997\n",
      "[step: 3457] loss: 52.74268341064453\n",
      "[step: 3457] loss: 0.006394487340003252\n",
      "[step: 3458] loss: 45.5972900390625\n",
      "[step: 3458] loss: 0.006394342519342899\n",
      "[step: 3459] loss: 46.770591735839844\n",
      "[step: 3459] loss: 0.006394209340214729\n",
      "[step: 3460] loss: 51.15790557861328\n",
      "[step: 3460] loss: 0.006394069641828537\n",
      "[step: 3461] loss: 48.88467788696289\n",
      "[step: 3461] loss: 0.006393936462700367\n",
      "[step: 3462] loss: 44.719459533691406\n",
      "[step: 3462] loss: 0.006393801886588335\n",
      "[step: 3463] loss: 47.08246612548828\n",
      "[step: 3463] loss: 0.006393651943653822\n",
      "[step: 3464] loss: 49.46099090576172\n",
      "[step: 3464] loss: 0.006393518298864365\n",
      "[step: 3465] loss: 46.225433349609375\n",
      "[step: 3465] loss: 0.006393378600478172\n",
      "[step: 3466] loss: 44.652645111083984\n",
      "[step: 3466] loss: 0.006393244490027428\n",
      "[step: 3467] loss: 47.03413009643555\n",
      "[step: 3467] loss: 0.00639310572296381\n",
      "[step: 3468] loss: 47.20454788208008\n",
      "[step: 3468] loss: 0.0063929627649486065\n",
      "[step: 3469] loss: 44.940284729003906\n",
      "[step: 3469] loss: 0.006392820738255978\n",
      "[step: 3470] loss: 44.92377471923828\n",
      "[step: 3470] loss: 0.0063926889561116695\n",
      "[step: 3471] loss: 46.439697265625\n",
      "[step: 3471] loss: 0.0063925511203706264\n",
      "[step: 3472] loss: 45.810935974121094\n",
      "[step: 3472] loss: 0.006392417009919882\n",
      "[step: 3473] loss: 44.45258331298828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3473] loss: 0.006392274983227253\n",
      "[step: 3474] loss: 45.166175842285156\n",
      "[step: 3474] loss: 0.006392131093889475\n",
      "[step: 3475] loss: 45.848209381103516\n",
      "[step: 3475] loss: 0.006391997914761305\n",
      "[step: 3476] loss: 44.82134246826172\n",
      "[step: 3476] loss: 0.006391859147697687\n",
      "[step: 3477] loss: 44.460052490234375\n",
      "[step: 3477] loss: 0.006391719449311495\n",
      "[step: 3478] loss: 45.23445510864258\n",
      "[step: 3478] loss: 0.006391582544893026\n",
      "[step: 3479] loss: 45.075599670410156\n",
      "[step: 3479] loss: 0.00639144703745842\n",
      "[step: 3480] loss: 44.371707916259766\n",
      "[step: 3480] loss: 0.006391302216798067\n",
      "[step: 3481] loss: 44.597900390625\n",
      "[step: 3481] loss: 0.006391169503331184\n",
      "[step: 3482] loss: 44.9675178527832\n",
      "[step: 3482] loss: 0.0063910274766385555\n",
      "[step: 3483] loss: 44.55812454223633\n",
      "[step: 3483] loss: 0.0063908942975103855\n",
      "[step: 3484] loss: 44.28059768676758\n",
      "[step: 3484] loss: 0.006390745285898447\n",
      "[step: 3485] loss: 44.6151123046875\n",
      "[step: 3485] loss: 0.006390609312802553\n",
      "[step: 3486] loss: 44.658084869384766\n",
      "[step: 3486] loss: 0.006390476133674383\n",
      "[step: 3487] loss: 44.291236877441406\n",
      "[step: 3487] loss: 0.006390335503965616\n",
      "[step: 3488] loss: 44.295265197753906\n",
      "[step: 3488] loss: 0.006390194874256849\n",
      "[step: 3489] loss: 44.5276985168457\n",
      "[step: 3489] loss: 0.00639005983248353\n",
      "[step: 3490] loss: 44.40235137939453\n",
      "[step: 3490] loss: 0.006389920599758625\n",
      "[step: 3491] loss: 44.18578338623047\n",
      "[step: 3491] loss: 0.0063897850923240185\n",
      "[step: 3492] loss: 44.27536392211914\n",
      "[step: 3492] loss: 0.006389647256582975\n",
      "[step: 3493] loss: 44.379310607910156\n",
      "[step: 3493] loss: 0.006389503367245197\n",
      "[step: 3494] loss: 44.23849868774414\n",
      "[step: 3494] loss: 0.006389367859810591\n",
      "[step: 3495] loss: 44.13071823120117\n",
      "[step: 3495] loss: 0.006389234215021133\n",
      "[step: 3496] loss: 44.22041320800781\n",
      "[step: 3496] loss: 0.006389094982296228\n",
      "[step: 3497] loss: 44.25289535522461\n",
      "[step: 3497] loss: 0.0063889529556035995\n",
      "[step: 3498] loss: 44.134376525878906\n",
      "[step: 3498] loss: 0.006388812325894833\n",
      "[step: 3499] loss: 44.08623504638672\n",
      "[step: 3499] loss: 0.006388676818460226\n",
      "[step: 3500] loss: 44.15117645263672\n",
      "[step: 3500] loss: 0.0063885413110256195\n",
      "[step: 3501] loss: 44.148780822753906\n",
      "[step: 3501] loss: 0.006388403940945864\n",
      "[step: 3502] loss: 44.06382751464844\n",
      "[step: 3502] loss: 0.006388259585946798\n",
      "[step: 3503] loss: 44.039634704589844\n",
      "[step: 3503] loss: 0.006388125941157341\n",
      "[step: 3504] loss: 44.08005905151367\n",
      "[step: 3504] loss: 0.006387986242771149\n",
      "[step: 3505] loss: 44.06924057006836\n",
      "[step: 3505] loss: 0.0063878512009978294\n",
      "[step: 3506] loss: 44.00770950317383\n",
      "[step: 3506] loss: 0.006387713365256786\n",
      "[step: 3507] loss: 43.98955535888672\n",
      "[step: 3507] loss: 0.006387574598193169\n",
      "[step: 3508] loss: 44.01242446899414\n",
      "[step: 3508] loss: 0.006387428380548954\n",
      "[step: 3509] loss: 44.00194549560547\n",
      "[step: 3509] loss: 0.006387295667082071\n",
      "[step: 3510] loss: 43.95805358886719\n",
      "[step: 3510] loss: 0.006387157365679741\n",
      "[step: 3511] loss: 43.9390983581543\n",
      "[step: 3511] loss: 0.006387018598616123\n",
      "[step: 3512] loss: 43.94975280761719\n",
      "[step: 3512] loss: 0.006386886350810528\n",
      "[step: 3513] loss: 43.94336700439453\n",
      "[step: 3513] loss: 0.006386742927134037\n",
      "[step: 3514] loss: 43.91150665283203\n",
      "[step: 3514] loss: 0.006386614870280027\n",
      "[step: 3515] loss: 43.890785217285156\n",
      "[step: 3515] loss: 0.0063864742405712605\n",
      "[step: 3516] loss: 43.89227294921875\n",
      "[step: 3516] loss: 0.006386335473507643\n",
      "[step: 3517] loss: 43.88831329345703\n",
      "[step: 3517] loss: 0.006386210676282644\n",
      "[step: 3518] loss: 43.865966796875\n",
      "[step: 3518] loss: 0.00638608681038022\n",
      "[step: 3519] loss: 43.84489440917969\n",
      "[step: 3519] loss: 0.006385971326380968\n",
      "[step: 3520] loss: 43.83900833129883\n",
      "[step: 3520] loss: 0.006385875400155783\n",
      "[step: 3521] loss: 43.83564376831055\n",
      "[step: 3521] loss: 0.00638579810038209\n",
      "[step: 3522] loss: 43.82063293457031\n",
      "[step: 3522] loss: 0.006385765038430691\n",
      "[step: 3523] loss: 43.80105209350586\n",
      "[step: 3523] loss: 0.00638580834493041\n",
      "[step: 3524] loss: 43.789833068847656\n",
      "[step: 3524] loss: 0.006385974120348692\n",
      "[step: 3525] loss: 43.78456115722656\n",
      "[step: 3525] loss: 0.006386375520378351\n",
      "[step: 3526] loss: 43.774513244628906\n",
      "[step: 3526] loss: 0.006387148052453995\n",
      "[step: 3527] loss: 43.758304595947266\n",
      "[step: 3527] loss: 0.006388639099895954\n",
      "[step: 3528] loss: 43.74402618408203\n",
      "[step: 3528] loss: 0.006391224451363087\n",
      "[step: 3529] loss: 43.73539733886719\n",
      "[step: 3529] loss: 0.006395970471203327\n",
      "[step: 3530] loss: 43.72740173339844\n",
      "[step: 3530] loss: 0.006403553299605846\n",
      "[step: 3531] loss: 43.715065002441406\n",
      "[step: 3531] loss: 0.006416628137230873\n",
      "[step: 3532] loss: 43.700721740722656\n",
      "[step: 3532] loss: 0.006433478090912104\n",
      "[step: 3533] loss: 43.68907928466797\n",
      "[step: 3533] loss: 0.006456459406763315\n",
      "[step: 3534] loss: 43.680233001708984\n",
      "[step: 3534] loss: 0.006467944476753473\n",
      "[step: 3535] loss: 43.670501708984375\n",
      "[step: 3535] loss: 0.006464814301580191\n",
      "[step: 3536] loss: 43.658233642578125\n",
      "[step: 3536] loss: 0.006431233137845993\n",
      "[step: 3537] loss: 43.645442962646484\n",
      "[step: 3537] loss: 0.006395959295332432\n",
      "[step: 3538] loss: 43.63452911376953\n",
      "[step: 3538] loss: 0.006384601816534996\n",
      "[step: 3539] loss: 43.624977111816406\n",
      "[step: 3539] loss: 0.006400746293365955\n",
      "[step: 3540] loss: 43.61473846435547\n",
      "[step: 3540] loss: 0.006420018617063761\n",
      "[step: 3541] loss: 43.60301208496094\n",
      "[step: 3541] loss: 0.006416283547878265\n",
      "[step: 3542] loss: 43.591060638427734\n",
      "[step: 3542] loss: 0.006396509241312742\n",
      "[step: 3543] loss: 43.58024215698242\n",
      "[step: 3543] loss: 0.006383534055203199\n",
      "[step: 3544] loss: 43.57023620605469\n",
      "[step: 3544] loss: 0.006390158087015152\n",
      "[step: 3545] loss: 43.55986785888672\n",
      "[step: 3545] loss: 0.006403181701898575\n",
      "[step: 3546] loss: 43.548667907714844\n",
      "[step: 3546] loss: 0.00640356820076704\n",
      "[step: 3547] loss: 43.537261962890625\n",
      "[step: 3547] loss: 0.006392181385308504\n",
      "[step: 3548] loss: 43.52635955810547\n",
      "[step: 3548] loss: 0.006382893770933151\n",
      "[step: 3549] loss: 43.51603698730469\n",
      "[step: 3549] loss: 0.0063856313936412334\n",
      "[step: 3550] loss: 43.50566101074219\n",
      "[step: 3550] loss: 0.006393694318830967\n",
      "[step: 3551] loss: 43.494815826416016\n",
      "[step: 3551] loss: 0.006395039614289999\n",
      "[step: 3552] loss: 43.48374938964844\n",
      "[step: 3552] loss: 0.0063886442221701145\n",
      "[step: 3553] loss: 43.472877502441406\n",
      "[step: 3553] loss: 0.006382287014275789\n",
      "[step: 3554] loss: 43.462303161621094\n",
      "[step: 3554] loss: 0.006382979452610016\n",
      "[step: 3555] loss: 43.45191955566406\n",
      "[step: 3555] loss: 0.00638783723115921\n",
      "[step: 3556] loss: 43.44134521484375\n",
      "[step: 3556] loss: 0.006389459129422903\n",
      "[step: 3557] loss: 43.430580139160156\n",
      "[step: 3557] loss: 0.006386104039847851\n",
      "[step: 3558] loss: 43.41973114013672\n",
      "[step: 3558] loss: 0.006381795275956392\n",
      "[step: 3559] loss: 43.40906524658203\n",
      "[step: 3559] loss: 0.006381277926266193\n",
      "[step: 3560] loss: 43.398555755615234\n",
      "[step: 3560] loss: 0.006383906584233046\n",
      "[step: 3561] loss: 43.38811111450195\n",
      "[step: 3561] loss: 0.006385564338415861\n",
      "[step: 3562] loss: 43.377567291259766\n",
      "[step: 3562] loss: 0.006384202744811773\n",
      "[step: 3563] loss: 43.36692810058594\n",
      "[step: 3563] loss: 0.006381381768733263\n",
      "[step: 3564] loss: 43.35624313354492\n",
      "[step: 3564] loss: 0.0063802157528698444\n",
      "[step: 3565] loss: 43.345664978027344\n",
      "[step: 3565] loss: 0.006381301674991846\n",
      "[step: 3566] loss: 43.33518981933594\n",
      "[step: 3566] loss: 0.0063826460391283035\n",
      "[step: 3567] loss: 43.324737548828125\n",
      "[step: 3567] loss: 0.006382481195032597\n",
      "[step: 3568] loss: 43.314231872558594\n",
      "[step: 3568] loss: 0.006380877457559109\n",
      "[step: 3569] loss: 43.30370330810547\n",
      "[step: 3569] loss: 0.006379605270922184\n",
      "[step: 3570] loss: 43.29313659667969\n",
      "[step: 3570] loss: 0.0063796998001635075\n",
      "[step: 3571] loss: 43.28260803222656\n",
      "[step: 3571] loss: 0.006380509119480848\n",
      "[step: 3572] loss: 43.2720947265625\n",
      "[step: 3572] loss: 0.0063808374106884\n",
      "[step: 3573] loss: 43.26167297363281\n",
      "[step: 3573] loss: 0.006380158476531506\n",
      "[step: 3574] loss: 43.25121307373047\n",
      "[step: 3574] loss: 0.006379157770425081\n",
      "[step: 3575] loss: 43.24081802368164\n",
      "[step: 3575] loss: 0.006378751713782549\n",
      "[step: 3576] loss: 43.230289459228516\n",
      "[step: 3576] loss: 0.0063790129497647285\n",
      "[step: 3577] loss: 43.21980285644531\n",
      "[step: 3577] loss: 0.006379369180649519\n",
      "[step: 3578] loss: 43.20940017700195\n",
      "[step: 3578] loss: 0.006379249040037394\n",
      "[step: 3579] loss: 43.198944091796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3579] loss: 0.006378684658557177\n",
      "[step: 3580] loss: 43.188533782958984\n",
      "[step: 3580] loss: 0.006378146819770336\n",
      "[step: 3581] loss: 43.17815017700195\n",
      "[step: 3581] loss: 0.006377996876835823\n",
      "[step: 3582] loss: 43.16778564453125\n",
      "[step: 3582] loss: 0.006378143560141325\n",
      "[step: 3583] loss: 43.15739440917969\n",
      "[step: 3583] loss: 0.006378231104463339\n",
      "[step: 3584] loss: 43.14692687988281\n",
      "[step: 3584] loss: 0.006378039717674255\n",
      "[step: 3585] loss: 43.13652038574219\n",
      "[step: 3585] loss: 0.006377647165209055\n",
      "[step: 3586] loss: 43.12611389160156\n",
      "[step: 3586] loss: 0.006377323996275663\n",
      "[step: 3587] loss: 43.11576843261719\n",
      "[step: 3587] loss: 0.0063772001303732395\n",
      "[step: 3588] loss: 43.10540771484375\n",
      "[step: 3588] loss: 0.006377220153808594\n",
      "[step: 3589] loss: 43.095062255859375\n",
      "[step: 3589] loss: 0.006377204321324825\n",
      "[step: 3590] loss: 43.08476638793945\n",
      "[step: 3590] loss: 0.006377041805535555\n",
      "[step: 3591] loss: 43.07440948486328\n",
      "[step: 3591] loss: 0.006376778241246939\n",
      "[step: 3592] loss: 43.06403350830078\n",
      "[step: 3592] loss: 0.006376537028700113\n",
      "[step: 3593] loss: 43.05373001098633\n",
      "[step: 3593] loss: 0.00637639919295907\n",
      "[step: 3594] loss: 43.04339599609375\n",
      "[step: 3594] loss: 0.006376340053975582\n",
      "[step: 3595] loss: 43.0330924987793\n",
      "[step: 3595] loss: 0.006376280449330807\n",
      "[step: 3596] loss: 43.022789001464844\n",
      "[step: 3596] loss: 0.0063761561177670956\n",
      "[step: 3597] loss: 43.012481689453125\n",
      "[step: 3597] loss: 0.00637596333399415\n",
      "[step: 3598] loss: 43.00217056274414\n",
      "[step: 3598] loss: 0.006375765427947044\n",
      "[step: 3599] loss: 42.99188232421875\n",
      "[step: 3599] loss: 0.006375616416335106\n",
      "[step: 3600] loss: 42.9815673828125\n",
      "[step: 3600] loss: 0.006375503726303577\n",
      "[step: 3601] loss: 42.971336364746094\n",
      "[step: 3601] loss: 0.006375419441610575\n",
      "[step: 3602] loss: 42.96102523803711\n",
      "[step: 3602] loss: 0.006375310011208057\n",
      "[step: 3603] loss: 42.95077896118164\n",
      "[step: 3603] loss: 0.006375162862241268\n",
      "[step: 3604] loss: 42.94049072265625\n",
      "[step: 3604] loss: 0.006374994292855263\n",
      "[step: 3605] loss: 42.93022537231445\n",
      "[step: 3605] loss: 0.006374842021614313\n",
      "[step: 3606] loss: 42.919979095458984\n",
      "[step: 3606] loss: 0.006374707445502281\n",
      "[step: 3607] loss: 42.90971755981445\n",
      "[step: 3607] loss: 0.006374592427164316\n",
      "[step: 3608] loss: 42.89950180053711\n",
      "[step: 3608] loss: 0.006374488119035959\n",
      "[step: 3609] loss: 42.889259338378906\n",
      "[step: 3609] loss: 0.00637436006218195\n",
      "[step: 3610] loss: 42.8790283203125\n",
      "[step: 3610] loss: 0.006374226417392492\n",
      "[step: 3611] loss: 42.86885070800781\n",
      "[step: 3611] loss: 0.006374082528054714\n",
      "[step: 3612] loss: 42.85860824584961\n",
      "[step: 3612] loss: 0.006373933516442776\n",
      "[step: 3613] loss: 42.84838104248047\n",
      "[step: 3613] loss: 0.006373798009008169\n",
      "[step: 3614] loss: 42.83819580078125\n",
      "[step: 3614] loss: 0.006373682525008917\n",
      "[step: 3615] loss: 42.82798385620117\n",
      "[step: 3615] loss: 0.006373561918735504\n",
      "[step: 3616] loss: 42.81779479980469\n",
      "[step: 3616] loss: 0.006373436655849218\n",
      "[step: 3617] loss: 42.8076171875\n",
      "[step: 3617] loss: 0.0063733067363500595\n",
      "[step: 3618] loss: 42.79742431640625\n",
      "[step: 3618] loss: 0.006373167037963867\n",
      "[step: 3619] loss: 42.78728485107422\n",
      "[step: 3619] loss: 0.0063730343244969845\n",
      "[step: 3620] loss: 42.777034759521484\n",
      "[step: 3620] loss: 0.006372897420078516\n",
      "[step: 3621] loss: 42.76689910888672\n",
      "[step: 3621] loss: 0.006372764706611633\n",
      "[step: 3622] loss: 42.75676345825195\n",
      "[step: 3622] loss: 0.006372638512402773\n",
      "[step: 3623] loss: 42.74657440185547\n",
      "[step: 3623] loss: 0.006372520700097084\n",
      "[step: 3624] loss: 42.73645782470703\n",
      "[step: 3624] loss: 0.006372387520968914\n",
      "[step: 3625] loss: 42.72629165649414\n",
      "[step: 3625] loss: 0.006372259464114904\n",
      "[step: 3626] loss: 42.71615982055664\n",
      "[step: 3626] loss: 0.006372125819325447\n",
      "[step: 3627] loss: 42.70602798461914\n",
      "[step: 3627] loss: 0.006371993105858564\n",
      "[step: 3628] loss: 42.69590759277344\n",
      "[step: 3628] loss: 0.0063718631863594055\n",
      "[step: 3629] loss: 42.68586730957031\n",
      "[step: 3629] loss: 0.006371733266860247\n",
      "[step: 3630] loss: 42.67575454711914\n",
      "[step: 3630] loss: 0.0063716089352965355\n",
      "[step: 3631] loss: 42.66572189331055\n",
      "[step: 3631] loss: 0.006371476221829653\n",
      "[step: 3632] loss: 42.65576934814453\n",
      "[step: 3632] loss: 0.006371355149894953\n",
      "[step: 3633] loss: 42.6458740234375\n",
      "[step: 3633] loss: 0.006371224299073219\n",
      "[step: 3634] loss: 42.63615417480469\n",
      "[step: 3634] loss: 0.006371096707880497\n",
      "[step: 3635] loss: 42.62665557861328\n",
      "[step: 3635] loss: 0.006370963994413614\n",
      "[step: 3636] loss: 42.617652893066406\n",
      "[step: 3636] loss: 0.006370830349624157\n",
      "[step: 3637] loss: 42.60948181152344\n",
      "[step: 3637] loss: 0.0063707055523991585\n",
      "[step: 3638] loss: 42.60276794433594\n",
      "[step: 3638] loss: 0.006370576564222574\n",
      "[step: 3639] loss: 42.59882736206055\n",
      "[step: 3639] loss: 0.006370444316416979\n",
      "[step: 3640] loss: 42.59990310668945\n",
      "[step: 3640] loss: 0.006370309740304947\n",
      "[step: 3641] loss: 42.610565185546875\n",
      "[step: 3641] loss: 0.006370186805725098\n",
      "[step: 3642] loss: 42.638954162597656\n",
      "[step: 3642] loss: 0.006370063871145248\n",
      "[step: 3643] loss: 42.70248794555664\n",
      "[step: 3643] loss: 0.006369934882968664\n",
      "[step: 3644] loss: 42.83118438720703\n",
      "[step: 3644] loss: 0.006369803100824356\n",
      "[step: 3645] loss: 43.094215393066406\n",
      "[step: 3645] loss: 0.006369675509631634\n",
      "[step: 3646] loss: 43.59790802001953\n",
      "[step: 3646] loss: 0.006369543261826038\n",
      "[step: 3647] loss: 44.61686706542969\n",
      "[step: 3647] loss: 0.006369409617036581\n",
      "[step: 3648] loss: 46.44953536987305\n",
      "[step: 3648] loss: 0.0063692801631987095\n",
      "[step: 3649] loss: 50.01930236816406\n",
      "[step: 3649] loss: 0.006369156762957573\n",
      "[step: 3650] loss: 55.130950927734375\n",
      "[step: 3650] loss: 0.006369022186845541\n",
      "[step: 3651] loss: 62.86311340332031\n",
      "[step: 3651] loss: 0.006368888542056084\n",
      "[step: 3652] loss: 65.51131439208984\n",
      "[step: 3652] loss: 0.006368769332766533\n",
      "[step: 3653] loss: 62.008148193359375\n",
      "[step: 3653] loss: 0.006368638947606087\n",
      "[step: 3654] loss: 48.76838684082031\n",
      "[step: 3654] loss: 0.006368511356413364\n",
      "[step: 3655] loss: 42.95441436767578\n",
      "[step: 3655] loss: 0.0063683814369142056\n",
      "[step: 3656] loss: 49.09682846069336\n",
      "[step: 3656] loss: 0.006368252914398909\n",
      "[step: 3657] loss: 53.37604522705078\n",
      "[step: 3657] loss: 0.006368126254528761\n",
      "[step: 3658] loss: 48.55110168457031\n",
      "[step: 3658] loss: 0.006367993541061878\n",
      "[step: 3659] loss: 42.92021179199219\n",
      "[step: 3659] loss: 0.006367871072143316\n",
      "[step: 3660] loss: 46.1956787109375\n",
      "[step: 3660] loss: 0.006367741618305445\n",
      "[step: 3661] loss: 49.61534881591797\n",
      "[step: 3661] loss: 0.006367608904838562\n",
      "[step: 3662] loss: 45.217140197753906\n",
      "[step: 3662] loss: 0.006367481779307127\n",
      "[step: 3663] loss: 42.751548767089844\n",
      "[step: 3663] loss: 0.006367354653775692\n",
      "[step: 3664] loss: 45.96730422973633\n",
      "[step: 3664] loss: 0.0063672238029539585\n",
      "[step: 3665] loss: 46.56990432739258\n",
      "[step: 3665] loss: 0.006367096211761236\n",
      "[step: 3666] loss: 43.46464920043945\n",
      "[step: 3666] loss: 0.006366967689245939\n",
      "[step: 3667] loss: 42.88242721557617\n",
      "[step: 3667] loss: 0.006366839166730642\n",
      "[step: 3668] loss: 45.105499267578125\n",
      "[step: 3668] loss: 0.006366711109876633\n",
      "[step: 3669] loss: 44.87226867675781\n",
      "[step: 3669] loss: 0.006366581656038761\n",
      "[step: 3670] loss: 42.71002960205078\n",
      "[step: 3670] loss: 0.006366455927491188\n",
      "[step: 3671] loss: 43.15955352783203\n",
      "[step: 3671] loss: 0.006366318557411432\n",
      "[step: 3672] loss: 44.537818908691406\n",
      "[step: 3672] loss: 0.006366189569234848\n",
      "[step: 3673] loss: 43.52732849121094\n",
      "[step: 3673] loss: 0.006366065703332424\n",
      "[step: 3674] loss: 42.44861602783203\n",
      "[step: 3674] loss: 0.0063659315928816795\n",
      "[step: 3675] loss: 43.270896911621094\n",
      "[step: 3675] loss: 0.006365809123963118\n",
      "[step: 3676] loss: 43.68564224243164\n",
      "[step: 3676] loss: 0.006365677807480097\n",
      "[step: 3677] loss: 42.73911666870117\n",
      "[step: 3677] loss: 0.006365546956658363\n",
      "[step: 3678] loss: 42.501155853271484\n",
      "[step: 3678] loss: 0.006365417502820492\n",
      "[step: 3679] loss: 43.19855499267578\n",
      "[step: 3679] loss: 0.00636528804898262\n",
      "[step: 3680] loss: 43.054351806640625\n",
      "[step: 3680] loss: 0.006365164183080196\n",
      "[step: 3681] loss: 42.394737243652344\n",
      "[step: 3681] loss: 0.00636503379791975\n",
      "[step: 3682] loss: 42.60627746582031\n",
      "[step: 3682] loss: 0.006364909932017326\n",
      "[step: 3683] loss: 42.97359085083008\n",
      "[step: 3683] loss: 0.006364776287227869\n",
      "[step: 3684] loss: 42.58525848388672\n",
      "[step: 3684] loss: 0.006364648230373859\n",
      "[step: 3685] loss: 42.313316345214844\n",
      "[step: 3685] loss: 0.006364517379552126\n",
      "[step: 3686] loss: 42.61544418334961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3686] loss: 0.006364389788359404\n",
      "[step: 3687] loss: 42.67330551147461\n",
      "[step: 3687] loss: 0.006364266853779554\n",
      "[step: 3688] loss: 42.34680938720703\n",
      "[step: 3688] loss: 0.006364138331264257\n",
      "[step: 3689] loss: 42.308799743652344\n",
      "[step: 3689] loss: 0.006364012602716684\n",
      "[step: 3690] loss: 42.52480697631836\n",
      "[step: 3690] loss: 0.006363890133798122\n",
      "[step: 3691] loss: 42.4593505859375\n",
      "[step: 3691] loss: 0.0063637662678956985\n",
      "[step: 3692] loss: 42.24198913574219\n",
      "[step: 3692] loss: 0.006363654043525457\n",
      "[step: 3693] loss: 42.28820037841797\n",
      "[step: 3693] loss: 0.006363534834235907\n",
      "[step: 3694] loss: 42.41081237792969\n",
      "[step: 3694] loss: 0.006363433785736561\n",
      "[step: 3695] loss: 42.30793762207031\n",
      "[step: 3695] loss: 0.006363346707075834\n",
      "[step: 3696] loss: 42.18241882324219\n",
      "[step: 3696] loss: 0.006363286636769772\n",
      "[step: 3697] loss: 42.24159240722656\n",
      "[step: 3697] loss: 0.00636327313259244\n",
      "[step: 3698] loss: 42.299564361572266\n",
      "[step: 3698] loss: 0.00636333180591464\n",
      "[step: 3699] loss: 42.21160125732422\n",
      "[step: 3699] loss: 0.006363508757203817\n",
      "[step: 3700] loss: 42.137840270996094\n",
      "[step: 3700] loss: 0.006363904103636742\n",
      "[step: 3701] loss: 42.183868408203125\n",
      "[step: 3701] loss: 0.006364643108099699\n",
      "[step: 3702] loss: 42.21123504638672\n",
      "[step: 3702] loss: 0.006366018671542406\n",
      "[step: 3703] loss: 42.143333435058594\n",
      "[step: 3703] loss: 0.006368343252688646\n",
      "[step: 3704] loss: 42.096309661865234\n",
      "[step: 3704] loss: 0.0063725123181939125\n",
      "[step: 3705] loss: 42.12528991699219\n",
      "[step: 3705] loss: 0.00637897290289402\n",
      "[step: 3706] loss: 42.13798522949219\n",
      "[step: 3706] loss: 0.006390038412064314\n",
      "[step: 3707] loss: 42.09037780761719\n",
      "[step: 3707] loss: 0.006404084153473377\n",
      "[step: 3708] loss: 42.054107666015625\n",
      "[step: 3708] loss: 0.006423936691135168\n",
      "[step: 3709] loss: 42.06817626953125\n",
      "[step: 3709] loss: 0.00643573934212327\n",
      "[step: 3710] loss: 42.076416015625\n",
      "[step: 3710] loss: 0.006438365671783686\n",
      "[step: 3711] loss: 42.0439453125\n",
      "[step: 3711] loss: 0.006414123810827732\n",
      "[step: 3712] loss: 42.01334762573242\n",
      "[step: 3712] loss: 0.006382123567163944\n",
      "[step: 3713] loss: 42.01569366455078\n",
      "[step: 3713] loss: 0.006362542975693941\n",
      "[step: 3714] loss: 42.021053314208984\n",
      "[step: 3714] loss: 0.006368170492351055\n",
      "[step: 3715] loss: 42.00086212158203\n",
      "[step: 3715] loss: 0.006386523600667715\n",
      "[step: 3716] loss: 41.974403381347656\n",
      "[step: 3716] loss: 0.006393953692167997\n",
      "[step: 3717] loss: 41.967960357666016\n",
      "[step: 3717] loss: 0.006383730098605156\n",
      "[step: 3718] loss: 41.970375061035156\n",
      "[step: 3718] loss: 0.0063663050532341\n",
      "[step: 3719] loss: 41.95853805541992\n",
      "[step: 3719] loss: 0.006361166946589947\n",
      "[step: 3720] loss: 41.9368896484375\n",
      "[step: 3720] loss: 0.006369888782501221\n",
      "[step: 3721] loss: 41.92465591430664\n",
      "[step: 3721] loss: 0.006378666963428259\n",
      "[step: 3722] loss: 41.92268371582031\n",
      "[step: 3722] loss: 0.006377236917614937\n",
      "[step: 3723] loss: 41.91606140136719\n",
      "[step: 3723] loss: 0.0063669742085039616\n",
      "[step: 3724] loss: 41.89988708496094\n",
      "[step: 3724] loss: 0.006360198836773634\n",
      "[step: 3725] loss: 41.885066986083984\n",
      "[step: 3725] loss: 0.00636262958869338\n",
      "[step: 3726] loss: 41.878299713134766\n",
      "[step: 3726] loss: 0.0063688503578305244\n",
      "[step: 3727] loss: 41.87311553955078\n",
      "[step: 3727] loss: 0.006370737683027983\n",
      "[step: 3728] loss: 41.86214828491211\n",
      "[step: 3728] loss: 0.006365919951349497\n",
      "[step: 3729] loss: 41.84788513183594\n",
      "[step: 3729] loss: 0.006360407918691635\n",
      "[step: 3730] loss: 41.837196350097656\n",
      "[step: 3730] loss: 0.006359566003084183\n",
      "[step: 3731] loss: 41.83063888549805\n",
      "[step: 3731] loss: 0.006362851243466139\n",
      "[step: 3732] loss: 41.82283020019531\n",
      "[step: 3732] loss: 0.006365557201206684\n",
      "[step: 3733] loss: 41.81130599975586\n",
      "[step: 3733] loss: 0.006364286411553621\n",
      "[step: 3734] loss: 41.79926300048828\n",
      "[step: 3734] loss: 0.006360798608511686\n",
      "[step: 3735] loss: 41.79005432128906\n",
      "[step: 3735] loss: 0.006358566228300333\n",
      "[step: 3736] loss: 41.78253173828125\n",
      "[step: 3736] loss: 0.006359287537634373\n",
      "[step: 3737] loss: 41.773624420166016\n",
      "[step: 3737] loss: 0.006361324340105057\n",
      "[step: 3738] loss: 41.76275634765625\n",
      "[step: 3738] loss: 0.006362018641084433\n",
      "[step: 3739] loss: 41.751983642578125\n",
      "[step: 3739] loss: 0.006360684521496296\n",
      "[step: 3740] loss: 41.742942810058594\n",
      "[step: 3740] loss: 0.006358638405799866\n",
      "[step: 3741] loss: 41.73478698730469\n",
      "[step: 3741] loss: 0.006357767153531313\n",
      "[step: 3742] loss: 41.725624084472656\n",
      "[step: 3742] loss: 0.006358419079333544\n",
      "[step: 3743] loss: 41.71539306640625\n",
      "[step: 3743] loss: 0.006359416991472244\n",
      "[step: 3744] loss: 41.70524597167969\n",
      "[step: 3744] loss: 0.006359534803777933\n",
      "[step: 3745] loss: 41.69607162475586\n",
      "[step: 3745] loss: 0.006358571350574493\n",
      "[step: 3746] loss: 41.68743896484375\n",
      "[step: 3746] loss: 0.006357455626130104\n",
      "[step: 3747] loss: 41.67847442626953\n",
      "[step: 3747] loss: 0.006357033271342516\n",
      "[step: 3748] loss: 41.668739318847656\n",
      "[step: 3748] loss: 0.00635739229619503\n",
      "[step: 3749] loss: 41.65892028808594\n",
      "[step: 3749] loss: 0.006357868202030659\n",
      "[step: 3750] loss: 41.64958953857422\n",
      "[step: 3750] loss: 0.006357812788337469\n",
      "[step: 3751] loss: 41.64076232910156\n",
      "[step: 3751] loss: 0.006357240490615368\n",
      "[step: 3752] loss: 41.63182067871094\n",
      "[step: 3752] loss: 0.006356584373861551\n",
      "[step: 3753] loss: 41.622493743896484\n",
      "[step: 3753] loss: 0.0063562854193151\n",
      "[step: 3754] loss: 41.61293411254883\n",
      "[step: 3754] loss: 0.0063564064912498\n",
      "[step: 3755] loss: 41.603538513183594\n",
      "[step: 3755] loss: 0.006356606725603342\n",
      "[step: 3756] loss: 41.594451904296875\n",
      "[step: 3756] loss: 0.006356579717248678\n",
      "[step: 3757] loss: 41.585540771484375\n",
      "[step: 3757] loss: 0.006356256082653999\n",
      "[step: 3758] loss: 41.5764274597168\n",
      "[step: 3758] loss: 0.006355833727866411\n",
      "[step: 3759] loss: 41.567115783691406\n",
      "[step: 3759] loss: 0.006355553399771452\n",
      "[step: 3760] loss: 41.55778503417969\n",
      "[step: 3760] loss: 0.006355497986078262\n",
      "[step: 3761] loss: 41.54862594604492\n",
      "[step: 3761] loss: 0.006355545949190855\n",
      "[step: 3762] loss: 41.53956985473633\n",
      "[step: 3762] loss: 0.006355538964271545\n",
      "[step: 3763] loss: 41.5305290222168\n",
      "[step: 3763] loss: 0.006355377845466137\n",
      "[step: 3764] loss: 41.521453857421875\n",
      "[step: 3764] loss: 0.006355104967951775\n",
      "[step: 3765] loss: 41.512298583984375\n",
      "[step: 3765] loss: 0.0063548521138727665\n",
      "[step: 3766] loss: 41.50312042236328\n",
      "[step: 3766] loss: 0.006354697979986668\n",
      "[step: 3767] loss: 41.49394989013672\n",
      "[step: 3767] loss: 0.006354642566293478\n",
      "[step: 3768] loss: 41.48497009277344\n",
      "[step: 3768] loss: 0.006354599725455046\n",
      "[step: 3769] loss: 41.47589874267578\n",
      "[step: 3769] loss: 0.006354518234729767\n",
      "[step: 3770] loss: 41.466896057128906\n",
      "[step: 3770] loss: 0.006354362238198519\n",
      "[step: 3771] loss: 41.457801818847656\n",
      "[step: 3771] loss: 0.0063541647978127\n",
      "[step: 3772] loss: 41.44870376586914\n",
      "[step: 3772] loss: 0.0063539789989590645\n",
      "[step: 3773] loss: 41.43958282470703\n",
      "[step: 3773] loss: 0.00635384488850832\n",
      "[step: 3774] loss: 41.43058776855469\n",
      "[step: 3774] loss: 0.006353758741170168\n",
      "[step: 3775] loss: 41.42157745361328\n",
      "[step: 3775] loss: 0.006353677250444889\n",
      "[step: 3776] loss: 41.41255569458008\n",
      "[step: 3776] loss: 0.006353584583848715\n",
      "[step: 3777] loss: 41.40353012084961\n",
      "[step: 3777] loss: 0.006353446748107672\n",
      "[step: 3778] loss: 41.39453125\n",
      "[step: 3778] loss: 0.0063532935455441475\n",
      "[step: 3779] loss: 41.38550567626953\n",
      "[step: 3779] loss: 0.006353137083351612\n",
      "[step: 3780] loss: 41.37643814086914\n",
      "[step: 3780] loss: 0.006353003438562155\n",
      "[step: 3781] loss: 41.367454528808594\n",
      "[step: 3781] loss: 0.0063528805039823055\n",
      "[step: 3782] loss: 41.35847854614258\n",
      "[step: 3782] loss: 0.0063527775928378105\n",
      "[step: 3783] loss: 41.349510192871094\n",
      "[step: 3783] loss: 0.006352678872644901\n",
      "[step: 3784] loss: 41.340538024902344\n",
      "[step: 3784] loss: 0.0063525615260005\n",
      "[step: 3785] loss: 41.331573486328125\n",
      "[step: 3785] loss: 0.006352433934807777\n",
      "[step: 3786] loss: 41.32259750366211\n",
      "[step: 3786] loss: 0.006352298893034458\n",
      "[step: 3787] loss: 41.3135986328125\n",
      "[step: 3787] loss: 0.006352168042212725\n",
      "[step: 3788] loss: 41.3046875\n",
      "[step: 3788] loss: 0.006352030206471682\n",
      "[step: 3789] loss: 41.29570770263672\n",
      "[step: 3789] loss: 0.006351915653795004\n",
      "[step: 3790] loss: 41.28673553466797\n",
      "[step: 3790] loss: 0.006351800169795752\n",
      "[step: 3791] loss: 41.277809143066406\n",
      "[step: 3791] loss: 0.006351681426167488\n",
      "[step: 3792] loss: 41.26885223388672\n",
      "[step: 3792] loss: 0.006351572927087545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3793] loss: 41.25989532470703\n",
      "[step: 3793] loss: 0.006351449992507696\n",
      "[step: 3794] loss: 41.25102615356445\n",
      "[step: 3794] loss: 0.00635133171454072\n",
      "[step: 3795] loss: 41.2420654296875\n",
      "[step: 3795] loss: 0.0063512008637189865\n",
      "[step: 3796] loss: 41.233150482177734\n",
      "[step: 3796] loss: 0.0063510797917842865\n",
      "[step: 3797] loss: 41.22422409057617\n",
      "[step: 3797] loss: 0.006350950337946415\n",
      "[step: 3798] loss: 41.215293884277344\n",
      "[step: 3798] loss: 0.006350819021463394\n",
      "[step: 3799] loss: 41.20645523071289\n",
      "[step: 3799] loss: 0.006350707728415728\n",
      "[step: 3800] loss: 41.19754409790039\n",
      "[step: 3800] loss: 0.006350589916110039\n",
      "[step: 3801] loss: 41.188594818115234\n",
      "[step: 3801] loss: 0.006350474897772074\n",
      "[step: 3802] loss: 41.17972183227539\n",
      "[step: 3802] loss: 0.006350349169224501\n",
      "[step: 3803] loss: 41.17086410522461\n",
      "[step: 3803] loss: 0.0063502308912575245\n",
      "[step: 3804] loss: 41.161949157714844\n",
      "[step: 3804] loss: 0.006350105628371239\n",
      "[step: 3805] loss: 41.153079986572266\n",
      "[step: 3805] loss: 0.006349984090775251\n",
      "[step: 3806] loss: 41.14417266845703\n",
      "[step: 3806] loss: 0.006349865347146988\n",
      "[step: 3807] loss: 41.13534164428711\n",
      "[step: 3807] loss: 0.006349742878228426\n",
      "[step: 3808] loss: 41.126468658447266\n",
      "[step: 3808] loss: 0.006349623203277588\n",
      "[step: 3809] loss: 41.11760711669922\n",
      "[step: 3809] loss: 0.006349498871713877\n",
      "[step: 3810] loss: 41.10871887207031\n",
      "[step: 3810] loss: 0.0063493805937469006\n",
      "[step: 3811] loss: 41.099876403808594\n",
      "[step: 3811] loss: 0.006349257659167051\n",
      "[step: 3812] loss: 41.09103012084961\n",
      "[step: 3812] loss: 0.006349143106490374\n",
      "[step: 3813] loss: 41.082176208496094\n",
      "[step: 3813] loss: 0.006349022500216961\n",
      "[step: 3814] loss: 41.073387145996094\n",
      "[step: 3814] loss: 0.006348906084895134\n",
      "[step: 3815] loss: 41.06451416015625\n",
      "[step: 3815] loss: 0.006348779890686274\n",
      "[step: 3816] loss: 41.055702209472656\n",
      "[step: 3816] loss: 0.006348660681396723\n",
      "[step: 3817] loss: 41.04682922363281\n",
      "[step: 3817] loss: 0.006348539609462023\n",
      "[step: 3818] loss: 41.03803253173828\n",
      "[step: 3818] loss: 0.006348418537527323\n",
      "[step: 3819] loss: 41.02922058105469\n",
      "[step: 3819] loss: 0.006348293274641037\n",
      "[step: 3820] loss: 41.02033996582031\n",
      "[step: 3820] loss: 0.006348178256303072\n",
      "[step: 3821] loss: 41.01155090332031\n",
      "[step: 3821] loss: 0.006348058581352234\n",
      "[step: 3822] loss: 41.00273895263672\n",
      "[step: 3822] loss: 0.006347935646772385\n",
      "[step: 3823] loss: 40.993927001953125\n",
      "[step: 3823] loss: 0.006347818300127983\n",
      "[step: 3824] loss: 40.98512268066406\n",
      "[step: 3824] loss: 0.006347695365548134\n",
      "[step: 3825] loss: 40.976341247558594\n",
      "[step: 3825] loss: 0.006347574759274721\n",
      "[step: 3826] loss: 40.967559814453125\n",
      "[step: 3826] loss: 0.006347452290356159\n",
      "[step: 3827] loss: 40.95872497558594\n",
      "[step: 3827] loss: 0.006347332149744034\n",
      "[step: 3828] loss: 40.94994354248047\n",
      "[step: 3828] loss: 0.006347215734422207\n",
      "[step: 3829] loss: 40.9411506652832\n",
      "[step: 3829] loss: 0.006347090005874634\n",
      "[step: 3830] loss: 40.93238067626953\n",
      "[step: 3830] loss: 0.006346968002617359\n",
      "[step: 3831] loss: 40.923614501953125\n",
      "[step: 3831] loss: 0.0063468534499406815\n",
      "[step: 3832] loss: 40.914817810058594\n",
      "[step: 3832] loss: 0.00634672911837697\n",
      "[step: 3833] loss: 40.90604782104492\n",
      "[step: 3833] loss: 0.006346606183797121\n",
      "[step: 3834] loss: 40.89725875854492\n",
      "[step: 3834] loss: 0.006346486043184996\n",
      "[step: 3835] loss: 40.88851547241211\n",
      "[step: 3835] loss: 0.006346367299556732\n",
      "[step: 3836] loss: 40.879756927490234\n",
      "[step: 3836] loss: 0.006346249021589756\n",
      "[step: 3837] loss: 40.87107467651367\n",
      "[step: 3837] loss: 0.006346126552671194\n",
      "[step: 3838] loss: 40.86229705810547\n",
      "[step: 3838] loss: 0.006346005480736494\n",
      "[step: 3839] loss: 40.85359191894531\n",
      "[step: 3839] loss: 0.006345889996737242\n",
      "[step: 3840] loss: 40.844947814941406\n",
      "[step: 3840] loss: 0.006345762871205807\n",
      "[step: 3841] loss: 40.836368560791016\n",
      "[step: 3841] loss: 0.006345649249851704\n",
      "[step: 3842] loss: 40.82789611816406\n",
      "[step: 3842] loss: 0.006345527246594429\n",
      "[step: 3843] loss: 40.81964111328125\n",
      "[step: 3843] loss: 0.006345413159579039\n",
      "[step: 3844] loss: 40.81172561645508\n",
      "[step: 3844] loss: 0.006345294415950775\n",
      "[step: 3845] loss: 40.804439544677734\n",
      "[step: 3845] loss: 0.006345175672322512\n",
      "[step: 3846] loss: 40.79827117919922\n",
      "[step: 3846] loss: 0.006345069967210293\n",
      "[step: 3847] loss: 40.79423522949219\n",
      "[step: 3847] loss: 0.006344946101307869\n",
      "[step: 3848] loss: 40.793975830078125\n",
      "[step: 3848] loss: 0.006344841793179512\n",
      "[step: 3849] loss: 40.80098342895508\n",
      "[step: 3849] loss: 0.006344732362776995\n",
      "[step: 3850] loss: 40.821372985839844\n",
      "[step: 3850] loss: 0.006344636436551809\n",
      "[step: 3851] loss: 40.868247985839844\n",
      "[step: 3851] loss: 0.006344554014503956\n",
      "[step: 3852] loss: 40.9641227722168\n",
      "[step: 3852] loss: 0.006344492081552744\n",
      "[step: 3853] loss: 41.15989685058594\n",
      "[step: 3853] loss: 0.0063444506376981735\n",
      "[step: 3854] loss: 41.5372314453125\n",
      "[step: 3854] loss: 0.00634445995092392\n",
      "[step: 3855] loss: 42.29720687866211\n",
      "[step: 3855] loss: 0.0063445535488426685\n",
      "[step: 3856] loss: 43.69215393066406\n",
      "[step: 3856] loss: 0.006344769615679979\n",
      "[step: 3857] loss: 46.43402862548828\n",
      "[step: 3857] loss: 0.006345172878354788\n",
      "[step: 3858] loss: 50.70507049560547\n",
      "[step: 3858] loss: 0.006345926783978939\n",
      "[step: 3859] loss: 57.749271392822266\n",
      "[step: 3859] loss: 0.006347173359245062\n",
      "[step: 3860] loss: 62.735172271728516\n",
      "[step: 3860] loss: 0.006349361501634121\n",
      "[step: 3861] loss: 63.91466522216797\n",
      "[step: 3861] loss: 0.006352704484015703\n",
      "[step: 3862] loss: 52.52958679199219\n",
      "[step: 3862] loss: 0.006358432583510876\n",
      "[step: 3863] loss: 42.14030456542969\n",
      "[step: 3863] loss: 0.006366183515638113\n",
      "[step: 3864] loss: 43.42741012573242\n",
      "[step: 3864] loss: 0.006378424819558859\n",
      "[step: 3865] loss: 50.598289489746094\n",
      "[step: 3865] loss: 0.006390394642949104\n",
      "[step: 3866] loss: 50.47533416748047\n",
      "[step: 3866] loss: 0.00640401616692543\n",
      "[step: 3867] loss: 42.78744888305664\n",
      "[step: 3867] loss: 0.006404165178537369\n",
      "[step: 3868] loss: 42.05821228027344\n",
      "[step: 3868] loss: 0.006393736694008112\n",
      "[step: 3869] loss: 47.076171875\n",
      "[step: 3869] loss: 0.006369167007505894\n",
      "[step: 3870] loss: 45.72405242919922\n",
      "[step: 3870] loss: 0.006348380818963051\n",
      "[step: 3871] loss: 41.289669036865234\n",
      "[step: 3871] loss: 0.006343035958707333\n",
      "[step: 3872] loss: 42.234622955322266\n",
      "[step: 3872] loss: 0.00635274313390255\n",
      "[step: 3873] loss: 45.04499816894531\n",
      "[step: 3873] loss: 0.0063653020188212395\n",
      "[step: 3874] loss: 43.47006607055664\n",
      "[step: 3874] loss: 0.006367092952132225\n",
      "[step: 3875] loss: 40.805992126464844\n",
      "[step: 3875] loss: 0.006358064711093903\n",
      "[step: 3876] loss: 42.24565887451172\n",
      "[step: 3876] loss: 0.00634592492133379\n",
      "[step: 3877] loss: 43.7926139831543\n",
      "[step: 3877] loss: 0.0063418978825211525\n",
      "[step: 3878] loss: 41.847816467285156\n",
      "[step: 3878] loss: 0.006347018759697676\n",
      "[step: 3879] loss: 40.78031921386719\n",
      "[step: 3879] loss: 0.006353795528411865\n",
      "[step: 3880] loss: 42.234580993652344\n",
      "[step: 3880] loss: 0.00635530985891819\n",
      "[step: 3881] loss: 42.453121185302734\n",
      "[step: 3881] loss: 0.006349698640406132\n",
      "[step: 3882] loss: 41.02760696411133\n",
      "[step: 3882] loss: 0.0063431523740291595\n",
      "[step: 3883] loss: 40.89459991455078\n",
      "[step: 3883] loss: 0.006341034080833197\n",
      "[step: 3884] loss: 41.870872497558594\n",
      "[step: 3884] loss: 0.00634385272860527\n",
      "[step: 3885] loss: 41.56342315673828\n",
      "[step: 3885] loss: 0.006347633432596922\n",
      "[step: 3886] loss: 40.669639587402344\n",
      "[step: 3886] loss: 0.0063481563702225685\n",
      "[step: 3887] loss: 41.05021667480469\n",
      "[step: 3887] loss: 0.00634530046954751\n",
      "[step: 3888] loss: 41.53474426269531\n",
      "[step: 3888] loss: 0.006341621279716492\n",
      "[step: 3889] loss: 40.937522888183594\n",
      "[step: 3889] loss: 0.006340225227177143\n",
      "[step: 3890] loss: 40.60771942138672\n",
      "[step: 3890] loss: 0.006341485772281885\n",
      "[step: 3891] loss: 41.065494537353516\n",
      "[step: 3891] loss: 0.006343498360365629\n",
      "[step: 3892] loss: 41.0924186706543\n",
      "[step: 3892] loss: 0.006344236899167299\n",
      "[step: 3893] loss: 40.630367279052734\n",
      "[step: 3893] loss: 0.006342912558466196\n",
      "[step: 3894] loss: 40.64219665527344\n",
      "[step: 3894] loss: 0.0063408599235117435\n",
      "[step: 3895] loss: 40.93924331665039\n",
      "[step: 3895] loss: 0.00633957190439105\n",
      "[step: 3896] loss: 40.79518508911133\n",
      "[step: 3896] loss: 0.006339690648019314\n",
      "[step: 3897] loss: 40.51863098144531\n",
      "[step: 3897] loss: 0.0063406615518033504\n",
      "[step: 3898] loss: 40.641170501708984\n",
      "[step: 3898] loss: 0.006341360043734312\n",
      "[step: 3899] loss: 40.789031982421875\n",
      "[step: 3899] loss: 0.00634115282446146\n",
      "[step: 3900] loss: 40.604591369628906\n",
      "[step: 3900] loss: 0.006340144667774439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3901] loss: 40.472801208496094\n",
      "[step: 3901] loss: 0.006339132785797119\n",
      "[step: 3902] loss: 40.598472595214844\n",
      "[step: 3902] loss: 0.006338674575090408\n",
      "[step: 3903] loss: 40.63993835449219\n",
      "[step: 3903] loss: 0.006338873878121376\n",
      "[step: 3904] loss: 40.49414825439453\n",
      "[step: 3904] loss: 0.006339315790683031\n",
      "[step: 3905] loss: 40.43962478637695\n",
      "[step: 3905] loss: 0.006339491810649633\n",
      "[step: 3906] loss: 40.52854537963867\n",
      "[step: 3906] loss: 0.006339231040328741\n",
      "[step: 3907] loss: 40.532981872558594\n",
      "[step: 3907] loss: 0.006338647101074457\n",
      "[step: 3908] loss: 40.42796325683594\n",
      "[step: 3908] loss: 0.0063381134532392025\n",
      "[step: 3909] loss: 40.40226745605469\n",
      "[step: 3909] loss: 0.006337872240692377\n",
      "[step: 3910] loss: 40.461124420166016\n",
      "[step: 3910] loss: 0.006337922066450119\n",
      "[step: 3911] loss: 40.451866149902344\n",
      "[step: 3911] loss: 0.0063380650244653225\n",
      "[step: 3912] loss: 40.37855529785156\n",
      "[step: 3912] loss: 0.006338111590594053\n",
      "[step: 3913] loss: 40.3623161315918\n",
      "[step: 3913] loss: 0.0063379425555467606\n",
      "[step: 3914] loss: 40.3980712890625\n",
      "[step: 3914] loss: 0.006337625440210104\n",
      "[step: 3915] loss: 40.38914108276367\n",
      "[step: 3915] loss: 0.00633729062974453\n",
      "[step: 3916] loss: 40.33795928955078\n",
      "[step: 3916] loss: 0.006337068509310484\n",
      "[step: 3917] loss: 40.32157897949219\n",
      "[step: 3917] loss: 0.006336974911391735\n",
      "[step: 3918] loss: 40.34156799316406\n",
      "[step: 3918] loss: 0.006336980499327183\n",
      "[step: 3919] loss: 40.336814880371094\n",
      "[step: 3919] loss: 0.006336968857795\n",
      "[step: 3920] loss: 40.300880432128906\n",
      "[step: 3920] loss: 0.006336890626698732\n",
      "[step: 3921] loss: 40.28211975097656\n",
      "[step: 3921] loss: 0.006336729042232037\n",
      "[step: 3922] loss: 40.29126739501953\n",
      "[step: 3922] loss: 0.006336512044072151\n",
      "[step: 3923] loss: 40.29009246826172\n",
      "[step: 3923] loss: 0.006336305756121874\n",
      "[step: 3924] loss: 40.265480041503906\n",
      "[step: 3924] loss: 0.006336144171655178\n",
      "[step: 3925] loss: 40.246009826660156\n",
      "[step: 3925] loss: 0.006336042191833258\n",
      "[step: 3926] loss: 40.2462158203125\n",
      "[step: 3926] loss: 0.006335971876978874\n",
      "[step: 3927] loss: 40.246376037597656\n",
      "[step: 3927] loss: 0.006335912272334099\n",
      "[step: 3928] loss: 40.2308349609375\n",
      "[step: 3928] loss: 0.006335819140076637\n",
      "[step: 3929] loss: 40.21207046508789\n",
      "[step: 3929] loss: 0.006335698999464512\n",
      "[step: 3930] loss: 40.20559310913086\n",
      "[step: 3930] loss: 0.006335550453513861\n",
      "[step: 3931] loss: 40.20469665527344\n",
      "[step: 3931] loss: 0.006335392128676176\n",
      "[step: 3932] loss: 40.19546890258789\n",
      "[step: 3932] loss: 0.006335239391773939\n",
      "[step: 3933] loss: 40.179664611816406\n",
      "[step: 3933] loss: 0.006335110403597355\n",
      "[step: 3934] loss: 40.16893005371094\n",
      "[step: 3934] loss: 0.006335000973194838\n",
      "[step: 3935] loss: 40.16487121582031\n",
      "[step: 3935] loss: 0.006334907375276089\n",
      "[step: 3936] loss: 40.159122467041016\n",
      "[step: 3936] loss: 0.006334816105663776\n",
      "[step: 3937] loss: 40.14747619628906\n",
      "[step: 3937] loss: 0.006334709003567696\n",
      "[step: 3938] loss: 40.13519287109375\n",
      "[step: 3938] loss: 0.0063345967791974545\n",
      "[step: 3939] loss: 40.1275520324707\n",
      "[step: 3939] loss: 0.0063344817608594894\n",
      "[step: 3940] loss: 40.12225341796875\n",
      "[step: 3940] loss: 0.0063343471847474575\n",
      "[step: 3941] loss: 40.11408996582031\n",
      "[step: 3941] loss: 0.006334217730909586\n",
      "[step: 3942] loss: 40.10308074951172\n",
      "[step: 3942] loss: 0.0063340854831039906\n",
      "[step: 3943] loss: 40.093177795410156\n",
      "[step: 3943] loss: 0.006333969999104738\n",
      "[step: 3944] loss: 40.086063385009766\n",
      "[step: 3944] loss: 0.006333853583782911\n",
      "[step: 3945] loss: 40.07941818237305\n",
      "[step: 3945] loss: 0.006333743222057819\n",
      "[step: 3946] loss: 40.070777893066406\n",
      "[step: 3946] loss: 0.006333642173558474\n",
      "[step: 3947] loss: 40.06083679199219\n",
      "[step: 3947] loss: 0.006333534140139818\n",
      "[step: 3948] loss: 40.0518798828125\n",
      "[step: 3948] loss: 0.006333422847092152\n",
      "[step: 3949] loss: 40.04454803466797\n",
      "[step: 3949] loss: 0.006333307828754187\n",
      "[step: 3950] loss: 40.0372314453125\n",
      "[step: 3950] loss: 0.006333193276077509\n",
      "[step: 3951] loss: 40.02870178222656\n",
      "[step: 3951] loss: 0.006333075929433107\n",
      "[step: 3952] loss: 40.01948547363281\n",
      "[step: 3952] loss: 0.006332959048449993\n",
      "[step: 3953] loss: 40.01091384887695\n",
      "[step: 3953] loss: 0.006332834716886282\n",
      "[step: 3954] loss: 40.00324249267578\n",
      "[step: 3954] loss: 0.006332718301564455\n",
      "[step: 3955] loss: 39.99565887451172\n",
      "[step: 3955] loss: 0.006332601420581341\n",
      "[step: 3956] loss: 39.987396240234375\n",
      "[step: 3956] loss: 0.006332486867904663\n",
      "[step: 3957] loss: 39.97871017456055\n",
      "[step: 3957] loss: 0.0063323755748569965\n",
      "[step: 3958] loss: 39.97023010253906\n",
      "[step: 3958] loss: 0.006332256831228733\n",
      "[step: 3959] loss: 39.96232604980469\n",
      "[step: 3959] loss: 0.006332145072519779\n",
      "[step: 3960] loss: 39.95462417602539\n",
      "[step: 3960] loss: 0.0063320342451334\n",
      "[step: 3961] loss: 39.946510314941406\n",
      "[step: 3961] loss: 0.006331917364150286\n",
      "[step: 3962] loss: 39.938194274902344\n",
      "[step: 3962] loss: 0.006331804674118757\n",
      "[step: 3963] loss: 39.92981719970703\n",
      "[step: 3963] loss: 0.006331699434667826\n",
      "[step: 3964] loss: 39.92173767089844\n",
      "[step: 3964] loss: 0.006331577897071838\n",
      "[step: 3965] loss: 39.913902282714844\n",
      "[step: 3965] loss: 0.006331465672701597\n",
      "[step: 3966] loss: 39.90597915649414\n",
      "[step: 3966] loss: 0.006331354379653931\n",
      "[step: 3967] loss: 39.897857666015625\n",
      "[step: 3967] loss: 0.006331240758299828\n",
      "[step: 3968] loss: 39.88959503173828\n",
      "[step: 3968] loss: 0.006331129930913448\n",
      "[step: 3969] loss: 39.88153076171875\n",
      "[step: 3969] loss: 0.0063310107216238976\n",
      "[step: 3970] loss: 39.87355041503906\n",
      "[step: 3970] loss: 0.006330900359898806\n",
      "[step: 3971] loss: 39.86561584472656\n",
      "[step: 3971] loss: 0.006330783478915691\n",
      "[step: 3972] loss: 39.857662200927734\n",
      "[step: 3972] loss: 0.0063306777738034725\n",
      "[step: 3973] loss: 39.849613189697266\n",
      "[step: 3973] loss: 0.006330559030175209\n",
      "[step: 3974] loss: 39.84149932861328\n",
      "[step: 3974] loss: 0.006330449134111404\n",
      "[step: 3975] loss: 39.83344650268555\n",
      "[step: 3975] loss: 0.006330336909741163\n",
      "[step: 3976] loss: 39.825496673583984\n",
      "[step: 3976] loss: 0.006330227013677359\n",
      "[step: 3977] loss: 39.81754684448242\n",
      "[step: 3977] loss: 0.006330122705549002\n",
      "[step: 3978] loss: 39.80962371826172\n",
      "[step: 3978] loss: 0.006330014206469059\n",
      "[step: 3979] loss: 39.8016357421875\n",
      "[step: 3979] loss: 0.006329914554953575\n",
      "[step: 3980] loss: 39.793617248535156\n",
      "[step: 3980] loss: 0.006329812109470367\n",
      "[step: 3981] loss: 39.78563690185547\n",
      "[step: 3981] loss: 0.006329714320600033\n",
      "[step: 3982] loss: 39.77764892578125\n",
      "[step: 3982] loss: 0.00632963003590703\n",
      "[step: 3983] loss: 39.769775390625\n",
      "[step: 3983] loss: 0.006329553667455912\n",
      "[step: 3984] loss: 39.76182556152344\n",
      "[step: 3984] loss: 0.0063295019790530205\n",
      "[step: 3985] loss: 39.75389099121094\n",
      "[step: 3985] loss: 0.006329468917101622\n",
      "[step: 3986] loss: 39.74592590332031\n",
      "[step: 3986] loss: 0.006329482886940241\n",
      "[step: 3987] loss: 39.73792266845703\n",
      "[step: 3987] loss: 0.006329561583697796\n",
      "[step: 3988] loss: 39.730018615722656\n",
      "[step: 3988] loss: 0.006329739931970835\n",
      "[step: 3989] loss: 39.72209167480469\n",
      "[step: 3989] loss: 0.006330061703920364\n",
      "[step: 3990] loss: 39.71416091918945\n",
      "[step: 3990] loss: 0.006330643780529499\n",
      "[step: 3991] loss: 39.706336975097656\n",
      "[step: 3991] loss: 0.006331556476652622\n",
      "[step: 3992] loss: 39.69842529296875\n",
      "[step: 3992] loss: 0.0063331290148198605\n",
      "[step: 3993] loss: 39.690486907958984\n",
      "[step: 3993] loss: 0.006335405632853508\n",
      "[step: 3994] loss: 39.682552337646484\n",
      "[step: 3994] loss: 0.006339217070490122\n",
      "[step: 3995] loss: 39.674644470214844\n",
      "[step: 3995] loss: 0.006344246212393045\n",
      "[step: 3996] loss: 39.66680145263672\n",
      "[step: 3996] loss: 0.006352235563099384\n",
      "[step: 3997] loss: 39.658905029296875\n",
      "[step: 3997] loss: 0.006360570900142193\n",
      "[step: 3998] loss: 39.65102767944336\n",
      "[step: 3998] loss: 0.006371539086103439\n",
      "[step: 3999] loss: 39.64313507080078\n",
      "[step: 3999] loss: 0.006376268342137337\n",
      "RMSE: 0.03150423616170883\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 25\n",
    "output_dim = 25\n",
    "learning_rate = 0.001\n",
    "iterations = 4000\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set \n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE \n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    test_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
