{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6d038b9841fd>:89: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 23487.666015625\n",
      "[step: 0] loss: 0.5357515811920166\n",
      "[step: 1] loss: 10543.6708984375\n",
      "[step: 1] loss: 0.18657314777374268\n",
      "[step: 2] loss: 5960.580078125\n",
      "[step: 2] loss: 0.08881929516792297\n",
      "[step: 3] loss: 4311.20556640625\n",
      "[step: 3] loss: 0.06655864417552948\n",
      "[step: 4] loss: 3199.855224609375\n",
      "[step: 4] loss: 0.05936209112405777\n",
      "[step: 5] loss: 2464.709228515625\n",
      "[step: 5] loss: 0.04926927015185356\n",
      "[step: 6] loss: 2108.736572265625\n",
      "[step: 6] loss: 0.04052507504820824\n",
      "[step: 7] loss: 1985.2215576171875\n",
      "[step: 7] loss: 0.03440467268228531\n",
      "[step: 8] loss: 1888.045166015625\n",
      "[step: 8] loss: 0.03055683523416519\n",
      "[step: 9] loss: 1785.1380615234375\n",
      "[step: 9] loss: 0.02783109061419964\n",
      "[step: 10] loss: 1686.0806884765625\n",
      "[step: 10] loss: 0.025544999167323112\n",
      "[step: 11] loss: 1561.22802734375\n",
      "[step: 11] loss: 0.023795193061232567\n",
      "[step: 12] loss: 1428.800537109375\n",
      "[step: 12] loss: 0.022581366822123528\n",
      "[step: 13] loss: 1340.221435546875\n",
      "[step: 13] loss: 0.02162306196987629\n",
      "[step: 14] loss: 1225.8018798828125\n",
      "[step: 14] loss: 0.020891016349196434\n",
      "[step: 15] loss: 1116.486328125\n",
      "[step: 15] loss: 0.020600298419594765\n",
      "[step: 16] loss: 1026.2421875\n",
      "[step: 16] loss: 0.02068200521171093\n",
      "[step: 17] loss: 952.0330810546875\n",
      "[step: 17] loss: 0.02061629481613636\n",
      "[step: 18] loss: 895.66455078125\n",
      "[step: 18] loss: 0.02013274095952511\n",
      "[step: 19] loss: 871.7080688476562\n",
      "[step: 19] loss: 0.01962321810424328\n",
      "[step: 20] loss: 838.9493408203125\n",
      "[step: 20] loss: 0.018723230808973312\n",
      "[step: 21] loss: 801.16943359375\n",
      "[step: 21] loss: 0.017932074144482613\n",
      "[step: 22] loss: 772.7586059570312\n",
      "[step: 22] loss: 0.016997912898659706\n",
      "[step: 23] loss: 739.3090209960938\n",
      "[step: 23] loss: 0.016429992392659187\n",
      "[step: 24] loss: 717.3489990234375\n",
      "[step: 24] loss: 0.015881860628724098\n",
      "[step: 25] loss: 681.0491943359375\n",
      "[step: 25] loss: 0.015599846839904785\n",
      "[step: 26] loss: 654.8339233398438\n",
      "[step: 26] loss: 0.01515257079154253\n",
      "[step: 27] loss: 619.7078857421875\n",
      "[step: 27] loss: 0.015010351315140724\n",
      "[step: 28] loss: 596.318359375\n",
      "[step: 28] loss: 0.0148856146261096\n",
      "[step: 29] loss: 558.9888916015625\n",
      "[step: 29] loss: 0.014436781406402588\n",
      "[step: 30] loss: 530.3295288085938\n",
      "[step: 30] loss: 0.014674164354801178\n",
      "[step: 31] loss: 490.97869873046875\n",
      "[step: 31] loss: 0.014427423477172852\n",
      "[step: 32] loss: 474.1390380859375\n",
      "[step: 32] loss: 0.014118451625108719\n",
      "[step: 33] loss: 446.6539306640625\n",
      "[step: 33] loss: 0.014007148332893848\n",
      "[step: 34] loss: 430.3324279785156\n",
      "[step: 34] loss: 0.013543348759412766\n",
      "[step: 35] loss: 412.04327392578125\n",
      "[step: 35] loss: 0.013443817384541035\n",
      "[step: 36] loss: 395.1906433105469\n",
      "[step: 36] loss: 0.01303952932357788\n",
      "[step: 37] loss: 383.05615234375\n",
      "[step: 37] loss: 0.012887930497527122\n",
      "[step: 38] loss: 373.3397521972656\n",
      "[step: 38] loss: 0.012566267512738705\n",
      "[step: 39] loss: 360.37109375\n",
      "[step: 39] loss: 0.012386748567223549\n",
      "[step: 40] loss: 352.1933898925781\n",
      "[step: 40] loss: 0.012243479490280151\n",
      "[step: 41] loss: 341.99822998046875\n",
      "[step: 41] loss: 0.01207722071558237\n",
      "[step: 42] loss: 333.67205810546875\n",
      "[step: 42] loss: 0.012008527293801308\n",
      "[step: 43] loss: 325.45916748046875\n",
      "[step: 43] loss: 0.011810329742729664\n",
      "[step: 44] loss: 319.8727111816406\n",
      "[step: 44] loss: 0.01175062544643879\n",
      "[step: 45] loss: 310.225341796875\n",
      "[step: 45] loss: 0.011546945199370384\n",
      "[step: 46] loss: 304.1680603027344\n",
      "[step: 46] loss: 0.011491076089441776\n",
      "[step: 47] loss: 297.22906494140625\n",
      "[step: 47] loss: 0.011298640631139278\n",
      "[step: 48] loss: 291.15509033203125\n",
      "[step: 48] loss: 0.011228752322494984\n",
      "[step: 49] loss: 285.2230224609375\n",
      "[step: 49] loss: 0.011055076494812965\n",
      "[step: 50] loss: 279.93890380859375\n",
      "[step: 50] loss: 0.01096209604293108\n",
      "[step: 51] loss: 275.52984619140625\n",
      "[step: 51] loss: 0.010834177024662495\n",
      "[step: 52] loss: 271.24224853515625\n",
      "[step: 52] loss: 0.010716233402490616\n",
      "[step: 53] loss: 266.9761962890625\n",
      "[step: 53] loss: 0.010649965144693851\n",
      "[step: 54] loss: 262.675048828125\n",
      "[step: 54] loss: 0.010501129552721977\n",
      "[step: 55] loss: 259.490966796875\n",
      "[step: 55] loss: 0.01044917106628418\n",
      "[step: 56] loss: 255.5446319580078\n",
      "[step: 56] loss: 0.010349668562412262\n",
      "[step: 57] loss: 252.31011962890625\n",
      "[step: 57] loss: 0.01026859600096941\n",
      "[step: 58] loss: 248.87225341796875\n",
      "[step: 58] loss: 0.010216990485787392\n",
      "[step: 59] loss: 245.54425048828125\n",
      "[step: 59] loss: 0.01010818500071764\n",
      "[step: 60] loss: 242.0521697998047\n",
      "[step: 60] loss: 0.010048947297036648\n",
      "[step: 61] loss: 238.9439697265625\n",
      "[step: 61] loss: 0.009983164258301258\n",
      "[step: 62] loss: 235.81637573242188\n",
      "[step: 62] loss: 0.009887045249342918\n",
      "[step: 63] loss: 233.27658081054688\n",
      "[step: 63] loss: 0.009835299104452133\n",
      "[step: 64] loss: 231.1851806640625\n",
      "[step: 64] loss: 0.009766168892383575\n",
      "[step: 65] loss: 228.89088439941406\n",
      "[step: 65] loss: 0.009679955430328846\n",
      "[step: 66] loss: 227.44857788085938\n",
      "[step: 66] loss: 0.009632802568376064\n",
      "[step: 67] loss: 227.20266723632812\n",
      "[step: 67] loss: 0.009572564624249935\n",
      "[step: 68] loss: 230.05166625976562\n",
      "[step: 68] loss: 0.009498830884695053\n",
      "[step: 69] loss: 239.23329162597656\n",
      "[step: 69] loss: 0.00945814698934555\n",
      "[step: 70] loss: 240.78225708007812\n",
      "[step: 70] loss: 0.009412738494575024\n",
      "[step: 71] loss: 231.3346710205078\n",
      "[step: 71] loss: 0.009344709105789661\n",
      "[step: 72] loss: 214.20266723632812\n",
      "[step: 72] loss: 0.009294415824115276\n",
      "[step: 73] loss: 222.24952697753906\n",
      "[step: 73] loss: 0.009255404584109783\n",
      "[step: 74] loss: 228.1500244140625\n",
      "[step: 74] loss: 0.009199589490890503\n",
      "[step: 75] loss: 212.32786560058594\n",
      "[step: 75] loss: 0.009139586240053177\n",
      "[step: 76] loss: 211.60635375976562\n",
      "[step: 76] loss: 0.009093343280255795\n",
      "[step: 77] loss: 219.87884521484375\n",
      "[step: 77] loss: 0.009058232419192791\n",
      "[step: 78] loss: 207.32608032226562\n",
      "[step: 78] loss: 0.009021173231303692\n",
      "[step: 79] loss: 205.97894287109375\n",
      "[step: 79] loss: 0.00897498894482851\n",
      "[step: 80] loss: 211.63262939453125\n",
      "[step: 80] loss: 0.00892785657197237\n",
      "[step: 81] loss: 202.74525451660156\n",
      "[step: 81] loss: 0.008885796181857586\n",
      "[step: 82] loss: 201.0262451171875\n",
      "[step: 82] loss: 0.008849427103996277\n",
      "[step: 83] loss: 204.87445068359375\n",
      "[step: 83] loss: 0.008816057816147804\n",
      "[step: 84] loss: 198.695068359375\n",
      "[step: 84] loss: 0.008781760931015015\n",
      "[step: 85] loss: 196.14419555664062\n",
      "[step: 85] loss: 0.008746710605919361\n",
      "[step: 86] loss: 199.50921630859375\n",
      "[step: 86] loss: 0.008709505200386047\n",
      "[step: 87] loss: 194.7124481201172\n",
      "[step: 87] loss: 0.008672717958688736\n",
      "[step: 88] loss: 192.38375854492188\n",
      "[step: 88] loss: 0.00863704364746809\n",
      "[step: 89] loss: 194.06576538085938\n",
      "[step: 89] loss: 0.008603114634752274\n",
      "[step: 90] loss: 192.041015625\n",
      "[step: 90] loss: 0.008570677600800991\n",
      "[step: 91] loss: 188.56878662109375\n",
      "[step: 91] loss: 0.00853924173861742\n",
      "[step: 92] loss: 189.02139282226562\n",
      "[step: 92] loss: 0.008509011939167976\n",
      "[step: 93] loss: 189.0501708984375\n",
      "[step: 93] loss: 0.008479966782033443\n",
      "[step: 94] loss: 185.87306213378906\n",
      "[step: 94] loss: 0.008452063426375389\n",
      "[step: 95] loss: 184.5687255859375\n",
      "[step: 95] loss: 0.00842551700770855\n",
      "[step: 96] loss: 184.8739013671875\n",
      "[step: 96] loss: 0.008401432074606419\n",
      "[step: 97] loss: 184.1475372314453\n",
      "[step: 97] loss: 0.008382857777178288\n",
      "[step: 98] loss: 181.83871459960938\n",
      "[step: 98] loss: 0.008380437269806862\n",
      "[step: 99] loss: 180.26536560058594\n",
      "[step: 99] loss: 0.008415121585130692\n",
      "[step: 100] loss: 180.37387084960938\n",
      "[step: 100] loss: 0.008562644943594933\n",
      "[step: 101] loss: 179.963134765625\n",
      "[step: 101] loss: 0.008709235116839409\n",
      "[step: 102] loss: 178.53939819335938\n",
      "[step: 102] loss: 0.008764730766415596\n",
      "[step: 103] loss: 176.83660888671875\n",
      "[step: 103] loss: 0.008287624455988407\n",
      "[step: 104] loss: 175.568603515625\n",
      "[step: 104] loss: 0.008370956405997276\n",
      "[step: 105] loss: 175.0561981201172\n",
      "[step: 105] loss: 0.008509954437613487\n",
      "[step: 106] loss: 174.90342712402344\n",
      "[step: 106] loss: 0.008190474472939968\n",
      "[step: 107] loss: 174.52447509765625\n",
      "[step: 107] loss: 0.008414586074650288\n",
      "[step: 108] loss: 173.76544189453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 108] loss: 0.008242701180279255\n",
      "[step: 109] loss: 173.05545043945312\n",
      "[step: 109] loss: 0.008228112943470478\n",
      "[step: 110] loss: 172.146728515625\n",
      "[step: 110] loss: 0.008263071067631245\n",
      "[step: 111] loss: 171.40940856933594\n",
      "[step: 111] loss: 0.008104214444756508\n",
      "[step: 112] loss: 170.7532958984375\n",
      "[step: 112] loss: 0.008221983909606934\n",
      "[step: 113] loss: 170.73123168945312\n",
      "[step: 113] loss: 0.008058041334152222\n",
      "[step: 114] loss: 171.35189819335938\n",
      "[step: 114] loss: 0.008149145171046257\n",
      "[step: 115] loss: 173.88233947753906\n",
      "[step: 115] loss: 0.008068486116826534\n",
      "[step: 116] loss: 178.29710388183594\n",
      "[step: 116] loss: 0.008042276836931705\n",
      "[step: 117] loss: 187.82443237304688\n",
      "[step: 117] loss: 0.008078396320343018\n",
      "[step: 118] loss: 190.8101806640625\n",
      "[step: 118] loss: 0.007970314472913742\n",
      "[step: 119] loss: 186.78309631347656\n",
      "[step: 119] loss: 0.008027669042348862\n",
      "[step: 120] loss: 168.62725830078125\n",
      "[step: 120] loss: 0.007959937676787376\n",
      "[step: 121] loss: 163.61459350585938\n",
      "[step: 121] loss: 0.007947803474962711\n",
      "[step: 122] loss: 173.39727783203125\n",
      "[step: 122] loss: 0.007961621508002281\n",
      "[step: 123] loss: 174.40228271484375\n",
      "[step: 123] loss: 0.00789431482553482\n",
      "[step: 124] loss: 165.43531799316406\n",
      "[step: 124] loss: 0.007927358150482178\n",
      "[step: 125] loss: 161.82223510742188\n",
      "[step: 125] loss: 0.007887685671448708\n",
      "[step: 126] loss: 166.43063354492188\n",
      "[step: 126] loss: 0.007865389809012413\n",
      "[step: 127] loss: 166.25180053710938\n",
      "[step: 127] loss: 0.007882624864578247\n",
      "[step: 128] loss: 161.85047912597656\n",
      "[step: 128] loss: 0.007833506911993027\n",
      "[step: 129] loss: 161.56683349609375\n",
      "[step: 129] loss: 0.007837122306227684\n",
      "[step: 130] loss: 160.00772094726562\n",
      "[step: 130] loss: 0.007831369526684284\n",
      "[step: 131] loss: 159.71707153320312\n",
      "[step: 131] loss: 0.007794596720486879\n",
      "[step: 132] loss: 160.66665649414062\n",
      "[step: 132] loss: 0.007801902014762163\n",
      "[step: 133] loss: 155.98597717285156\n",
      "[step: 133] loss: 0.007788143120706081\n",
      "[step: 134] loss: 156.64596557617188\n",
      "[step: 134] loss: 0.007761570159345865\n",
      "[step: 135] loss: 156.75241088867188\n",
      "[step: 135] loss: 0.00776525866240263\n",
      "[step: 136] loss: 155.62771606445312\n",
      "[step: 136] loss: 0.00775357149541378\n",
      "[step: 137] loss: 155.92694091796875\n",
      "[step: 137] loss: 0.007731158286333084\n",
      "[step: 138] loss: 152.71697998046875\n",
      "[step: 138] loss: 0.00772932218387723\n",
      "[step: 139] loss: 153.3820343017578\n",
      "[step: 139] loss: 0.00772329606115818\n",
      "[step: 140] loss: 152.69223022460938\n",
      "[step: 140] loss: 0.007704098243266344\n",
      "[step: 141] loss: 152.40048217773438\n",
      "[step: 141] loss: 0.007695294450968504\n",
      "[step: 142] loss: 153.19850158691406\n",
      "[step: 142] loss: 0.007693363353610039\n",
      "[step: 143] loss: 151.27879333496094\n",
      "[step: 143] loss: 0.007681258954107761\n",
      "[step: 144] loss: 151.28013610839844\n",
      "[step: 144] loss: 0.00766676664352417\n",
      "[step: 145] loss: 149.9904022216797\n",
      "[step: 145] loss: 0.007661901880055666\n",
      "[step: 146] loss: 148.87017822265625\n",
      "[step: 146] loss: 0.007657849695533514\n",
      "[step: 147] loss: 148.6288604736328\n",
      "[step: 147] loss: 0.007646186277270317\n",
      "[step: 148] loss: 147.32540893554688\n",
      "[step: 148] loss: 0.007634148932993412\n",
      "[step: 149] loss: 147.11758422851562\n",
      "[step: 149] loss: 0.0076277246698737144\n",
      "[step: 150] loss: 146.69956970214844\n",
      "[step: 150] loss: 0.0076237632893025875\n",
      "[step: 151] loss: 146.0224151611328\n",
      "[step: 151] loss: 0.007617409341037273\n",
      "[step: 152] loss: 146.85427856445312\n",
      "[step: 152] loss: 0.007607472129166126\n",
      "[step: 153] loss: 148.2851104736328\n",
      "[step: 153] loss: 0.0075974240899086\n",
      "[step: 154] loss: 154.40237426757812\n",
      "[step: 154] loss: 0.007589234504848719\n",
      "[step: 155] loss: 170.38592529296875\n",
      "[step: 155] loss: 0.007583138067275286\n",
      "[step: 156] loss: 205.42445373535156\n",
      "[step: 156] loss: 0.0075785089284181595\n",
      "[step: 157] loss: 218.7286376953125\n",
      "[step: 157] loss: 0.007574932649731636\n",
      "[step: 158] loss: 177.2454833984375\n",
      "[step: 158] loss: 0.007573799695819616\n",
      "[step: 159] loss: 145.02249145507812\n",
      "[step: 159] loss: 0.007575609255582094\n",
      "[step: 160] loss: 179.4732666015625\n",
      "[step: 160] loss: 0.007588966749608517\n",
      "[step: 161] loss: 172.41061401367188\n",
      "[step: 161] loss: 0.0076102325692772865\n",
      "[step: 162] loss: 145.85821533203125\n",
      "[step: 162] loss: 0.00766333844512701\n",
      "[step: 163] loss: 171.38844299316406\n",
      "[step: 163] loss: 0.007672579493373632\n",
      "[step: 164] loss: 157.50125122070312\n",
      "[step: 164] loss: 0.007657763548195362\n",
      "[step: 165] loss: 146.60330200195312\n",
      "[step: 165] loss: 0.007564523722976446\n",
      "[step: 166] loss: 165.39321899414062\n",
      "[step: 166] loss: 0.00751841813325882\n",
      "[step: 167] loss: 142.43692016601562\n",
      "[step: 167] loss: 0.007553587667644024\n",
      "[step: 168] loss: 153.31785583496094\n",
      "[step: 168] loss: 0.007575808558613062\n",
      "[step: 169] loss: 150.24166870117188\n",
      "[step: 169] loss: 0.00753891933709383\n",
      "[step: 170] loss: 141.45303344726562\n",
      "[step: 170] loss: 0.0074974726885557175\n",
      "[step: 171] loss: 151.26861572265625\n",
      "[step: 171] loss: 0.007514017168432474\n",
      "[step: 172] loss: 140.38937377929688\n",
      "[step: 172] loss: 0.007535963784903288\n",
      "[step: 173] loss: 144.37240600585938\n",
      "[step: 173] loss: 0.007506313268095255\n",
      "[step: 174] loss: 144.1283721923828\n",
      "[step: 174] loss: 0.007476678118109703\n",
      "[step: 175] loss: 138.7280731201172\n",
      "[step: 175] loss: 0.007484196685254574\n",
      "[step: 176] loss: 142.8213653564453\n",
      "[step: 176] loss: 0.007498129270970821\n",
      "[step: 177] loss: 139.5200653076172\n",
      "[step: 177] loss: 0.00748654967173934\n",
      "[step: 178] loss: 137.37872314453125\n",
      "[step: 178] loss: 0.007460796274244785\n",
      "[step: 179] loss: 140.98013305664062\n",
      "[step: 179] loss: 0.007453701924532652\n",
      "[step: 180] loss: 135.21803283691406\n",
      "[step: 180] loss: 0.007463459391146898\n",
      "[step: 181] loss: 137.8470916748047\n",
      "[step: 181] loss: 0.007465145085006952\n",
      "[step: 182] loss: 136.82626342773438\n",
      "[step: 182] loss: 0.007452322170138359\n",
      "[step: 183] loss: 134.10940551757812\n",
      "[step: 183] loss: 0.007435315754264593\n",
      "[step: 184] loss: 136.293701171875\n",
      "[step: 184] loss: 0.00742893572896719\n",
      "[step: 185] loss: 133.877197265625\n",
      "[step: 185] loss: 0.00743242260068655\n",
      "[step: 186] loss: 133.31033325195312\n",
      "[step: 186] loss: 0.007435382343828678\n",
      "[step: 187] loss: 134.2191162109375\n",
      "[step: 187] loss: 0.0074324519373476505\n",
      "[step: 188] loss: 132.0835723876953\n",
      "[step: 188] loss: 0.007422124035656452\n",
      "[step: 189] loss: 132.0665283203125\n",
      "[step: 189] loss: 0.00741097517311573\n",
      "[step: 190] loss: 132.55230712890625\n",
      "[step: 190] loss: 0.007402408868074417\n",
      "[step: 191] loss: 130.29351806640625\n",
      "[step: 191] loss: 0.007397872861474752\n",
      "[step: 192] loss: 131.10610961914062\n",
      "[step: 192] loss: 0.007396587170660496\n",
      "[step: 193] loss: 130.53118896484375\n",
      "[step: 193] loss: 0.007397265639156103\n",
      "[step: 194] loss: 129.3054656982422\n",
      "[step: 194] loss: 0.007400221191346645\n",
      "[step: 195] loss: 129.5028076171875\n",
      "[step: 195] loss: 0.007405492477118969\n",
      "[step: 196] loss: 129.11611938476562\n",
      "[step: 196] loss: 0.007419081404805183\n",
      "[step: 197] loss: 128.1517333984375\n",
      "[step: 197] loss: 0.0074394745752215385\n",
      "[step: 198] loss: 127.9462890625\n",
      "[step: 198] loss: 0.007485039532184601\n",
      "[step: 199] loss: 127.93695068359375\n",
      "[step: 199] loss: 0.007524074055254459\n",
      "[step: 200] loss: 126.94973754882812\n",
      "[step: 200] loss: 0.007574590388685465\n",
      "[step: 201] loss: 126.53849792480469\n",
      "[step: 201] loss: 0.007520198822021484\n",
      "[step: 202] loss: 126.68907928466797\n",
      "[step: 202] loss: 0.0074276188388466835\n",
      "[step: 203] loss: 125.84513854980469\n",
      "[step: 203] loss: 0.007357980590313673\n",
      "[step: 204] loss: 125.3431625366211\n",
      "[step: 204] loss: 0.007388551719486713\n",
      "[step: 205] loss: 125.20616149902344\n",
      "[step: 205] loss: 0.007440644316375256\n",
      "[step: 206] loss: 124.8963394165039\n",
      "[step: 206] loss: 0.007406118791550398\n",
      "[step: 207] loss: 124.29644775390625\n",
      "[step: 207] loss: 0.007349457126110792\n",
      "[step: 208] loss: 123.78752136230469\n",
      "[step: 208] loss: 0.007352404296398163\n",
      "[step: 209] loss: 123.70104217529297\n",
      "[step: 209] loss: 0.007387087680399418\n",
      "[step: 210] loss: 123.36936950683594\n",
      "[step: 210] loss: 0.0073832981288433075\n",
      "[step: 211] loss: 122.7872314453125\n",
      "[step: 211] loss: 0.007341545075178146\n",
      "[step: 212] loss: 122.38838958740234\n",
      "[step: 212] loss: 0.007327560801059008\n",
      "[step: 213] loss: 122.12312316894531\n",
      "[step: 213] loss: 0.007348904851824045\n",
      "[step: 214] loss: 121.85624694824219\n",
      "[step: 214] loss: 0.007359095383435488\n",
      "[step: 215] loss: 121.54354858398438\n",
      "[step: 215] loss: 0.007341745775192976\n",
      "[step: 216] loss: 121.0409927368164\n",
      "[step: 216] loss: 0.007315742317587137\n",
      "[step: 217] loss: 120.6195068359375\n",
      "[step: 217] loss: 0.007310858462005854\n",
      "[step: 218] loss: 120.34608459472656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 218] loss: 0.007323427125811577\n",
      "[step: 219] loss: 120.06436920166016\n",
      "[step: 219] loss: 0.007329947780817747\n",
      "[step: 220] loss: 119.79732513427734\n",
      "[step: 220] loss: 0.007321934215724468\n",
      "[step: 221] loss: 119.503662109375\n",
      "[step: 221] loss: 0.007304364815354347\n",
      "[step: 222] loss: 119.136474609375\n",
      "[step: 222] loss: 0.0072929104790091515\n",
      "[step: 223] loss: 118.75481414794922\n",
      "[step: 223] loss: 0.007292208261787891\n",
      "[step: 224] loss: 118.41272735595703\n",
      "[step: 224] loss: 0.007298001553863287\n",
      "[step: 225] loss: 118.05220031738281\n",
      "[step: 225] loss: 0.007303834427148104\n",
      "[step: 226] loss: 117.70454406738281\n",
      "[step: 226] loss: 0.007303827442228794\n",
      "[step: 227] loss: 117.39717102050781\n",
      "[step: 227] loss: 0.007300115190446377\n",
      "[step: 228] loss: 117.07806396484375\n",
      "[step: 228] loss: 0.00729186600074172\n",
      "[step: 229] loss: 116.75946807861328\n",
      "[step: 229] loss: 0.00728365033864975\n",
      "[step: 230] loss: 116.46839904785156\n",
      "[step: 230] loss: 0.007275510113686323\n",
      "[step: 231] loss: 116.19541931152344\n",
      "[step: 231] loss: 0.007269011810421944\n",
      "[step: 232] loss: 115.96476745605469\n",
      "[step: 232] loss: 0.0072638955898582935\n",
      "[step: 233] loss: 115.88096618652344\n",
      "[step: 233] loss: 0.007259916048496962\n",
      "[step: 234] loss: 116.2755355834961\n",
      "[step: 234] loss: 0.007256769109517336\n",
      "[step: 235] loss: 118.0421142578125\n",
      "[step: 235] loss: 0.007254155818372965\n",
      "[step: 236] loss: 124.54164123535156\n",
      "[step: 236] loss: 0.00725199980661273\n",
      "[step: 237] loss: 140.05615234375\n",
      "[step: 237] loss: 0.007250663358718157\n",
      "[step: 238] loss: 175.81631469726562\n",
      "[step: 238] loss: 0.007251102477312088\n",
      "[step: 239] loss: 175.56320190429688\n",
      "[step: 239] loss: 0.007255671080201864\n",
      "[step: 240] loss: 138.03675842285156\n",
      "[step: 240] loss: 0.007272405084222555\n",
      "[step: 241] loss: 121.8399887084961\n",
      "[step: 241] loss: 0.007316096685826778\n",
      "[step: 242] loss: 136.77688598632812\n",
      "[step: 242] loss: 0.007445886731147766\n",
      "[step: 243] loss: 144.09906005859375\n",
      "[step: 243] loss: 0.0076565914787352085\n",
      "[step: 244] loss: 121.58011627197266\n",
      "[step: 244] loss: 0.008033372461795807\n",
      "[step: 245] loss: 122.670654296875\n",
      "[step: 245] loss: 0.007802947424352169\n",
      "[step: 246] loss: 136.87232971191406\n",
      "[step: 246] loss: 0.007345587946474552\n",
      "[step: 247] loss: 124.80258178710938\n",
      "[step: 247] loss: 0.007354419678449631\n",
      "[step: 248] loss: 116.83499908447266\n",
      "[step: 248] loss: 0.0075661432929337025\n",
      "[step: 249] loss: 129.30081176757812\n",
      "[step: 249] loss: 0.007314841728657484\n",
      "[step: 250] loss: 121.94853210449219\n",
      "[step: 250] loss: 0.007359407376497984\n",
      "[step: 251] loss: 114.23283386230469\n",
      "[step: 251] loss: 0.007427378557622433\n",
      "[step: 252] loss: 123.13047790527344\n",
      "[step: 252] loss: 0.007259943522512913\n",
      "[step: 253] loss: 120.13749694824219\n",
      "[step: 253] loss: 0.007361521478742361\n",
      "[step: 254] loss: 113.60111999511719\n",
      "[step: 254] loss: 0.0073273666203022\n",
      "[step: 255] loss: 118.35871887207031\n",
      "[step: 255] loss: 0.007245294284075499\n",
      "[step: 256] loss: 117.22113037109375\n",
      "[step: 256] loss: 0.0073401746340096\n",
      "[step: 257] loss: 112.23678588867188\n",
      "[step: 257] loss: 0.007265185471624136\n",
      "[step: 258] loss: 114.76251220703125\n",
      "[step: 258] loss: 0.007243570405989885\n",
      "[step: 259] loss: 114.37013244628906\n",
      "[step: 259] loss: 0.007292948663234711\n",
      "[step: 260] loss: 112.16815185546875\n",
      "[step: 260] loss: 0.007241338025778532\n",
      "[step: 261] loss: 111.35665893554688\n",
      "[step: 261] loss: 0.0072221094742417336\n",
      "[step: 262] loss: 112.90357971191406\n",
      "[step: 262] loss: 0.007268871180713177\n",
      "[step: 263] loss: 111.150634765625\n",
      "[step: 263] loss: 0.00722021097317338\n",
      "[step: 264] loss: 109.77027893066406\n",
      "[step: 264] loss: 0.007215880323201418\n",
      "[step: 265] loss: 110.88325500488281\n",
      "[step: 265] loss: 0.007241199724376202\n",
      "[step: 266] loss: 110.91984558105469\n",
      "[step: 266] loss: 0.007215994410216808\n",
      "[step: 267] loss: 108.2486572265625\n",
      "[step: 267] loss: 0.007195072248578072\n",
      "[step: 268] loss: 110.34347534179688\n",
      "[step: 268] loss: 0.007221858482807875\n",
      "[step: 269] loss: 110.93028259277344\n",
      "[step: 269] loss: 0.007207473739981651\n",
      "[step: 270] loss: 107.81454467773438\n",
      "[step: 270] loss: 0.00718751410022378\n",
      "[step: 271] loss: 107.90875244140625\n",
      "[step: 271] loss: 0.007196363992989063\n",
      "[step: 272] loss: 110.10968017578125\n",
      "[step: 272] loss: 0.00720251677557826\n",
      "[step: 273] loss: 107.10540008544922\n",
      "[step: 273] loss: 0.0071824393235147\n",
      "[step: 274] loss: 106.58015441894531\n",
      "[step: 274] loss: 0.007177586201578379\n",
      "[step: 275] loss: 107.50149536132812\n",
      "[step: 275] loss: 0.007186358794569969\n",
      "[step: 276] loss: 106.8360824584961\n",
      "[step: 276] loss: 0.007179069798439741\n",
      "[step: 277] loss: 105.40557861328125\n",
      "[step: 277] loss: 0.007168279495090246\n",
      "[step: 278] loss: 105.53133392333984\n",
      "[step: 278] loss: 0.007168869022279978\n",
      "[step: 279] loss: 105.58146667480469\n",
      "[step: 279] loss: 0.007173731457442045\n",
      "[step: 280] loss: 105.07611846923828\n",
      "[step: 280] loss: 0.007163436152040958\n",
      "[step: 281] loss: 104.20663452148438\n",
      "[step: 281] loss: 0.00715795299038291\n",
      "[step: 282] loss: 104.1597671508789\n",
      "[step: 282] loss: 0.007160290610045195\n",
      "[step: 283] loss: 104.40741729736328\n",
      "[step: 283] loss: 0.007160093169659376\n",
      "[step: 284] loss: 103.75193786621094\n",
      "[step: 284] loss: 0.00715294573456049\n",
      "[step: 285] loss: 103.12120056152344\n",
      "[step: 285] loss: 0.007147558499127626\n",
      "[step: 286] loss: 103.0150375366211\n",
      "[step: 286] loss: 0.007149175275117159\n",
      "[step: 287] loss: 103.14300537109375\n",
      "[step: 287] loss: 0.007148467004299164\n",
      "[step: 288] loss: 102.66436767578125\n",
      "[step: 288] loss: 0.007144335191696882\n",
      "[step: 289] loss: 102.16149139404297\n",
      "[step: 289] loss: 0.007138762157410383\n",
      "[step: 290] loss: 101.90847778320312\n",
      "[step: 290] loss: 0.007136811967939138\n",
      "[step: 291] loss: 101.85140228271484\n",
      "[step: 291] loss: 0.007137270178645849\n",
      "[step: 292] loss: 101.66917419433594\n",
      "[step: 292] loss: 0.0071357120759785175\n",
      "[step: 293] loss: 101.30760192871094\n",
      "[step: 293] loss: 0.007132284808903933\n",
      "[step: 294] loss: 100.8877182006836\n",
      "[step: 294] loss: 0.007127807009965181\n",
      "[step: 295] loss: 100.59857177734375\n",
      "[step: 295] loss: 0.0071252742782235146\n",
      "[step: 296] loss: 100.53485107421875\n",
      "[step: 296] loss: 0.0071242437697947025\n",
      "[step: 297] loss: 100.32526397705078\n",
      "[step: 297] loss: 0.007123250979930162\n",
      "[step: 298] loss: 100.02178192138672\n",
      "[step: 298] loss: 0.007121846545487642\n",
      "[step: 299] loss: 99.65045928955078\n",
      "[step: 299] loss: 0.007119162939488888\n",
      "[step: 300] loss: 99.37213134765625\n",
      "[step: 300] loss: 0.007116077933460474\n",
      "[step: 301] loss: 99.17192077636719\n",
      "[step: 301] loss: 0.007113054394721985\n",
      "[step: 302] loss: 98.97001647949219\n",
      "[step: 302] loss: 0.007110212929546833\n",
      "[step: 303] loss: 98.79449462890625\n",
      "[step: 303] loss: 0.007108128163963556\n",
      "[step: 304] loss: 98.53242492675781\n",
      "[step: 304] loss: 0.007106185425072908\n",
      "[step: 305] loss: 98.23854064941406\n",
      "[step: 305] loss: 0.007104562595486641\n",
      "[step: 306] loss: 97.9306411743164\n",
      "[step: 306] loss: 0.007103285286575556\n",
      "[step: 307] loss: 97.65420532226562\n",
      "[step: 307] loss: 0.0071020713075995445\n",
      "[step: 308] loss: 97.41984558105469\n",
      "[step: 308] loss: 0.007101600058376789\n",
      "[step: 309] loss: 97.1785888671875\n",
      "[step: 309] loss: 0.007101908326148987\n",
      "[step: 310] loss: 96.96388244628906\n",
      "[step: 310] loss: 0.0071043274365365505\n",
      "[step: 311] loss: 96.75270080566406\n",
      "[step: 311] loss: 0.007110105827450752\n",
      "[step: 312] loss: 96.54890441894531\n",
      "[step: 312] loss: 0.007125173695385456\n",
      "[step: 313] loss: 96.34149169921875\n",
      "[step: 313] loss: 0.007152388337999582\n",
      "[step: 314] loss: 96.14109802246094\n",
      "[step: 314] loss: 0.007217064034193754\n",
      "[step: 315] loss: 95.96709442138672\n",
      "[step: 315] loss: 0.007300885859876871\n",
      "[step: 316] loss: 95.85980224609375\n",
      "[step: 316] loss: 0.007454771548509598\n",
      "[step: 317] loss: 95.8042221069336\n",
      "[step: 317] loss: 0.007447574753314257\n",
      "[step: 318] loss: 95.96890258789062\n",
      "[step: 318] loss: 0.007332723122090101\n",
      "[step: 319] loss: 96.34817504882812\n",
      "[step: 319] loss: 0.00712229497730732\n",
      "[step: 320] loss: 97.62105560302734\n",
      "[step: 320] loss: 0.007135030347853899\n",
      "[step: 321] loss: 99.26176452636719\n",
      "[step: 321] loss: 0.00726633844897151\n",
      "[step: 322] loss: 103.92876434326172\n",
      "[step: 322] loss: 0.007201113272458315\n",
      "[step: 323] loss: 106.91825866699219\n",
      "[step: 323] loss: 0.007090095896273851\n",
      "[step: 324] loss: 113.67962646484375\n",
      "[step: 324] loss: 0.007131021469831467\n",
      "[step: 325] loss: 110.59896087646484\n",
      "[step: 325] loss: 0.0071955169551074505\n",
      "[step: 326] loss: 105.35145568847656\n",
      "[step: 326] loss: 0.007144469302147627\n",
      "[step: 327] loss: 97.32378387451172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 327] loss: 0.0070769754238426685\n",
      "[step: 328] loss: 93.13146209716797\n",
      "[step: 328] loss: 0.007110888604074717\n",
      "[step: 329] loss: 94.27710723876953\n",
      "[step: 329] loss: 0.007157403975725174\n",
      "[step: 330] loss: 98.3546142578125\n",
      "[step: 330] loss: 0.007117704953998327\n",
      "[step: 331] loss: 101.55908203125\n",
      "[step: 331] loss: 0.007067367900162935\n",
      "[step: 332] loss: 98.72811889648438\n",
      "[step: 332] loss: 0.007084497716277838\n",
      "[step: 333] loss: 94.16868591308594\n",
      "[step: 333] loss: 0.0071160560473799706\n",
      "[step: 334] loss: 91.43917846679688\n",
      "[step: 334] loss: 0.007100773509591818\n",
      "[step: 335] loss: 92.56117248535156\n",
      "[step: 335] loss: 0.007062881253659725\n",
      "[step: 336] loss: 95.11174011230469\n",
      "[step: 336] loss: 0.007062991615384817\n",
      "[step: 337] loss: 95.54942321777344\n",
      "[step: 337] loss: 0.007088192272931337\n",
      "[step: 338] loss: 93.99620056152344\n",
      "[step: 338] loss: 0.007087424863129854\n",
      "[step: 339] loss: 91.16932678222656\n",
      "[step: 339] loss: 0.007063454017043114\n",
      "[step: 340] loss: 90.06597900390625\n",
      "[step: 340] loss: 0.007048074621707201\n",
      "[step: 341] loss: 90.6891860961914\n",
      "[step: 341] loss: 0.0070572602562606335\n",
      "[step: 342] loss: 91.70297241210938\n",
      "[step: 342] loss: 0.00706992344930768\n",
      "[step: 343] loss: 92.20892333984375\n",
      "[step: 343] loss: 0.007064015604555607\n",
      "[step: 344] loss: 91.32029724121094\n",
      "[step: 344] loss: 0.0070481328293681145\n",
      "[step: 345] loss: 90.17620849609375\n",
      "[step: 345] loss: 0.007038729265332222\n",
      "[step: 346] loss: 88.97383880615234\n",
      "[step: 346] loss: 0.007043308112770319\n",
      "[step: 347] loss: 88.35441589355469\n",
      "[step: 347] loss: 0.007051060441881418\n",
      "[step: 348] loss: 88.25736999511719\n",
      "[step: 348] loss: 0.007049546577036381\n",
      "[step: 349] loss: 88.55543518066406\n",
      "[step: 349] loss: 0.0070406608283519745\n",
      "[step: 350] loss: 89.1343002319336\n",
      "[step: 350] loss: 0.007031321991235018\n",
      "[step: 351] loss: 89.25765991210938\n",
      "[step: 351] loss: 0.0070285489782691\n",
      "[step: 352] loss: 89.28199768066406\n",
      "[step: 352] loss: 0.007031234446913004\n",
      "[step: 353] loss: 88.4692153930664\n",
      "[step: 353] loss: 0.00703467708081007\n",
      "[step: 354] loss: 87.66252136230469\n",
      "[step: 354] loss: 0.007035591173917055\n",
      "[step: 355] loss: 86.70496368408203\n",
      "[step: 355] loss: 0.007031963672488928\n",
      "[step: 356] loss: 86.08454895019531\n",
      "[step: 356] loss: 0.007026627194136381\n",
      "[step: 357] loss: 85.70951843261719\n",
      "[step: 357] loss: 0.007020867895334959\n",
      "[step: 358] loss: 85.46405029296875\n",
      "[step: 358] loss: 0.007016640622168779\n",
      "[step: 359] loss: 85.26374816894531\n",
      "[step: 359] loss: 0.007014364004135132\n",
      "[step: 360] loss: 85.06944274902344\n",
      "[step: 360] loss: 0.007013596594333649\n",
      "[step: 361] loss: 84.89883422851562\n",
      "[step: 361] loss: 0.007013955619186163\n",
      "[step: 362] loss: 84.83612060546875\n",
      "[step: 362] loss: 0.007014854811131954\n",
      "[step: 363] loss: 85.10563659667969\n",
      "[step: 363] loss: 0.007016513030976057\n",
      "[step: 364] loss: 85.82301330566406\n",
      "[step: 364] loss: 0.0070189256221055984\n",
      "[step: 365] loss: 87.8129653930664\n",
      "[step: 365] loss: 0.0070237754844129086\n",
      "[step: 366] loss: 91.41947937011719\n",
      "[step: 366] loss: 0.007031132932752371\n",
      "[step: 367] loss: 99.43892669677734\n",
      "[step: 367] loss: 0.00704682944342494\n",
      "[step: 368] loss: 106.29093933105469\n",
      "[step: 368] loss: 0.007068526931107044\n",
      "[step: 369] loss: 113.07870483398438\n",
      "[step: 369] loss: 0.0071125151589512825\n",
      "[step: 370] loss: 103.94483184814453\n",
      "[step: 370] loss: 0.007157373242080212\n",
      "[step: 371] loss: 91.83846282958984\n",
      "[step: 371] loss: 0.007229158654808998\n",
      "[step: 372] loss: 88.46253967285156\n",
      "[step: 372] loss: 0.007227191235870123\n",
      "[step: 373] loss: 93.02626037597656\n",
      "[step: 373] loss: 0.007184845395386219\n",
      "[step: 374] loss: 99.15275573730469\n",
      "[step: 374] loss: 0.0070637590251863\n",
      "[step: 375] loss: 89.34883880615234\n",
      "[step: 375] loss: 0.006999020930379629\n",
      "[step: 376] loss: 86.85406494140625\n",
      "[step: 376] loss: 0.007035079412162304\n",
      "[step: 377] loss: 95.68647766113281\n",
      "[step: 377] loss: 0.0070943268947303295\n",
      "[step: 378] loss: 93.87255096435547\n",
      "[step: 378] loss: 0.007100390736013651\n",
      "[step: 379] loss: 86.5946044921875\n",
      "[step: 379] loss: 0.007036867085844278\n",
      "[step: 380] loss: 84.2307357788086\n",
      "[step: 380] loss: 0.006990457884967327\n",
      "[step: 381] loss: 93.29110717773438\n",
      "[step: 381] loss: 0.006998855154961348\n",
      "[step: 382] loss: 92.0714340209961\n",
      "[step: 382] loss: 0.007036937866359949\n",
      "[step: 383] loss: 83.30057525634766\n",
      "[step: 383] loss: 0.007060598116368055\n",
      "[step: 384] loss: 88.30781555175781\n",
      "[step: 384] loss: 0.007038671523332596\n",
      "[step: 385] loss: 92.51486206054688\n",
      "[step: 385] loss: 0.007001126185059547\n",
      "[step: 386] loss: 81.43267059326172\n",
      "[step: 386] loss: 0.006978678051382303\n",
      "[step: 387] loss: 84.57382202148438\n",
      "[step: 387] loss: 0.006987010594457388\n",
      "[step: 388] loss: 87.81075286865234\n",
      "[step: 388] loss: 0.007008865475654602\n",
      "[step: 389] loss: 81.07902526855469\n",
      "[step: 389] loss: 0.007016411982476711\n",
      "[step: 390] loss: 84.06612396240234\n",
      "[step: 390] loss: 0.007005553692579269\n",
      "[step: 391] loss: 87.54777526855469\n",
      "[step: 391] loss: 0.006983534432947636\n",
      "[step: 392] loss: 80.6148452758789\n",
      "[step: 392] loss: 0.006969692185521126\n",
      "[step: 393] loss: 83.2559814453125\n",
      "[step: 393] loss: 0.006970408838242292\n",
      "[step: 394] loss: 86.74100494384766\n",
      "[step: 394] loss: 0.006980150938034058\n",
      "[step: 395] loss: 79.35903930664062\n",
      "[step: 395] loss: 0.0069894990883767605\n",
      "[step: 396] loss: 83.11404418945312\n",
      "[step: 396] loss: 0.006989673711359501\n",
      "[step: 397] loss: 83.43653869628906\n",
      "[step: 397] loss: 0.006982996594160795\n",
      "[step: 398] loss: 78.30351257324219\n",
      "[step: 398] loss: 0.006971448659896851\n",
      "[step: 399] loss: 82.09688568115234\n",
      "[step: 399] loss: 0.006961850915104151\n",
      "[step: 400] loss: 82.00518798828125\n",
      "[step: 400] loss: 0.0069569475017488\n",
      "[step: 401] loss: 77.77059173583984\n",
      "[step: 401] loss: 0.006956994067877531\n",
      "[step: 402] loss: 81.6841812133789\n",
      "[step: 402] loss: 0.006960263475775719\n",
      "[step: 403] loss: 80.93602752685547\n",
      "[step: 403] loss: 0.006964682601392269\n",
      "[step: 404] loss: 77.21330261230469\n",
      "[step: 404] loss: 0.00696976576000452\n",
      "[step: 405] loss: 80.73609924316406\n",
      "[step: 405] loss: 0.00697385473176837\n",
      "[step: 406] loss: 79.27100372314453\n",
      "[step: 406] loss: 0.006979352794587612\n",
      "[step: 407] loss: 76.75927734375\n",
      "[step: 407] loss: 0.006983455270528793\n",
      "[step: 408] loss: 79.73500061035156\n",
      "[step: 408] loss: 0.006990727502852678\n",
      "[step: 409] loss: 78.39869689941406\n",
      "[step: 409] loss: 0.006995282601565123\n",
      "[step: 410] loss: 76.24783325195312\n",
      "[step: 410] loss: 0.007003603968769312\n",
      "[step: 411] loss: 79.08154296875\n",
      "[step: 411] loss: 0.00700516439974308\n",
      "[step: 412] loss: 77.95408630371094\n",
      "[step: 412] loss: 0.007008202373981476\n",
      "[step: 413] loss: 75.86390686035156\n",
      "[step: 413] loss: 0.007000305224210024\n",
      "[step: 414] loss: 78.17073059082031\n",
      "[step: 414] loss: 0.006991848815232515\n",
      "[step: 415] loss: 77.26469421386719\n",
      "[step: 415] loss: 0.006975471507757902\n",
      "[step: 416] loss: 75.07725524902344\n",
      "[step: 416] loss: 0.00696071470156312\n",
      "[step: 417] loss: 77.20610046386719\n",
      "[step: 417] loss: 0.006946753244847059\n",
      "[step: 418] loss: 76.57527160644531\n",
      "[step: 418] loss: 0.006937253754585981\n",
      "[step: 419] loss: 74.57366943359375\n",
      "[step: 419] loss: 0.0069318474270403385\n",
      "[step: 420] loss: 76.62214660644531\n",
      "[step: 420] loss: 0.006929915864020586\n",
      "[step: 421] loss: 76.4332275390625\n",
      "[step: 421] loss: 0.006930569652467966\n",
      "[step: 422] loss: 74.31048583984375\n",
      "[step: 422] loss: 0.0069334860891103745\n",
      "[step: 423] loss: 76.30685424804688\n",
      "[step: 423] loss: 0.006939596496522427\n",
      "[step: 424] loss: 76.14532470703125\n",
      "[step: 424] loss: 0.0069502913393080235\n",
      "[step: 425] loss: 74.1121597290039\n",
      "[step: 425] loss: 0.0069717406295239925\n",
      "[step: 426] loss: 76.01298522949219\n",
      "[step: 426] loss: 0.0070059290155768394\n",
      "[step: 427] loss: 77.21256256103516\n",
      "[step: 427] loss: 0.007073775865137577\n",
      "[step: 428] loss: 76.30462646484375\n",
      "[step: 428] loss: 0.007151501718908548\n",
      "[step: 429] loss: 80.27867889404297\n",
      "[step: 429] loss: 0.007268092129379511\n",
      "[step: 430] loss: 84.96855163574219\n",
      "[step: 430] loss: 0.007255754433572292\n",
      "[step: 431] loss: 91.33686828613281\n",
      "[step: 431] loss: 0.007150198332965374\n",
      "[step: 432] loss: 106.28426361083984\n",
      "[step: 432] loss: 0.006969383917748928\n",
      "[step: 433] loss: 121.24015808105469\n",
      "[step: 433] loss: 0.006947136949747801\n",
      "[step: 434] loss: 118.63874816894531\n",
      "[step: 434] loss: 0.007059899624437094\n",
      "[step: 435] loss: 99.22911071777344\n",
      "[step: 435] loss: 0.007086305413395166\n",
      "[step: 436] loss: 76.41377258300781\n",
      "[step: 436] loss: 0.007001486606895924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 437] loss: 83.72055053710938\n",
      "[step: 437] loss: 0.00692030368372798\n",
      "[step: 438] loss: 99.2134017944336\n",
      "[step: 438] loss: 0.006947655696421862\n",
      "[step: 439] loss: 84.15455627441406\n",
      "[step: 439] loss: 0.007019482087343931\n",
      "[step: 440] loss: 76.675048828125\n",
      "[step: 440] loss: 0.007019786164164543\n",
      "[step: 441] loss: 83.74314880371094\n",
      "[step: 441] loss: 0.006959341932088137\n",
      "[step: 442] loss: 85.44319152832031\n",
      "[step: 442] loss: 0.006911184173077345\n",
      "[step: 443] loss: 79.48114013671875\n",
      "[step: 443] loss: 0.006930484902113676\n",
      "[step: 444] loss: 75.67027282714844\n",
      "[step: 444] loss: 0.0069717164151370525\n",
      "[step: 445] loss: 84.21612548828125\n",
      "[step: 445] loss: 0.006965982262045145\n",
      "[step: 446] loss: 82.9920883178711\n",
      "[step: 446] loss: 0.006926306523382664\n",
      "[step: 447] loss: 75.11137390136719\n",
      "[step: 447] loss: 0.006902365945279598\n",
      "[step: 448] loss: 80.16683959960938\n",
      "[step: 448] loss: 0.00691800843924284\n",
      "[step: 449] loss: 81.05003356933594\n",
      "[step: 449] loss: 0.006941545754671097\n",
      "[step: 450] loss: 75.99134826660156\n",
      "[step: 450] loss: 0.006935575511306524\n",
      "[step: 451] loss: 75.82600402832031\n",
      "[step: 451] loss: 0.006910711992532015\n",
      "[step: 452] loss: 77.40513610839844\n",
      "[step: 452] loss: 0.006895646918565035\n",
      "[step: 453] loss: 76.36170959472656\n",
      "[step: 453] loss: 0.006903494242578745\n",
      "[step: 454] loss: 73.91972351074219\n",
      "[step: 454] loss: 0.0069176931865513325\n",
      "[step: 455] loss: 73.9681625366211\n",
      "[step: 455] loss: 0.006917113438248634\n",
      "[step: 456] loss: 74.83677673339844\n",
      "[step: 456] loss: 0.006903911009430885\n",
      "[step: 457] loss: 73.6219482421875\n",
      "[step: 457] loss: 0.006890601944178343\n",
      "[step: 458] loss: 71.51563262939453\n",
      "[step: 458] loss: 0.006888465955853462\n",
      "[step: 459] loss: 73.02566528320312\n",
      "[step: 459] loss: 0.0068953875452280045\n",
      "[step: 460] loss: 73.40123748779297\n",
      "[step: 460] loss: 0.006901675369590521\n",
      "[step: 461] loss: 70.30834197998047\n",
      "[step: 461] loss: 0.006901801563799381\n",
      "[step: 462] loss: 71.8145980834961\n",
      "[step: 462] loss: 0.006894555874168873\n",
      "[step: 463] loss: 72.0230712890625\n",
      "[step: 463] loss: 0.006885665934532881\n",
      "[step: 464] loss: 70.39430236816406\n",
      "[step: 464] loss: 0.006879750173538923\n",
      "[step: 465] loss: 70.91719055175781\n",
      "[step: 465] loss: 0.0068787154741585255\n",
      "[step: 466] loss: 70.32572937011719\n",
      "[step: 466] loss: 0.006881106644868851\n",
      "[step: 467] loss: 70.57368469238281\n",
      "[step: 467] loss: 0.00688411807641387\n",
      "[step: 468] loss: 70.42298889160156\n",
      "[step: 468] loss: 0.00688578188419342\n",
      "[step: 469] loss: 69.14387512207031\n",
      "[step: 469] loss: 0.006884818896651268\n",
      "[step: 470] loss: 70.07008361816406\n",
      "[step: 470] loss: 0.006882382091134787\n",
      "[step: 471] loss: 70.01943969726562\n",
      "[step: 471] loss: 0.006878601852804422\n",
      "[step: 472] loss: 68.83345031738281\n",
      "[step: 472] loss: 0.006874897517263889\n",
      "[step: 473] loss: 69.16461181640625\n",
      "[step: 473] loss: 0.006871381308883429\n",
      "[step: 474] loss: 69.21859741210938\n",
      "[step: 474] loss: 0.006868517026305199\n",
      "[step: 475] loss: 68.84391021728516\n",
      "[step: 475] loss: 0.006866178475320339\n",
      "[step: 476] loss: 68.59243774414062\n",
      "[step: 476] loss: 0.006864325609058142\n",
      "[step: 477] loss: 68.41645812988281\n",
      "[step: 477] loss: 0.006862782407552004\n",
      "[step: 478] loss: 68.39949035644531\n",
      "[step: 478] loss: 0.006861411966383457\n",
      "[step: 479] loss: 68.35651397705078\n",
      "[step: 479] loss: 0.006860180292278528\n",
      "[step: 480] loss: 68.03999328613281\n",
      "[step: 480] loss: 0.006859030108898878\n",
      "[step: 481] loss: 67.76593017578125\n",
      "[step: 481] loss: 0.0068579670041799545\n",
      "[step: 482] loss: 67.85444641113281\n",
      "[step: 482] loss: 0.00685708224773407\n",
      "[step: 483] loss: 67.84852600097656\n",
      "[step: 483] loss: 0.006856550928205252\n",
      "[step: 484] loss: 67.44549560546875\n",
      "[step: 484] loss: 0.006856787484139204\n",
      "[step: 485] loss: 67.34376525878906\n",
      "[step: 485] loss: 0.006858948152512312\n",
      "[step: 486] loss: 67.36637115478516\n",
      "[step: 486] loss: 0.00686542596668005\n",
      "[step: 487] loss: 67.1599349975586\n",
      "[step: 487] loss: 0.006883623544126749\n",
      "[step: 488] loss: 67.108154296875\n",
      "[step: 488] loss: 0.0069268848747015\n",
      "[step: 489] loss: 67.00445556640625\n",
      "[step: 489] loss: 0.007043374236673117\n",
      "[step: 490] loss: 66.72575378417969\n",
      "[step: 490] loss: 0.007258859928697348\n",
      "[step: 491] loss: 66.71802520751953\n",
      "[step: 491] loss: 0.007712922524660826\n",
      "[step: 492] loss: 66.72315979003906\n",
      "[step: 492] loss: 0.007798806764185429\n",
      "[step: 493] loss: 66.50990295410156\n",
      "[step: 493] loss: 0.007390658836811781\n",
      "[step: 494] loss: 66.37080383300781\n",
      "[step: 494] loss: 0.006934307981282473\n",
      "[step: 495] loss: 66.28437805175781\n",
      "[step: 495] loss: 0.007280415389686823\n",
      "[step: 496] loss: 66.16128540039062\n",
      "[step: 496] loss: 0.007317445706576109\n",
      "[step: 497] loss: 66.07821655273438\n",
      "[step: 497] loss: 0.006904542446136475\n",
      "[step: 498] loss: 66.00911712646484\n",
      "[step: 498] loss: 0.00711987167596817\n",
      "[step: 499] loss: 65.89569091796875\n",
      "[step: 499] loss: 0.007263985462486744\n",
      "[step: 500] loss: 65.74453735351562\n",
      "[step: 500] loss: 0.006938015576452017\n",
      "[step: 501] loss: 65.62711334228516\n",
      "[step: 501] loss: 0.006970262620598078\n",
      "[step: 502] loss: 65.56185913085938\n",
      "[step: 502] loss: 0.007146286778151989\n",
      "[step: 503] loss: 65.46864318847656\n",
      "[step: 503] loss: 0.006937742233276367\n",
      "[step: 504] loss: 65.36158752441406\n",
      "[step: 504] loss: 0.006923784501850605\n",
      "[step: 505] loss: 65.28767395019531\n",
      "[step: 505] loss: 0.00705755827948451\n",
      "[step: 506] loss: 65.18778991699219\n",
      "[step: 506] loss: 0.006925818510353565\n",
      "[step: 507] loss: 65.05248260498047\n",
      "[step: 507] loss: 0.00689304294064641\n",
      "[step: 508] loss: 64.95292663574219\n",
      "[step: 508] loss: 0.006991703994572163\n",
      "[step: 509] loss: 64.87010192871094\n",
      "[step: 509] loss: 0.006905918009579182\n",
      "[step: 510] loss: 64.76605224609375\n",
      "[step: 510] loss: 0.006873812060803175\n",
      "[step: 511] loss: 64.67217254638672\n",
      "[step: 511] loss: 0.006950824521481991\n",
      "[step: 512] loss: 64.59564971923828\n",
      "[step: 512] loss: 0.006880985572934151\n",
      "[step: 513] loss: 64.51022338867188\n",
      "[step: 513] loss: 0.006866185925900936\n",
      "[step: 514] loss: 64.41659545898438\n",
      "[step: 514] loss: 0.00691800145432353\n",
      "[step: 515] loss: 64.32669067382812\n",
      "[step: 515] loss: 0.006860602181404829\n",
      "[step: 516] loss: 64.2408447265625\n",
      "[step: 516] loss: 0.006860832683742046\n",
      "[step: 517] loss: 64.15633392333984\n",
      "[step: 517] loss: 0.006893876940011978\n",
      "[step: 518] loss: 64.06816101074219\n",
      "[step: 518] loss: 0.0068466379307210445\n",
      "[step: 519] loss: 63.9847297668457\n",
      "[step: 519] loss: 0.006856576073914766\n",
      "[step: 520] loss: 63.91656494140625\n",
      "[step: 520] loss: 0.0068697272799909115\n",
      "[step: 521] loss: 63.86250305175781\n",
      "[step: 521] loss: 0.006843411363661289\n",
      "[step: 522] loss: 63.82720184326172\n",
      "[step: 522] loss: 0.006844196002930403\n",
      "[step: 523] loss: 63.843685150146484\n",
      "[step: 523] loss: 0.006853046827018261\n",
      "[step: 524] loss: 63.96490478515625\n",
      "[step: 524] loss: 0.00684082368388772\n",
      "[step: 525] loss: 64.27978515625\n",
      "[step: 525] loss: 0.006833982188254595\n",
      "[step: 526] loss: 64.9969482421875\n",
      "[step: 526] loss: 0.0068384455516934395\n",
      "[step: 527] loss: 66.54405212402344\n",
      "[step: 527] loss: 0.006838909815996885\n",
      "[step: 528] loss: 69.88609313964844\n",
      "[step: 528] loss: 0.0068261162377893925\n",
      "[step: 529] loss: 76.4454116821289\n",
      "[step: 529] loss: 0.006826527416706085\n",
      "[step: 530] loss: 89.05335998535156\n",
      "[step: 530] loss: 0.00683317007496953\n",
      "[step: 531] loss: 105.66509246826172\n",
      "[step: 531] loss: 0.006821450777351856\n",
      "[step: 532] loss: 120.93977355957031\n",
      "[step: 532] loss: 0.0068182614631950855\n",
      "[step: 533] loss: 107.46763610839844\n",
      "[step: 533] loss: 0.0068254051730036736\n",
      "[step: 534] loss: 77.41875457763672\n",
      "[step: 534] loss: 0.006819778122007847\n",
      "[step: 535] loss: 64.77511596679688\n",
      "[step: 535] loss: 0.0068129138089716434\n",
      "[step: 536] loss: 81.84761047363281\n",
      "[step: 536] loss: 0.0068161324597895145\n",
      "[step: 537] loss: 89.15425109863281\n",
      "[step: 537] loss: 0.0068155573680996895\n",
      "[step: 538] loss: 70.30026245117188\n",
      "[step: 538] loss: 0.006812059320509434\n",
      "[step: 539] loss: 65.58602142333984\n",
      "[step: 539] loss: 0.00680895708501339\n",
      "[step: 540] loss: 79.09129333496094\n",
      "[step: 540] loss: 0.006808513309806585\n",
      "[step: 541] loss: 74.96243286132812\n",
      "[step: 541] loss: 0.006809921469539404\n",
      "[step: 542] loss: 63.62007141113281\n",
      "[step: 542] loss: 0.006806232500821352\n",
      "[step: 543] loss: 69.6839599609375\n",
      "[step: 543] loss: 0.006802483461797237\n",
      "[step: 544] loss: 73.68882751464844\n",
      "[step: 544] loss: 0.006803867872804403\n",
      "[step: 545] loss: 65.30015563964844\n",
      "[step: 545] loss: 0.006803838536143303\n",
      "[step: 546] loss: 64.65695190429688\n",
      "[step: 546] loss: 0.006800366099923849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 547] loss: 69.58760833740234\n",
      "[step: 547] loss: 0.006798751186579466\n",
      "[step: 548] loss: 66.46578979492188\n",
      "[step: 548] loss: 0.00679786317050457\n",
      "[step: 549] loss: 63.50956344604492\n",
      "[step: 549] loss: 0.00679731834679842\n",
      "[step: 550] loss: 65.7865982055664\n",
      "[step: 550] loss: 0.006796861998736858\n",
      "[step: 551] loss: 65.96237182617188\n",
      "[step: 551] loss: 0.006794405169785023\n",
      "[step: 552] loss: 63.92911148071289\n",
      "[step: 552] loss: 0.006792362313717604\n",
      "[step: 553] loss: 63.64130401611328\n",
      "[step: 553] loss: 0.00679220724850893\n",
      "[step: 554] loss: 64.4566421508789\n",
      "[step: 554] loss: 0.0067917765118181705\n",
      "[step: 555] loss: 64.25634765625\n",
      "[step: 555] loss: 0.006790295708924532\n",
      "[step: 556] loss: 62.70469284057617\n",
      "[step: 556] loss: 0.0067888833582401276\n",
      "[step: 557] loss: 62.81721115112305\n",
      "[step: 557] loss: 0.006787362042814493\n",
      "[step: 558] loss: 63.878658294677734\n",
      "[step: 558] loss: 0.006786163430660963\n",
      "[step: 559] loss: 62.50114440917969\n",
      "[step: 559] loss: 0.006785798352211714\n",
      "[step: 560] loss: 61.61958312988281\n",
      "[step: 560] loss: 0.006785007659345865\n",
      "[step: 561] loss: 62.90095520019531\n",
      "[step: 561] loss: 0.006783594377338886\n",
      "[step: 562] loss: 62.39214324951172\n",
      "[step: 562] loss: 0.006782282143831253\n",
      "[step: 563] loss: 61.076629638671875\n",
      "[step: 563] loss: 0.006781098432838917\n",
      "[step: 564] loss: 61.83306884765625\n",
      "[step: 564] loss: 0.006779936142265797\n",
      "[step: 565] loss: 62.033809661865234\n",
      "[step: 565] loss: 0.006779109127819538\n",
      "[step: 566] loss: 60.974205017089844\n",
      "[step: 566] loss: 0.00677843764424324\n",
      "[step: 567] loss: 61.050575256347656\n",
      "[step: 567] loss: 0.006777392700314522\n",
      "[step: 568] loss: 61.428123474121094\n",
      "[step: 568] loss: 0.006776268593966961\n",
      "[step: 569] loss: 60.910491943359375\n",
      "[step: 569] loss: 0.006775184068828821\n",
      "[step: 570] loss: 60.680816650390625\n",
      "[step: 570] loss: 0.006773997098207474\n",
      "[step: 571] loss: 60.78711700439453\n",
      "[step: 571] loss: 0.006772852037101984\n",
      "[step: 572] loss: 60.62915802001953\n",
      "[step: 572] loss: 0.006771892309188843\n",
      "[step: 573] loss: 60.51850128173828\n",
      "[step: 573] loss: 0.006771007087081671\n",
      "[step: 574] loss: 60.36244201660156\n",
      "[step: 574] loss: 0.006770052015781403\n",
      "[step: 575] loss: 60.177818298339844\n",
      "[step: 575] loss: 0.006769163068383932\n",
      "[step: 576] loss: 60.259098052978516\n",
      "[step: 576] loss: 0.006768312305212021\n",
      "[step: 577] loss: 60.16019821166992\n",
      "[step: 577] loss: 0.006767358630895615\n",
      "[step: 578] loss: 59.81928634643555\n",
      "[step: 578] loss: 0.006766404956579208\n",
      "[step: 579] loss: 59.83954620361328\n",
      "[step: 579] loss: 0.006765489466488361\n",
      "[step: 580] loss: 59.915016174316406\n",
      "[step: 580] loss: 0.006764567457139492\n",
      "[step: 581] loss: 59.651824951171875\n",
      "[step: 581] loss: 0.0067636254243552685\n",
      "[step: 582] loss: 59.484474182128906\n",
      "[step: 582] loss: 0.006762741133570671\n",
      "[step: 583] loss: 59.526771545410156\n",
      "[step: 583] loss: 0.006761956959962845\n",
      "[step: 584] loss: 59.437278747558594\n",
      "[step: 584] loss: 0.0067612421698868275\n",
      "[step: 585] loss: 59.29220199584961\n",
      "[step: 585] loss: 0.006760693155229092\n",
      "[step: 586] loss: 59.21165466308594\n",
      "[step: 586] loss: 0.006760524585843086\n",
      "[step: 587] loss: 59.10847473144531\n",
      "[step: 587] loss: 0.006760892923921347\n",
      "[step: 588] loss: 59.03378677368164\n",
      "[step: 588] loss: 0.0067624435760080814\n",
      "[step: 589] loss: 58.9987678527832\n",
      "[step: 589] loss: 0.006765918806195259\n",
      "[step: 590] loss: 58.87400436401367\n",
      "[step: 590] loss: 0.0067740632221102715\n",
      "[step: 591] loss: 58.732826232910156\n",
      "[step: 591] loss: 0.006789033301174641\n",
      "[step: 592] loss: 58.694644927978516\n",
      "[step: 592] loss: 0.006822474300861359\n",
      "[step: 593] loss: 58.65122985839844\n",
      "[step: 593] loss: 0.006874376442283392\n",
      "[step: 594] loss: 58.53345489501953\n",
      "[step: 594] loss: 0.006980799604207277\n",
      "[step: 595] loss: 58.431549072265625\n",
      "[step: 595] loss: 0.007069880608469248\n",
      "[step: 596] loss: 58.359737396240234\n",
      "[step: 596] loss: 0.007158804684877396\n",
      "[step: 597] loss: 58.27510070800781\n",
      "[step: 597] loss: 0.007030480075627565\n",
      "[step: 598] loss: 58.202491760253906\n",
      "[step: 598] loss: 0.006836124695837498\n",
      "[step: 599] loss: 58.13817596435547\n",
      "[step: 599] loss: 0.006771970074623823\n",
      "[step: 600] loss: 58.041236877441406\n",
      "[step: 600] loss: 0.006880625616759062\n",
      "[step: 601] loss: 57.937217712402344\n",
      "[step: 601] loss: 0.006970221642404795\n",
      "[step: 602] loss: 57.86763000488281\n",
      "[step: 602] loss: 0.006895530968904495\n",
      "[step: 603] loss: 57.803924560546875\n",
      "[step: 603] loss: 0.00678380997851491\n",
      "[step: 604] loss: 57.721309661865234\n",
      "[step: 604] loss: 0.006762021686881781\n",
      "[step: 605] loss: 57.63853454589844\n",
      "[step: 605] loss: 0.006830149795860052\n",
      "[step: 606] loss: 57.55989074707031\n",
      "[step: 606] loss: 0.006870113778859377\n",
      "[step: 607] loss: 57.47187042236328\n",
      "[step: 607] loss: 0.006811304017901421\n",
      "[step: 608] loss: 57.386993408203125\n",
      "[step: 608] loss: 0.006750384345650673\n",
      "[step: 609] loss: 57.31681442260742\n",
      "[step: 609] loss: 0.006768517196178436\n",
      "[step: 610] loss: 57.2464714050293\n",
      "[step: 610] loss: 0.00681333290413022\n",
      "[step: 611] loss: 57.16557693481445\n",
      "[step: 611] loss: 0.0068047973327338696\n",
      "[step: 612] loss: 57.08483123779297\n",
      "[step: 612] loss: 0.006756999995559454\n",
      "[step: 613] loss: 57.00717544555664\n",
      "[step: 613] loss: 0.006746381521224976\n",
      "[step: 614] loss: 56.92548751831055\n",
      "[step: 614] loss: 0.006773591507226229\n",
      "[step: 615] loss: 56.8425178527832\n",
      "[step: 615] loss: 0.006781728472560644\n",
      "[step: 616] loss: 56.76611328125\n",
      "[step: 616] loss: 0.006760742049664259\n",
      "[step: 617] loss: 56.69255447387695\n",
      "[step: 617] loss: 0.006741891149431467\n",
      "[step: 618] loss: 56.616207122802734\n",
      "[step: 618] loss: 0.006746741011738777\n",
      "[step: 619] loss: 56.53983688354492\n",
      "[step: 619] loss: 0.0067606293596327305\n",
      "[step: 620] loss: 56.46672439575195\n",
      "[step: 620] loss: 0.006760433316230774\n",
      "[step: 621] loss: 56.393951416015625\n",
      "[step: 621] loss: 0.006746687460690737\n",
      "[step: 622] loss: 56.320884704589844\n",
      "[step: 622] loss: 0.006733461748808622\n",
      "[step: 623] loss: 56.253753662109375\n",
      "[step: 623] loss: 0.006736696697771549\n",
      "[step: 624] loss: 56.20173263549805\n",
      "[step: 624] loss: 0.006747663486748934\n",
      "[step: 625] loss: 56.17731857299805\n",
      "[step: 625] loss: 0.006747476290911436\n",
      "[step: 626] loss: 56.22837829589844\n",
      "[step: 626] loss: 0.006736852694302797\n",
      "[step: 627] loss: 56.43388366699219\n",
      "[step: 627] loss: 0.0067278859205543995\n",
      "[step: 628] loss: 57.133304595947266\n",
      "[step: 628] loss: 0.006729047745466232\n",
      "[step: 629] loss: 58.514869689941406\n",
      "[step: 629] loss: 0.006734153721481562\n",
      "[step: 630] loss: 62.632568359375\n",
      "[step: 630] loss: 0.00673584034666419\n",
      "[step: 631] loss: 65.30896759033203\n",
      "[step: 631] loss: 0.006733008194714785\n",
      "[step: 632] loss: 71.16856384277344\n",
      "[step: 632] loss: 0.006727069616317749\n",
      "[step: 633] loss: 62.572288513183594\n",
      "[step: 633] loss: 0.0067222327925264835\n",
      "[step: 634] loss: 56.7899169921875\n",
      "[step: 634] loss: 0.006720973644405603\n",
      "[step: 635] loss: 58.73515319824219\n",
      "[step: 635] loss: 0.006723575294017792\n",
      "[step: 636] loss: 63.06802749633789\n",
      "[step: 636] loss: 0.00672603864222765\n",
      "[step: 637] loss: 68.37995147705078\n",
      "[step: 637] loss: 0.006725329905748367\n",
      "[step: 638] loss: 61.33000946044922\n",
      "[step: 638] loss: 0.006722358521074057\n",
      "[step: 639] loss: 59.21235275268555\n",
      "[step: 639] loss: 0.006718823686242104\n",
      "[step: 640] loss: 64.30218505859375\n",
      "[step: 640] loss: 0.006716154050081968\n",
      "[step: 641] loss: 66.0369644165039\n",
      "[step: 641] loss: 0.006714096292853355\n",
      "[step: 642] loss: 63.34400939941406\n",
      "[step: 642] loss: 0.006713208742439747\n",
      "[step: 643] loss: 60.453582763671875\n",
      "[step: 643] loss: 0.006713513284921646\n",
      "[step: 644] loss: 65.66604614257812\n",
      "[step: 644] loss: 0.006714421324431896\n",
      "[step: 645] loss: 72.40667724609375\n",
      "[step: 645] loss: 0.0067151314578950405\n",
      "[step: 646] loss: 73.45985412597656\n",
      "[step: 646] loss: 0.006715091411024332\n",
      "[step: 647] loss: 75.70659637451172\n",
      "[step: 647] loss: 0.006715136580169201\n",
      "[step: 648] loss: 82.22415924072266\n",
      "[step: 648] loss: 0.006715006660670042\n",
      "[step: 649] loss: 80.67058563232422\n",
      "[step: 649] loss: 0.006714993622153997\n",
      "[step: 650] loss: 70.54046630859375\n",
      "[step: 650] loss: 0.006714573595672846\n",
      "[step: 651] loss: 61.39146423339844\n",
      "[step: 651] loss: 0.006714648101478815\n",
      "[step: 652] loss: 59.22541427612305\n",
      "[step: 652] loss: 0.006715062540024519\n",
      "[step: 653] loss: 59.77074432373047\n",
      "[step: 653] loss: 0.006717001087963581\n",
      "[step: 654] loss: 62.61608123779297\n",
      "[step: 654] loss: 0.006719578988850117\n",
      "[step: 655] loss: 65.54298400878906\n",
      "[step: 655] loss: 0.0067251394502818584\n",
      "[step: 656] loss: 61.533782958984375\n",
      "[step: 656] loss: 0.006732731591910124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 657] loss: 55.28125762939453\n",
      "[step: 657] loss: 0.006747817154973745\n",
      "[step: 658] loss: 56.3736686706543\n",
      "[step: 658] loss: 0.006766072474420071\n",
      "[step: 659] loss: 60.435516357421875\n",
      "[step: 659] loss: 0.006797293666750193\n",
      "[step: 660] loss: 59.58259963989258\n",
      "[step: 660] loss: 0.006823270116001368\n",
      "[step: 661] loss: 57.64949417114258\n",
      "[step: 661] loss: 0.006857153493911028\n",
      "[step: 662] loss: 56.250282287597656\n",
      "[step: 662] loss: 0.006851124111562967\n",
      "[step: 663] loss: 54.96535110473633\n",
      "[step: 663] loss: 0.006823263596743345\n",
      "[step: 664] loss: 56.307151794433594\n",
      "[step: 664] loss: 0.006759841460734606\n",
      "[step: 665] loss: 57.57341003417969\n",
      "[step: 665] loss: 0.006709984038025141\n",
      "[step: 666] loss: 56.69017791748047\n",
      "[step: 666] loss: 0.006696966476738453\n",
      "[step: 667] loss: 54.73418045043945\n",
      "[step: 667] loss: 0.006719061639159918\n",
      "[step: 668] loss: 54.15990447998047\n",
      "[step: 668] loss: 0.006756687071174383\n",
      "[step: 669] loss: 54.81608581542969\n",
      "[step: 669] loss: 0.0067830076441168785\n",
      "[step: 670] loss: 55.66191864013672\n",
      "[step: 670] loss: 0.006793744396418333\n",
      "[step: 671] loss: 55.386173248291016\n",
      "[step: 671] loss: 0.006769826170057058\n",
      "[step: 672] loss: 53.961692810058594\n",
      "[step: 672] loss: 0.006730017252266407\n",
      "[step: 673] loss: 53.460113525390625\n",
      "[step: 673] loss: 0.0066962833516299725\n",
      "[step: 674] loss: 53.92308807373047\n",
      "[step: 674] loss: 0.0066917939111590385\n",
      "[step: 675] loss: 54.20880889892578\n",
      "[step: 675] loss: 0.0067122397013008595\n",
      "[step: 676] loss: 53.96799850463867\n",
      "[step: 676] loss: 0.006734669208526611\n",
      "[step: 677] loss: 53.78691864013672\n",
      "[step: 677] loss: 0.006742966827005148\n",
      "[step: 678] loss: 53.41026306152344\n",
      "[step: 678] loss: 0.006729274056851864\n",
      "[step: 679] loss: 52.98329162597656\n",
      "[step: 679] loss: 0.006707517430186272\n",
      "[step: 680] loss: 52.95210266113281\n",
      "[step: 680] loss: 0.0066895573399960995\n",
      "[step: 681] loss: 53.16452407836914\n",
      "[step: 681] loss: 0.006685159634798765\n",
      "[step: 682] loss: 53.24275207519531\n",
      "[step: 682] loss: 0.006692030467092991\n",
      "[step: 683] loss: 53.14692687988281\n",
      "[step: 683] loss: 0.00670215068385005\n",
      "[step: 684] loss: 52.8128776550293\n",
      "[step: 684] loss: 0.006709659937769175\n",
      "[step: 685] loss: 52.402950286865234\n",
      "[step: 685] loss: 0.006710235960781574\n",
      "[step: 686] loss: 52.27606201171875\n",
      "[step: 686] loss: 0.006707104854285717\n",
      "[step: 687] loss: 52.371498107910156\n",
      "[step: 687] loss: 0.0066982596181333065\n",
      "[step: 688] loss: 52.468109130859375\n",
      "[step: 688] loss: 0.006688318215310574\n",
      "[step: 689] loss: 52.41008758544922\n",
      "[step: 689] loss: 0.006679217796772718\n",
      "[step: 690] loss: 52.343162536621094\n",
      "[step: 690] loss: 0.006674851290881634\n",
      "[step: 691] loss: 52.19245910644531\n",
      "[step: 691] loss: 0.006675980519503355\n",
      "[step: 692] loss: 52.012535095214844\n",
      "[step: 692] loss: 0.006680740043520927\n",
      "[step: 693] loss: 51.8850212097168\n",
      "[step: 693] loss: 0.006686939392238855\n",
      "[step: 694] loss: 51.7688102722168\n",
      "[step: 694] loss: 0.006691901478916407\n",
      "[step: 695] loss: 51.63493728637695\n",
      "[step: 695] loss: 0.006696613505482674\n",
      "[step: 696] loss: 51.534183502197266\n",
      "[step: 696] loss: 0.006697910837829113\n",
      "[step: 697] loss: 51.51055145263672\n",
      "[step: 697] loss: 0.006698889192193747\n",
      "[step: 698] loss: 51.495872497558594\n",
      "[step: 698] loss: 0.006697310600429773\n",
      "[step: 699] loss: 51.49287414550781\n",
      "[step: 699] loss: 0.0066962651908397675\n",
      "[step: 700] loss: 51.498573303222656\n",
      "[step: 700] loss: 0.0066934507340192795\n",
      "[step: 701] loss: 51.50634765625\n",
      "[step: 701] loss: 0.006690692622214556\n",
      "[step: 702] loss: 51.500762939453125\n",
      "[step: 702] loss: 0.006686783861368895\n",
      "[step: 703] loss: 51.53197479248047\n",
      "[step: 703] loss: 0.006683894898742437\n",
      "[step: 704] loss: 51.638916015625\n",
      "[step: 704] loss: 0.006681123748421669\n",
      "[step: 705] loss: 51.846466064453125\n",
      "[step: 705] loss: 0.006680070888251066\n",
      "[step: 706] loss: 52.24281311035156\n",
      "[step: 706] loss: 0.006679282523691654\n",
      "[step: 707] loss: 52.98269271850586\n",
      "[step: 707] loss: 0.006680501624941826\n",
      "[step: 708] loss: 54.22971725463867\n",
      "[step: 708] loss: 0.006682003848254681\n",
      "[step: 709] loss: 56.41889953613281\n",
      "[step: 709] loss: 0.0066866460256278515\n",
      "[step: 710] loss: 59.80963897705078\n",
      "[step: 710] loss: 0.006692481692880392\n",
      "[step: 711] loss: 65.23120880126953\n",
      "[step: 711] loss: 0.006704328581690788\n",
      "[step: 712] loss: 71.96412658691406\n",
      "[step: 712] loss: 0.006718040909618139\n",
      "[step: 713] loss: 78.92277526855469\n",
      "[step: 713] loss: 0.0067405421286821365\n",
      "[step: 714] loss: 83.65492248535156\n",
      "[step: 714] loss: 0.006759903859347105\n",
      "[step: 715] loss: 80.24127960205078\n",
      "[step: 715] loss: 0.006784473545849323\n",
      "[step: 716] loss: 67.24481201171875\n",
      "[step: 716] loss: 0.006787483114749193\n",
      "[step: 717] loss: 54.601844787597656\n",
      "[step: 717] loss: 0.006778554990887642\n",
      "[step: 718] loss: 53.25434112548828\n",
      "[step: 718] loss: 0.006738914176821709\n",
      "[step: 719] loss: 61.09637451171875\n",
      "[step: 719] loss: 0.00669474434107542\n",
      "[step: 720] loss: 64.54264831542969\n",
      "[step: 720] loss: 0.006660958286374807\n",
      "[step: 721] loss: 58.077171325683594\n",
      "[step: 721] loss: 0.006652318872511387\n",
      "[step: 722] loss: 51.057437896728516\n",
      "[step: 722] loss: 0.006665926892310381\n",
      "[step: 723] loss: 52.97516632080078\n",
      "[step: 723] loss: 0.006690497975796461\n",
      "[step: 724] loss: 58.60930252075195\n",
      "[step: 724] loss: 0.006718876771628857\n",
      "[step: 725] loss: 57.9236946105957\n",
      "[step: 725] loss: 0.00673541659489274\n",
      "[step: 726] loss: 52.473968505859375\n",
      "[step: 726] loss: 0.006740153767168522\n",
      "[step: 727] loss: 50.552146911621094\n",
      "[step: 727] loss: 0.006719225086271763\n",
      "[step: 728] loss: 53.642433166503906\n",
      "[step: 728] loss: 0.006687644403427839\n",
      "[step: 729] loss: 55.111961364746094\n",
      "[step: 729] loss: 0.00665941322222352\n",
      "[step: 730] loss: 52.29558181762695\n",
      "[step: 730] loss: 0.006650432012975216\n",
      "[step: 731] loss: 49.97611999511719\n",
      "[step: 731] loss: 0.006660659797489643\n",
      "[step: 732] loss: 51.29095458984375\n",
      "[step: 732] loss: 0.006679409649223089\n",
      "[step: 733] loss: 53.19340515136719\n",
      "[step: 733] loss: 0.0066987331956624985\n",
      "[step: 734] loss: 52.324195861816406\n",
      "[step: 734] loss: 0.006704084109514952\n",
      "[step: 735] loss: 50.215450286865234\n",
      "[step: 735] loss: 0.006698848679661751\n",
      "[step: 736] loss: 49.76248550415039\n",
      "[step: 736] loss: 0.006678481586277485\n",
      "[step: 737] loss: 50.92762756347656\n",
      "[step: 737] loss: 0.006657000631093979\n",
      "[step: 738] loss: 51.3740348815918\n",
      "[step: 738] loss: 0.006642529740929604\n",
      "[step: 739] loss: 50.23933410644531\n",
      "[step: 739] loss: 0.006639830768108368\n",
      "[step: 740] loss: 49.262725830078125\n",
      "[step: 740] loss: 0.006646796129643917\n",
      "[step: 741] loss: 49.66526794433594\n",
      "[step: 741] loss: 0.006658087484538555\n",
      "[step: 742] loss: 50.48724365234375\n",
      "[step: 742] loss: 0.006670175585895777\n",
      "[step: 743] loss: 50.346221923828125\n",
      "[step: 743] loss: 0.006675106007605791\n",
      "[step: 744] loss: 49.41108703613281\n",
      "[step: 744] loss: 0.0066749476827681065\n",
      "[step: 745] loss: 48.967193603515625\n",
      "[step: 745] loss: 0.006665440276265144\n",
      "[step: 746] loss: 49.29136276245117\n",
      "[step: 746] loss: 0.0066544427536427975\n",
      "[step: 747] loss: 49.63494110107422\n",
      "[step: 747] loss: 0.006643736734986305\n",
      "[step: 748] loss: 49.41631317138672\n",
      "[step: 748] loss: 0.006637279409915209\n",
      "[step: 749] loss: 48.857791900634766\n",
      "[step: 749] loss: 0.006635462399572134\n",
      "[step: 750] loss: 48.57417297363281\n",
      "[step: 750] loss: 0.0066375345923006535\n",
      "[step: 751] loss: 48.759605407714844\n",
      "[step: 751] loss: 0.006641105283051729\n",
      "[step: 752] loss: 49.016380310058594\n",
      "[step: 752] loss: 0.006643849425017834\n",
      "[step: 753] loss: 49.01658630371094\n",
      "[step: 753] loss: 0.0066444710828363895\n",
      "[step: 754] loss: 48.76581954956055\n",
      "[step: 754] loss: 0.006641892716288567\n",
      "[step: 755] loss: 48.723060607910156\n",
      "[step: 755] loss: 0.00663809385150671\n",
      "[step: 756] loss: 48.9872932434082\n",
      "[step: 756] loss: 0.006633569952100515\n",
      "[step: 757] loss: 49.8447151184082\n",
      "[step: 757] loss: 0.00663127051666379\n",
      "[step: 758] loss: 50.658607482910156\n",
      "[step: 758] loss: 0.006632053758949041\n",
      "[step: 759] loss: 52.29206848144531\n",
      "[step: 759] loss: 0.006637432146817446\n",
      "[step: 760] loss: 52.03997039794922\n",
      "[step: 760] loss: 0.006647639907896519\n",
      "[step: 761] loss: 52.09914016723633\n",
      "[step: 761] loss: 0.006665130145847797\n",
      "[step: 762] loss: 49.69435501098633\n",
      "[step: 762] loss: 0.006690998561680317\n",
      "[step: 763] loss: 48.3089714050293\n",
      "[step: 763] loss: 0.006734014023095369\n",
      "[step: 764] loss: 48.02770233154297\n",
      "[step: 764] loss: 0.006790440529584885\n",
      "[step: 765] loss: 48.670204162597656\n",
      "[step: 765] loss: 0.006871581543236971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 766] loss: 49.61228942871094\n",
      "[step: 766] loss: 0.006918654311448336\n",
      "[step: 767] loss: 49.61750793457031\n",
      "[step: 767] loss: 0.0069211493246257305\n",
      "[step: 768] loss: 49.19386672973633\n",
      "[step: 768] loss: 0.006800398230552673\n",
      "[step: 769] loss: 48.12019348144531\n",
      "[step: 769] loss: 0.006676579359918833\n",
      "[step: 770] loss: 47.695068359375\n",
      "[step: 770] loss: 0.006648183334618807\n",
      "[step: 771] loss: 47.83924865722656\n",
      "[step: 771] loss: 0.006716825533658266\n",
      "[step: 772] loss: 48.2098388671875\n",
      "[step: 772] loss: 0.0067945560440421104\n",
      "[step: 773] loss: 48.48943328857422\n",
      "[step: 773] loss: 0.006798698101192713\n",
      "[step: 774] loss: 48.41755294799805\n",
      "[step: 774] loss: 0.006751782260835171\n",
      "[step: 775] loss: 48.194454193115234\n",
      "[step: 775] loss: 0.006669207010418177\n",
      "[step: 776] loss: 47.66015625\n",
      "[step: 776] loss: 0.006639198865741491\n",
      "[step: 777] loss: 47.36029815673828\n",
      "[step: 777] loss: 0.006676580291241407\n",
      "[step: 778] loss: 47.243560791015625\n",
      "[step: 778] loss: 0.006711559370160103\n",
      "[step: 779] loss: 47.42515563964844\n",
      "[step: 779] loss: 0.006700098514556885\n",
      "[step: 780] loss: 47.75722885131836\n",
      "[step: 780] loss: 0.006648951675742865\n",
      "[step: 781] loss: 48.139076232910156\n",
      "[step: 781] loss: 0.006624139379709959\n",
      "[step: 782] loss: 48.478515625\n",
      "[step: 782] loss: 0.006643605884164572\n",
      "[step: 783] loss: 48.76869201660156\n",
      "[step: 783] loss: 0.0066650016233325005\n",
      "[step: 784] loss: 49.04559326171875\n",
      "[step: 784] loss: 0.006658162921667099\n",
      "[step: 785] loss: 49.461753845214844\n",
      "[step: 785] loss: 0.00662990240380168\n",
      "[step: 786] loss: 50.22366714477539\n",
      "[step: 786] loss: 0.0066146631725132465\n",
      "[step: 787] loss: 51.75920867919922\n",
      "[step: 787] loss: 0.00662169000133872\n",
      "[step: 788] loss: 53.88824462890625\n",
      "[step: 788] loss: 0.006635725032538176\n",
      "[step: 789] loss: 58.554229736328125\n",
      "[step: 789] loss: 0.006638491526246071\n",
      "[step: 790] loss: 58.847843170166016\n",
      "[step: 790] loss: 0.006624278146773577\n",
      "[step: 791] loss: 59.100242614746094\n",
      "[step: 791] loss: 0.0066103460267186165\n",
      "[step: 792] loss: 54.46172332763672\n",
      "[step: 792] loss: 0.006606486160308123\n",
      "[step: 793] loss: 52.155670166015625\n",
      "[step: 793] loss: 0.006612051278352737\n",
      "[step: 794] loss: 52.34330749511719\n",
      "[step: 794] loss: 0.006618449930101633\n",
      "[step: 795] loss: 52.89189910888672\n",
      "[step: 795] loss: 0.0066182794980704784\n",
      "[step: 796] loss: 52.49093246459961\n",
      "[step: 796] loss: 0.006613886449486017\n",
      "[step: 797] loss: 48.19575881958008\n",
      "[step: 797] loss: 0.006606475915759802\n",
      "[step: 798] loss: 46.62469482421875\n",
      "[step: 798] loss: 0.0066001517698168755\n",
      "[step: 799] loss: 48.943992614746094\n",
      "[step: 799] loss: 0.006597863044589758\n",
      "[step: 800] loss: 51.69158935546875\n",
      "[step: 800] loss: 0.006599379237741232\n",
      "[step: 801] loss: 51.83249282836914\n",
      "[step: 801] loss: 0.006602948531508446\n",
      "[step: 802] loss: 48.46538162231445\n",
      "[step: 802] loss: 0.0066057899966835976\n",
      "[step: 803] loss: 46.6824951171875\n",
      "[step: 803] loss: 0.006605600938200951\n",
      "[step: 804] loss: 48.12730407714844\n",
      "[step: 804] loss: 0.006600832566618919\n",
      "[step: 805] loss: 49.310001373291016\n",
      "[step: 805] loss: 0.006594750564545393\n",
      "[step: 806] loss: 48.62909698486328\n",
      "[step: 806] loss: 0.006589265074580908\n",
      "[step: 807] loss: 45.971153259277344\n",
      "[step: 807] loss: 0.006586698815226555\n",
      "[step: 808] loss: 45.717384338378906\n",
      "[step: 808] loss: 0.006587459705770016\n",
      "[step: 809] loss: 47.49987030029297\n",
      "[step: 809] loss: 0.00658979220315814\n",
      "[step: 810] loss: 47.87617874145508\n",
      "[step: 810] loss: 0.006592261604964733\n",
      "[step: 811] loss: 47.118690490722656\n",
      "[step: 811] loss: 0.006593432277441025\n",
      "[step: 812] loss: 45.91438674926758\n",
      "[step: 812] loss: 0.006593689322471619\n",
      "[step: 813] loss: 46.39599609375\n",
      "[step: 813] loss: 0.006592564284801483\n",
      "[step: 814] loss: 47.455055236816406\n",
      "[step: 814] loss: 0.006591548211872578\n",
      "[step: 815] loss: 47.44403839111328\n",
      "[step: 815] loss: 0.006590339355170727\n",
      "[step: 816] loss: 46.56377410888672\n",
      "[step: 816] loss: 0.006589836440980434\n",
      "[step: 817] loss: 45.892539978027344\n",
      "[step: 817] loss: 0.006589359603822231\n",
      "[step: 818] loss: 46.26586151123047\n",
      "[step: 818] loss: 0.006589129567146301\n",
      "[step: 819] loss: 47.27626419067383\n",
      "[step: 819] loss: 0.00658875098451972\n",
      "[step: 820] loss: 47.32267761230469\n",
      "[step: 820] loss: 0.006588737480342388\n",
      "[step: 821] loss: 47.396881103515625\n",
      "[step: 821] loss: 0.006589150987565517\n",
      "[step: 822] loss: 47.34368896484375\n",
      "[step: 822] loss: 0.006591259501874447\n",
      "[step: 823] loss: 48.462745666503906\n",
      "[step: 823] loss: 0.006594897247850895\n",
      "[step: 824] loss: 50.380615234375\n",
      "[step: 824] loss: 0.006602751091122627\n",
      "[step: 825] loss: 52.72224426269531\n",
      "[step: 825] loss: 0.006613465026021004\n",
      "[step: 826] loss: 55.15264129638672\n",
      "[step: 826] loss: 0.0066336700692772865\n",
      "[step: 827] loss: 58.18324279785156\n",
      "[step: 827] loss: 0.006655687000602484\n",
      "[step: 828] loss: 61.87073516845703\n",
      "[step: 828] loss: 0.0066904048435389996\n",
      "[step: 829] loss: 65.54736328125\n",
      "[step: 829] loss: 0.006707431748509407\n",
      "[step: 830] loss: 62.80301284790039\n",
      "[step: 830] loss: 0.006717555224895477\n",
      "[step: 831] loss: 55.89146423339844\n",
      "[step: 831] loss: 0.006685645319521427\n",
      "[step: 832] loss: 48.125511169433594\n",
      "[step: 832] loss: 0.006644188426434994\n",
      "[step: 833] loss: 45.435752868652344\n",
      "[step: 833] loss: 0.006609308999031782\n",
      "[step: 834] loss: 48.268184661865234\n",
      "[step: 834] loss: 0.006600542925298214\n",
      "[step: 835] loss: 51.539581298828125\n",
      "[step: 835] loss: 0.00660568755120039\n",
      "[step: 836] loss: 52.979835510253906\n",
      "[step: 836] loss: 0.006605926435440779\n",
      "[step: 837] loss: 50.512489318847656\n",
      "[step: 837] loss: 0.0065971617586910725\n",
      "[step: 838] loss: 47.15478515625\n",
      "[step: 838] loss: 0.00658894469961524\n",
      "[step: 839] loss: 45.35902404785156\n",
      "[step: 839] loss: 0.00660031707957387\n",
      "[step: 840] loss: 45.883460998535156\n",
      "[step: 840] loss: 0.006622291635721922\n",
      "[step: 841] loss: 48.04353713989258\n",
      "[step: 841] loss: 0.006638919003307819\n",
      "[step: 842] loss: 49.12355041503906\n",
      "[step: 842] loss: 0.006627956405282021\n",
      "[step: 843] loss: 48.12290954589844\n",
      "[step: 843] loss: 0.006604819092899561\n",
      "[step: 844] loss: 45.69490051269531\n",
      "[step: 844] loss: 0.006584585178643465\n",
      "[step: 845] loss: 44.0079231262207\n",
      "[step: 845] loss: 0.006582139525562525\n",
      "[step: 846] loss: 44.29377746582031\n",
      "[step: 846] loss: 0.0065852380357682705\n",
      "[step: 847] loss: 45.817901611328125\n",
      "[step: 847] loss: 0.006580968853086233\n",
      "[step: 848] loss: 46.807830810546875\n",
      "[step: 848] loss: 0.006566222757101059\n",
      "[step: 849] loss: 46.207359313964844\n",
      "[step: 849] loss: 0.006556755397468805\n",
      "[step: 850] loss: 44.683998107910156\n",
      "[step: 850] loss: 0.006560612469911575\n",
      "[step: 851] loss: 43.47727966308594\n",
      "[step: 851] loss: 0.006570624653249979\n",
      "[step: 852] loss: 43.394371032714844\n",
      "[step: 852] loss: 0.006577719934284687\n",
      "[step: 853] loss: 44.17027282714844\n",
      "[step: 853] loss: 0.006576767656952143\n",
      "[step: 854] loss: 44.856361389160156\n",
      "[step: 854] loss: 0.006578588392585516\n",
      "[step: 855] loss: 44.87249755859375\n",
      "[step: 855] loss: 0.006588426418602467\n",
      "[step: 856] loss: 44.270381927490234\n",
      "[step: 856] loss: 0.006607008166611195\n",
      "[step: 857] loss: 43.589439392089844\n",
      "[step: 857] loss: 0.006626056041568518\n",
      "[step: 858] loss: 43.25837326049805\n",
      "[step: 858] loss: 0.006645940709859133\n",
      "[step: 859] loss: 43.37347412109375\n",
      "[step: 859] loss: 0.0066611082293093204\n",
      "[step: 860] loss: 43.59581756591797\n",
      "[step: 860] loss: 0.006679735612124205\n",
      "[step: 861] loss: 43.68282699584961\n",
      "[step: 861] loss: 0.006678123492747545\n",
      "[step: 862] loss: 43.48677444458008\n",
      "[step: 862] loss: 0.00666356785222888\n",
      "[step: 863] loss: 43.12415313720703\n",
      "[step: 863] loss: 0.006615799386054277\n",
      "[step: 864] loss: 42.77796173095703\n",
      "[step: 864] loss: 0.006570662837475538\n",
      "[step: 865] loss: 42.611549377441406\n",
      "[step: 865] loss: 0.006548296194523573\n",
      "[step: 866] loss: 42.64152526855469\n",
      "[step: 866] loss: 0.006553338840603828\n",
      "[step: 867] loss: 42.7966423034668\n",
      "[step: 867] loss: 0.006574793253093958\n",
      "[step: 868] loss: 43.034114837646484\n",
      "[step: 868] loss: 0.006598351057618856\n",
      "[step: 869] loss: 43.163818359375\n",
      "[step: 869] loss: 0.0066233123652637005\n",
      "[step: 870] loss: 43.326499938964844\n",
      "[step: 870] loss: 0.006637725979089737\n",
      "[step: 871] loss: 43.24701690673828\n",
      "[step: 871] loss: 0.006639807019382715\n",
      "[step: 872] loss: 43.33928298950195\n",
      "[step: 872] loss: 0.006617074832320213\n",
      "[step: 873] loss: 43.24279022216797\n",
      "[step: 873] loss: 0.006581485271453857\n",
      "[step: 874] loss: 43.542572021484375\n",
      "[step: 874] loss: 0.0065494319424033165\n",
      "[step: 875] loss: 43.876670837402344\n",
      "[step: 875] loss: 0.0065391636453568935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 876] loss: 45.00678253173828\n",
      "[step: 876] loss: 0.006551402620971203\n",
      "[step: 877] loss: 46.664878845214844\n",
      "[step: 877] loss: 0.006573868449777365\n",
      "[step: 878] loss: 50.07313537597656\n",
      "[step: 878] loss: 0.006594395264983177\n",
      "[step: 879] loss: 54.868377685546875\n",
      "[step: 879] loss: 0.006603231653571129\n",
      "[step: 880] loss: 62.60187530517578\n",
      "[step: 880] loss: 0.00660101929679513\n",
      "[step: 881] loss: 69.98629760742188\n",
      "[step: 881] loss: 0.006585872732102871\n",
      "[step: 882] loss: 74.58684539794922\n",
      "[step: 882] loss: 0.006568056531250477\n",
      "[step: 883] loss: 69.30184936523438\n",
      "[step: 883] loss: 0.006550510413944721\n",
      "[step: 884] loss: 55.374053955078125\n",
      "[step: 884] loss: 0.006540444679558277\n",
      "[step: 885] loss: 44.396148681640625\n",
      "[step: 885] loss: 0.00653812475502491\n",
      "[step: 886] loss: 46.27375411987305\n",
      "[step: 886] loss: 0.006542399525642395\n",
      "[step: 887] loss: 55.45956802368164\n",
      "[step: 887] loss: 0.006551826372742653\n",
      "[step: 888] loss: 56.279483795166016\n",
      "[step: 888] loss: 0.006564175710082054\n",
      "[step: 889] loss: 49.333168029785156\n",
      "[step: 889] loss: 0.006579292472451925\n",
      "[step: 890] loss: 44.02200698852539\n",
      "[step: 890] loss: 0.006585812661796808\n",
      "[step: 891] loss: 46.6132698059082\n",
      "[step: 891] loss: 0.006586885545402765\n",
      "[step: 892] loss: 50.58100128173828\n",
      "[step: 892] loss: 0.006573285907506943\n",
      "[step: 893] loss: 49.60553741455078\n",
      "[step: 893] loss: 0.0065566240809857845\n",
      "[step: 894] loss: 46.33046340942383\n",
      "[step: 894] loss: 0.006539598107337952\n",
      "[step: 895] loss: 43.063995361328125\n",
      "[step: 895] loss: 0.006529462989419699\n",
      "[step: 896] loss: 44.740928649902344\n",
      "[step: 896] loss: 0.00652634771540761\n",
      "[step: 897] loss: 48.179931640625\n",
      "[step: 897] loss: 0.006527639925479889\n",
      "[step: 898] loss: 46.091270446777344\n",
      "[step: 898] loss: 0.006531362887471914\n",
      "[step: 899] loss: 42.38514709472656\n",
      "[step: 899] loss: 0.006536285392940044\n",
      "[step: 900] loss: 42.44906997680664\n",
      "[step: 900] loss: 0.006543941795825958\n",
      "[step: 901] loss: 45.05005645751953\n",
      "[step: 901] loss: 0.006553088780492544\n",
      "[step: 902] loss: 45.23690414428711\n",
      "[step: 902] loss: 0.006564601324498653\n",
      "[step: 903] loss: 42.7459831237793\n",
      "[step: 903] loss: 0.006574960425496101\n",
      "[step: 904] loss: 42.34272003173828\n",
      "[step: 904] loss: 0.006584095768630505\n",
      "[step: 905] loss: 42.84872817993164\n",
      "[step: 905] loss: 0.006586222909390926\n",
      "[step: 906] loss: 42.72493362426758\n",
      "[step: 906] loss: 0.006583914626389742\n",
      "[step: 907] loss: 43.03575897216797\n",
      "[step: 907] loss: 0.006573632825165987\n",
      "[step: 908] loss: 43.002532958984375\n",
      "[step: 908] loss: 0.00656163552775979\n",
      "[step: 909] loss: 41.765228271484375\n",
      "[step: 909] loss: 0.006547378376126289\n",
      "[step: 910] loss: 41.09071350097656\n",
      "[step: 910] loss: 0.006536857225000858\n",
      "[step: 911] loss: 41.95124816894531\n",
      "[step: 911] loss: 0.00652832817286253\n",
      "[step: 912] loss: 42.39430236816406\n",
      "[step: 912] loss: 0.00652349554002285\n",
      "[step: 913] loss: 41.720420837402344\n",
      "[step: 913] loss: 0.006519670598208904\n",
      "[step: 914] loss: 41.30194091796875\n",
      "[step: 914] loss: 0.00651659956201911\n",
      "[step: 915] loss: 41.31913375854492\n",
      "[step: 915] loss: 0.006513780448585749\n",
      "[step: 916] loss: 41.055423736572266\n",
      "[step: 916] loss: 0.006511454004794359\n",
      "[step: 917] loss: 40.99810028076172\n",
      "[step: 917] loss: 0.006510251667350531\n",
      "[step: 918] loss: 41.39349365234375\n",
      "[step: 918] loss: 0.006510530598461628\n",
      "[step: 919] loss: 41.390541076660156\n",
      "[step: 919] loss: 0.006512841209769249\n",
      "[step: 920] loss: 40.90281295776367\n",
      "[step: 920] loss: 0.006517738103866577\n",
      "[step: 921] loss: 40.587242126464844\n",
      "[step: 921] loss: 0.006527206394821405\n",
      "[step: 922] loss: 40.619285583496094\n",
      "[step: 922] loss: 0.006541921757161617\n",
      "[step: 923] loss: 40.52272033691406\n",
      "[step: 923] loss: 0.006569516379386187\n",
      "[step: 924] loss: 40.472129821777344\n",
      "[step: 924] loss: 0.006606482435017824\n",
      "[step: 925] loss: 40.6612663269043\n",
      "[step: 925] loss: 0.006669062655419111\n",
      "[step: 926] loss: 40.810699462890625\n",
      "[step: 926] loss: 0.006720866076648235\n",
      "[step: 927] loss: 40.73365020751953\n",
      "[step: 927] loss: 0.006762242875993252\n",
      "[step: 928] loss: 40.78761291503906\n",
      "[step: 928] loss: 0.0067179324105381966\n",
      "[step: 929] loss: 40.967071533203125\n",
      "[step: 929] loss: 0.006627530325204134\n",
      "[step: 930] loss: 41.53089904785156\n",
      "[step: 930] loss: 0.006549983285367489\n",
      "[step: 931] loss: 41.90534973144531\n",
      "[step: 931] loss: 0.006545217242091894\n",
      "[step: 932] loss: 43.36126708984375\n",
      "[step: 932] loss: 0.006590561475604773\n",
      "[step: 933] loss: 44.28250503540039\n",
      "[step: 933] loss: 0.0066202059388160706\n",
      "[step: 934] loss: 46.08592987060547\n",
      "[step: 934] loss: 0.006629645824432373\n",
      "[step: 935] loss: 45.27996063232422\n",
      "[step: 935] loss: 0.006613222882151604\n",
      "[step: 936] loss: 44.50614929199219\n",
      "[step: 936] loss: 0.006611413788050413\n",
      "[step: 937] loss: 42.86386489868164\n",
      "[step: 937] loss: 0.006599725689738989\n",
      "[step: 938] loss: 42.32383346557617\n",
      "[step: 938] loss: 0.006574082653969526\n",
      "[step: 939] loss: 43.59154510498047\n",
      "[step: 939] loss: 0.006533006206154823\n",
      "[step: 940] loss: 46.31291961669922\n",
      "[step: 940] loss: 0.006518574431538582\n",
      "[step: 941] loss: 50.677303314208984\n",
      "[step: 941] loss: 0.006555221509188414\n",
      "[step: 942] loss: 50.910194396972656\n",
      "[step: 942] loss: 0.00658014789223671\n",
      "[step: 943] loss: 49.98607635498047\n",
      "[step: 943] loss: 0.006555314175784588\n",
      "[step: 944] loss: 47.57093811035156\n",
      "[step: 944] loss: 0.006506193429231644\n",
      "[step: 945] loss: 45.635841369628906\n",
      "[step: 945] loss: 0.006499164272099733\n",
      "[step: 946] loss: 44.24152374267578\n",
      "[step: 946] loss: 0.00652686832472682\n",
      "[step: 947] loss: 43.03041076660156\n",
      "[step: 947] loss: 0.006531068589538336\n",
      "[step: 948] loss: 42.26149368286133\n",
      "[step: 948] loss: 0.006507000420242548\n",
      "[step: 949] loss: 41.519378662109375\n",
      "[step: 949] loss: 0.006496828980743885\n",
      "[step: 950] loss: 42.35407257080078\n",
      "[step: 950] loss: 0.006512399297207594\n",
      "[step: 951] loss: 43.71846008300781\n",
      "[step: 951] loss: 0.006516787223517895\n",
      "[step: 952] loss: 43.875885009765625\n",
      "[step: 952] loss: 0.006499971263110638\n",
      "[step: 953] loss: 42.13803482055664\n",
      "[step: 953] loss: 0.0064869713969528675\n",
      "[step: 954] loss: 39.97496032714844\n",
      "[step: 954] loss: 0.006493629887700081\n",
      "[step: 955] loss: 39.183406829833984\n",
      "[step: 955] loss: 0.0064997682347893715\n",
      "[step: 956] loss: 39.98894500732422\n",
      "[step: 956] loss: 0.006491077598184347\n",
      "[step: 957] loss: 41.209510803222656\n",
      "[step: 957] loss: 0.006482325028628111\n",
      "[step: 958] loss: 41.63356018066406\n",
      "[step: 958] loss: 0.006486831698566675\n",
      "[step: 959] loss: 41.0728645324707\n",
      "[step: 959] loss: 0.006493456196039915\n",
      "[step: 960] loss: 40.1484375\n",
      "[step: 960] loss: 0.006490407511591911\n",
      "[step: 961] loss: 39.83700180053711\n",
      "[step: 961] loss: 0.006484226323664188\n",
      "[step: 962] loss: 40.13069152832031\n",
      "[step: 962] loss: 0.006485908757895231\n",
      "[step: 963] loss: 40.75444030761719\n",
      "[step: 963] loss: 0.0064924489706754684\n",
      "[step: 964] loss: 40.262107849121094\n",
      "[step: 964] loss: 0.0064931935630738735\n",
      "[step: 965] loss: 39.601959228515625\n",
      "[step: 965] loss: 0.006492353044450283\n",
      "[step: 966] loss: 38.91625213623047\n",
      "[step: 966] loss: 0.006496539805084467\n",
      "[step: 967] loss: 38.74165344238281\n",
      "[step: 967] loss: 0.00650949077680707\n",
      "[step: 968] loss: 38.949363708496094\n",
      "[step: 968] loss: 0.006526189390569925\n",
      "[step: 969] loss: 39.19499969482422\n",
      "[step: 969] loss: 0.006548344157636166\n",
      "[step: 970] loss: 39.21179962158203\n",
      "[step: 970] loss: 0.006578919477760792\n",
      "[step: 971] loss: 38.898765563964844\n",
      "[step: 971] loss: 0.006626690737903118\n",
      "[step: 972] loss: 38.53919982910156\n",
      "[step: 972] loss: 0.006652184762060642\n",
      "[step: 973] loss: 38.32176971435547\n",
      "[step: 973] loss: 0.006654773838818073\n",
      "[step: 974] loss: 38.31721496582031\n",
      "[step: 974] loss: 0.00659938994795084\n",
      "[step: 975] loss: 38.46461486816406\n",
      "[step: 975] loss: 0.006530976854264736\n",
      "[step: 976] loss: 38.6688232421875\n",
      "[step: 976] loss: 0.006482512224465609\n",
      "[step: 977] loss: 38.912742614746094\n",
      "[step: 977] loss: 0.006476691924035549\n",
      "[step: 978] loss: 39.29709243774414\n",
      "[step: 978] loss: 0.0065085552632808685\n",
      "[step: 979] loss: 40.142555236816406\n",
      "[step: 979] loss: 0.006558431312441826\n",
      "[step: 980] loss: 42.35507583618164\n",
      "[step: 980] loss: 0.006609885487705469\n",
      "[step: 981] loss: 47.76853942871094\n",
      "[step: 981] loss: 0.006624337285757065\n",
      "[step: 982] loss: 61.929176330566406\n",
      "[step: 982] loss: 0.006586561910808086\n",
      "[step: 983] loss: 81.11224365234375\n",
      "[step: 983] loss: 0.006512380205094814\n",
      "[step: 984] loss: 109.10559844970703\n",
      "[step: 984] loss: 0.006469343323260546\n",
      "[step: 985] loss: 107.80296325683594\n",
      "[step: 985] loss: 0.006486228667199612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 986] loss: 87.0636215209961\n",
      "[step: 986] loss: 0.006534473039209843\n",
      "[step: 987] loss: 68.64192962646484\n",
      "[step: 987] loss: 0.00657733203843236\n",
      "[step: 988] loss: 64.14006805419922\n",
      "[step: 988] loss: 0.006583172362297773\n",
      "[step: 989] loss: 71.15074157714844\n",
      "[step: 989] loss: 0.0065515656024217606\n",
      "[step: 990] loss: 60.62156677246094\n",
      "[step: 990] loss: 0.006493222899734974\n",
      "[step: 991] loss: 59.993194580078125\n",
      "[step: 991] loss: 0.006462414283305407\n",
      "[step: 992] loss: 56.48023223876953\n",
      "[step: 992] loss: 0.006478538736701012\n",
      "[step: 993] loss: 60.123287200927734\n",
      "[step: 993] loss: 0.0065157231874763966\n",
      "[step: 994] loss: 54.460113525390625\n",
      "[step: 994] loss: 0.00654068635776639\n",
      "[step: 995] loss: 53.86503982543945\n",
      "[step: 995] loss: 0.006528419442474842\n",
      "[step: 996] loss: 50.782554626464844\n",
      "[step: 996] loss: 0.006494386121630669\n",
      "[step: 997] loss: 52.80331802368164\n",
      "[step: 997] loss: 0.006462583784013987\n",
      "[step: 998] loss: 48.802772521972656\n",
      "[step: 998] loss: 0.006457490846514702\n",
      "[step: 999] loss: 47.568729400634766\n",
      "[step: 999] loss: 0.006476052571088076\n",
      "[step: 1000] loss: 49.784202575683594\n",
      "[step: 1000] loss: 0.006496497429907322\n",
      "[step: 1001] loss: 43.12054443359375\n",
      "[step: 1001] loss: 0.006505373865365982\n",
      "[step: 1002] loss: 50.40013885498047\n",
      "[step: 1002] loss: 0.006495397072285414\n",
      "[step: 1003] loss: 42.91869354248047\n",
      "[step: 1003] loss: 0.006474841386079788\n",
      "[step: 1004] loss: 44.93096923828125\n",
      "[step: 1004] loss: 0.006455505732446909\n",
      "[step: 1005] loss: 46.31769561767578\n",
      "[step: 1005] loss: 0.006450142711400986\n",
      "[step: 1006] loss: 42.996376037597656\n",
      "[step: 1006] loss: 0.0064582000486552715\n",
      "[step: 1007] loss: 43.945377349853516\n",
      "[step: 1007] loss: 0.006470783613622189\n",
      "[step: 1008] loss: 43.98896789550781\n",
      "[step: 1008] loss: 0.006480787880718708\n",
      "[step: 1009] loss: 40.5293083190918\n",
      "[step: 1009] loss: 0.006482037715613842\n",
      "[step: 1010] loss: 44.04431915283203\n",
      "[step: 1010] loss: 0.006474449764937162\n",
      "[step: 1011] loss: 40.83733367919922\n",
      "[step: 1011] loss: 0.006460952572524548\n",
      "[step: 1012] loss: 40.29850769042969\n",
      "[step: 1012] loss: 0.006449160166084766\n",
      "[step: 1013] loss: 42.092201232910156\n",
      "[step: 1013] loss: 0.006443149875849485\n",
      "[step: 1014] loss: 39.62730026245117\n",
      "[step: 1014] loss: 0.006443589925765991\n",
      "[step: 1015] loss: 40.69480895996094\n",
      "[step: 1015] loss: 0.006448622792959213\n",
      "[step: 1016] loss: 39.912498474121094\n",
      "[step: 1016] loss: 0.006455364637076855\n",
      "[step: 1017] loss: 39.64505386352539\n",
      "[step: 1017] loss: 0.006461871787905693\n",
      "[step: 1018] loss: 39.47124481201172\n",
      "[step: 1018] loss: 0.006466730032116175\n",
      "[step: 1019] loss: 39.29120635986328\n",
      "[step: 1019] loss: 0.006470717489719391\n",
      "[step: 1020] loss: 39.55646514892578\n",
      "[step: 1020] loss: 0.006470676977187395\n",
      "[step: 1021] loss: 38.508636474609375\n",
      "[step: 1021] loss: 0.0064690494909882545\n",
      "[step: 1022] loss: 38.7451057434082\n",
      "[step: 1022] loss: 0.0064627500250935555\n",
      "[step: 1023] loss: 39.04325866699219\n",
      "[step: 1023] loss: 0.0064557334408164024\n",
      "[step: 1024] loss: 38.02178955078125\n",
      "[step: 1024] loss: 0.006447477266192436\n",
      "[step: 1025] loss: 38.549171447753906\n",
      "[step: 1025] loss: 0.006441229488700628\n",
      "[step: 1026] loss: 38.34234619140625\n",
      "[step: 1026] loss: 0.00643668370321393\n",
      "[step: 1027] loss: 37.622154235839844\n",
      "[step: 1027] loss: 0.006433816161006689\n",
      "[step: 1028] loss: 38.09489440917969\n",
      "[step: 1028] loss: 0.006432072725147009\n",
      "[step: 1029] loss: 37.85441589355469\n",
      "[step: 1029] loss: 0.0064310613088309765\n",
      "[step: 1030] loss: 37.715179443359375\n",
      "[step: 1030] loss: 0.006430431269109249\n",
      "[step: 1031] loss: 37.534942626953125\n",
      "[step: 1031] loss: 0.006429929751902819\n",
      "[step: 1032] loss: 37.44220733642578\n",
      "[step: 1032] loss: 0.006429565604776144\n",
      "[step: 1033] loss: 37.5970458984375\n",
      "[step: 1033] loss: 0.006429540459066629\n",
      "[step: 1034] loss: 37.086387634277344\n",
      "[step: 1034] loss: 0.006430287845432758\n",
      "[step: 1035] loss: 37.23565673828125\n",
      "[step: 1035] loss: 0.006432253401726484\n",
      "[step: 1036] loss: 37.276710510253906\n",
      "[step: 1036] loss: 0.0064368718303740025\n",
      "[step: 1037] loss: 37.05511474609375\n",
      "[step: 1037] loss: 0.006446142680943012\n",
      "[step: 1038] loss: 36.9183349609375\n",
      "[step: 1038] loss: 0.006465556100010872\n",
      "[step: 1039] loss: 36.97225570678711\n",
      "[step: 1039] loss: 0.006500791292637587\n",
      "[step: 1040] loss: 36.8477783203125\n",
      "[step: 1040] loss: 0.0065682437270879745\n",
      "[step: 1041] loss: 36.76866912841797\n",
      "[step: 1041] loss: 0.006657877471297979\n",
      "[step: 1042] loss: 36.67566680908203\n",
      "[step: 1042] loss: 0.006763813551515341\n",
      "[step: 1043] loss: 36.631553649902344\n",
      "[step: 1043] loss: 0.006762417033314705\n",
      "[step: 1044] loss: 36.71104431152344\n",
      "[step: 1044] loss: 0.0066350600682199\n",
      "[step: 1045] loss: 36.48711395263672\n",
      "[step: 1045] loss: 0.006479443516582251\n",
      "[step: 1046] loss: 36.48319625854492\n",
      "[step: 1046] loss: 0.006459220312535763\n",
      "[step: 1047] loss: 36.485618591308594\n",
      "[step: 1047] loss: 0.006556060630828142\n",
      "[step: 1048] loss: 36.41094970703125\n",
      "[step: 1048] loss: 0.006651338655501604\n",
      "[step: 1049] loss: 36.37001037597656\n",
      "[step: 1049] loss: 0.006701957434415817\n",
      "[step: 1050] loss: 36.289058685302734\n",
      "[step: 1050] loss: 0.006601371336728334\n",
      "[step: 1051] loss: 36.28282928466797\n",
      "[step: 1051] loss: 0.006505575962364674\n",
      "[step: 1052] loss: 36.279052734375\n",
      "[step: 1052] loss: 0.006531637627631426\n",
      "[step: 1053] loss: 36.25560760498047\n",
      "[step: 1053] loss: 0.006592919584363699\n",
      "[step: 1054] loss: 36.23518371582031\n",
      "[step: 1054] loss: 0.006563818547874689\n",
      "[step: 1055] loss: 36.34504318237305\n",
      "[step: 1055] loss: 0.006463708821684122\n",
      "[step: 1056] loss: 36.563026428222656\n",
      "[step: 1056] loss: 0.006498072762042284\n",
      "[step: 1057] loss: 36.91330337524414\n",
      "[step: 1057] loss: 0.006572195794433355\n",
      "[step: 1058] loss: 37.9058837890625\n",
      "[step: 1058] loss: 0.006529820151627064\n",
      "[step: 1059] loss: 39.310264587402344\n",
      "[step: 1059] loss: 0.0064413174986839294\n",
      "[step: 1060] loss: 42.90885925292969\n",
      "[step: 1060] loss: 0.0064636943861842155\n",
      "[step: 1061] loss: 46.9405517578125\n",
      "[step: 1061] loss: 0.006535512860864401\n",
      "[step: 1062] loss: 55.22332000732422\n",
      "[step: 1062] loss: 0.006502317264676094\n",
      "[step: 1063] loss: 55.632652282714844\n",
      "[step: 1063] loss: 0.006431016139686108\n",
      "[step: 1064] loss: 51.92781066894531\n",
      "[step: 1064] loss: 0.006458488758653402\n",
      "[step: 1065] loss: 42.926361083984375\n",
      "[step: 1065] loss: 0.00649351067841053\n",
      "[step: 1066] loss: 37.018211364746094\n",
      "[step: 1066] loss: 0.006443238817155361\n",
      "[step: 1067] loss: 39.05944061279297\n",
      "[step: 1067] loss: 0.006421979516744614\n",
      "[step: 1068] loss: 44.482364654541016\n",
      "[step: 1068] loss: 0.006451252847909927\n",
      "[step: 1069] loss: 45.843658447265625\n",
      "[step: 1069] loss: 0.006444485858082771\n",
      "[step: 1070] loss: 40.92107391357422\n",
      "[step: 1070] loss: 0.006416570395231247\n",
      "[step: 1071] loss: 36.33814239501953\n",
      "[step: 1071] loss: 0.006425841711461544\n",
      "[step: 1072] loss: 37.05702209472656\n",
      "[step: 1072] loss: 0.006436571478843689\n",
      "[step: 1073] loss: 40.611759185791016\n",
      "[step: 1073] loss: 0.006419628392904997\n",
      "[step: 1074] loss: 41.885475158691406\n",
      "[step: 1074] loss: 0.006412541028112173\n",
      "[step: 1075] loss: 39.25373077392578\n",
      "[step: 1075] loss: 0.006422912236303091\n",
      "[step: 1076] loss: 36.50931930541992\n",
      "[step: 1076] loss: 0.006422007456421852\n",
      "[step: 1077] loss: 36.3140869140625\n",
      "[step: 1077] loss: 0.0064070760272443295\n",
      "[step: 1078] loss: 38.005149841308594\n",
      "[step: 1078] loss: 0.0064107757061719894\n",
      "[step: 1079] loss: 39.04588317871094\n",
      "[step: 1079] loss: 0.006417210213840008\n",
      "[step: 1080] loss: 38.124656677246094\n",
      "[step: 1080] loss: 0.006407325156033039\n",
      "[step: 1081] loss: 36.61627197265625\n",
      "[step: 1081] loss: 0.0064030978828668594\n",
      "[step: 1082] loss: 35.66526794433594\n",
      "[step: 1082] loss: 0.006407357286661863\n",
      "[step: 1083] loss: 36.3011589050293\n",
      "[step: 1083] loss: 0.006407490000128746\n",
      "[step: 1084] loss: 37.465553283691406\n",
      "[step: 1084] loss: 0.006399865262210369\n",
      "[step: 1085] loss: 37.49956512451172\n",
      "[step: 1085] loss: 0.006399634759873152\n",
      "[step: 1086] loss: 36.77069854736328\n",
      "[step: 1086] loss: 0.006403067149221897\n",
      "[step: 1087] loss: 35.506797790527344\n",
      "[step: 1087] loss: 0.00640050508081913\n",
      "[step: 1088] loss: 35.24528503417969\n",
      "[step: 1088] loss: 0.006395765580236912\n",
      "[step: 1089] loss: 35.918182373046875\n",
      "[step: 1089] loss: 0.006396087817847729\n",
      "[step: 1090] loss: 36.60298538208008\n",
      "[step: 1090] loss: 0.0063979108817875385\n",
      "[step: 1091] loss: 36.93265914916992\n",
      "[step: 1091] loss: 0.0063953460194170475\n",
      "[step: 1092] loss: 36.23603057861328\n",
      "[step: 1092] loss: 0.006392233539372683\n",
      "[step: 1093] loss: 35.50238800048828\n",
      "[step: 1093] loss: 0.006392807699739933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1094] loss: 35.085655212402344\n",
      "[step: 1094] loss: 0.006393624935299158\n",
      "[step: 1095] loss: 35.02873611450195\n",
      "[step: 1095] loss: 0.006391954608261585\n",
      "[step: 1096] loss: 35.21726989746094\n",
      "[step: 1096] loss: 0.006390486843883991\n",
      "[step: 1097] loss: 35.43863296508789\n",
      "[step: 1097] loss: 0.0063917916268110275\n",
      "[step: 1098] loss: 35.660701751708984\n",
      "[step: 1098] loss: 0.006394758354872465\n",
      "[step: 1099] loss: 35.765464782714844\n",
      "[step: 1099] loss: 0.006397892255336046\n",
      "[step: 1100] loss: 35.77386474609375\n",
      "[step: 1100] loss: 0.006405831314623356\n",
      "[step: 1101] loss: 35.601409912109375\n",
      "[step: 1101] loss: 0.006424768827855587\n",
      "[step: 1102] loss: 35.32720184326172\n",
      "[step: 1102] loss: 0.006462476681917906\n",
      "[step: 1103] loss: 35.017234802246094\n",
      "[step: 1103] loss: 0.006534506566822529\n",
      "[step: 1104] loss: 34.77637481689453\n",
      "[step: 1104] loss: 0.006652424577623606\n",
      "[step: 1105] loss: 34.6361083984375\n",
      "[step: 1105] loss: 0.006805249024182558\n",
      "[step: 1106] loss: 34.554100036621094\n",
      "[step: 1106] loss: 0.006872138939797878\n",
      "[step: 1107] loss: 34.48925018310547\n",
      "[step: 1107] loss: 0.006718793883919716\n",
      "[step: 1108] loss: 34.422698974609375\n",
      "[step: 1108] loss: 0.006470028311014175\n",
      "[step: 1109] loss: 34.346153259277344\n",
      "[step: 1109] loss: 0.006438728887587786\n",
      "[step: 1110] loss: 34.29856491088867\n",
      "[step: 1110] loss: 0.006603904068470001\n",
      "[step: 1111] loss: 34.2763671875\n",
      "[step: 1111] loss: 0.0067488583736121655\n",
      "[step: 1112] loss: 34.284507751464844\n",
      "[step: 1112] loss: 0.006768323015421629\n",
      "[step: 1113] loss: 34.33710861206055\n",
      "[step: 1113] loss: 0.006552266422659159\n",
      "[step: 1114] loss: 34.43452835083008\n",
      "[step: 1114] loss: 0.006440339609980583\n",
      "[step: 1115] loss: 34.707611083984375\n",
      "[step: 1115] loss: 0.006556149572134018\n",
      "[step: 1116] loss: 35.30717468261719\n",
      "[step: 1116] loss: 0.006620003841817379\n",
      "[step: 1117] loss: 36.92674255371094\n",
      "[step: 1117] loss: 0.006503233220428228\n",
      "[step: 1118] loss: 40.267784118652344\n",
      "[step: 1118] loss: 0.006406718865036964\n",
      "[step: 1119] loss: 48.51882553100586\n",
      "[step: 1119] loss: 0.006465763784945011\n",
      "[step: 1120] loss: 60.58968734741211\n",
      "[step: 1120] loss: 0.0065447185188531876\n",
      "[step: 1121] loss: 77.42353057861328\n",
      "[step: 1121] loss: 0.0064750234596431255\n",
      "[step: 1122] loss: 79.03117370605469\n",
      "[step: 1122] loss: 0.006400397513061762\n",
      "[step: 1123] loss: 61.40428161621094\n",
      "[step: 1123] loss: 0.006448869127780199\n",
      "[step: 1124] loss: 43.987945556640625\n",
      "[step: 1124] loss: 0.006491435691714287\n",
      "[step: 1125] loss: 46.32137680053711\n",
      "[step: 1125] loss: 0.006454126909375191\n",
      "[step: 1126] loss: 57.817962646484375\n",
      "[step: 1126] loss: 0.00639589736238122\n",
      "[step: 1127] loss: 52.116722106933594\n",
      "[step: 1127] loss: 0.006413762457668781\n",
      "[step: 1128] loss: 45.48744201660156\n",
      "[step: 1128] loss: 0.0064434329979121685\n",
      "[step: 1129] loss: 45.07592010498047\n",
      "[step: 1129] loss: 0.006432714406400919\n",
      "[step: 1130] loss: 46.701927185058594\n",
      "[step: 1130] loss: 0.006390587892383337\n",
      "[step: 1131] loss: 47.729835510253906\n",
      "[step: 1131] loss: 0.006395820993930101\n",
      "[step: 1132] loss: 41.94190216064453\n",
      "[step: 1132] loss: 0.006416283547878265\n",
      "[step: 1133] loss: 39.3897590637207\n",
      "[step: 1133] loss: 0.0064224256202578545\n",
      "[step: 1134] loss: 45.22917938232422\n",
      "[step: 1134] loss: 0.006390097551047802\n",
      "[step: 1135] loss: 43.54289627075195\n",
      "[step: 1135] loss: 0.006380659062415361\n",
      "[step: 1136] loss: 36.02283477783203\n",
      "[step: 1136] loss: 0.006390274036675692\n",
      "[step: 1137] loss: 40.09488296508789\n",
      "[step: 1137] loss: 0.006403832230716944\n",
      "[step: 1138] loss: 43.47557067871094\n",
      "[step: 1138] loss: 0.006384965498000383\n",
      "[step: 1139] loss: 37.145294189453125\n",
      "[step: 1139] loss: 0.006375570315867662\n",
      "[step: 1140] loss: 36.217857360839844\n",
      "[step: 1140] loss: 0.006375911179929972\n",
      "[step: 1141] loss: 40.14228820800781\n",
      "[step: 1141] loss: 0.006385072134435177\n",
      "[step: 1142] loss: 38.775390625\n",
      "[step: 1142] loss: 0.006379494443535805\n",
      "[step: 1143] loss: 35.07648468017578\n",
      "[step: 1143] loss: 0.006376313976943493\n",
      "[step: 1144] loss: 37.36100387573242\n",
      "[step: 1144] loss: 0.006371790077537298\n",
      "[step: 1145] loss: 38.161529541015625\n",
      "[step: 1145] loss: 0.00637159263715148\n",
      "[step: 1146] loss: 35.58702087402344\n",
      "[step: 1146] loss: 0.006368197035044432\n",
      "[step: 1147] loss: 35.90619659423828\n",
      "[step: 1147] loss: 0.006371555384248495\n",
      "[step: 1148] loss: 36.282188415527344\n",
      "[step: 1148] loss: 0.006373575888574123\n",
      "[step: 1149] loss: 35.7540397644043\n",
      "[step: 1149] loss: 0.006371192168444395\n",
      "[step: 1150] loss: 35.52403259277344\n",
      "[step: 1150] loss: 0.006364299450069666\n",
      "[step: 1151] loss: 35.495872497558594\n",
      "[step: 1151] loss: 0.006361728999763727\n",
      "[step: 1152] loss: 34.752601623535156\n",
      "[step: 1152] loss: 0.00636184262111783\n",
      "[step: 1153] loss: 34.39341354370117\n",
      "[step: 1153] loss: 0.006364101078361273\n",
      "[step: 1154] loss: 35.247406005859375\n",
      "[step: 1154] loss: 0.00636427104473114\n",
      "[step: 1155] loss: 35.029510498046875\n",
      "[step: 1155] loss: 0.006362691521644592\n",
      "[step: 1156] loss: 33.942684173583984\n",
      "[step: 1156] loss: 0.006359749939292669\n",
      "[step: 1157] loss: 33.93378448486328\n",
      "[step: 1157] loss: 0.006357226055115461\n",
      "[step: 1158] loss: 34.547508239746094\n",
      "[step: 1158] loss: 0.006356468424201012\n",
      "[step: 1159] loss: 34.40485382080078\n",
      "[step: 1159] loss: 0.006355947814881802\n",
      "[step: 1160] loss: 33.67438507080078\n",
      "[step: 1160] loss: 0.006355702877044678\n",
      "[step: 1161] loss: 33.4915771484375\n",
      "[step: 1161] loss: 0.006355048157274723\n",
      "[step: 1162] loss: 33.92198181152344\n",
      "[step: 1162] loss: 0.00635536340996623\n",
      "[step: 1163] loss: 33.954105377197266\n",
      "[step: 1163] loss: 0.006354873068630695\n",
      "[step: 1164] loss: 33.652191162109375\n",
      "[step: 1164] loss: 0.006353945937007666\n",
      "[step: 1165] loss: 33.55568313598633\n",
      "[step: 1165] loss: 0.006351730786263943\n",
      "[step: 1166] loss: 33.54985427856445\n",
      "[step: 1166] loss: 0.006350256968289614\n",
      "[step: 1167] loss: 33.34044647216797\n",
      "[step: 1167] loss: 0.006349178496748209\n",
      "[step: 1168] loss: 33.1236457824707\n",
      "[step: 1168] loss: 0.006348861381411552\n",
      "[step: 1169] loss: 33.29485321044922\n",
      "[step: 1169] loss: 0.006348252296447754\n",
      "[step: 1170] loss: 33.46626281738281\n",
      "[step: 1170] loss: 0.0063477386720478535\n",
      "[step: 1171] loss: 33.379398345947266\n",
      "[step: 1171] loss: 0.006347422953695059\n",
      "[step: 1172] loss: 33.17208480834961\n",
      "[step: 1172] loss: 0.006347157992422581\n",
      "[step: 1173] loss: 33.01375198364258\n",
      "[step: 1173] loss: 0.006346903741359711\n",
      "[step: 1174] loss: 32.99924850463867\n",
      "[step: 1174] loss: 0.006346529349684715\n",
      "[step: 1175] loss: 32.96192169189453\n",
      "[step: 1175] loss: 0.006346511654555798\n",
      "[step: 1176] loss: 32.81005096435547\n",
      "[step: 1176] loss: 0.006346169859170914\n",
      "[step: 1177] loss: 32.71851348876953\n",
      "[step: 1177] loss: 0.006346084643155336\n",
      "[step: 1178] loss: 32.755950927734375\n",
      "[step: 1178] loss: 0.006346000358462334\n",
      "[step: 1179] loss: 32.8287467956543\n",
      "[step: 1179] loss: 0.006346550770103931\n",
      "[step: 1180] loss: 32.854312896728516\n",
      "[step: 1180] loss: 0.006347426678985357\n",
      "[step: 1181] loss: 32.855751037597656\n",
      "[step: 1181] loss: 0.006349269766360521\n",
      "[step: 1182] loss: 32.93873596191406\n",
      "[step: 1182] loss: 0.006352008320391178\n",
      "[step: 1183] loss: 33.17488098144531\n",
      "[step: 1183] loss: 0.006356863770633936\n",
      "[step: 1184] loss: 33.56873321533203\n",
      "[step: 1184] loss: 0.006363776046782732\n",
      "[step: 1185] loss: 34.23698043823242\n",
      "[step: 1185] loss: 0.006376608274877071\n",
      "[step: 1186] loss: 35.43527603149414\n",
      "[step: 1186] loss: 0.006391207221895456\n",
      "[step: 1187] loss: 37.536197662353516\n",
      "[step: 1187] loss: 0.006414082832634449\n",
      "[step: 1188] loss: 40.49793243408203\n",
      "[step: 1188] loss: 0.00643057469278574\n",
      "[step: 1189] loss: 44.772735595703125\n",
      "[step: 1189] loss: 0.006448188330978155\n",
      "[step: 1190] loss: 45.93562316894531\n",
      "[step: 1190] loss: 0.006442067213356495\n",
      "[step: 1191] loss: 43.60464096069336\n",
      "[step: 1191] loss: 0.006422528997063637\n",
      "[step: 1192] loss: 38.06511688232422\n",
      "[step: 1192] loss: 0.0063896034844219685\n",
      "[step: 1193] loss: 33.87964630126953\n",
      "[step: 1193] loss: 0.006360589060932398\n",
      "[step: 1194] loss: 34.16344451904297\n",
      "[step: 1194] loss: 0.006345190107822418\n",
      "[step: 1195] loss: 36.82749938964844\n",
      "[step: 1195] loss: 0.006343759596347809\n",
      "[step: 1196] loss: 38.57667922973633\n",
      "[step: 1196] loss: 0.006350633688271046\n",
      "[step: 1197] loss: 37.44586944580078\n",
      "[step: 1197] loss: 0.006358671002089977\n",
      "[step: 1198] loss: 34.63179016113281\n",
      "[step: 1198] loss: 0.006368834525346756\n",
      "[step: 1199] loss: 32.82050323486328\n",
      "[step: 1199] loss: 0.00637799734249711\n",
      "[step: 1200] loss: 33.569419860839844\n",
      "[step: 1200] loss: 0.006385694723576307\n",
      "[step: 1201] loss: 35.48572540283203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1201] loss: 0.00639003049582243\n",
      "[step: 1202] loss: 35.92715072631836\n",
      "[step: 1202] loss: 0.006388956215232611\n",
      "[step: 1203] loss: 34.23594284057617\n",
      "[step: 1203] loss: 0.006381349638104439\n",
      "[step: 1204] loss: 32.49156188964844\n",
      "[step: 1204] loss: 0.006368894595652819\n",
      "[step: 1205] loss: 32.43719482421875\n",
      "[step: 1205] loss: 0.006353241857141256\n",
      "[step: 1206] loss: 33.646583557128906\n",
      "[step: 1206] loss: 0.00633937818929553\n",
      "[step: 1207] loss: 34.434425354003906\n",
      "[step: 1207] loss: 0.006330446805804968\n",
      "[step: 1208] loss: 33.93791580200195\n",
      "[step: 1208] loss: 0.006327811162918806\n",
      "[step: 1209] loss: 32.84267044067383\n",
      "[step: 1209] loss: 0.006329862400889397\n",
      "[step: 1210] loss: 32.188758850097656\n",
      "[step: 1210] loss: 0.00633343867957592\n",
      "[step: 1211] loss: 32.36418151855469\n",
      "[step: 1211] loss: 0.006335605401545763\n",
      "[step: 1212] loss: 32.88172149658203\n",
      "[step: 1212] loss: 0.006335014011710882\n",
      "[step: 1213] loss: 33.166015625\n",
      "[step: 1213] loss: 0.006332279648631811\n",
      "[step: 1214] loss: 33.11865234375\n",
      "[step: 1214] loss: 0.006328829098492861\n",
      "[step: 1215] loss: 32.88121032714844\n",
      "[step: 1215] loss: 0.006326968315988779\n",
      "[step: 1216] loss: 32.55189514160156\n",
      "[step: 1216] loss: 0.006328287068754435\n",
      "[step: 1217] loss: 32.28907775878906\n",
      "[step: 1217] loss: 0.00633402680978179\n",
      "[step: 1218] loss: 31.968725204467773\n",
      "[step: 1218] loss: 0.006343821529299021\n",
      "[step: 1219] loss: 31.805278778076172\n",
      "[step: 1219] loss: 0.006362453103065491\n",
      "[step: 1220] loss: 31.84149742126465\n",
      "[step: 1220] loss: 0.0063820346258580685\n",
      "[step: 1221] loss: 32.05400848388672\n",
      "[step: 1221] loss: 0.006416962016373873\n",
      "[step: 1222] loss: 32.37044906616211\n",
      "[step: 1222] loss: 0.006443263031542301\n",
      "[step: 1223] loss: 32.540653228759766\n",
      "[step: 1223] loss: 0.00647727632895112\n",
      "[step: 1224] loss: 32.70330047607422\n",
      "[step: 1224] loss: 0.006490901578217745\n",
      "[step: 1225] loss: 32.719444274902344\n",
      "[step: 1225] loss: 0.006486887577921152\n",
      "[step: 1226] loss: 32.84954071044922\n",
      "[step: 1226] loss: 0.0064673712477087975\n",
      "[step: 1227] loss: 33.051300048828125\n",
      "[step: 1227] loss: 0.006438477896153927\n",
      "[step: 1228] loss: 33.35779571533203\n",
      "[step: 1228] loss: 0.006403010804206133\n",
      "[step: 1229] loss: 33.79558563232422\n",
      "[step: 1229] loss: 0.006374016869813204\n",
      "[step: 1230] loss: 34.23396301269531\n",
      "[step: 1230] loss: 0.0063522276468575\n",
      "[step: 1231] loss: 34.80192947387695\n",
      "[step: 1231] loss: 0.006346361245959997\n",
      "[step: 1232] loss: 35.40812683105469\n",
      "[step: 1232] loss: 0.006366197019815445\n",
      "[step: 1233] loss: 36.11553192138672\n",
      "[step: 1233] loss: 0.006395857781171799\n",
      "[step: 1234] loss: 36.71712875366211\n",
      "[step: 1234] loss: 0.006407884415239096\n",
      "[step: 1235] loss: 36.64800262451172\n",
      "[step: 1235] loss: 0.006386404857039452\n",
      "[step: 1236] loss: 35.984703063964844\n",
      "[step: 1236] loss: 0.0063494909554719925\n",
      "[step: 1237] loss: 34.469398498535156\n",
      "[step: 1237] loss: 0.006322167348116636\n",
      "[step: 1238] loss: 32.85011291503906\n",
      "[step: 1238] loss: 0.006323339883238077\n",
      "[step: 1239] loss: 31.718666076660156\n",
      "[step: 1239] loss: 0.006337596569210291\n",
      "[step: 1240] loss: 31.379043579101562\n",
      "[step: 1240] loss: 0.00633970508351922\n",
      "[step: 1241] loss: 31.719276428222656\n",
      "[step: 1241] loss: 0.006329016759991646\n",
      "[step: 1242] loss: 32.349998474121094\n",
      "[step: 1242] loss: 0.006321755703538656\n",
      "[step: 1243] loss: 33.02875518798828\n",
      "[step: 1243] loss: 0.00632948474958539\n",
      "[step: 1244] loss: 33.497955322265625\n",
      "[step: 1244] loss: 0.00634358124807477\n",
      "[step: 1245] loss: 33.670509338378906\n",
      "[step: 1245] loss: 0.006354824174195528\n",
      "[step: 1246] loss: 33.50330352783203\n",
      "[step: 1246] loss: 0.00634651118889451\n",
      "[step: 1247] loss: 32.969696044921875\n",
      "[step: 1247] loss: 0.006340722553431988\n",
      "[step: 1248] loss: 32.31078338623047\n",
      "[step: 1248] loss: 0.006336212158203125\n",
      "[step: 1249] loss: 31.65865707397461\n",
      "[step: 1249] loss: 0.006338254548609257\n",
      "[step: 1250] loss: 31.200490951538086\n",
      "[step: 1250] loss: 0.0063384296372532845\n",
      "[step: 1251] loss: 30.979122161865234\n",
      "[step: 1251] loss: 0.006334817968308926\n",
      "[step: 1252] loss: 30.976238250732422\n",
      "[step: 1252] loss: 0.006328956224024296\n",
      "[step: 1253] loss: 31.134658813476562\n",
      "[step: 1253] loss: 0.006323732901364565\n",
      "[step: 1254] loss: 31.405061721801758\n",
      "[step: 1254] loss: 0.006320039741694927\n",
      "[step: 1255] loss: 31.779403686523438\n",
      "[step: 1255] loss: 0.006317404098808765\n",
      "[step: 1256] loss: 32.24829864501953\n",
      "[step: 1256] loss: 0.006314869970083237\n",
      "[step: 1257] loss: 32.94750213623047\n",
      "[step: 1257] loss: 0.006314636208117008\n",
      "[step: 1258] loss: 33.95654296875\n",
      "[step: 1258] loss: 0.006313680205494165\n",
      "[step: 1259] loss: 35.61100387573242\n",
      "[step: 1259] loss: 0.006313738878816366\n",
      "[step: 1260] loss: 38.03833770751953\n",
      "[step: 1260] loss: 0.006312250625342131\n",
      "[step: 1261] loss: 41.094696044921875\n",
      "[step: 1261] loss: 0.006313242483884096\n",
      "[step: 1262] loss: 44.075721740722656\n",
      "[step: 1262] loss: 0.0063177491538226604\n",
      "[step: 1263] loss: 44.41508483886719\n",
      "[step: 1263] loss: 0.006327243056148291\n",
      "[step: 1264] loss: 41.20719909667969\n",
      "[step: 1264] loss: 0.006342173554003239\n",
      "[step: 1265] loss: 35.936180114746094\n",
      "[step: 1265] loss: 0.006364880129694939\n",
      "[step: 1266] loss: 32.298126220703125\n",
      "[step: 1266] loss: 0.006396045908331871\n",
      "[step: 1267] loss: 32.53506851196289\n",
      "[step: 1267] loss: 0.006442076060920954\n",
      "[step: 1268] loss: 34.78357696533203\n",
      "[step: 1268] loss: 0.00647758087143302\n",
      "[step: 1269] loss: 36.49927520751953\n",
      "[step: 1269] loss: 0.006494807545095682\n",
      "[step: 1270] loss: 35.620304107666016\n",
      "[step: 1270] loss: 0.006451771594583988\n",
      "[step: 1271] loss: 33.72489929199219\n",
      "[step: 1271] loss: 0.006376559380441904\n",
      "[step: 1272] loss: 32.47865295410156\n",
      "[step: 1272] loss: 0.006317867897450924\n",
      "[step: 1273] loss: 32.4045295715332\n",
      "[step: 1273] loss: 0.006305595859885216\n",
      "[step: 1274] loss: 33.005767822265625\n",
      "[step: 1274] loss: 0.006332667078822851\n",
      "[step: 1275] loss: 33.04710388183594\n",
      "[step: 1275] loss: 0.006375549361109734\n",
      "[step: 1276] loss: 32.74071502685547\n",
      "[step: 1276] loss: 0.006427686661481857\n",
      "[step: 1277] loss: 32.485572814941406\n",
      "[step: 1277] loss: 0.0064314077608287334\n",
      "[step: 1278] loss: 32.37495803833008\n",
      "[step: 1278] loss: 0.006396427284926176\n",
      "[step: 1279] loss: 32.160667419433594\n",
      "[step: 1279] loss: 0.006347879767417908\n",
      "[step: 1280] loss: 31.673675537109375\n",
      "[step: 1280] loss: 0.00632242439314723\n",
      "[step: 1281] loss: 31.189136505126953\n",
      "[step: 1281] loss: 0.006327532697468996\n",
      "[step: 1282] loss: 31.069183349609375\n",
      "[step: 1282] loss: 0.0063479323871433735\n",
      "[step: 1283] loss: 31.465087890625\n",
      "[step: 1283] loss: 0.006385828368365765\n",
      "[step: 1284] loss: 31.92727279663086\n",
      "[step: 1284] loss: 0.00640075420960784\n",
      "[step: 1285] loss: 31.991443634033203\n",
      "[step: 1285] loss: 0.0063915979117155075\n",
      "[step: 1286] loss: 31.484210968017578\n",
      "[step: 1286] loss: 0.006355394143611193\n",
      "[step: 1287] loss: 30.76386070251465\n",
      "[step: 1287] loss: 0.0063169184140861034\n",
      "[step: 1288] loss: 30.32332420349121\n",
      "[step: 1288] loss: 0.0063046482391655445\n",
      "[step: 1289] loss: 30.356746673583984\n",
      "[step: 1289] loss: 0.0063160075806081295\n",
      "[step: 1290] loss: 30.675813674926758\n",
      "[step: 1290] loss: 0.00634652329608798\n",
      "[step: 1291] loss: 30.930593490600586\n",
      "[step: 1291] loss: 0.006367968861013651\n",
      "[step: 1292] loss: 30.90768051147461\n",
      "[step: 1292] loss: 0.00636496813967824\n",
      "[step: 1293] loss: 30.68958282470703\n",
      "[step: 1293] loss: 0.006334724370390177\n",
      "[step: 1294] loss: 30.502044677734375\n",
      "[step: 1294] loss: 0.006299782544374466\n",
      "[step: 1295] loss: 30.553375244140625\n",
      "[step: 1295] loss: 0.006288216914981604\n",
      "[step: 1296] loss: 30.922630310058594\n",
      "[step: 1296] loss: 0.006297748070210218\n",
      "[step: 1297] loss: 31.613937377929688\n",
      "[step: 1297] loss: 0.00631733750924468\n",
      "[step: 1298] loss: 32.78539276123047\n",
      "[step: 1298] loss: 0.006334400735795498\n",
      "[step: 1299] loss: 34.48963165283203\n",
      "[step: 1299] loss: 0.006338451988995075\n",
      "[step: 1300] loss: 37.4285888671875\n",
      "[step: 1300] loss: 0.006325394846498966\n",
      "[step: 1301] loss: 42.00801086425781\n",
      "[step: 1301] loss: 0.006308563519269228\n",
      "[step: 1302] loss: 47.156192779541016\n",
      "[step: 1302] loss: 0.006296119652688503\n",
      "[step: 1303] loss: 51.78501892089844\n",
      "[step: 1303] loss: 0.0062915557064116\n",
      "[step: 1304] loss: 50.10008239746094\n",
      "[step: 1304] loss: 0.006291239522397518\n",
      "[step: 1305] loss: 42.55034255981445\n",
      "[step: 1305] loss: 0.00629263324663043\n",
      "[step: 1306] loss: 34.17420196533203\n",
      "[step: 1306] loss: 0.006294654682278633\n",
      "[step: 1307] loss: 31.843128204345703\n",
      "[step: 1307] loss: 0.006295325700193644\n",
      "[step: 1308] loss: 35.83787536621094\n",
      "[step: 1308] loss: 0.006297199986875057\n",
      "[step: 1309] loss: 40.14582824707031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1309] loss: 0.006300896871834993\n",
      "[step: 1310] loss: 39.51825714111328\n",
      "[step: 1310] loss: 0.00630751671269536\n",
      "[step: 1311] loss: 33.80494689941406\n",
      "[step: 1311] loss: 0.006307815667241812\n",
      "[step: 1312] loss: 30.506208419799805\n",
      "[step: 1312] loss: 0.006309757009148598\n",
      "[step: 1313] loss: 32.7668342590332\n",
      "[step: 1313] loss: 0.006300357636064291\n",
      "[step: 1314] loss: 35.882164001464844\n",
      "[step: 1314] loss: 0.006291075609624386\n",
      "[step: 1315] loss: 35.14917755126953\n",
      "[step: 1315] loss: 0.006282637361437082\n",
      "[step: 1316] loss: 31.676319122314453\n",
      "[step: 1316] loss: 0.006277889013290405\n",
      "[step: 1317] loss: 30.27377700805664\n",
      "[step: 1317] loss: 0.006275479681789875\n",
      "[step: 1318] loss: 32.074302673339844\n",
      "[step: 1318] loss: 0.006274839863181114\n",
      "[step: 1319] loss: 33.67015075683594\n",
      "[step: 1319] loss: 0.006276529282331467\n",
      "[step: 1320] loss: 32.719635009765625\n",
      "[step: 1320] loss: 0.006277843844145536\n",
      "[step: 1321] loss: 30.751657485961914\n",
      "[step: 1321] loss: 0.006278934422880411\n",
      "[step: 1322] loss: 30.334346771240234\n",
      "[step: 1322] loss: 0.0062795039266347885\n",
      "[step: 1323] loss: 31.47488021850586\n",
      "[step: 1323] loss: 0.006279578432440758\n",
      "[step: 1324] loss: 32.13458251953125\n",
      "[step: 1324] loss: 0.006279555149376392\n",
      "[step: 1325] loss: 31.482006072998047\n",
      "[step: 1325] loss: 0.006281272042542696\n",
      "[step: 1326] loss: 30.424144744873047\n",
      "[step: 1326] loss: 0.006283557042479515\n",
      "[step: 1327] loss: 30.152982711791992\n",
      "[step: 1327] loss: 0.006290156859904528\n",
      "[step: 1328] loss: 30.762954711914062\n",
      "[step: 1328] loss: 0.006299744360148907\n",
      "[step: 1329] loss: 31.135669708251953\n",
      "[step: 1329] loss: 0.006319030188024044\n",
      "[step: 1330] loss: 30.733699798583984\n",
      "[step: 1330] loss: 0.0063481987453997135\n",
      "[step: 1331] loss: 30.042098999023438\n",
      "[step: 1331] loss: 0.006399609614163637\n",
      "[step: 1332] loss: 29.69487190246582\n",
      "[step: 1332] loss: 0.006465309299528599\n",
      "[step: 1333] loss: 29.99472427368164\n",
      "[step: 1333] loss: 0.006539492402225733\n",
      "[step: 1334] loss: 30.430171966552734\n",
      "[step: 1334] loss: 0.006582482252269983\n",
      "[step: 1335] loss: 30.480295181274414\n",
      "[step: 1335] loss: 0.006540508475154638\n",
      "[step: 1336] loss: 30.07027816772461\n",
      "[step: 1336] loss: 0.0064271483570337296\n",
      "[step: 1337] loss: 29.578195571899414\n",
      "[step: 1337] loss: 0.006328928750008345\n",
      "[step: 1338] loss: 29.44729232788086\n",
      "[step: 1338] loss: 0.006312547251582146\n",
      "[step: 1339] loss: 29.6417236328125\n",
      "[step: 1339] loss: 0.00637722946703434\n",
      "[step: 1340] loss: 29.86925506591797\n",
      "[step: 1340] loss: 0.006474779453128576\n",
      "[step: 1341] loss: 29.875545501708984\n",
      "[step: 1341] loss: 0.006472878158092499\n",
      "[step: 1342] loss: 29.62799835205078\n",
      "[step: 1342] loss: 0.006389628630131483\n",
      "[step: 1343] loss: 29.378742218017578\n",
      "[step: 1343] loss: 0.0063090152107179165\n",
      "[step: 1344] loss: 29.279861450195312\n",
      "[step: 1344] loss: 0.00632284814491868\n",
      "[step: 1345] loss: 29.372013092041016\n",
      "[step: 1345] loss: 0.006383622996509075\n",
      "[step: 1346] loss: 29.524200439453125\n",
      "[step: 1346] loss: 0.006420206744223833\n",
      "[step: 1347] loss: 29.632465362548828\n",
      "[step: 1347] loss: 0.006427684333175421\n",
      "[step: 1348] loss: 29.679824829101562\n",
      "[step: 1348] loss: 0.006360335741192102\n",
      "[step: 1349] loss: 29.81147575378418\n",
      "[step: 1349] loss: 0.006307580042630434\n",
      "[step: 1350] loss: 30.246212005615234\n",
      "[step: 1350] loss: 0.006305639166384935\n",
      "[step: 1351] loss: 31.502899169921875\n",
      "[step: 1351] loss: 0.006349371280521154\n",
      "[step: 1352] loss: 34.302978515625\n",
      "[step: 1352] loss: 0.006395678035914898\n",
      "[step: 1353] loss: 39.97041702270508\n",
      "[step: 1353] loss: 0.006365995854139328\n",
      "[step: 1354] loss: 49.34831237792969\n",
      "[step: 1354] loss: 0.0063005657866597176\n",
      "[step: 1355] loss: 58.77002716064453\n",
      "[step: 1355] loss: 0.006276198662817478\n",
      "[step: 1356] loss: 58.20196533203125\n",
      "[step: 1356] loss: 0.00631187716498971\n",
      "[step: 1357] loss: 44.72335433959961\n",
      "[step: 1357] loss: 0.006351345218718052\n",
      "[step: 1358] loss: 32.19153594970703\n",
      "[step: 1358] loss: 0.006323022767901421\n",
      "[step: 1359] loss: 33.549739837646484\n",
      "[step: 1359] loss: 0.006273272912949324\n",
      "[step: 1360] loss: 41.88328552246094\n",
      "[step: 1360] loss: 0.006272302940487862\n",
      "[step: 1361] loss: 42.13573455810547\n",
      "[step: 1361] loss: 0.006294766906648874\n",
      "[step: 1362] loss: 35.153228759765625\n",
      "[step: 1362] loss: 0.0063176327385008335\n",
      "[step: 1363] loss: 31.85472297668457\n",
      "[step: 1363] loss: 0.006292198318988085\n",
      "[step: 1364] loss: 34.632347106933594\n",
      "[step: 1364] loss: 0.006272522732615471\n",
      "[step: 1365] loss: 36.63384246826172\n",
      "[step: 1365] loss: 0.006279677618294954\n",
      "[step: 1366] loss: 34.735897064208984\n",
      "[step: 1366] loss: 0.006278573535382748\n",
      "[step: 1367] loss: 32.103477478027344\n",
      "[step: 1367] loss: 0.0062848846428096294\n",
      "[step: 1368] loss: 31.954465866088867\n",
      "[step: 1368] loss: 0.006278396118432283\n",
      "[step: 1369] loss: 33.06649398803711\n",
      "[step: 1369] loss: 0.006279896013438702\n",
      "[step: 1370] loss: 32.42352294921875\n",
      "[step: 1370] loss: 0.006275394465774298\n",
      "[step: 1371] loss: 31.171117782592773\n",
      "[step: 1371] loss: 0.006261731032282114\n",
      "[step: 1372] loss: 31.470903396606445\n",
      "[step: 1372] loss: 0.006262480281293392\n",
      "[step: 1373] loss: 31.723901748657227\n",
      "[step: 1373] loss: 0.0062677194364368916\n",
      "[step: 1374] loss: 30.712970733642578\n",
      "[step: 1374] loss: 0.006276923231780529\n",
      "[step: 1375] loss: 30.11490249633789\n",
      "[step: 1375] loss: 0.006270966958254576\n",
      "[step: 1376] loss: 30.584213256835938\n",
      "[step: 1376] loss: 0.006260449532419443\n",
      "[step: 1377] loss: 30.95054817199707\n",
      "[step: 1377] loss: 0.006255329120904207\n",
      "[step: 1378] loss: 30.39657211303711\n",
      "[step: 1378] loss: 0.006255695130676031\n",
      "[step: 1379] loss: 29.57054901123047\n",
      "[step: 1379] loss: 0.006258411332964897\n",
      "[step: 1380] loss: 29.416305541992188\n",
      "[step: 1380] loss: 0.006254236213862896\n",
      "[step: 1381] loss: 30.061065673828125\n",
      "[step: 1381] loss: 0.006254211999475956\n",
      "[step: 1382] loss: 30.236827850341797\n",
      "[step: 1382] loss: 0.0062551177106797695\n",
      "[step: 1383] loss: 29.479904174804688\n",
      "[step: 1383] loss: 0.00625783484429121\n",
      "[step: 1384] loss: 28.995201110839844\n",
      "[step: 1384] loss: 0.00625575240701437\n",
      "[step: 1385] loss: 29.302051544189453\n",
      "[step: 1385] loss: 0.0062523894011974335\n",
      "[step: 1386] loss: 29.576087951660156\n",
      "[step: 1386] loss: 0.006249476224184036\n",
      "[step: 1387] loss: 29.302734375\n",
      "[step: 1387] loss: 0.006248081102967262\n",
      "[step: 1388] loss: 28.88929557800293\n",
      "[step: 1388] loss: 0.0062484825029969215\n",
      "[step: 1389] loss: 28.81719207763672\n",
      "[step: 1389] loss: 0.006246301345527172\n",
      "[step: 1390] loss: 29.094863891601562\n",
      "[step: 1390] loss: 0.00624430924654007\n",
      "[step: 1391] loss: 29.253767013549805\n",
      "[step: 1391] loss: 0.0062423027120530605\n",
      "[step: 1392] loss: 28.992542266845703\n",
      "[step: 1392] loss: 0.006242902018129826\n",
      "[step: 1393] loss: 28.628538131713867\n",
      "[step: 1393] loss: 0.006242972332984209\n",
      "[step: 1394] loss: 28.62134552001953\n",
      "[step: 1394] loss: 0.0062425266951322556\n",
      "[step: 1395] loss: 28.802705764770508\n",
      "[step: 1395] loss: 0.006241339724510908\n",
      "[step: 1396] loss: 28.803842544555664\n",
      "[step: 1396] loss: 0.0062403022311627865\n",
      "[step: 1397] loss: 28.612167358398438\n",
      "[step: 1397] loss: 0.006240368355065584\n",
      "[step: 1398] loss: 28.45380210876465\n",
      "[step: 1398] loss: 0.006240778136998415\n",
      "[step: 1399] loss: 28.436479568481445\n",
      "[step: 1399] loss: 0.006241622380912304\n",
      "[step: 1400] loss: 28.4764404296875\n",
      "[step: 1400] loss: 0.006242127623409033\n",
      "[step: 1401] loss: 28.49432373046875\n",
      "[step: 1401] loss: 0.006243918091058731\n",
      "[step: 1402] loss: 28.444135665893555\n",
      "[step: 1402] loss: 0.006248126737773418\n",
      "[step: 1403] loss: 28.370115280151367\n",
      "[step: 1403] loss: 0.006257456727325916\n",
      "[step: 1404] loss: 28.33156967163086\n",
      "[step: 1404] loss: 0.0062758298590779305\n",
      "[step: 1405] loss: 28.3138427734375\n",
      "[step: 1405] loss: 0.006310723256319761\n",
      "[step: 1406] loss: 28.27964210510254\n",
      "[step: 1406] loss: 0.006377554498612881\n",
      "[step: 1407] loss: 28.247058868408203\n",
      "[step: 1407] loss: 0.006485387217253447\n",
      "[step: 1408] loss: 28.270477294921875\n",
      "[step: 1408] loss: 0.006634888239204884\n",
      "[step: 1409] loss: 28.350345611572266\n",
      "[step: 1409] loss: 0.006735896226018667\n",
      "[step: 1410] loss: 28.493457794189453\n",
      "[step: 1410] loss: 0.00667394045740366\n",
      "[step: 1411] loss: 28.790830612182617\n",
      "[step: 1411] loss: 0.006464571226388216\n",
      "[step: 1412] loss: 29.61804962158203\n",
      "[step: 1412] loss: 0.0063338871113955975\n",
      "[step: 1413] loss: 31.84514617919922\n",
      "[step: 1413] loss: 0.0064050136134028435\n",
      "[step: 1414] loss: 36.91510772705078\n",
      "[step: 1414] loss: 0.006581460125744343\n",
      "[step: 1415] loss: 48.843162536621094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1415] loss: 0.0067421915009617805\n",
      "[step: 1416] loss: 64.81866455078125\n",
      "[step: 1416] loss: 0.006458932533860207\n",
      "[step: 1417] loss: 79.61070251464844\n",
      "[step: 1417] loss: 0.0063661797903478146\n",
      "[step: 1418] loss: 71.29302215576172\n",
      "[step: 1418] loss: 0.006504019256681204\n",
      "[step: 1419] loss: 44.815032958984375\n",
      "[step: 1419] loss: 0.0066088223829865456\n",
      "[step: 1420] loss: 33.17868423461914\n",
      "[step: 1420] loss: 0.006398014724254608\n",
      "[step: 1421] loss: 44.07344055175781\n",
      "[step: 1421] loss: 0.006302828900516033\n",
      "[step: 1422] loss: 55.413597106933594\n",
      "[step: 1422] loss: 0.006358989980071783\n",
      "[step: 1423] loss: 42.403282165527344\n",
      "[step: 1423] loss: 0.006406312342733145\n",
      "[step: 1424] loss: 30.78524398803711\n",
      "[step: 1424] loss: 0.006312162149697542\n",
      "[step: 1425] loss: 39.252891540527344\n",
      "[step: 1425] loss: 0.006291604600846767\n",
      "[step: 1426] loss: 43.3912353515625\n",
      "[step: 1426] loss: 0.006343543529510498\n",
      "[step: 1427] loss: 33.219451904296875\n",
      "[step: 1427] loss: 0.006340700667351484\n",
      "[step: 1428] loss: 31.868898391723633\n",
      "[step: 1428] loss: 0.006287724711000919\n",
      "[step: 1429] loss: 37.58865737915039\n",
      "[step: 1429] loss: 0.006276408210396767\n",
      "[step: 1430] loss: 34.94549560546875\n",
      "[step: 1430] loss: 0.00629989430308342\n",
      "[step: 1431] loss: 31.2471923828125\n",
      "[step: 1431] loss: 0.006290059071034193\n",
      "[step: 1432] loss: 32.287601470947266\n",
      "[step: 1432] loss: 0.006280218716710806\n",
      "[step: 1433] loss: 34.251556396484375\n",
      "[step: 1433] loss: 0.006271186750382185\n",
      "[step: 1434] loss: 32.65684509277344\n",
      "[step: 1434] loss: 0.0062748645432293415\n",
      "[step: 1435] loss: 29.855167388916016\n",
      "[step: 1435] loss: 0.006257954519242048\n",
      "[step: 1436] loss: 32.05638885498047\n",
      "[step: 1436] loss: 0.006268166471272707\n",
      "[step: 1437] loss: 33.382240295410156\n",
      "[step: 1437] loss: 0.006268192548304796\n",
      "[step: 1438] loss: 29.78378677368164\n",
      "[step: 1438] loss: 0.006251256447285414\n",
      "[step: 1439] loss: 29.76061248779297\n",
      "[step: 1439] loss: 0.00624054716899991\n",
      "[step: 1440] loss: 31.87691307067871\n",
      "[step: 1440] loss: 0.00625028507784009\n",
      "[step: 1441] loss: 30.18594741821289\n",
      "[step: 1441] loss: 0.006259770132601261\n",
      "[step: 1442] loss: 29.051055908203125\n",
      "[step: 1442] loss: 0.006245841272175312\n",
      "[step: 1443] loss: 29.862539291381836\n",
      "[step: 1443] loss: 0.006238321308046579\n",
      "[step: 1444] loss: 29.970001220703125\n",
      "[step: 1444] loss: 0.006241274997591972\n",
      "[step: 1445] loss: 29.347780227661133\n",
      "[step: 1445] loss: 0.006243242882192135\n",
      "[step: 1446] loss: 28.649137496948242\n",
      "[step: 1446] loss: 0.006233404390513897\n",
      "[step: 1447] loss: 29.053359985351562\n",
      "[step: 1447] loss: 0.0062329331412911415\n",
      "[step: 1448] loss: 29.426136016845703\n",
      "[step: 1448] loss: 0.006237545050680637\n",
      "[step: 1449] loss: 28.458690643310547\n",
      "[step: 1449] loss: 0.006239152513444424\n",
      "[step: 1450] loss: 28.35413932800293\n",
      "[step: 1450] loss: 0.006234780419617891\n",
      "[step: 1451] loss: 28.918237686157227\n",
      "[step: 1451] loss: 0.006229361053556204\n",
      "[step: 1452] loss: 28.57015037536621\n",
      "[step: 1452] loss: 0.0062309629283845425\n",
      "[step: 1453] loss: 28.250934600830078\n",
      "[step: 1453] loss: 0.006227834150195122\n",
      "[step: 1454] loss: 28.205814361572266\n",
      "[step: 1454] loss: 0.006224720273166895\n",
      "[step: 1455] loss: 28.30971336364746\n",
      "[step: 1455] loss: 0.006225235760211945\n",
      "[step: 1456] loss: 28.38187599182129\n",
      "[step: 1456] loss: 0.006227822974324226\n",
      "[step: 1457] loss: 27.996780395507812\n",
      "[step: 1457] loss: 0.006228305399417877\n",
      "[step: 1458] loss: 27.911739349365234\n",
      "[step: 1458] loss: 0.006225172895938158\n",
      "[step: 1459] loss: 28.168411254882812\n",
      "[step: 1459] loss: 0.0062255156226456165\n",
      "[step: 1460] loss: 28.01514434814453\n",
      "[step: 1460] loss: 0.006225223653018475\n",
      "[step: 1461] loss: 27.793142318725586\n",
      "[step: 1461] loss: 0.00622426625341177\n",
      "[step: 1462] loss: 27.779712677001953\n",
      "[step: 1462] loss: 0.006221012677997351\n",
      "[step: 1463] loss: 27.826427459716797\n",
      "[step: 1463] loss: 0.0062196203507483006\n",
      "[step: 1464] loss: 27.823793411254883\n",
      "[step: 1464] loss: 0.006219534669071436\n",
      "[step: 1465] loss: 27.710927963256836\n",
      "[step: 1465] loss: 0.006218301597982645\n",
      "[step: 1466] loss: 27.612911224365234\n",
      "[step: 1466] loss: 0.006216827780008316\n",
      "[step: 1467] loss: 27.630197525024414\n",
      "[step: 1467] loss: 0.006215908098965883\n",
      "[step: 1468] loss: 27.68338394165039\n",
      "[step: 1468] loss: 0.006215665955096483\n",
      "[step: 1469] loss: 27.60027313232422\n",
      "[step: 1469] loss: 0.00621555931866169\n",
      "[step: 1470] loss: 27.500240325927734\n",
      "[step: 1470] loss: 0.006214272230863571\n",
      "[step: 1471] loss: 27.52075958251953\n",
      "[step: 1471] loss: 0.00621338514611125\n",
      "[step: 1472] loss: 27.52901840209961\n",
      "[step: 1472] loss: 0.006213227286934853\n",
      "[step: 1473] loss: 27.501514434814453\n",
      "[step: 1473] loss: 0.0062131802551448345\n",
      "[step: 1474] loss: 27.489093780517578\n",
      "[step: 1474] loss: 0.006212851498275995\n",
      "[step: 1475] loss: 27.473609924316406\n",
      "[step: 1475] loss: 0.006212182343006134\n",
      "[step: 1476] loss: 27.519020080566406\n",
      "[step: 1476] loss: 0.0062121558003127575\n",
      "[step: 1477] loss: 27.65769386291504\n",
      "[step: 1477] loss: 0.006212545558810234\n",
      "[step: 1478] loss: 27.862201690673828\n",
      "[step: 1478] loss: 0.006213558372110128\n",
      "[step: 1479] loss: 28.240402221679688\n",
      "[step: 1479] loss: 0.006215043365955353\n",
      "[step: 1480] loss: 29.028095245361328\n",
      "[step: 1480] loss: 0.006218822207301855\n",
      "[step: 1481] loss: 30.45822525024414\n",
      "[step: 1481] loss: 0.00622511375695467\n",
      "[step: 1482] loss: 32.792564392089844\n",
      "[step: 1482] loss: 0.006240301299840212\n",
      "[step: 1483] loss: 36.086936950683594\n",
      "[step: 1483] loss: 0.006253421306610107\n",
      "[step: 1484] loss: 39.11372756958008\n",
      "[step: 1484] loss: 0.0062851859256625175\n",
      "[step: 1485] loss: 39.309364318847656\n",
      "[step: 1485] loss: 0.006290308199822903\n",
      "[step: 1486] loss: 35.631107330322266\n",
      "[step: 1486] loss: 0.006301452871412039\n",
      "[step: 1487] loss: 30.15447998046875\n",
      "[step: 1487] loss: 0.00627452228218317\n",
      "[step: 1488] loss: 27.888328552246094\n",
      "[step: 1488] loss: 0.006248627323657274\n",
      "[step: 1489] loss: 29.64752197265625\n",
      "[step: 1489] loss: 0.006224007811397314\n",
      "[step: 1490] loss: 32.1573486328125\n",
      "[step: 1490] loss: 0.006217021960765123\n",
      "[step: 1491] loss: 32.4000244140625\n",
      "[step: 1491] loss: 0.0062286462634801865\n",
      "[step: 1492] loss: 30.456562042236328\n",
      "[step: 1492] loss: 0.006251291371881962\n",
      "[step: 1493] loss: 28.570354461669922\n",
      "[step: 1493] loss: 0.0062795644626021385\n",
      "[step: 1494] loss: 28.068458557128906\n",
      "[step: 1494] loss: 0.006281580775976181\n",
      "[step: 1495] loss: 28.964704513549805\n",
      "[step: 1495] loss: 0.006275771651417017\n",
      "[step: 1496] loss: 29.93558120727539\n",
      "[step: 1496] loss: 0.0062710936181247234\n",
      "[step: 1497] loss: 29.71213722229004\n",
      "[step: 1497] loss: 0.006275319028645754\n",
      "[step: 1498] loss: 28.620197296142578\n",
      "[step: 1498] loss: 0.006285727955400944\n",
      "[step: 1499] loss: 27.61440658569336\n",
      "[step: 1499] loss: 0.0062941377982497215\n",
      "[step: 1500] loss: 27.605873107910156\n",
      "[step: 1500] loss: 0.006283838767558336\n",
      "[step: 1501] loss: 28.353012084960938\n",
      "[step: 1501] loss: 0.006257752887904644\n",
      "[step: 1502] loss: 28.835617065429688\n",
      "[step: 1502] loss: 0.006225507240742445\n",
      "[step: 1503] loss: 28.51042938232422\n",
      "[step: 1503] loss: 0.006205115932971239\n",
      "[step: 1504] loss: 27.640079498291016\n",
      "[step: 1504] loss: 0.0062059322372078896\n",
      "[step: 1505] loss: 27.120586395263672\n",
      "[step: 1505] loss: 0.006220616400241852\n",
      "[step: 1506] loss: 27.278823852539062\n",
      "[step: 1506] loss: 0.0062343464232981205\n",
      "[step: 1507] loss: 27.73806381225586\n",
      "[step: 1507] loss: 0.006238131783902645\n",
      "[step: 1508] loss: 28.031261444091797\n",
      "[step: 1508] loss: 0.006225036457180977\n",
      "[step: 1509] loss: 27.91724967956543\n",
      "[step: 1509] loss: 0.0062078447081148624\n",
      "[step: 1510] loss: 27.656715393066406\n",
      "[step: 1510] loss: 0.006199682597070932\n",
      "[step: 1511] loss: 27.418346405029297\n",
      "[step: 1511] loss: 0.006204345263540745\n",
      "[step: 1512] loss: 27.239078521728516\n",
      "[step: 1512] loss: 0.006215647794306278\n",
      "[step: 1513] loss: 27.06155014038086\n",
      "[step: 1513] loss: 0.006221247371286154\n",
      "[step: 1514] loss: 26.900856018066406\n",
      "[step: 1514] loss: 0.006222486030310392\n",
      "[step: 1515] loss: 26.8692626953125\n",
      "[step: 1515] loss: 0.006211420055478811\n",
      "[step: 1516] loss: 26.98992347717285\n",
      "[step: 1516] loss: 0.006203948054462671\n",
      "[step: 1517] loss: 27.196575164794922\n",
      "[step: 1517] loss: 0.006203210446983576\n",
      "[step: 1518] loss: 27.37974739074707\n",
      "[step: 1518] loss: 0.0062103732489049435\n",
      "[step: 1519] loss: 27.48154067993164\n",
      "[step: 1519] loss: 0.0062209852039813995\n",
      "[step: 1520] loss: 27.599853515625\n",
      "[step: 1520] loss: 0.0062325093895196915\n",
      "[step: 1521] loss: 27.77364158630371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1521] loss: 0.00624463427811861\n",
      "[step: 1522] loss: 28.05538558959961\n",
      "[step: 1522] loss: 0.006261705886572599\n",
      "[step: 1523] loss: 28.436946868896484\n",
      "[step: 1523] loss: 0.006287435069680214\n",
      "[step: 1524] loss: 28.82192611694336\n",
      "[step: 1524] loss: 0.006342371925711632\n",
      "[step: 1525] loss: 29.356441497802734\n",
      "[step: 1525] loss: 0.00635774340480566\n",
      "[step: 1526] loss: 29.815807342529297\n",
      "[step: 1526] loss: 0.006362272892147303\n",
      "[step: 1527] loss: 30.32880210876465\n",
      "[step: 1527] loss: 0.0063099851831793785\n",
      "[step: 1528] loss: 30.56857681274414\n",
      "[step: 1528] loss: 0.006252883467823267\n",
      "[step: 1529] loss: 30.558961868286133\n",
      "[step: 1529] loss: 0.0062199789099395275\n",
      "[step: 1530] loss: 30.03335952758789\n",
      "[step: 1530] loss: 0.006216959562152624\n",
      "[step: 1531] loss: 29.1549129486084\n",
      "[step: 1531] loss: 0.006232671905308962\n",
      "[step: 1532] loss: 28.069950103759766\n",
      "[step: 1532] loss: 0.006247979123145342\n",
      "[step: 1533] loss: 27.165273666381836\n",
      "[step: 1533] loss: 0.006276315543800592\n",
      "[step: 1534] loss: 26.631900787353516\n",
      "[step: 1534] loss: 0.00628246832638979\n",
      "[step: 1535] loss: 26.532363891601562\n",
      "[step: 1535] loss: 0.006276258267462254\n",
      "[step: 1536] loss: 26.75861358642578\n",
      "[step: 1536] loss: 0.006258359178900719\n",
      "[step: 1537] loss: 27.142921447753906\n",
      "[step: 1537] loss: 0.006234992761164904\n",
      "[step: 1538] loss: 27.533672332763672\n",
      "[step: 1538] loss: 0.006215725559741259\n",
      "[step: 1539] loss: 27.812725067138672\n",
      "[step: 1539] loss: 0.0062036397866904736\n",
      "[step: 1540] loss: 27.97787094116211\n",
      "[step: 1540] loss: 0.006209226790815592\n",
      "[step: 1541] loss: 27.94034194946289\n",
      "[step: 1541] loss: 0.006221832241863012\n",
      "[step: 1542] loss: 27.810230255126953\n",
      "[step: 1542] loss: 0.006233958061784506\n",
      "[step: 1543] loss: 27.554378509521484\n",
      "[step: 1543] loss: 0.006239041686058044\n",
      "[step: 1544] loss: 27.283336639404297\n",
      "[step: 1544] loss: 0.0062334006652235985\n",
      "[step: 1545] loss: 26.99884605407715\n",
      "[step: 1545] loss: 0.006223532371222973\n",
      "[step: 1546] loss: 26.752696990966797\n",
      "[step: 1546] loss: 0.006214665714651346\n",
      "[step: 1547] loss: 26.55896759033203\n",
      "[step: 1547] loss: 0.006208142265677452\n",
      "[step: 1548] loss: 26.416749954223633\n",
      "[step: 1548] loss: 0.006208845879882574\n",
      "[step: 1549] loss: 26.32064437866211\n",
      "[step: 1549] loss: 0.006205542478710413\n",
      "[step: 1550] loss: 26.257055282592773\n",
      "[step: 1550] loss: 0.006204970646649599\n",
      "[step: 1551] loss: 26.216379165649414\n",
      "[step: 1551] loss: 0.006199709139764309\n",
      "[step: 1552] loss: 26.189620971679688\n",
      "[step: 1552] loss: 0.006192841101437807\n",
      "[step: 1553] loss: 26.170324325561523\n",
      "[step: 1553] loss: 0.006187534891068935\n",
      "[step: 1554] loss: 26.15373992919922\n",
      "[step: 1554] loss: 0.006184706464409828\n",
      "[step: 1555] loss: 26.138343811035156\n",
      "[step: 1555] loss: 0.0061855497770011425\n",
      "[step: 1556] loss: 26.123870849609375\n",
      "[step: 1556] loss: 0.006188774015754461\n",
      "[step: 1557] loss: 26.112537384033203\n",
      "[step: 1557] loss: 0.006191282067447901\n",
      "[step: 1558] loss: 26.111427307128906\n",
      "[step: 1558] loss: 0.0061913966201245785\n",
      "[step: 1559] loss: 26.134815216064453\n",
      "[step: 1559] loss: 0.006190109997987747\n",
      "[step: 1560] loss: 26.216190338134766\n",
      "[step: 1560] loss: 0.00618775375187397\n",
      "[step: 1561] loss: 26.43587875366211\n",
      "[step: 1561] loss: 0.006187643390148878\n",
      "[step: 1562] loss: 26.98119354248047\n",
      "[step: 1562] loss: 0.006190835032612085\n",
      "[step: 1563] loss: 28.369308471679688\n",
      "[step: 1563] loss: 0.006200659554451704\n",
      "[step: 1564] loss: 31.622718811035156\n",
      "[step: 1564] loss: 0.0062165227718651295\n",
      "[step: 1565] loss: 39.52639389038086\n",
      "[step: 1565] loss: 0.006247788202017546\n",
      "[step: 1566] loss: 53.44013595581055\n",
      "[step: 1566] loss: 0.00628596730530262\n",
      "[step: 1567] loss: 73.92047882080078\n",
      "[step: 1567] loss: 0.006350419949740171\n",
      "[step: 1568] loss: 81.11531066894531\n",
      "[step: 1568] loss: 0.006403633393347263\n",
      "[step: 1569] loss: 62.37632369995117\n",
      "[step: 1569] loss: 0.0064423708245158195\n",
      "[step: 1570] loss: 34.73152160644531\n",
      "[step: 1570] loss: 0.006424451246857643\n",
      "[step: 1571] loss: 31.389850616455078\n",
      "[step: 1571] loss: 0.00634700758382678\n",
      "[step: 1572] loss: 46.85260772705078\n",
      "[step: 1572] loss: 0.006251571699976921\n",
      "[step: 1573] loss: 46.94911193847656\n",
      "[step: 1573] loss: 0.006200205069035292\n",
      "[step: 1574] loss: 33.00996017456055\n",
      "[step: 1574] loss: 0.006216316018253565\n",
      "[step: 1575] loss: 30.183719635009766\n",
      "[step: 1575] loss: 0.006272546015679836\n",
      "[step: 1576] loss: 39.54288864135742\n",
      "[step: 1576] loss: 0.00634227879345417\n",
      "[step: 1577] loss: 39.29805374145508\n",
      "[step: 1577] loss: 0.006342809647321701\n",
      "[step: 1578] loss: 29.29859161376953\n",
      "[step: 1578] loss: 0.006274272687733173\n",
      "[step: 1579] loss: 30.310409545898438\n",
      "[step: 1579] loss: 0.0061980136670172215\n",
      "[step: 1580] loss: 36.208351135253906\n",
      "[step: 1580] loss: 0.006189570762217045\n",
      "[step: 1581] loss: 32.38154983520508\n",
      "[step: 1581] loss: 0.006246288772672415\n",
      "[step: 1582] loss: 27.469802856445312\n",
      "[step: 1582] loss: 0.006307917647063732\n",
      "[step: 1583] loss: 30.951871871948242\n",
      "[step: 1583] loss: 0.006336107384413481\n",
      "[step: 1584] loss: 33.03864669799805\n",
      "[step: 1584] loss: 0.006284390576183796\n",
      "[step: 1585] loss: 28.361446380615234\n",
      "[step: 1585] loss: 0.006210793741047382\n",
      "[step: 1586] loss: 27.46275520324707\n",
      "[step: 1586] loss: 0.006189167033880949\n",
      "[step: 1587] loss: 30.82408905029297\n",
      "[step: 1587] loss: 0.006233392283320427\n",
      "[step: 1588] loss: 29.950942993164062\n",
      "[step: 1588] loss: 0.006290399003773928\n",
      "[step: 1589] loss: 26.987834930419922\n",
      "[step: 1589] loss: 0.00631050206720829\n",
      "[step: 1590] loss: 27.593202590942383\n",
      "[step: 1590] loss: 0.00629393570125103\n",
      "[step: 1591] loss: 29.057659149169922\n",
      "[step: 1591] loss: 0.006220153532922268\n",
      "[step: 1592] loss: 28.293052673339844\n",
      "[step: 1592] loss: 0.00619156938046217\n",
      "[step: 1593] loss: 27.0621337890625\n",
      "[step: 1593] loss: 0.006234010215848684\n",
      "[step: 1594] loss: 27.08446502685547\n",
      "[step: 1594] loss: 0.006287124007940292\n",
      "[step: 1595] loss: 27.559446334838867\n",
      "[step: 1595] loss: 0.006313837133347988\n",
      "[step: 1596] loss: 27.879777908325195\n",
      "[step: 1596] loss: 0.006256578955799341\n",
      "[step: 1597] loss: 26.928707122802734\n",
      "[step: 1597] loss: 0.006192849483340979\n",
      "[step: 1598] loss: 26.28515625\n",
      "[step: 1598] loss: 0.006199197843670845\n",
      "[step: 1599] loss: 26.914833068847656\n",
      "[step: 1599] loss: 0.006245695054531097\n",
      "[step: 1600] loss: 27.308643341064453\n",
      "[step: 1600] loss: 0.006293318700045347\n",
      "[step: 1601] loss: 26.53390884399414\n",
      "[step: 1601] loss: 0.006251183804124594\n",
      "[step: 1602] loss: 26.0176944732666\n",
      "[step: 1602] loss: 0.0061961449682712555\n",
      "[step: 1603] loss: 26.453262329101562\n",
      "[step: 1603] loss: 0.006188459228724241\n",
      "[step: 1604] loss: 26.64190101623535\n",
      "[step: 1604] loss: 0.006209817249327898\n",
      "[step: 1605] loss: 26.299230575561523\n",
      "[step: 1605] loss: 0.006240367889404297\n",
      "[step: 1606] loss: 26.15545654296875\n",
      "[step: 1606] loss: 0.0062176878564059734\n",
      "[step: 1607] loss: 26.067184448242188\n",
      "[step: 1607] loss: 0.006190876476466656\n",
      "[step: 1608] loss: 25.932180404663086\n",
      "[step: 1608] loss: 0.006179461255669594\n",
      "[step: 1609] loss: 26.084299087524414\n",
      "[step: 1609] loss: 0.006190108601003885\n",
      "[step: 1610] loss: 26.19895362854004\n",
      "[step: 1610] loss: 0.006203189492225647\n",
      "[step: 1611] loss: 25.934371948242188\n",
      "[step: 1611] loss: 0.0061937300488352776\n",
      "[step: 1612] loss: 25.678415298461914\n",
      "[step: 1612] loss: 0.006183878984302282\n",
      "[step: 1613] loss: 25.780384063720703\n",
      "[step: 1613] loss: 0.006172819063067436\n",
      "[step: 1614] loss: 25.900962829589844\n",
      "[step: 1614] loss: 0.006172655150294304\n",
      "[step: 1615] loss: 25.830642700195312\n",
      "[step: 1615] loss: 0.006176657043397427\n",
      "[step: 1616] loss: 25.753625869750977\n",
      "[step: 1616] loss: 0.006180198397487402\n",
      "[step: 1617] loss: 25.698564529418945\n",
      "[step: 1617] loss: 0.006182617042213678\n",
      "[step: 1618] loss: 25.604576110839844\n",
      "[step: 1618] loss: 0.006177091505378485\n",
      "[step: 1619] loss: 25.543975830078125\n",
      "[step: 1619] loss: 0.006169235799461603\n",
      "[step: 1620] loss: 25.605192184448242\n",
      "[step: 1620] loss: 0.0061648255214095116\n",
      "[step: 1621] loss: 25.66974639892578\n",
      "[step: 1621] loss: 0.006163383834064007\n",
      "[step: 1622] loss: 25.589338302612305\n",
      "[step: 1622] loss: 0.006165245547890663\n",
      "[step: 1623] loss: 25.488130569458008\n",
      "[step: 1623] loss: 0.006168180610984564\n",
      "[step: 1624] loss: 25.458290100097656\n",
      "[step: 1624] loss: 0.006169670261442661\n",
      "[step: 1625] loss: 25.44121551513672\n",
      "[step: 1625] loss: 0.0061689503490924835\n",
      "[step: 1626] loss: 25.40918731689453\n",
      "[step: 1626] loss: 0.006169195752590895\n",
      "[step: 1627] loss: 25.400196075439453\n",
      "[step: 1627] loss: 0.006167115177959204\n",
      "[step: 1628] loss: 25.42475700378418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1628] loss: 0.006164604797959328\n",
      "[step: 1629] loss: 25.42353057861328\n",
      "[step: 1629] loss: 0.006163457874208689\n",
      "[step: 1630] loss: 25.371986389160156\n",
      "[step: 1630] loss: 0.006163007579743862\n",
      "[step: 1631] loss: 25.31906509399414\n",
      "[step: 1631] loss: 0.0061645214445889\n",
      "[step: 1632] loss: 25.3018798828125\n",
      "[step: 1632] loss: 0.006166170351207256\n",
      "[step: 1633] loss: 25.29339599609375\n",
      "[step: 1633] loss: 0.0061697764322161674\n",
      "[step: 1634] loss: 25.28687286376953\n",
      "[step: 1634] loss: 0.006173389498144388\n",
      "[step: 1635] loss: 25.306079864501953\n",
      "[step: 1635] loss: 0.00618044612929225\n",
      "[step: 1636] loss: 25.37976837158203\n",
      "[step: 1636] loss: 0.006190645042806864\n",
      "[step: 1637] loss: 25.523242950439453\n",
      "[step: 1637] loss: 0.0062085287645459175\n",
      "[step: 1638] loss: 25.780630111694336\n",
      "[step: 1638] loss: 0.0062392884865403175\n",
      "[step: 1639] loss: 26.325199127197266\n",
      "[step: 1639] loss: 0.00628676125779748\n",
      "[step: 1640] loss: 27.417137145996094\n",
      "[step: 1640] loss: 0.006362611427903175\n",
      "[step: 1641] loss: 29.719646453857422\n",
      "[step: 1641] loss: 0.006456639152020216\n",
      "[step: 1642] loss: 33.87872314453125\n",
      "[step: 1642] loss: 0.006556996610015631\n",
      "[step: 1643] loss: 41.53818893432617\n",
      "[step: 1643] loss: 0.006595503073185682\n",
      "[step: 1644] loss: 51.08948516845703\n",
      "[step: 1644] loss: 0.006539523601531982\n",
      "[step: 1645] loss: 58.31324005126953\n",
      "[step: 1645] loss: 0.0063592600636184216\n",
      "[step: 1646] loss: 53.23664855957031\n",
      "[step: 1646] loss: 0.006243521347641945\n",
      "[step: 1647] loss: 37.772239685058594\n",
      "[step: 1647] loss: 0.006280438043177128\n",
      "[step: 1648] loss: 27.765155792236328\n",
      "[step: 1648] loss: 0.0063492716290056705\n",
      "[step: 1649] loss: 32.21021270751953\n",
      "[step: 1649] loss: 0.006354496814310551\n",
      "[step: 1650] loss: 40.468170166015625\n",
      "[step: 1650] loss: 0.006311065051704645\n",
      "[step: 1651] loss: 36.73876953125\n",
      "[step: 1651] loss: 0.006335078738629818\n",
      "[step: 1652] loss: 27.770999908447266\n",
      "[step: 1652] loss: 0.0063010710291564465\n",
      "[step: 1653] loss: 27.959110260009766\n",
      "[step: 1653] loss: 0.006236137822270393\n",
      "[step: 1654] loss: 34.10099792480469\n",
      "[step: 1654] loss: 0.0062845260836184025\n",
      "[step: 1655] loss: 32.42242431640625\n",
      "[step: 1655] loss: 0.006346066482365131\n",
      "[step: 1656] loss: 26.429027557373047\n",
      "[step: 1656] loss: 0.006403446663171053\n",
      "[step: 1657] loss: 28.602174758911133\n",
      "[step: 1657] loss: 0.006324508227407932\n",
      "[step: 1658] loss: 31.61117935180664\n",
      "[step: 1658] loss: 0.006310733035206795\n",
      "[step: 1659] loss: 28.204452514648438\n",
      "[step: 1659] loss: 0.0063087898306548595\n",
      "[step: 1660] loss: 26.61140251159668\n",
      "[step: 1660] loss: 0.00625309394672513\n",
      "[step: 1661] loss: 28.50539207458496\n",
      "[step: 1661] loss: 0.00626008864492178\n",
      "[step: 1662] loss: 28.33654022216797\n",
      "[step: 1662] loss: 0.006272262427955866\n",
      "[step: 1663] loss: 26.750160217285156\n",
      "[step: 1663] loss: 0.006215426605194807\n",
      "[step: 1664] loss: 26.31777572631836\n",
      "[step: 1664] loss: 0.006228152196854353\n",
      "[step: 1665] loss: 26.796646118164062\n",
      "[step: 1665] loss: 0.006240985356271267\n",
      "[step: 1666] loss: 27.223052978515625\n",
      "[step: 1666] loss: 0.006198660004884005\n",
      "[step: 1667] loss: 26.694091796875\n",
      "[step: 1667] loss: 0.0062035066075623035\n",
      "[step: 1668] loss: 25.719478607177734\n",
      "[step: 1668] loss: 0.006213880144059658\n",
      "[step: 1669] loss: 26.161907196044922\n",
      "[step: 1669] loss: 0.006190874148160219\n",
      "[step: 1670] loss: 26.919414520263672\n",
      "[step: 1670] loss: 0.006187716498970985\n",
      "[step: 1671] loss: 25.87076187133789\n",
      "[step: 1671] loss: 0.006191338412463665\n",
      "[step: 1672] loss: 25.150177001953125\n",
      "[step: 1672] loss: 0.006182954180985689\n",
      "[step: 1673] loss: 25.91863250732422\n",
      "[step: 1673] loss: 0.006172582041472197\n",
      "[step: 1674] loss: 26.050121307373047\n",
      "[step: 1674] loss: 0.006178181618452072\n",
      "[step: 1675] loss: 25.291105270385742\n",
      "[step: 1675] loss: 0.006176120601594448\n",
      "[step: 1676] loss: 25.186473846435547\n",
      "[step: 1676] loss: 0.006165002938359976\n",
      "[step: 1677] loss: 25.618633270263672\n",
      "[step: 1677] loss: 0.006168348249047995\n",
      "[step: 1678] loss: 25.551637649536133\n",
      "[step: 1678] loss: 0.0061679380014538765\n",
      "[step: 1679] loss: 25.251285552978516\n",
      "[step: 1679] loss: 0.006160961929708719\n",
      "[step: 1680] loss: 25.220943450927734\n",
      "[step: 1680] loss: 0.0061576408334076405\n",
      "[step: 1681] loss: 25.234132766723633\n",
      "[step: 1681] loss: 0.0061614313162863255\n",
      "[step: 1682] loss: 25.281776428222656\n",
      "[step: 1682] loss: 0.006157132796943188\n",
      "[step: 1683] loss: 25.28874397277832\n",
      "[step: 1683] loss: 0.006151861511170864\n",
      "[step: 1684] loss: 25.13658332824707\n",
      "[step: 1684] loss: 0.006156441755592823\n",
      "[step: 1685] loss: 25.117752075195312\n",
      "[step: 1685] loss: 0.006153008434921503\n",
      "[step: 1686] loss: 25.38221549987793\n",
      "[step: 1686] loss: 0.006149087566882372\n",
      "[step: 1687] loss: 25.532392501831055\n",
      "[step: 1687] loss: 0.006150349043309689\n",
      "[step: 1688] loss: 25.53754425048828\n",
      "[step: 1688] loss: 0.006150375120341778\n",
      "[step: 1689] loss: 25.835281372070312\n",
      "[step: 1689] loss: 0.0061464570462703705\n",
      "[step: 1690] loss: 26.58765983581543\n",
      "[step: 1690] loss: 0.006145828869193792\n",
      "[step: 1691] loss: 27.508058547973633\n",
      "[step: 1691] loss: 0.006147580686956644\n",
      "[step: 1692] loss: 28.785640716552734\n",
      "[step: 1692] loss: 0.006144822109490633\n",
      "[step: 1693] loss: 30.290937423706055\n",
      "[step: 1693] loss: 0.006142675410956144\n",
      "[step: 1694] loss: 32.11027908325195\n",
      "[step: 1694] loss: 0.006143114063888788\n",
      "[step: 1695] loss: 32.761905670166016\n",
      "[step: 1695] loss: 0.006143730133771896\n",
      "[step: 1696] loss: 32.130767822265625\n",
      "[step: 1696] loss: 0.00614157784730196\n",
      "[step: 1697] loss: 29.673999786376953\n",
      "[step: 1697] loss: 0.006140304263681173\n",
      "[step: 1698] loss: 27.113601684570312\n",
      "[step: 1698] loss: 0.006141199730336666\n",
      "[step: 1699] loss: 25.5395565032959\n",
      "[step: 1699] loss: 0.006140156649053097\n",
      "[step: 1700] loss: 25.54366683959961\n",
      "[step: 1700] loss: 0.0061394874937832355\n",
      "[step: 1701] loss: 26.38436508178711\n",
      "[step: 1701] loss: 0.006139356642961502\n",
      "[step: 1702] loss: 27.12579917907715\n",
      "[step: 1702] loss: 0.006139723118394613\n",
      "[step: 1703] loss: 27.298809051513672\n",
      "[step: 1703] loss: 0.0061407387256622314\n",
      "[step: 1704] loss: 26.76198387145996\n",
      "[step: 1704] loss: 0.006142244208604097\n",
      "[step: 1705] loss: 26.026538848876953\n",
      "[step: 1705] loss: 0.006145363673567772\n",
      "[step: 1706] loss: 25.458271026611328\n",
      "[step: 1706] loss: 0.006152232177555561\n",
      "[step: 1707] loss: 25.321002960205078\n",
      "[step: 1707] loss: 0.006161200813949108\n",
      "[step: 1708] loss: 25.49819564819336\n",
      "[step: 1708] loss: 0.006182556040585041\n",
      "[step: 1709] loss: 25.67977523803711\n",
      "[step: 1709] loss: 0.006205683574080467\n",
      "[step: 1710] loss: 25.68341636657715\n",
      "[step: 1710] loss: 0.006250785663723946\n",
      "[step: 1711] loss: 25.471630096435547\n",
      "[step: 1711] loss: 0.006276413332670927\n",
      "[step: 1712] loss: 25.24803352355957\n",
      "[step: 1712] loss: 0.0063043320551514626\n",
      "[step: 1713] loss: 25.06310272216797\n",
      "[step: 1713] loss: 0.006301628425717354\n",
      "[step: 1714] loss: 24.94369125366211\n",
      "[step: 1714] loss: 0.006267837714403868\n",
      "[step: 1715] loss: 24.857093811035156\n",
      "[step: 1715] loss: 0.006210485007613897\n",
      "[step: 1716] loss: 24.816999435424805\n",
      "[step: 1716] loss: 0.0061585502699017525\n",
      "[step: 1717] loss: 24.87169647216797\n",
      "[step: 1717] loss: 0.006136066280305386\n",
      "[step: 1718] loss: 25.022449493408203\n",
      "[step: 1718] loss: 0.006143662612885237\n",
      "[step: 1719] loss: 25.229612350463867\n",
      "[step: 1719] loss: 0.006166341248899698\n",
      "[step: 1720] loss: 25.384380340576172\n",
      "[step: 1720] loss: 0.006189072038978338\n",
      "[step: 1721] loss: 25.470640182495117\n",
      "[step: 1721] loss: 0.006208198610693216\n",
      "[step: 1722] loss: 25.40531349182129\n",
      "[step: 1722] loss: 0.006213109474629164\n",
      "[step: 1723] loss: 25.295509338378906\n",
      "[step: 1723] loss: 0.006214320659637451\n",
      "[step: 1724] loss: 25.137069702148438\n",
      "[step: 1724] loss: 0.006177016068249941\n",
      "[step: 1725] loss: 25.051776885986328\n",
      "[step: 1725] loss: 0.006144255865365267\n",
      "[step: 1726] loss: 25.010032653808594\n",
      "[step: 1726] loss: 0.0061344061978161335\n",
      "[step: 1727] loss: 25.062541961669922\n",
      "[step: 1727] loss: 0.006142416037619114\n",
      "[step: 1728] loss: 25.19639015197754\n",
      "[step: 1728] loss: 0.006162707693874836\n",
      "[step: 1729] loss: 25.433868408203125\n",
      "[step: 1729] loss: 0.006178280338644981\n",
      "[step: 1730] loss: 25.765247344970703\n",
      "[step: 1730] loss: 0.006195039488375187\n",
      "[step: 1731] loss: 26.185440063476562\n",
      "[step: 1731] loss: 0.006190401967614889\n",
      "[step: 1732] loss: 26.728199005126953\n",
      "[step: 1732] loss: 0.006176908500492573\n",
      "[step: 1733] loss: 27.41350555419922\n",
      "[step: 1733] loss: 0.006157827563583851\n",
      "[step: 1734] loss: 28.173091888427734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1734] loss: 0.006142186466604471\n",
      "[step: 1735] loss: 29.024084091186523\n",
      "[step: 1735] loss: 0.006133235525339842\n",
      "[step: 1736] loss: 29.59764862060547\n",
      "[step: 1736] loss: 0.006129960063844919\n",
      "[step: 1737] loss: 29.964599609375\n",
      "[step: 1737] loss: 0.006133313290774822\n",
      "[step: 1738] loss: 29.45920753479004\n",
      "[step: 1738] loss: 0.006140908692032099\n",
      "[step: 1739] loss: 28.604461669921875\n",
      "[step: 1739] loss: 0.006151887588202953\n",
      "[step: 1740] loss: 27.138038635253906\n",
      "[step: 1740] loss: 0.006161919329315424\n",
      "[step: 1741] loss: 25.9246826171875\n",
      "[step: 1741] loss: 0.006170056294649839\n",
      "[step: 1742] loss: 25.086769104003906\n",
      "[step: 1742] loss: 0.006170997861772776\n",
      "[step: 1743] loss: 24.82898712158203\n",
      "[step: 1743] loss: 0.006170997396111488\n",
      "[step: 1744] loss: 24.99825668334961\n",
      "[step: 1744] loss: 0.006163987331092358\n",
      "[step: 1745] loss: 25.32905387878418\n",
      "[step: 1745] loss: 0.006158147938549519\n",
      "[step: 1746] loss: 25.619422912597656\n",
      "[step: 1746] loss: 0.006147671025246382\n",
      "[step: 1747] loss: 25.723670959472656\n",
      "[step: 1747] loss: 0.0061393738724291325\n",
      "[step: 1748] loss: 25.69964599609375\n",
      "[step: 1748] loss: 0.006131175439804792\n",
      "[step: 1749] loss: 25.462099075317383\n",
      "[step: 1749] loss: 0.0061270915903151035\n",
      "[step: 1750] loss: 25.18115234375\n",
      "[step: 1750] loss: 0.006123387720435858\n",
      "[step: 1751] loss: 24.81503677368164\n",
      "[step: 1751] loss: 0.006121156271547079\n",
      "[step: 1752] loss: 24.483394622802734\n",
      "[step: 1752] loss: 0.00612000422552228\n",
      "[step: 1753] loss: 24.230976104736328\n",
      "[step: 1753] loss: 0.006120198406279087\n",
      "[step: 1754] loss: 24.116161346435547\n",
      "[step: 1754] loss: 0.00612122006714344\n",
      "[step: 1755] loss: 24.167551040649414\n",
      "[step: 1755] loss: 0.0061230892315506935\n",
      "[step: 1756] loss: 24.37350082397461\n",
      "[step: 1756] loss: 0.006126290187239647\n",
      "[step: 1757] loss: 24.70589828491211\n",
      "[step: 1757] loss: 0.006130018271505833\n",
      "[step: 1758] loss: 25.106220245361328\n",
      "[step: 1758] loss: 0.006137962453067303\n",
      "[step: 1759] loss: 25.613513946533203\n",
      "[step: 1759] loss: 0.006145384628325701\n",
      "[step: 1760] loss: 26.141530990600586\n",
      "[step: 1760] loss: 0.006161140277981758\n",
      "[step: 1761] loss: 26.894702911376953\n",
      "[step: 1761] loss: 0.006177392788231373\n",
      "[step: 1762] loss: 27.70094871520996\n",
      "[step: 1762] loss: 0.0062073091976344585\n",
      "[step: 1763] loss: 28.9147891998291\n",
      "[step: 1763] loss: 0.0062342798337340355\n",
      "[step: 1764] loss: 30.117313385009766\n",
      "[step: 1764] loss: 0.006272260565310717\n",
      "[step: 1765] loss: 31.55138397216797\n",
      "[step: 1765] loss: 0.006306086201220751\n",
      "[step: 1766] loss: 32.37805938720703\n",
      "[step: 1766] loss: 0.006327908486127853\n",
      "[step: 1767] loss: 32.414310455322266\n",
      "[step: 1767] loss: 0.006314093712717295\n",
      "[step: 1768] loss: 30.94658660888672\n",
      "[step: 1768] loss: 0.006261484697461128\n",
      "[step: 1769] loss: 28.365507125854492\n",
      "[step: 1769] loss: 0.0061812796629965305\n",
      "[step: 1770] loss: 25.742822647094727\n",
      "[step: 1770] loss: 0.006127603817731142\n",
      "[step: 1771] loss: 24.24933624267578\n",
      "[step: 1771] loss: 0.006127919536083937\n",
      "[step: 1772] loss: 24.357376098632812\n",
      "[step: 1772] loss: 0.0061693633906543255\n",
      "[step: 1773] loss: 25.548295974731445\n",
      "[step: 1773] loss: 0.006216955836862326\n",
      "[step: 1774] loss: 26.820262908935547\n",
      "[step: 1774] loss: 0.006235429551452398\n",
      "[step: 1775] loss: 27.22265625\n",
      "[step: 1775] loss: 0.006226120982319117\n",
      "[step: 1776] loss: 26.492904663085938\n",
      "[step: 1776] loss: 0.006185583770275116\n",
      "[step: 1777] loss: 25.13915252685547\n",
      "[step: 1777] loss: 0.006160425487905741\n",
      "[step: 1778] loss: 24.177597045898438\n",
      "[step: 1778] loss: 0.006143917795270681\n",
      "[step: 1779] loss: 24.036582946777344\n",
      "[step: 1779] loss: 0.006144756451249123\n",
      "[step: 1780] loss: 24.455158233642578\n",
      "[step: 1780] loss: 0.0061502112075686455\n",
      "[step: 1781] loss: 24.857908248901367\n",
      "[step: 1781] loss: 0.006156067363917828\n",
      "[step: 1782] loss: 24.97182273864746\n",
      "[step: 1782] loss: 0.006171788088977337\n",
      "[step: 1783] loss: 24.78107261657715\n",
      "[step: 1783] loss: 0.00616902532055974\n",
      "[step: 1784] loss: 24.474510192871094\n",
      "[step: 1784] loss: 0.006166667211800814\n",
      "[step: 1785] loss: 24.269859313964844\n",
      "[step: 1785] loss: 0.006136710289865732\n",
      "[step: 1786] loss: 24.28271484375\n",
      "[step: 1786] loss: 0.006117838900536299\n",
      "[step: 1787] loss: 24.523391723632812\n",
      "[step: 1787] loss: 0.006118348799645901\n",
      "[step: 1788] loss: 24.800966262817383\n",
      "[step: 1788] loss: 0.006135368719696999\n",
      "[step: 1789] loss: 24.917814254760742\n",
      "[step: 1789] loss: 0.006160815246403217\n",
      "[step: 1790] loss: 24.829788208007812\n",
      "[step: 1790] loss: 0.0061575863510370255\n",
      "[step: 1791] loss: 24.613479614257812\n",
      "[step: 1791] loss: 0.006148097570985556\n",
      "[step: 1792] loss: 24.410362243652344\n",
      "[step: 1792] loss: 0.0061301542446017265\n",
      "[step: 1793] loss: 24.381229400634766\n",
      "[step: 1793] loss: 0.006124400999397039\n",
      "[step: 1794] loss: 24.586849212646484\n",
      "[step: 1794] loss: 0.006126344669610262\n",
      "[step: 1795] loss: 25.177574157714844\n",
      "[step: 1795] loss: 0.006128475069999695\n",
      "[step: 1796] loss: 26.190834045410156\n",
      "[step: 1796] loss: 0.00612941337749362\n",
      "[step: 1797] loss: 27.821189880371094\n",
      "[step: 1797] loss: 0.006123424042016268\n",
      "[step: 1798] loss: 30.0072021484375\n",
      "[step: 1798] loss: 0.006122062914073467\n",
      "[step: 1799] loss: 33.026275634765625\n",
      "[step: 1799] loss: 0.00612215418368578\n",
      "[step: 1800] loss: 35.69870376586914\n",
      "[step: 1800] loss: 0.006128855049610138\n",
      "[step: 1801] loss: 38.2728385925293\n",
      "[step: 1801] loss: 0.006135931238532066\n",
      "[step: 1802] loss: 37.53415298461914\n",
      "[step: 1802] loss: 0.006140848621726036\n",
      "[step: 1803] loss: 34.30608367919922\n",
      "[step: 1803] loss: 0.006142250262200832\n",
      "[step: 1804] loss: 28.71493148803711\n",
      "[step: 1804] loss: 0.00614058505743742\n",
      "[step: 1805] loss: 24.80340576171875\n",
      "[step: 1805] loss: 0.00613789726048708\n",
      "[step: 1806] loss: 24.955684661865234\n",
      "[step: 1806] loss: 0.0061372509226202965\n",
      "[step: 1807] loss: 27.670635223388672\n",
      "[step: 1807] loss: 0.006137000396847725\n",
      "[step: 1808] loss: 29.56148910522461\n",
      "[step: 1808] loss: 0.006145270075649023\n",
      "[step: 1809] loss: 28.57359504699707\n",
      "[step: 1809] loss: 0.0061429571360349655\n",
      "[step: 1810] loss: 26.26830291748047\n",
      "[step: 1810] loss: 0.006145379971712828\n",
      "[step: 1811] loss: 24.3930721282959\n",
      "[step: 1811] loss: 0.0061308820731937885\n",
      "[step: 1812] loss: 24.270862579345703\n",
      "[step: 1812] loss: 0.006120841484516859\n",
      "[step: 1813] loss: 25.514854431152344\n",
      "[step: 1813] loss: 0.006114041432738304\n",
      "[step: 1814] loss: 26.688365936279297\n",
      "[step: 1814] loss: 0.006112608592957258\n",
      "[step: 1815] loss: 26.62063980102539\n",
      "[step: 1815] loss: 0.006113768555223942\n",
      "[step: 1816] loss: 25.108673095703125\n",
      "[step: 1816] loss: 0.006115703843533993\n",
      "[step: 1817] loss: 23.68228530883789\n",
      "[step: 1817] loss: 0.006116283591836691\n",
      "[step: 1818] loss: 23.586294174194336\n",
      "[step: 1818] loss: 0.006114713381975889\n",
      "[step: 1819] loss: 24.50627899169922\n",
      "[step: 1819] loss: 0.006111671682447195\n",
      "[step: 1820] loss: 25.137815475463867\n",
      "[step: 1820] loss: 0.006107613909989595\n",
      "[step: 1821] loss: 24.786346435546875\n",
      "[step: 1821] loss: 0.006103849969804287\n",
      "[step: 1822] loss: 24.175743103027344\n",
      "[step: 1822] loss: 0.006101218052208424\n",
      "[step: 1823] loss: 23.962312698364258\n",
      "[step: 1823] loss: 0.006099889986217022\n",
      "[step: 1824] loss: 24.12432289123535\n",
      "[step: 1824] loss: 0.006099659483879805\n",
      "[step: 1825] loss: 24.070728302001953\n",
      "[step: 1825] loss: 0.006100485101342201\n",
      "[step: 1826] loss: 23.831310272216797\n",
      "[step: 1826] loss: 0.00610268535092473\n",
      "[step: 1827] loss: 23.611541748046875\n",
      "[step: 1827] loss: 0.006106378044933081\n",
      "[step: 1828] loss: 23.511672973632812\n",
      "[step: 1828] loss: 0.006113293580710888\n",
      "[step: 1829] loss: 23.486047744750977\n",
      "[step: 1829] loss: 0.0061240908689796925\n",
      "[step: 1830] loss: 23.477617263793945\n",
      "[step: 1830] loss: 0.006147311069071293\n",
      "[step: 1831] loss: 23.562332153320312\n",
      "[step: 1831] loss: 0.00617904681712389\n",
      "[step: 1832] loss: 23.6980037689209\n",
      "[step: 1832] loss: 0.006250455975532532\n",
      "[step: 1833] loss: 23.757904052734375\n",
      "[step: 1833] loss: 0.006320078391581774\n",
      "[step: 1834] loss: 23.68781280517578\n",
      "[step: 1834] loss: 0.006424091290682554\n",
      "[step: 1835] loss: 23.62349510192871\n",
      "[step: 1835] loss: 0.006517586763948202\n",
      "[step: 1836] loss: 23.788301467895508\n",
      "[step: 1836] loss: 0.006539753172546625\n",
      "[step: 1837] loss: 24.268238067626953\n",
      "[step: 1837] loss: 0.0064296601340174675\n",
      "[step: 1838] loss: 25.000471115112305\n",
      "[step: 1838] loss: 0.006259015761315823\n",
      "[step: 1839] loss: 26.019664764404297\n",
      "[step: 1839] loss: 0.006148782558739185\n",
      "[step: 1840] loss: 27.437274932861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1840] loss: 0.0061990912072360516\n",
      "[step: 1841] loss: 29.676271438598633\n",
      "[step: 1841] loss: 0.006333618890494108\n",
      "[step: 1842] loss: 32.7305908203125\n",
      "[step: 1842] loss: 0.006357815582305193\n",
      "[step: 1843] loss: 36.23177719116211\n",
      "[step: 1843] loss: 0.006253522355109453\n",
      "[step: 1844] loss: 38.791648864746094\n",
      "[step: 1844] loss: 0.006150849629193544\n",
      "[step: 1845] loss: 37.684600830078125\n",
      "[step: 1845] loss: 0.006198662333190441\n",
      "[step: 1846] loss: 32.89370346069336\n",
      "[step: 1846] loss: 0.006248071789741516\n",
      "[step: 1847] loss: 26.652690887451172\n",
      "[step: 1847] loss: 0.006205975078046322\n",
      "[step: 1848] loss: 23.775039672851562\n",
      "[step: 1848] loss: 0.006195788737386465\n",
      "[step: 1849] loss: 25.698150634765625\n",
      "[step: 1849] loss: 0.006229191552847624\n",
      "[step: 1850] loss: 28.734310150146484\n",
      "[step: 1850] loss: 0.006236772984266281\n",
      "[step: 1851] loss: 28.582141876220703\n",
      "[step: 1851] loss: 0.006154022645205259\n",
      "[step: 1852] loss: 25.776744842529297\n",
      "[step: 1852] loss: 0.006136390846222639\n",
      "[step: 1853] loss: 24.314014434814453\n",
      "[step: 1853] loss: 0.006218400783836842\n",
      "[step: 1854] loss: 25.39021110534668\n",
      "[step: 1854] loss: 0.006210194900631905\n",
      "[step: 1855] loss: 26.362499237060547\n",
      "[step: 1855] loss: 0.006152734626084566\n",
      "[step: 1856] loss: 25.275001525878906\n",
      "[step: 1856] loss: 0.006141710095107555\n",
      "[step: 1857] loss: 23.771717071533203\n",
      "[step: 1857] loss: 0.006173768546432257\n",
      "[step: 1858] loss: 23.67458724975586\n",
      "[step: 1858] loss: 0.006179868709295988\n",
      "[step: 1859] loss: 24.535078048706055\n",
      "[step: 1859] loss: 0.0061207725666463375\n",
      "[step: 1860] loss: 24.900421142578125\n",
      "[step: 1860] loss: 0.006148454267531633\n",
      "[step: 1861] loss: 24.411855697631836\n",
      "[step: 1861] loss: 0.00620140740647912\n",
      "[step: 1862] loss: 23.869165420532227\n",
      "[step: 1862] loss: 0.006140839774161577\n",
      "[step: 1863] loss: 23.863121032714844\n",
      "[step: 1863] loss: 0.006103606894612312\n",
      "[step: 1864] loss: 24.302562713623047\n",
      "[step: 1864] loss: 0.0061332834884524345\n",
      "[step: 1865] loss: 24.58399772644043\n",
      "[step: 1865] loss: 0.006160642486065626\n",
      "[step: 1866] loss: 24.380905151367188\n",
      "[step: 1866] loss: 0.006133595481514931\n",
      "[step: 1867] loss: 23.97269058227539\n",
      "[step: 1867] loss: 0.006104621570557356\n",
      "[step: 1868] loss: 23.889448165893555\n",
      "[step: 1868] loss: 0.006120341829955578\n",
      "[step: 1869] loss: 24.387271881103516\n",
      "[step: 1869] loss: 0.006134336814284325\n",
      "[step: 1870] loss: 24.865478515625\n",
      "[step: 1870] loss: 0.006112613249570131\n",
      "[step: 1871] loss: 24.9975528717041\n",
      "[step: 1871] loss: 0.006106923334300518\n",
      "[step: 1872] loss: 25.13723373413086\n",
      "[step: 1872] loss: 0.006111546419560909\n",
      "[step: 1873] loss: 26.045330047607422\n",
      "[step: 1873] loss: 0.006111502181738615\n",
      "[step: 1874] loss: 27.56084442138672\n",
      "[step: 1874] loss: 0.006097055971622467\n",
      "[step: 1875] loss: 29.552947998046875\n",
      "[step: 1875] loss: 0.006100200116634369\n",
      "[step: 1876] loss: 30.745418548583984\n",
      "[step: 1876] loss: 0.006106862332671881\n",
      "[step: 1877] loss: 32.061988830566406\n",
      "[step: 1877] loss: 0.006102414336055517\n",
      "[step: 1878] loss: 31.841222763061523\n",
      "[step: 1878] loss: 0.006088055204600096\n",
      "[step: 1879] loss: 31.202003479003906\n",
      "[step: 1879] loss: 0.006091509014368057\n",
      "[step: 1880] loss: 28.679893493652344\n",
      "[step: 1880] loss: 0.006098788231611252\n",
      "[step: 1881] loss: 25.183795928955078\n",
      "[step: 1881] loss: 0.00609723711386323\n",
      "[step: 1882] loss: 23.429046630859375\n",
      "[step: 1882] loss: 0.006086132489144802\n",
      "[step: 1883] loss: 24.211523056030273\n",
      "[step: 1883] loss: 0.006084860768169165\n",
      "[step: 1884] loss: 25.686370849609375\n",
      "[step: 1884] loss: 0.0060907285660505295\n",
      "[step: 1885] loss: 25.984596252441406\n",
      "[step: 1885] loss: 0.0060910130850970745\n",
      "[step: 1886] loss: 25.441530227661133\n",
      "[step: 1886] loss: 0.0060861483216285706\n",
      "[step: 1887] loss: 24.93511199951172\n",
      "[step: 1887] loss: 0.006084492430090904\n",
      "[step: 1888] loss: 24.50125503540039\n",
      "[step: 1888] loss: 0.006086492445319891\n",
      "[step: 1889] loss: 23.69376564025879\n",
      "[step: 1889] loss: 0.006086794193834066\n",
      "[step: 1890] loss: 23.17095947265625\n",
      "[step: 1890] loss: 0.006087332498282194\n",
      "[step: 1891] loss: 23.60857391357422\n",
      "[step: 1891] loss: 0.006089268252253532\n",
      "[step: 1892] loss: 24.400022506713867\n",
      "[step: 1892] loss: 0.006095315329730511\n",
      "[step: 1893] loss: 24.59366798400879\n",
      "[step: 1893] loss: 0.0061029293574392796\n",
      "[step: 1894] loss: 23.910566329956055\n",
      "[step: 1894] loss: 0.006116964388638735\n",
      "[step: 1895] loss: 23.324352264404297\n",
      "[step: 1895] loss: 0.0061332411132752895\n",
      "[step: 1896] loss: 23.222755432128906\n",
      "[step: 1896] loss: 0.006170464213937521\n",
      "[step: 1897] loss: 23.179298400878906\n",
      "[step: 1897] loss: 0.006166120991110802\n",
      "[step: 1898] loss: 23.06344223022461\n",
      "[step: 1898] loss: 0.006164869759231806\n",
      "[step: 1899] loss: 23.087554931640625\n",
      "[step: 1899] loss: 0.006147137377411127\n",
      "[step: 1900] loss: 23.383813858032227\n",
      "[step: 1900] loss: 0.006126479245722294\n",
      "[step: 1901] loss: 23.494394302368164\n",
      "[step: 1901] loss: 0.006103644613176584\n",
      "[step: 1902] loss: 23.212308883666992\n",
      "[step: 1902] loss: 0.006084865424782038\n",
      "[step: 1903] loss: 22.759967803955078\n",
      "[step: 1903] loss: 0.006077397149056196\n",
      "[step: 1904] loss: 22.575469970703125\n",
      "[step: 1904] loss: 0.006080014631152153\n",
      "[step: 1905] loss: 22.62502670288086\n",
      "[step: 1905] loss: 0.0060869064182043076\n",
      "[step: 1906] loss: 22.668142318725586\n",
      "[step: 1906] loss: 0.006095536053180695\n",
      "[step: 1907] loss: 22.658634185791016\n",
      "[step: 1907] loss: 0.006105566862970591\n",
      "[step: 1908] loss: 22.721721649169922\n",
      "[step: 1908] loss: 0.006115930620580912\n",
      "[step: 1909] loss: 22.910430908203125\n",
      "[step: 1909] loss: 0.006133770104497671\n",
      "[step: 1910] loss: 23.080894470214844\n",
      "[step: 1910] loss: 0.006128828041255474\n",
      "[step: 1911] loss: 23.22981071472168\n",
      "[step: 1911] loss: 0.006127086468040943\n",
      "[step: 1912] loss: 23.39849853515625\n",
      "[step: 1912] loss: 0.0061157625168561935\n",
      "[step: 1913] loss: 23.90433120727539\n",
      "[step: 1913] loss: 0.006104859057813883\n",
      "[step: 1914] loss: 25.11371421813965\n",
      "[step: 1914] loss: 0.006092429626733065\n",
      "[step: 1915] loss: 27.24089813232422\n",
      "[step: 1915] loss: 0.0060815103352069855\n",
      "[step: 1916] loss: 31.63054656982422\n",
      "[step: 1916] loss: 0.006074817851185799\n",
      "[step: 1917] loss: 37.636531829833984\n",
      "[step: 1917] loss: 0.006073111668229103\n",
      "[step: 1918] loss: 47.18527603149414\n",
      "[step: 1918] loss: 0.006072987336665392\n",
      "[step: 1919] loss: 52.71535110473633\n",
      "[step: 1919] loss: 0.006072829011827707\n",
      "[step: 1920] loss: 51.86634063720703\n",
      "[step: 1920] loss: 0.006072785705327988\n",
      "[step: 1921] loss: 40.67619323730469\n",
      "[step: 1921] loss: 0.006074864882975817\n",
      "[step: 1922] loss: 27.74679946899414\n",
      "[step: 1922] loss: 0.006080807652324438\n",
      "[step: 1923] loss: 25.537513732910156\n",
      "[step: 1923] loss: 0.006087607238441706\n",
      "[step: 1924] loss: 32.43848419189453\n",
      "[step: 1924] loss: 0.006100459489971399\n",
      "[step: 1925] loss: 36.03397750854492\n",
      "[step: 1925] loss: 0.006109291687607765\n",
      "[step: 1926] loss: 30.85965347290039\n",
      "[step: 1926] loss: 0.0061288573779165745\n",
      "[step: 1927] loss: 25.104408264160156\n",
      "[step: 1927] loss: 0.006147488486021757\n",
      "[step: 1928] loss: 25.861255645751953\n",
      "[step: 1928] loss: 0.006174940150231123\n",
      "[step: 1929] loss: 29.41412353515625\n",
      "[step: 1929] loss: 0.006196002941578627\n",
      "[step: 1930] loss: 29.56209945678711\n",
      "[step: 1930] loss: 0.006212061271071434\n",
      "[step: 1931] loss: 26.304841995239258\n",
      "[step: 1931] loss: 0.0062084682285785675\n",
      "[step: 1932] loss: 24.200334548950195\n",
      "[step: 1932] loss: 0.006189433857798576\n",
      "[step: 1933] loss: 25.041362762451172\n",
      "[step: 1933] loss: 0.006155470386147499\n",
      "[step: 1934] loss: 27.01158905029297\n",
      "[step: 1934] loss: 0.006122268736362457\n",
      "[step: 1935] loss: 26.33962631225586\n",
      "[step: 1935] loss: 0.006097850389778614\n",
      "[step: 1936] loss: 23.533388137817383\n",
      "[step: 1936] loss: 0.006087572779506445\n",
      "[step: 1937] loss: 23.628177642822266\n",
      "[step: 1937] loss: 0.006090334616601467\n",
      "[step: 1938] loss: 25.279191970825195\n",
      "[step: 1938] loss: 0.006105739623308182\n",
      "[step: 1939] loss: 25.104467391967773\n",
      "[step: 1939] loss: 0.006133955903351307\n",
      "[step: 1940] loss: 23.81281280517578\n",
      "[step: 1940] loss: 0.006149739492684603\n",
      "[step: 1941] loss: 22.87912368774414\n",
      "[step: 1941] loss: 0.00616030627861619\n",
      "[step: 1942] loss: 23.604204177856445\n",
      "[step: 1942] loss: 0.006132167298346758\n",
      "[step: 1943] loss: 24.466829299926758\n",
      "[step: 1943] loss: 0.006100200116634369\n",
      "[step: 1944] loss: 23.519804000854492\n",
      "[step: 1944] loss: 0.006077950354665518\n",
      "[step: 1945] loss: 22.59358787536621\n",
      "[step: 1945] loss: 0.0060714902356266975\n",
      "[step: 1946] loss: 23.13689613342285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1946] loss: 0.006075003184378147\n",
      "[step: 1947] loss: 23.506513595581055\n",
      "[step: 1947] loss: 0.006083854008466005\n",
      "[step: 1948] loss: 22.934192657470703\n",
      "[step: 1948] loss: 0.006098831072449684\n",
      "[step: 1949] loss: 22.768203735351562\n",
      "[step: 1949] loss: 0.006111486814916134\n",
      "[step: 1950] loss: 23.00847053527832\n",
      "[step: 1950] loss: 0.006120216101408005\n",
      "[step: 1951] loss: 22.882165908813477\n",
      "[step: 1951] loss: 0.006117669399827719\n",
      "[step: 1952] loss: 22.570526123046875\n",
      "[step: 1952] loss: 0.006110194604843855\n",
      "[step: 1953] loss: 22.546119689941406\n",
      "[step: 1953] loss: 0.006098173093050718\n",
      "[step: 1954] loss: 22.7725830078125\n",
      "[step: 1954] loss: 0.00609514070674777\n",
      "[step: 1955] loss: 22.89859390258789\n",
      "[step: 1955] loss: 0.0060851131565868855\n",
      "[step: 1956] loss: 22.629871368408203\n",
      "[step: 1956] loss: 0.006082474719733\n",
      "[step: 1957] loss: 22.45070457458496\n",
      "[step: 1957] loss: 0.006077130325138569\n",
      "[step: 1958] loss: 22.671791076660156\n",
      "[step: 1958] loss: 0.006075391545891762\n",
      "[step: 1959] loss: 22.93922233581543\n",
      "[step: 1959] loss: 0.006075534038245678\n",
      "[step: 1960] loss: 23.011211395263672\n",
      "[step: 1960] loss: 0.006076599005609751\n",
      "[step: 1961] loss: 23.273109436035156\n",
      "[step: 1961] loss: 0.006078767590224743\n",
      "[step: 1962] loss: 23.765300750732422\n",
      "[step: 1962] loss: 0.006080725230276585\n",
      "[step: 1963] loss: 24.380077362060547\n",
      "[step: 1963] loss: 0.006085465662181377\n",
      "[step: 1964] loss: 24.95332908630371\n",
      "[step: 1964] loss: 0.006088736932724714\n",
      "[step: 1965] loss: 25.902572631835938\n",
      "[step: 1965] loss: 0.006098889745771885\n",
      "[step: 1966] loss: 26.86376190185547\n",
      "[step: 1966] loss: 0.006100744474679232\n",
      "[step: 1967] loss: 27.892295837402344\n",
      "[step: 1967] loss: 0.006107109598815441\n",
      "[step: 1968] loss: 27.940444946289062\n",
      "[step: 1968] loss: 0.006102242972701788\n",
      "[step: 1969] loss: 27.234336853027344\n",
      "[step: 1969] loss: 0.006097226869314909\n",
      "[step: 1970] loss: 25.591283798217773\n",
      "[step: 1970] loss: 0.00608577998355031\n",
      "[step: 1971] loss: 23.915847778320312\n",
      "[step: 1971] loss: 0.006077067460864782\n",
      "[step: 1972] loss: 22.493366241455078\n",
      "[step: 1972] loss: 0.006066874600946903\n",
      "[step: 1973] loss: 22.074907302856445\n",
      "[step: 1973] loss: 0.006060446612536907\n",
      "[step: 1974] loss: 22.74807357788086\n",
      "[step: 1974] loss: 0.006057459861040115\n",
      "[step: 1975] loss: 23.72256851196289\n",
      "[step: 1975] loss: 0.006057469639927149\n",
      "[step: 1976] loss: 24.297409057617188\n",
      "[step: 1976] loss: 0.006059715989977121\n",
      "[step: 1977] loss: 24.120954513549805\n",
      "[step: 1977] loss: 0.006063823588192463\n",
      "[step: 1978] loss: 23.597454071044922\n",
      "[step: 1978] loss: 0.006068550515919924\n",
      "[step: 1979] loss: 22.877628326416016\n",
      "[step: 1979] loss: 0.006074529606848955\n",
      "[step: 1980] loss: 22.282772064208984\n",
      "[step: 1980] loss: 0.00608162023127079\n",
      "[step: 1981] loss: 21.96548843383789\n",
      "[step: 1981] loss: 0.006091008428484201\n",
      "[step: 1982] loss: 22.053762435913086\n",
      "[step: 1982] loss: 0.006102571729570627\n",
      "[step: 1983] loss: 22.44001007080078\n",
      "[step: 1983] loss: 0.006117502227425575\n",
      "[step: 1984] loss: 22.827043533325195\n",
      "[step: 1984] loss: 0.006134269293397665\n",
      "[step: 1985] loss: 23.01287841796875\n",
      "[step: 1985] loss: 0.0061620003543794155\n",
      "[step: 1986] loss: 22.940088272094727\n",
      "[step: 1986] loss: 0.006200545001775026\n",
      "[step: 1987] loss: 22.833106994628906\n",
      "[step: 1987] loss: 0.006274071522057056\n",
      "[step: 1988] loss: 22.728858947753906\n",
      "[step: 1988] loss: 0.0063600093126297\n",
      "[step: 1989] loss: 22.700958251953125\n",
      "[step: 1989] loss: 0.006406278815120459\n",
      "[step: 1990] loss: 22.728858947753906\n",
      "[step: 1990] loss: 0.006317550782114267\n",
      "[step: 1991] loss: 22.815174102783203\n",
      "[step: 1991] loss: 0.006167714949697256\n",
      "[step: 1992] loss: 22.917919158935547\n",
      "[step: 1992] loss: 0.006123885978013277\n",
      "[step: 1993] loss: 22.968555450439453\n",
      "[step: 1993] loss: 0.006246240809559822\n",
      "[step: 1994] loss: 22.93305206298828\n",
      "[step: 1994] loss: 0.006270808167755604\n",
      "[step: 1995] loss: 22.866506576538086\n",
      "[step: 1995] loss: 0.006201387848705053\n",
      "[step: 1996] loss: 22.857921600341797\n",
      "[step: 1996] loss: 0.006163607817143202\n",
      "[step: 1997] loss: 23.019344329833984\n",
      "[step: 1997] loss: 0.006220858544111252\n",
      "[step: 1998] loss: 23.339820861816406\n",
      "[step: 1998] loss: 0.006215882953256369\n",
      "[step: 1999] loss: 23.92020034790039\n",
      "[step: 1999] loss: 0.006129919085651636\n",
      "[step: 2000] loss: 24.665569305419922\n",
      "[step: 2000] loss: 0.006098082754760981\n",
      "[step: 2001] loss: 25.834733963012695\n",
      "[step: 2001] loss: 0.006143940147012472\n",
      "[step: 2002] loss: 27.077848434448242\n",
      "[step: 2002] loss: 0.006111315917223692\n",
      "[step: 2003] loss: 28.606639862060547\n",
      "[step: 2003] loss: 0.006069309543818235\n",
      "[step: 2004] loss: 29.403940200805664\n",
      "[step: 2004] loss: 0.006118866614997387\n",
      "[step: 2005] loss: 29.688003540039062\n",
      "[step: 2005] loss: 0.0061536491848528385\n",
      "[step: 2006] loss: 28.58386993408203\n",
      "[step: 2006] loss: 0.006165983621031046\n",
      "[step: 2007] loss: 26.83538055419922\n",
      "[step: 2007] loss: 0.006152550224214792\n",
      "[step: 2008] loss: 24.76717185974121\n",
      "[step: 2008] loss: 0.006156153045594692\n",
      "[step: 2009] loss: 23.129953384399414\n",
      "[step: 2009] loss: 0.0061164130456745625\n",
      "[step: 2010] loss: 22.536195755004883\n",
      "[step: 2010] loss: 0.006101209204643965\n",
      "[step: 2011] loss: 22.97825813293457\n",
      "[step: 2011] loss: 0.006108203902840614\n",
      "[step: 2012] loss: 23.913299560546875\n",
      "[step: 2012] loss: 0.006078673992305994\n",
      "[step: 2013] loss: 24.522144317626953\n",
      "[step: 2013] loss: 0.006110462825745344\n",
      "[step: 2014] loss: 24.572834014892578\n",
      "[step: 2014] loss: 0.006143753882497549\n",
      "[step: 2015] loss: 23.965591430664062\n",
      "[step: 2015] loss: 0.006109773647040129\n",
      "[step: 2016] loss: 23.125816345214844\n",
      "[step: 2016] loss: 0.006080461200326681\n",
      "[step: 2017] loss: 22.379243850708008\n",
      "[step: 2017] loss: 0.006090215872973204\n",
      "[step: 2018] loss: 21.982494354248047\n",
      "[step: 2018] loss: 0.00609011622145772\n",
      "[step: 2019] loss: 21.962289810180664\n",
      "[step: 2019] loss: 0.006115632597357035\n",
      "[step: 2020] loss: 22.22079086303711\n",
      "[step: 2020] loss: 0.006085881032049656\n",
      "[step: 2021] loss: 22.645469665527344\n",
      "[step: 2021] loss: 0.0060719880275428295\n",
      "[step: 2022] loss: 23.034568786621094\n",
      "[step: 2022] loss: 0.006108894478529692\n",
      "[step: 2023] loss: 23.211692810058594\n",
      "[step: 2023] loss: 0.0061185904778540134\n",
      "[step: 2024] loss: 23.144943237304688\n",
      "[step: 2024] loss: 0.006120243575423956\n",
      "[step: 2025] loss: 22.968111038208008\n",
      "[step: 2025] loss: 0.006089517381042242\n",
      "[step: 2026] loss: 22.7883358001709\n",
      "[step: 2026] loss: 0.006118506658822298\n",
      "[step: 2027] loss: 22.708681106567383\n",
      "[step: 2027] loss: 0.006130009889602661\n",
      "[step: 2028] loss: 22.5862979888916\n",
      "[step: 2028] loss: 0.006085793953388929\n",
      "[step: 2029] loss: 22.49374771118164\n",
      "[step: 2029] loss: 0.006062916945666075\n",
      "[step: 2030] loss: 22.372455596923828\n",
      "[step: 2030] loss: 0.0060661653988063335\n",
      "[step: 2031] loss: 22.30617332458496\n",
      "[step: 2031] loss: 0.0060898843221366405\n",
      "[step: 2032] loss: 22.214771270751953\n",
      "[step: 2032] loss: 0.006096599157899618\n",
      "[step: 2033] loss: 22.116069793701172\n",
      "[step: 2033] loss: 0.006067337468266487\n",
      "[step: 2034] loss: 21.951662063598633\n",
      "[step: 2034] loss: 0.006057786755263805\n",
      "[step: 2035] loss: 21.79264259338379\n",
      "[step: 2035] loss: 0.006094755604863167\n",
      "[step: 2036] loss: 21.658161163330078\n",
      "[step: 2036] loss: 0.006086898036301136\n",
      "[step: 2037] loss: 21.60211944580078\n",
      "[step: 2037] loss: 0.006082913838326931\n",
      "[step: 2038] loss: 21.615459442138672\n",
      "[step: 2038] loss: 0.00606964435428381\n",
      "[step: 2039] loss: 21.70627212524414\n",
      "[step: 2039] loss: 0.006084317807108164\n",
      "[step: 2040] loss: 21.888553619384766\n",
      "[step: 2040] loss: 0.006098141428083181\n",
      "[step: 2041] loss: 22.235912322998047\n",
      "[step: 2041] loss: 0.0060723694041371346\n",
      "[step: 2042] loss: 22.845623016357422\n",
      "[step: 2042] loss: 0.0060503664426505566\n",
      "[step: 2043] loss: 23.96088218688965\n",
      "[step: 2043] loss: 0.006055163219571114\n",
      "[step: 2044] loss: 25.828369140625\n",
      "[step: 2044] loss: 0.0060735950246453285\n",
      "[step: 2045] loss: 28.942955017089844\n",
      "[step: 2045] loss: 0.0060724359937012196\n",
      "[step: 2046] loss: 33.247840881347656\n",
      "[step: 2046] loss: 0.006053248420357704\n",
      "[step: 2047] loss: 38.97755432128906\n",
      "[step: 2047] loss: 0.006051645614206791\n",
      "[step: 2048] loss: 42.66224670410156\n",
      "[step: 2048] loss: 0.006069722585380077\n",
      "[step: 2049] loss: 43.72151184082031\n",
      "[step: 2049] loss: 0.006072371266782284\n",
      "[step: 2050] loss: 37.63157653808594\n",
      "[step: 2050] loss: 0.006062671076506376\n",
      "[step: 2051] loss: 28.935667037963867\n",
      "[step: 2051] loss: 0.006059750448912382\n",
      "[step: 2052] loss: 23.864948272705078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2052] loss: 0.006079118698835373\n",
      "[step: 2053] loss: 25.99291229248047\n",
      "[step: 2053] loss: 0.006126268766820431\n",
      "[step: 2054] loss: 31.207679748535156\n",
      "[step: 2054] loss: 0.00612106267362833\n",
      "[step: 2055] loss: 31.77459144592285\n",
      "[step: 2055] loss: 0.0061347465962171555\n",
      "[step: 2056] loss: 27.537487030029297\n",
      "[step: 2056] loss: 0.006166724022477865\n",
      "[step: 2057] loss: 22.856510162353516\n",
      "[step: 2057] loss: 0.006184716708958149\n",
      "[step: 2058] loss: 22.86526870727539\n",
      "[step: 2058] loss: 0.006194517016410828\n",
      "[step: 2059] loss: 25.837093353271484\n",
      "[step: 2059] loss: 0.0061834524385631084\n",
      "[step: 2060] loss: 26.809432983398438\n",
      "[step: 2060] loss: 0.006156783550977707\n",
      "[step: 2061] loss: 25.258817672729492\n",
      "[step: 2061] loss: 0.006122139282524586\n",
      "[step: 2062] loss: 23.023927688598633\n",
      "[step: 2062] loss: 0.006069916766136885\n",
      "[step: 2063] loss: 22.563447952270508\n",
      "[step: 2063] loss: 0.006036614533513784\n",
      "[step: 2064] loss: 23.844154357910156\n",
      "[step: 2064] loss: 0.006051696836948395\n",
      "[step: 2065] loss: 24.45743179321289\n",
      "[step: 2065] loss: 0.00607127184048295\n",
      "[step: 2066] loss: 23.430328369140625\n",
      "[step: 2066] loss: 0.006110591813921928\n",
      "[step: 2067] loss: 21.716794967651367\n",
      "[step: 2067] loss: 0.00611761724576354\n",
      "[step: 2068] loss: 21.88309097290039\n",
      "[step: 2068] loss: 0.006130731664597988\n",
      "[step: 2069] loss: 23.036331176757812\n",
      "[step: 2069] loss: 0.006111629772931337\n",
      "[step: 2070] loss: 22.86458969116211\n",
      "[step: 2070] loss: 0.006065970286726952\n",
      "[step: 2071] loss: 22.00387191772461\n",
      "[step: 2071] loss: 0.006042220164090395\n",
      "[step: 2072] loss: 21.733707427978516\n",
      "[step: 2072] loss: 0.00604286277666688\n",
      "[step: 2073] loss: 22.041309356689453\n",
      "[step: 2073] loss: 0.006055863108485937\n",
      "[step: 2074] loss: 22.138492584228516\n",
      "[step: 2074] loss: 0.0060592410154640675\n",
      "[step: 2075] loss: 21.894927978515625\n",
      "[step: 2075] loss: 0.006054155994206667\n",
      "[step: 2076] loss: 21.741493225097656\n",
      "[step: 2076] loss: 0.006053508259356022\n",
      "[step: 2077] loss: 21.63557243347168\n",
      "[step: 2077] loss: 0.006058697588741779\n",
      "[step: 2078] loss: 21.596973419189453\n",
      "[step: 2078] loss: 0.006058268714696169\n",
      "[step: 2079] loss: 21.64065170288086\n",
      "[step: 2079] loss: 0.006061702035367489\n",
      "[step: 2080] loss: 21.685558319091797\n",
      "[step: 2080] loss: 0.006046280264854431\n",
      "[step: 2081] loss: 21.670608520507812\n",
      "[step: 2081] loss: 0.006045857444405556\n",
      "[step: 2082] loss: 21.7623291015625\n",
      "[step: 2082] loss: 0.006046554539352655\n",
      "[step: 2083] loss: 22.028640747070312\n",
      "[step: 2083] loss: 0.006036388222128153\n",
      "[step: 2084] loss: 22.359636306762695\n",
      "[step: 2084] loss: 0.00603209063410759\n",
      "[step: 2085] loss: 22.81503677368164\n",
      "[step: 2085] loss: 0.006034158170223236\n",
      "[step: 2086] loss: 23.707077026367188\n",
      "[step: 2086] loss: 0.006042005494236946\n",
      "[step: 2087] loss: 25.13468360900879\n",
      "[step: 2087] loss: 0.00604304438456893\n",
      "[step: 2088] loss: 27.257186889648438\n",
      "[step: 2088] loss: 0.006042012479156256\n",
      "[step: 2089] loss: 29.444156646728516\n",
      "[step: 2089] loss: 0.006045873276889324\n",
      "[step: 2090] loss: 31.23995590209961\n",
      "[step: 2090] loss: 0.006062833592295647\n",
      "[step: 2091] loss: 31.11241912841797\n",
      "[step: 2091] loss: 0.006079497747123241\n",
      "[step: 2092] loss: 29.090694427490234\n",
      "[step: 2092] loss: 0.006105917040258646\n",
      "[step: 2093] loss: 26.403255462646484\n",
      "[step: 2093] loss: 0.006109090056270361\n",
      "[step: 2094] loss: 25.843088150024414\n",
      "[step: 2094] loss: 0.006124828476458788\n",
      "[step: 2095] loss: 26.86566925048828\n",
      "[step: 2095] loss: 0.006134505849331617\n",
      "[step: 2096] loss: 26.575881958007812\n",
      "[step: 2096] loss: 0.006131167057901621\n",
      "[step: 2097] loss: 24.323156356811523\n",
      "[step: 2097] loss: 0.006107641849666834\n",
      "[step: 2098] loss: 22.48407745361328\n",
      "[step: 2098] loss: 0.006081758067011833\n",
      "[step: 2099] loss: 22.926189422607422\n",
      "[step: 2099] loss: 0.006057137623429298\n",
      "[step: 2100] loss: 23.07303237915039\n",
      "[step: 2100] loss: 0.006043410860002041\n",
      "[step: 2101] loss: 22.217105865478516\n",
      "[step: 2101] loss: 0.006028139963746071\n",
      "[step: 2102] loss: 22.322404861450195\n",
      "[step: 2102] loss: 0.006019356660544872\n",
      "[step: 2103] loss: 23.26955795288086\n",
      "[step: 2103] loss: 0.006023433059453964\n",
      "[step: 2104] loss: 23.247421264648438\n",
      "[step: 2104] loss: 0.006033385172486305\n",
      "[step: 2105] loss: 22.048294067382812\n",
      "[step: 2105] loss: 0.006047157105058432\n",
      "[step: 2106] loss: 21.658958435058594\n",
      "[step: 2106] loss: 0.0060508968308568\n",
      "[step: 2107] loss: 21.809850692749023\n",
      "[step: 2107] loss: 0.0060587311163544655\n",
      "[step: 2108] loss: 21.4949951171875\n",
      "[step: 2108] loss: 0.006064968183636665\n",
      "[step: 2109] loss: 21.202646255493164\n",
      "[step: 2109] loss: 0.006072420161217451\n",
      "[step: 2110] loss: 21.698509216308594\n",
      "[step: 2110] loss: 0.00607425207272172\n",
      "[step: 2111] loss: 22.196392059326172\n",
      "[step: 2111] loss: 0.006073597352951765\n",
      "[step: 2112] loss: 21.94325828552246\n",
      "[step: 2112] loss: 0.006068097427487373\n",
      "[step: 2113] loss: 21.563800811767578\n",
      "[step: 2113] loss: 0.006067492067813873\n",
      "[step: 2114] loss: 21.661663055419922\n",
      "[step: 2114] loss: 0.006051906384527683\n",
      "[step: 2115] loss: 21.84286880493164\n",
      "[step: 2115] loss: 0.006036865059286356\n",
      "[step: 2116] loss: 21.977401733398438\n",
      "[step: 2116] loss: 0.006023148540407419\n",
      "[step: 2117] loss: 22.49958610534668\n",
      "[step: 2117] loss: 0.0060158222913742065\n",
      "[step: 2118] loss: 23.77763557434082\n",
      "[step: 2118] loss: 0.006012178026139736\n",
      "[step: 2119] loss: 25.255096435546875\n",
      "[step: 2119] loss: 0.00601097010076046\n",
      "[step: 2120] loss: 27.36042022705078\n",
      "[step: 2120] loss: 0.006011164281517267\n",
      "[step: 2121] loss: 29.22559356689453\n",
      "[step: 2121] loss: 0.006012708880007267\n",
      "[step: 2122] loss: 31.254379272460938\n",
      "[step: 2122] loss: 0.00601526303216815\n",
      "[step: 2123] loss: 31.071208953857422\n",
      "[step: 2123] loss: 0.006017300765961409\n",
      "[step: 2124] loss: 29.42761993408203\n",
      "[step: 2124] loss: 0.006021364592015743\n",
      "[step: 2125] loss: 26.13578987121582\n",
      "[step: 2125] loss: 0.006026554852724075\n",
      "[step: 2126] loss: 23.4827880859375\n",
      "[step: 2126] loss: 0.006037420593202114\n",
      "[step: 2127] loss: 22.661930084228516\n",
      "[step: 2127] loss: 0.006045232061296701\n",
      "[step: 2128] loss: 23.276893615722656\n",
      "[step: 2128] loss: 0.006066923029720783\n",
      "[step: 2129] loss: 24.225648880004883\n",
      "[step: 2129] loss: 0.006078392267227173\n",
      "[step: 2130] loss: 24.416797637939453\n",
      "[step: 2130] loss: 0.006106801796704531\n",
      "[step: 2131] loss: 24.17068099975586\n",
      "[step: 2131] loss: 0.006139787379652262\n",
      "[step: 2132] loss: 23.305431365966797\n",
      "[step: 2132] loss: 0.006194199435412884\n",
      "[step: 2133] loss: 22.798137664794922\n",
      "[step: 2133] loss: 0.006248651072382927\n",
      "[step: 2134] loss: 22.62445831298828\n",
      "[step: 2134] loss: 0.006305576767772436\n",
      "[step: 2135] loss: 22.786869049072266\n",
      "[step: 2135] loss: 0.006291515193879604\n",
      "[step: 2136] loss: 22.887948989868164\n",
      "[step: 2136] loss: 0.006216700188815594\n",
      "[step: 2137] loss: 22.355648040771484\n",
      "[step: 2137] loss: 0.006090898532420397\n",
      "[step: 2138] loss: 21.818073272705078\n",
      "[step: 2138] loss: 0.006023666355758905\n",
      "[step: 2139] loss: 21.687374114990234\n",
      "[step: 2139] loss: 0.0060490272007882595\n",
      "[step: 2140] loss: 22.053176879882812\n",
      "[step: 2140] loss: 0.006107104942202568\n",
      "[step: 2141] loss: 22.369369506835938\n",
      "[step: 2141] loss: 0.00612993398681283\n",
      "[step: 2142] loss: 22.291547775268555\n",
      "[step: 2142] loss: 0.006096895318478346\n",
      "[step: 2143] loss: 21.884685516357422\n",
      "[step: 2143] loss: 0.006076259072870016\n",
      "[step: 2144] loss: 21.35946273803711\n",
      "[step: 2144] loss: 0.0060662697069346905\n",
      "[step: 2145] loss: 20.911117553710938\n",
      "[step: 2145] loss: 0.006075923331081867\n",
      "[step: 2146] loss: 20.71820640563965\n",
      "[step: 2146] loss: 0.0060572270303964615\n",
      "[step: 2147] loss: 20.815101623535156\n",
      "[step: 2147] loss: 0.00601808400824666\n",
      "[step: 2148] loss: 21.07339096069336\n",
      "[step: 2148] loss: 0.0060192556120455265\n",
      "[step: 2149] loss: 21.355545043945312\n",
      "[step: 2149] loss: 0.006051541306078434\n",
      "[step: 2150] loss: 21.512619018554688\n",
      "[step: 2150] loss: 0.006067256908863783\n",
      "[step: 2151] loss: 21.533361434936523\n",
      "[step: 2151] loss: 0.006049867253750563\n",
      "[step: 2152] loss: 21.41535186767578\n",
      "[step: 2152] loss: 0.006044273730367422\n",
      "[step: 2153] loss: 21.44806480407715\n",
      "[step: 2153] loss: 0.006043939385563135\n",
      "[step: 2154] loss: 21.710233688354492\n",
      "[step: 2154] loss: 0.006045422051101923\n",
      "[step: 2155] loss: 22.257226943969727\n",
      "[step: 2155] loss: 0.006028398405760527\n",
      "[step: 2156] loss: 22.96670913696289\n",
      "[step: 2156] loss: 0.006011257879436016\n",
      "[step: 2157] loss: 23.880958557128906\n",
      "[step: 2157] loss: 0.006008653901517391\n",
      "[step: 2158] loss: 24.896221160888672\n",
      "[step: 2158] loss: 0.006013772916048765\n",
      "[step: 2159] loss: 26.077564239501953\n",
      "[step: 2159] loss: 0.006012863479554653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2160] loss: 26.983154296875\n",
      "[step: 2160] loss: 0.006008302327245474\n",
      "[step: 2161] loss: 27.881330490112305\n",
      "[step: 2161] loss: 0.006010623648762703\n",
      "[step: 2162] loss: 27.77271270751953\n",
      "[step: 2162] loss: 0.006019404157996178\n",
      "[step: 2163] loss: 27.246570587158203\n",
      "[step: 2163] loss: 0.006026551593095064\n",
      "[step: 2164] loss: 25.693540573120117\n",
      "[step: 2164] loss: 0.006030181422829628\n",
      "[step: 2165] loss: 23.856868743896484\n",
      "[step: 2165] loss: 0.006051687058061361\n",
      "[step: 2166] loss: 22.167810440063477\n",
      "[step: 2166] loss: 0.0060546440072357655\n",
      "[step: 2167] loss: 21.10091209411621\n",
      "[step: 2167] loss: 0.006064519286155701\n",
      "[step: 2168] loss: 20.989898681640625\n",
      "[step: 2168] loss: 0.006056865211576223\n",
      "[step: 2169] loss: 21.673717498779297\n",
      "[step: 2169] loss: 0.006049347575753927\n",
      "[step: 2170] loss: 22.66195297241211\n",
      "[step: 2170] loss: 0.0060399435460567474\n",
      "[step: 2171] loss: 23.327014923095703\n",
      "[step: 2171] loss: 0.006029966752976179\n",
      "[step: 2172] loss: 23.301944732666016\n",
      "[step: 2172] loss: 0.0060177636332809925\n",
      "[step: 2173] loss: 22.604045867919922\n",
      "[step: 2173] loss: 0.00600891700014472\n",
      "[step: 2174] loss: 21.649988174438477\n",
      "[step: 2174] loss: 0.006005296017974615\n",
      "[step: 2175] loss: 20.904640197753906\n",
      "[step: 2175] loss: 0.006003918591886759\n",
      "[step: 2176] loss: 20.608659744262695\n",
      "[step: 2176] loss: 0.006000591441988945\n",
      "[step: 2177] loss: 20.650665283203125\n",
      "[step: 2177] loss: 0.005997539963573217\n",
      "[step: 2178] loss: 20.834157943725586\n",
      "[step: 2178] loss: 0.005994412116706371\n",
      "[step: 2179] loss: 20.98282814025879\n",
      "[step: 2179] loss: 0.005994649138301611\n",
      "[step: 2180] loss: 21.07701873779297\n",
      "[step: 2180] loss: 0.005995647981762886\n",
      "[step: 2181] loss: 21.15057373046875\n",
      "[step: 2181] loss: 0.005994862411171198\n",
      "[step: 2182] loss: 21.246479034423828\n",
      "[step: 2182] loss: 0.005992588587105274\n",
      "[step: 2183] loss: 21.458499908447266\n",
      "[step: 2183] loss: 0.005991787649691105\n",
      "[step: 2184] loss: 21.71405029296875\n",
      "[step: 2184] loss: 0.005993652623146772\n",
      "[step: 2185] loss: 22.103933334350586\n",
      "[step: 2185] loss: 0.00599805498495698\n",
      "[step: 2186] loss: 22.50670051574707\n",
      "[step: 2186] loss: 0.00600268691778183\n",
      "[step: 2187] loss: 23.149566650390625\n",
      "[step: 2187] loss: 0.006012485828250647\n",
      "[step: 2188] loss: 23.873151779174805\n",
      "[step: 2188] loss: 0.0060232593677937984\n",
      "[step: 2189] loss: 24.916160583496094\n",
      "[step: 2189] loss: 0.006053805351257324\n",
      "[step: 2190] loss: 25.728008270263672\n",
      "[step: 2190] loss: 0.006077106576412916\n",
      "[step: 2191] loss: 26.52245330810547\n",
      "[step: 2191] loss: 0.006126442924141884\n",
      "[step: 2192] loss: 26.483165740966797\n",
      "[step: 2192] loss: 0.006153740454465151\n",
      "[step: 2193] loss: 26.047622680664062\n",
      "[step: 2193] loss: 0.006182197947055101\n",
      "[step: 2194] loss: 25.220993041992188\n",
      "[step: 2194] loss: 0.006184449885040522\n",
      "[step: 2195] loss: 24.807748794555664\n",
      "[step: 2195] loss: 0.006172026041895151\n",
      "[step: 2196] loss: 24.39071273803711\n",
      "[step: 2196] loss: 0.006157317664474249\n",
      "[step: 2197] loss: 24.64788055419922\n",
      "[step: 2197] loss: 0.006161975674331188\n",
      "[step: 2198] loss: 24.277070999145508\n",
      "[step: 2198] loss: 0.00617147795855999\n",
      "[step: 2199] loss: 23.560653686523438\n",
      "[step: 2199] loss: 0.0061447471380233765\n",
      "[step: 2200] loss: 22.506540298461914\n",
      "[step: 2200] loss: 0.006059711799025536\n",
      "[step: 2201] loss: 21.722190856933594\n",
      "[step: 2201] loss: 0.006001834291964769\n",
      "[step: 2202] loss: 21.65338706970215\n",
      "[step: 2202] loss: 0.006027770694345236\n",
      "[step: 2203] loss: 21.773218154907227\n",
      "[step: 2203] loss: 0.00607621856033802\n",
      "[step: 2204] loss: 21.764755249023438\n",
      "[step: 2204] loss: 0.006074503995478153\n",
      "[step: 2205] loss: 21.501834869384766\n",
      "[step: 2205] loss: 0.006045380607247353\n",
      "[step: 2206] loss: 21.507362365722656\n",
      "[step: 2206] loss: 0.006053153891116381\n",
      "[step: 2207] loss: 21.829986572265625\n",
      "[step: 2207] loss: 0.00606228644028306\n",
      "[step: 2208] loss: 22.163448333740234\n",
      "[step: 2208] loss: 0.006047647446393967\n",
      "[step: 2209] loss: 22.09940528869629\n",
      "[step: 2209] loss: 0.006020817440003157\n",
      "[step: 2210] loss: 21.553836822509766\n",
      "[step: 2210] loss: 0.006015257444232702\n",
      "[step: 2211] loss: 20.9964542388916\n",
      "[step: 2211] loss: 0.006003340240567923\n",
      "[step: 2212] loss: 20.788318634033203\n",
      "[step: 2212] loss: 0.005991083569824696\n",
      "[step: 2213] loss: 20.906532287597656\n",
      "[step: 2213] loss: 0.006003568880259991\n",
      "[step: 2214] loss: 21.213956832885742\n",
      "[step: 2214] loss: 0.006021489389240742\n",
      "[step: 2215] loss: 21.54400634765625\n",
      "[step: 2215] loss: 0.006031638942658901\n",
      "[step: 2216] loss: 21.879945755004883\n",
      "[step: 2216] loss: 0.00600597495213151\n",
      "[step: 2217] loss: 22.03588104248047\n",
      "[step: 2217] loss: 0.006004031747579575\n",
      "[step: 2218] loss: 22.295747756958008\n",
      "[step: 2218] loss: 0.006009100470691919\n",
      "[step: 2219] loss: 22.719058990478516\n",
      "[step: 2219] loss: 0.0060007572174072266\n",
      "[step: 2220] loss: 23.556739807128906\n",
      "[step: 2220] loss: 0.005999567918479443\n",
      "[step: 2221] loss: 24.482402801513672\n",
      "[step: 2221] loss: 0.0060028494335711\n",
      "[step: 2222] loss: 25.56690216064453\n",
      "[step: 2222] loss: 0.005999337416142225\n",
      "[step: 2223] loss: 26.750566482543945\n",
      "[step: 2223] loss: 0.005988801829516888\n",
      "[step: 2224] loss: 28.54104995727539\n",
      "[step: 2224] loss: 0.005988378543406725\n",
      "[step: 2225] loss: 30.188514709472656\n",
      "[step: 2225] loss: 0.005993615835905075\n",
      "[step: 2226] loss: 31.829673767089844\n",
      "[step: 2226] loss: 0.006000368390232325\n",
      "[step: 2227] loss: 29.796369552612305\n",
      "[step: 2227] loss: 0.0060065677389502525\n",
      "[step: 2228] loss: 26.485836029052734\n",
      "[step: 2228] loss: 0.006022579502314329\n",
      "[step: 2229] loss: 22.991897583007812\n",
      "[step: 2229] loss: 0.0060373651795089245\n",
      "[step: 2230] loss: 21.896364212036133\n",
      "[step: 2230] loss: 0.006058285478502512\n",
      "[step: 2231] loss: 22.228336334228516\n",
      "[step: 2231] loss: 0.006080297287553549\n",
      "[step: 2232] loss: 22.26669692993164\n",
      "[step: 2232] loss: 0.006121711805462837\n",
      "[step: 2233] loss: 22.040382385253906\n",
      "[step: 2233] loss: 0.006114193703979254\n",
      "[step: 2234] loss: 22.21599578857422\n",
      "[step: 2234] loss: 0.006090173497796059\n",
      "[step: 2235] loss: 22.677806854248047\n",
      "[step: 2235] loss: 0.006049260497093201\n",
      "[step: 2236] loss: 22.7904109954834\n",
      "[step: 2236] loss: 0.006009556353092194\n",
      "[step: 2237] loss: 22.11170768737793\n",
      "[step: 2237] loss: 0.005993098486214876\n",
      "[step: 2238] loss: 21.267139434814453\n",
      "[step: 2238] loss: 0.005993890110403299\n",
      "[step: 2239] loss: 21.253787994384766\n",
      "[step: 2239] loss: 0.0060032946057617664\n",
      "[step: 2240] loss: 21.83338165283203\n",
      "[step: 2240] loss: 0.006013296078890562\n",
      "[step: 2241] loss: 22.416038513183594\n",
      "[step: 2241] loss: 0.006032206118106842\n",
      "[step: 2242] loss: 22.158071517944336\n",
      "[step: 2242] loss: 0.006035025231540203\n",
      "[step: 2243] loss: 21.619686126708984\n",
      "[step: 2243] loss: 0.006043537519872189\n",
      "[step: 2244] loss: 21.30213165283203\n",
      "[step: 2244] loss: 0.006041965447366238\n",
      "[step: 2245] loss: 21.4160099029541\n",
      "[step: 2245] loss: 0.006034845486283302\n",
      "[step: 2246] loss: 21.256023406982422\n",
      "[step: 2246] loss: 0.006020986475050449\n",
      "[step: 2247] loss: 20.900623321533203\n",
      "[step: 2247] loss: 0.006005570292472839\n",
      "[step: 2248] loss: 20.7978515625\n",
      "[step: 2248] loss: 0.005990271922200918\n",
      "[step: 2249] loss: 21.100933074951172\n",
      "[step: 2249] loss: 0.005977540276944637\n",
      "[step: 2250] loss: 21.199567794799805\n",
      "[step: 2250] loss: 0.00597026152536273\n",
      "[step: 2251] loss: 20.92449378967285\n",
      "[step: 2251] loss: 0.005974726285785437\n",
      "[step: 2252] loss: 20.587657928466797\n",
      "[step: 2252] loss: 0.005980763118714094\n",
      "[step: 2253] loss: 20.618648529052734\n",
      "[step: 2253] loss: 0.005986101925373077\n",
      "[step: 2254] loss: 20.81602668762207\n",
      "[step: 2254] loss: 0.0059886048547923565\n",
      "[step: 2255] loss: 20.89620590209961\n",
      "[step: 2255] loss: 0.005990142002701759\n",
      "[step: 2256] loss: 20.82362174987793\n",
      "[step: 2256] loss: 0.005989706609398127\n",
      "[step: 2257] loss: 20.973682403564453\n",
      "[step: 2257] loss: 0.005993583705276251\n",
      "[step: 2258] loss: 21.392215728759766\n",
      "[step: 2258] loss: 0.00600752467289567\n",
      "[step: 2259] loss: 22.008289337158203\n",
      "[step: 2259] loss: 0.006021811626851559\n",
      "[step: 2260] loss: 22.56283187866211\n",
      "[step: 2260] loss: 0.0060551902279257774\n",
      "[step: 2261] loss: 23.451457977294922\n",
      "[step: 2261] loss: 0.006046084687113762\n",
      "[step: 2262] loss: 24.498931884765625\n",
      "[step: 2262] loss: 0.006036428268998861\n",
      "[step: 2263] loss: 26.232025146484375\n",
      "[step: 2263] loss: 0.006023698952049017\n",
      "[step: 2264] loss: 28.0006046295166\n",
      "[step: 2264] loss: 0.00600562384352088\n",
      "[step: 2265] loss: 29.889385223388672\n",
      "[step: 2265] loss: 0.006001313682645559\n",
      "[step: 2266] loss: 30.764307022094727\n",
      "[step: 2266] loss: 0.005998973734676838\n",
      "[step: 2267] loss: 30.313278198242188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2267] loss: 0.005990137811750174\n",
      "[step: 2268] loss: 28.106035232543945\n",
      "[step: 2268] loss: 0.005981932859867811\n",
      "[step: 2269] loss: 25.171401977539062\n",
      "[step: 2269] loss: 0.005979926325380802\n",
      "[step: 2270] loss: 22.368255615234375\n",
      "[step: 2270] loss: 0.005977804306894541\n",
      "[step: 2271] loss: 20.960390090942383\n",
      "[step: 2271] loss: 0.005974334664642811\n",
      "[step: 2272] loss: 20.993431091308594\n",
      "[step: 2272] loss: 0.005974388215690851\n",
      "[step: 2273] loss: 22.049041748046875\n",
      "[step: 2273] loss: 0.0059769717045128345\n",
      "[step: 2274] loss: 23.41463851928711\n",
      "[step: 2274] loss: 0.005980620626360178\n",
      "[step: 2275] loss: 24.01244354248047\n",
      "[step: 2275] loss: 0.005983611103147268\n",
      "[step: 2276] loss: 23.644458770751953\n",
      "[step: 2276] loss: 0.005994435865432024\n",
      "[step: 2277] loss: 22.161102294921875\n",
      "[step: 2277] loss: 0.006016944069415331\n",
      "[step: 2278] loss: 20.772077560424805\n",
      "[step: 2278] loss: 0.006024923641234636\n",
      "[step: 2279] loss: 20.146438598632812\n",
      "[step: 2279] loss: 0.006048356648534536\n",
      "[step: 2280] loss: 20.364431381225586\n",
      "[step: 2280] loss: 0.00605496671050787\n",
      "[step: 2281] loss: 20.981937408447266\n",
      "[step: 2281] loss: 0.006048797629773617\n",
      "[step: 2282] loss: 21.486482620239258\n",
      "[step: 2282] loss: 0.006036385428160429\n",
      "[step: 2283] loss: 21.877212524414062\n",
      "[step: 2283] loss: 0.0060209063813090324\n",
      "[step: 2284] loss: 22.07925033569336\n",
      "[step: 2284] loss: 0.0060156285762786865\n",
      "[step: 2285] loss: 22.17487144470215\n",
      "[step: 2285] loss: 0.006013520993292332\n",
      "[step: 2286] loss: 21.843603134155273\n",
      "[step: 2286] loss: 0.006027281284332275\n",
      "[step: 2287] loss: 21.233745574951172\n",
      "[step: 2287] loss: 0.006063108332455158\n",
      "[step: 2288] loss: 20.450454711914062\n",
      "[step: 2288] loss: 0.006160736083984375\n",
      "[step: 2289] loss: 19.953197479248047\n",
      "[step: 2289] loss: 0.006129487883299589\n",
      "[step: 2290] loss: 19.888708114624023\n",
      "[step: 2290] loss: 0.006145566701889038\n",
      "[step: 2291] loss: 20.043825149536133\n",
      "[step: 2291] loss: 0.006138492841273546\n",
      "[step: 2292] loss: 20.127647399902344\n",
      "[step: 2292] loss: 0.006074217148125172\n",
      "[step: 2293] loss: 20.037681579589844\n",
      "[step: 2293] loss: 0.0060529145412147045\n",
      "[step: 2294] loss: 19.963956832885742\n",
      "[step: 2294] loss: 0.00601298687979579\n",
      "[step: 2295] loss: 20.044330596923828\n",
      "[step: 2295] loss: 0.006007311400026083\n",
      "[step: 2296] loss: 20.300540924072266\n",
      "[step: 2296] loss: 0.006035699974745512\n",
      "[step: 2297] loss: 20.583574295043945\n",
      "[step: 2297] loss: 0.0060467892326414585\n",
      "[step: 2298] loss: 20.872928619384766\n",
      "[step: 2298] loss: 0.006011585704982281\n",
      "[step: 2299] loss: 21.068191528320312\n",
      "[step: 2299] loss: 0.006000084336847067\n",
      "[step: 2300] loss: 21.433574676513672\n",
      "[step: 2300] loss: 0.005985144060105085\n",
      "[step: 2301] loss: 21.923107147216797\n",
      "[step: 2301] loss: 0.005981202702969313\n",
      "[step: 2302] loss: 22.786998748779297\n",
      "[step: 2302] loss: 0.005989653989672661\n",
      "[step: 2303] loss: 23.860103607177734\n",
      "[step: 2303] loss: 0.006000046152621508\n",
      "[step: 2304] loss: 25.19556999206543\n",
      "[step: 2304] loss: 0.005981984548270702\n",
      "[step: 2305] loss: 26.509544372558594\n",
      "[step: 2305] loss: 0.005973614752292633\n",
      "[step: 2306] loss: 27.77398681640625\n",
      "[step: 2306] loss: 0.005980333779007196\n",
      "[step: 2307] loss: 28.237632751464844\n",
      "[step: 2307] loss: 0.005971598904579878\n",
      "[step: 2308] loss: 27.989439010620117\n",
      "[step: 2308] loss: 0.005971274804323912\n",
      "[step: 2309] loss: 26.179088592529297\n",
      "[step: 2309] loss: 0.005973879713565111\n",
      "[step: 2310] loss: 23.76169204711914\n",
      "[step: 2310] loss: 0.0059712971560657024\n",
      "[step: 2311] loss: 21.280332565307617\n",
      "[step: 2311] loss: 0.005966817494481802\n",
      "[step: 2312] loss: 19.91826629638672\n",
      "[step: 2312] loss: 0.005965427029877901\n",
      "[step: 2313] loss: 19.967369079589844\n",
      "[step: 2313] loss: 0.0059568015858531\n",
      "[step: 2314] loss: 20.98226547241211\n",
      "[step: 2314] loss: 0.005954265128821135\n",
      "[step: 2315] loss: 22.129499435424805\n",
      "[step: 2315] loss: 0.0059583550319075584\n",
      "[step: 2316] loss: 22.395694732666016\n",
      "[step: 2316] loss: 0.005964904557913542\n",
      "[step: 2317] loss: 21.842714309692383\n",
      "[step: 2317] loss: 0.005963875912129879\n",
      "[step: 2318] loss: 20.786266326904297\n",
      "[step: 2318] loss: 0.005962503608316183\n",
      "[step: 2319] loss: 20.0827579498291\n",
      "[step: 2319] loss: 0.00596517464146018\n",
      "[step: 2320] loss: 20.106487274169922\n",
      "[step: 2320] loss: 0.005970342084765434\n",
      "[step: 2321] loss: 20.76426887512207\n",
      "[step: 2321] loss: 0.00599227799102664\n",
      "[step: 2322] loss: 21.562759399414062\n",
      "[step: 2322] loss: 0.006026807241141796\n",
      "[step: 2323] loss: 22.21668243408203\n",
      "[step: 2323] loss: 0.006117170210927725\n",
      "[step: 2324] loss: 22.436729431152344\n",
      "[step: 2324] loss: 0.0062258606776595116\n",
      "[step: 2325] loss: 22.6992244720459\n",
      "[step: 2325] loss: 0.0064069353975355625\n",
      "[step: 2326] loss: 23.07058334350586\n",
      "[step: 2326] loss: 0.006489693187177181\n",
      "[step: 2327] loss: 24.47524070739746\n",
      "[step: 2327] loss: 0.006423445884138346\n",
      "[step: 2328] loss: 26.46377182006836\n",
      "[step: 2328] loss: 0.006221394054591656\n",
      "[step: 2329] loss: 29.751827239990234\n",
      "[step: 2329] loss: 0.006043458357453346\n",
      "[step: 2330] loss: 31.47333335876465\n",
      "[step: 2330] loss: 0.006104068364948034\n",
      "[step: 2331] loss: 32.28711700439453\n",
      "[step: 2331] loss: 0.006185524631291628\n",
      "[step: 2332] loss: 29.04961395263672\n",
      "[step: 2332] loss: 0.00615199888125062\n",
      "[step: 2333] loss: 24.773635864257812\n",
      "[step: 2333] loss: 0.0060526905581355095\n",
      "[step: 2334] loss: 21.571096420288086\n",
      "[step: 2334] loss: 0.006041360087692738\n",
      "[step: 2335] loss: 21.214859008789062\n",
      "[step: 2335] loss: 0.006063863635063171\n",
      "[step: 2336] loss: 22.770326614379883\n",
      "[step: 2336] loss: 0.006067868322134018\n",
      "[step: 2337] loss: 23.85947608947754\n",
      "[step: 2337] loss: 0.006032919976860285\n",
      "[step: 2338] loss: 24.002565383911133\n",
      "[step: 2338] loss: 0.0060059679672122\n",
      "[step: 2339] loss: 23.10013198852539\n",
      "[step: 2339] loss: 0.006025847513228655\n",
      "[step: 2340] loss: 22.20553970336914\n",
      "[step: 2340] loss: 0.006014551036059856\n",
      "[step: 2341] loss: 21.714174270629883\n",
      "[step: 2341] loss: 0.006004169583320618\n",
      "[step: 2342] loss: 21.610122680664062\n",
      "[step: 2342] loss: 0.005988015327602625\n",
      "[step: 2343] loss: 21.422210693359375\n",
      "[step: 2343] loss: 0.006007106974720955\n",
      "[step: 2344] loss: 21.085105895996094\n",
      "[step: 2344] loss: 0.005991390440613031\n",
      "[step: 2345] loss: 20.79862403869629\n",
      "[step: 2345] loss: 0.005986382719129324\n",
      "[step: 2346] loss: 20.595508575439453\n",
      "[step: 2346] loss: 0.005981012247502804\n",
      "[step: 2347] loss: 20.417701721191406\n",
      "[step: 2347] loss: 0.005976472981274128\n",
      "[step: 2348] loss: 20.222976684570312\n",
      "[step: 2348] loss: 0.00597419124096632\n",
      "[step: 2349] loss: 20.29092788696289\n",
      "[step: 2349] loss: 0.005978690460324287\n",
      "[step: 2350] loss: 20.483257293701172\n",
      "[step: 2350] loss: 0.005962872877717018\n",
      "[step: 2351] loss: 20.533287048339844\n",
      "[step: 2351] loss: 0.005956145003437996\n",
      "[step: 2352] loss: 20.323366165161133\n",
      "[step: 2352] loss: 0.005959419067949057\n",
      "[step: 2353] loss: 20.188209533691406\n",
      "[step: 2353] loss: 0.005960640497505665\n",
      "[step: 2354] loss: 20.320716857910156\n",
      "[step: 2354] loss: 0.005958929657936096\n",
      "[step: 2355] loss: 20.672359466552734\n",
      "[step: 2355] loss: 0.005959614180028439\n",
      "[step: 2356] loss: 20.941429138183594\n",
      "[step: 2356] loss: 0.0059523568488657475\n",
      "[step: 2357] loss: 21.149402618408203\n",
      "[step: 2357] loss: 0.005950308870524168\n",
      "[step: 2358] loss: 21.506975173950195\n",
      "[step: 2358] loss: 0.005952887702733278\n",
      "[step: 2359] loss: 22.160411834716797\n",
      "[step: 2359] loss: 0.005953791551291943\n",
      "[step: 2360] loss: 23.21538543701172\n",
      "[step: 2360] loss: 0.0059527079574763775\n",
      "[step: 2361] loss: 24.661611557006836\n",
      "[step: 2361] loss: 0.005953918676823378\n",
      "[step: 2362] loss: 26.42038917541504\n",
      "[step: 2362] loss: 0.005960374139249325\n",
      "[step: 2363] loss: 27.86388397216797\n",
      "[step: 2363] loss: 0.005969201680272818\n",
      "[step: 2364] loss: 27.92768669128418\n",
      "[step: 2364] loss: 0.005987047683447599\n",
      "[step: 2365] loss: 26.889690399169922\n",
      "[step: 2365] loss: 0.005990998353809118\n",
      "[step: 2366] loss: 25.2762451171875\n",
      "[step: 2366] loss: 0.00600258307531476\n",
      "[step: 2367] loss: 24.10399055480957\n",
      "[step: 2367] loss: 0.006002489011734724\n",
      "[step: 2368] loss: 23.15538215637207\n",
      "[step: 2368] loss: 0.006010290700942278\n",
      "[step: 2369] loss: 21.423240661621094\n",
      "[step: 2369] loss: 0.005999227985739708\n",
      "[step: 2370] loss: 19.89217185974121\n",
      "[step: 2370] loss: 0.005990117322653532\n",
      "[step: 2371] loss: 19.831932067871094\n",
      "[step: 2371] loss: 0.005974057596176863\n",
      "[step: 2372] loss: 21.103111267089844\n",
      "[step: 2372] loss: 0.005960556678473949\n",
      "[step: 2373] loss: 22.10071563720703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2373] loss: 0.005953038111329079\n",
      "[step: 2374] loss: 21.556114196777344\n",
      "[step: 2374] loss: 0.005946399178355932\n",
      "[step: 2375] loss: 20.454547882080078\n",
      "[step: 2375] loss: 0.005940471310168505\n",
      "[step: 2376] loss: 19.940942764282227\n",
      "[step: 2376] loss: 0.00593915767967701\n",
      "[step: 2377] loss: 20.117904663085938\n",
      "[step: 2377] loss: 0.005944002419710159\n",
      "[step: 2378] loss: 20.20616912841797\n",
      "[step: 2378] loss: 0.005951405968517065\n",
      "[step: 2379] loss: 19.805633544921875\n",
      "[step: 2379] loss: 0.005966938100755215\n",
      "[step: 2380] loss: 19.405858993530273\n",
      "[step: 2380] loss: 0.005980389192700386\n",
      "[step: 2381] loss: 19.460243225097656\n",
      "[step: 2381] loss: 0.005999239161610603\n",
      "[step: 2382] loss: 19.7991886138916\n",
      "[step: 2382] loss: 0.0060019725933671\n",
      "[step: 2383] loss: 19.91925048828125\n",
      "[step: 2383] loss: 0.006008929572999477\n",
      "[step: 2384] loss: 19.69758415222168\n",
      "[step: 2384] loss: 0.006006490904837847\n",
      "[step: 2385] loss: 19.484249114990234\n",
      "[step: 2385] loss: 0.005998645909130573\n",
      "[step: 2386] loss: 19.565776824951172\n",
      "[step: 2386] loss: 0.0059789614751935005\n",
      "[step: 2387] loss: 19.854965209960938\n",
      "[step: 2387] loss: 0.005959137808531523\n",
      "[step: 2388] loss: 20.049030303955078\n",
      "[step: 2388] loss: 0.005942066665738821\n",
      "[step: 2389] loss: 20.157569885253906\n",
      "[step: 2389] loss: 0.005933788605034351\n",
      "[step: 2390] loss: 20.455188751220703\n",
      "[step: 2390] loss: 0.005930056795477867\n",
      "[step: 2391] loss: 21.310043334960938\n",
      "[step: 2391] loss: 0.005930204410105944\n",
      "[step: 2392] loss: 22.577301025390625\n",
      "[step: 2392] loss: 0.005931952502578497\n",
      "[step: 2393] loss: 24.430850982666016\n",
      "[step: 2393] loss: 0.005934284534305334\n",
      "[step: 2394] loss: 26.22539520263672\n",
      "[step: 2394] loss: 0.005937600042670965\n",
      "[step: 2395] loss: 28.83920669555664\n",
      "[step: 2395] loss: 0.005941261537373066\n",
      "[step: 2396] loss: 30.260072708129883\n",
      "[step: 2396] loss: 0.005946125369518995\n",
      "[step: 2397] loss: 31.2661190032959\n",
      "[step: 2397] loss: 0.005950982216745615\n",
      "[step: 2398] loss: 29.755353927612305\n",
      "[step: 2398] loss: 0.005956336390227079\n",
      "[step: 2399] loss: 26.509204864501953\n",
      "[step: 2399] loss: 0.005960359703749418\n",
      "[step: 2400] loss: 22.854206085205078\n",
      "[step: 2400] loss: 0.005963987670838833\n",
      "[step: 2401] loss: 20.36180305480957\n",
      "[step: 2401] loss: 0.005965241696685553\n",
      "[step: 2402] loss: 20.210918426513672\n",
      "[step: 2402] loss: 0.005966817028820515\n",
      "[step: 2403] loss: 21.96946907043457\n",
      "[step: 2403] loss: 0.005964695941656828\n",
      "[step: 2404] loss: 23.696456909179688\n",
      "[step: 2404] loss: 0.005963362753391266\n",
      "[step: 2405] loss: 23.509241104125977\n",
      "[step: 2405] loss: 0.005958163645118475\n",
      "[step: 2406] loss: 22.086681365966797\n",
      "[step: 2406] loss: 0.005954604595899582\n",
      "[step: 2407] loss: 21.12295150756836\n",
      "[step: 2407] loss: 0.005951793398708105\n",
      "[step: 2408] loss: 20.974201202392578\n",
      "[step: 2408] loss: 0.005960612092167139\n",
      "[step: 2409] loss: 20.606908798217773\n",
      "[step: 2409] loss: 0.005976215936243534\n",
      "[step: 2410] loss: 20.09052276611328\n",
      "[step: 2410] loss: 0.006034001242369413\n",
      "[step: 2411] loss: 20.2601375579834\n",
      "[step: 2411] loss: 0.006035720929503441\n",
      "[step: 2412] loss: 20.846710205078125\n",
      "[step: 2412] loss: 0.006042084656655788\n",
      "[step: 2413] loss: 21.0411376953125\n",
      "[step: 2413] loss: 0.006045701447874308\n",
      "[step: 2414] loss: 20.372844696044922\n",
      "[step: 2414] loss: 0.006028946954756975\n",
      "[step: 2415] loss: 19.664798736572266\n",
      "[step: 2415] loss: 0.006012073718011379\n",
      "[step: 2416] loss: 19.358295440673828\n",
      "[step: 2416] loss: 0.005990294739603996\n",
      "[step: 2417] loss: 19.490192413330078\n",
      "[step: 2417] loss: 0.005960141774266958\n",
      "[step: 2418] loss: 19.799041748046875\n",
      "[step: 2418] loss: 0.005957235116511583\n",
      "[step: 2419] loss: 19.85740852355957\n",
      "[step: 2419] loss: 0.0059643518179655075\n",
      "[step: 2420] loss: 19.7861328125\n",
      "[step: 2420] loss: 0.0059599983505904675\n",
      "[step: 2421] loss: 19.716957092285156\n",
      "[step: 2421] loss: 0.005948286969214678\n",
      "[step: 2422] loss: 19.830322265625\n",
      "[step: 2422] loss: 0.005946024786680937\n",
      "[step: 2423] loss: 19.827455520629883\n",
      "[step: 2423] loss: 0.005943323951214552\n",
      "[step: 2424] loss: 19.576969146728516\n",
      "[step: 2424] loss: 0.0059412322007119656\n",
      "[step: 2425] loss: 19.26275634765625\n",
      "[step: 2425] loss: 0.005945916287600994\n",
      "[step: 2426] loss: 19.26343536376953\n",
      "[step: 2426] loss: 0.005939034279435873\n",
      "[step: 2427] loss: 19.62989044189453\n",
      "[step: 2427] loss: 0.005936391185969114\n",
      "[step: 2428] loss: 20.09832191467285\n",
      "[step: 2428] loss: 0.0059393648989498615\n",
      "[step: 2429] loss: 20.64533233642578\n",
      "[step: 2429] loss: 0.00594779197126627\n",
      "[step: 2430] loss: 21.477540969848633\n",
      "[step: 2430] loss: 0.005953541491180658\n",
      "[step: 2431] loss: 23.040870666503906\n",
      "[step: 2431] loss: 0.005960278213024139\n",
      "[step: 2432] loss: 25.27456283569336\n",
      "[step: 2432] loss: 0.005979722831398249\n",
      "[step: 2433] loss: 28.58955955505371\n",
      "[step: 2433] loss: 0.006022137124091387\n",
      "[step: 2434] loss: 30.72610855102539\n",
      "[step: 2434] loss: 0.006051857490092516\n",
      "[step: 2435] loss: 32.34465408325195\n",
      "[step: 2435] loss: 0.006092681083828211\n",
      "[step: 2436] loss: 30.813243865966797\n",
      "[step: 2436] loss: 0.00609534652903676\n",
      "[step: 2437] loss: 28.980623245239258\n",
      "[step: 2437] loss: 0.006084516178816557\n",
      "[step: 2438] loss: 28.09234619140625\n",
      "[step: 2438] loss: 0.006048886571079493\n",
      "[step: 2439] loss: 26.460365295410156\n",
      "[step: 2439] loss: 0.006003541871905327\n",
      "[step: 2440] loss: 23.147937774658203\n",
      "[step: 2440] loss: 0.00596098555251956\n",
      "[step: 2441] loss: 21.729080200195312\n",
      "[step: 2441] loss: 0.005937579553574324\n",
      "[step: 2442] loss: 23.581626892089844\n",
      "[step: 2442] loss: 0.005943258758634329\n",
      "[step: 2443] loss: 25.263038635253906\n",
      "[step: 2443] loss: 0.005967793520539999\n",
      "[step: 2444] loss: 22.209280014038086\n",
      "[step: 2444] loss: 0.00600012531504035\n",
      "[step: 2445] loss: 19.871997833251953\n",
      "[step: 2445] loss: 0.006008934695273638\n",
      "[step: 2446] loss: 20.913543701171875\n",
      "[step: 2446] loss: 0.006021320354193449\n",
      "[step: 2447] loss: 21.524272918701172\n",
      "[step: 2447] loss: 0.005972424987703562\n",
      "[step: 2448] loss: 20.485668182373047\n",
      "[step: 2448] loss: 0.00594303896650672\n",
      "[step: 2449] loss: 20.480947494506836\n",
      "[step: 2449] loss: 0.005935280118137598\n",
      "[step: 2450] loss: 21.34247589111328\n",
      "[step: 2450] loss: 0.005953227169811726\n",
      "[step: 2451] loss: 20.482318878173828\n",
      "[step: 2451] loss: 0.005987801123410463\n",
      "[step: 2452] loss: 19.38650131225586\n",
      "[step: 2452] loss: 0.0059877121821045876\n",
      "[step: 2453] loss: 19.88233184814453\n",
      "[step: 2453] loss: 0.005962300579994917\n",
      "[step: 2454] loss: 20.182254791259766\n",
      "[step: 2454] loss: 0.005941706709563732\n",
      "[step: 2455] loss: 19.49803352355957\n",
      "[step: 2455] loss: 0.005935853812843561\n",
      "[step: 2456] loss: 19.372112274169922\n",
      "[step: 2456] loss: 0.005928300321102142\n",
      "[step: 2457] loss: 19.88370704650879\n",
      "[step: 2457] loss: 0.005927324295043945\n",
      "[step: 2458] loss: 19.62106704711914\n",
      "[step: 2458] loss: 0.0059548974968492985\n",
      "[step: 2459] loss: 19.06344223022461\n",
      "[step: 2459] loss: 0.005932340398430824\n",
      "[step: 2460] loss: 19.21650505065918\n",
      "[step: 2460] loss: 0.005927604157477617\n",
      "[step: 2461] loss: 19.358154296875\n",
      "[step: 2461] loss: 0.005943617317825556\n",
      "[step: 2462] loss: 19.050291061401367\n",
      "[step: 2462] loss: 0.005950482562184334\n",
      "[step: 2463] loss: 19.00434684753418\n",
      "[step: 2463] loss: 0.005957178771495819\n",
      "[step: 2464] loss: 19.4189395904541\n",
      "[step: 2464] loss: 0.005936292465776205\n",
      "[step: 2465] loss: 19.55121612548828\n",
      "[step: 2465] loss: 0.005933180917054415\n",
      "[step: 2466] loss: 19.45867919921875\n",
      "[step: 2466] loss: 0.005932472180575132\n",
      "[step: 2467] loss: 19.798738479614258\n",
      "[step: 2467] loss: 0.005935169756412506\n",
      "[step: 2468] loss: 20.79522132873535\n",
      "[step: 2468] loss: 0.005945318378508091\n",
      "[step: 2469] loss: 22.079710006713867\n",
      "[step: 2469] loss: 0.005940628238022327\n",
      "[step: 2470] loss: 24.770811080932617\n",
      "[step: 2470] loss: 0.005918251350522041\n",
      "[step: 2471] loss: 28.732337951660156\n",
      "[step: 2471] loss: 0.005915970075875521\n",
      "[step: 2472] loss: 34.50855255126953\n",
      "[step: 2472] loss: 0.005924474447965622\n",
      "[step: 2473] loss: 38.57014083862305\n",
      "[step: 2473] loss: 0.005922437179833651\n",
      "[step: 2474] loss: 39.775596618652344\n",
      "[step: 2474] loss: 0.005917200818657875\n",
      "[step: 2475] loss: 32.69097137451172\n",
      "[step: 2475] loss: 0.005924171302467585\n",
      "[step: 2476] loss: 23.873220443725586\n",
      "[step: 2476] loss: 0.005926635582000017\n",
      "[step: 2477] loss: 19.539594650268555\n",
      "[step: 2477] loss: 0.005923878401517868\n",
      "[step: 2478] loss: 22.20387077331543\n",
      "[step: 2478] loss: 0.005933257285505533\n",
      "[step: 2479] loss: 27.136356353759766\n",
      "[step: 2479] loss: 0.005923672579228878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2480] loss: 28.16634750366211\n",
      "[step: 2480] loss: 0.005915157496929169\n",
      "[step: 2481] loss: 25.093660354614258\n",
      "[step: 2481] loss: 0.0059108431451022625\n",
      "[step: 2482] loss: 20.9443416595459\n",
      "[step: 2482] loss: 0.005913277622312307\n",
      "[step: 2483] loss: 19.325637817382812\n",
      "[step: 2483] loss: 0.0059133172035217285\n",
      "[step: 2484] loss: 21.299232482910156\n",
      "[step: 2484] loss: 0.005914107896387577\n",
      "[step: 2485] loss: 23.604372024536133\n",
      "[step: 2485] loss: 0.0059211538173258305\n",
      "[step: 2486] loss: 23.503490447998047\n",
      "[step: 2486] loss: 0.005941438023000956\n",
      "[step: 2487] loss: 21.1923885345459\n",
      "[step: 2487] loss: 0.005978605709969997\n",
      "[step: 2488] loss: 19.372516632080078\n",
      "[step: 2488] loss: 0.0060300566256046295\n",
      "[step: 2489] loss: 19.56588363647461\n",
      "[step: 2489] loss: 0.006098238285630941\n",
      "[step: 2490] loss: 20.572473526000977\n",
      "[step: 2490] loss: 0.006142128724604845\n",
      "[step: 2491] loss: 21.084762573242188\n",
      "[step: 2491] loss: 0.006165375933051109\n",
      "[step: 2492] loss: 20.613971710205078\n",
      "[step: 2492] loss: 0.006130054593086243\n",
      "[step: 2493] loss: 19.898677825927734\n",
      "[step: 2493] loss: 0.006141097750514746\n",
      "[step: 2494] loss: 19.582246780395508\n",
      "[step: 2494] loss: 0.006178927142173052\n",
      "[step: 2495] loss: 19.556344985961914\n",
      "[step: 2495] loss: 0.00629395293071866\n",
      "[step: 2496] loss: 19.54657745361328\n",
      "[step: 2496] loss: 0.006310510449111462\n",
      "[step: 2497] loss: 19.442211151123047\n",
      "[step: 2497] loss: 0.006196265574544668\n",
      "[step: 2498] loss: 19.394426345825195\n",
      "[step: 2498] loss: 0.006020198576152325\n",
      "[step: 2499] loss: 19.395009994506836\n",
      "[step: 2499] loss: 0.006026692688465118\n",
      "[step: 2500] loss: 19.21491241455078\n",
      "[step: 2500] loss: 0.006117118988186121\n",
      "[step: 2501] loss: 19.053680419921875\n",
      "[step: 2501] loss: 0.0060362196527421474\n",
      "[step: 2502] loss: 19.00950050354004\n",
      "[step: 2502] loss: 0.006065833382308483\n",
      "[step: 2503] loss: 19.0318660736084\n",
      "[step: 2503] loss: 0.006098762154579163\n",
      "[step: 2504] loss: 19.044719696044922\n",
      "[step: 2504] loss: 0.006020846776664257\n",
      "[step: 2505] loss: 19.050901412963867\n",
      "[step: 2505] loss: 0.005973594728857279\n",
      "[step: 2506] loss: 19.02997589111328\n",
      "[step: 2506] loss: 0.006044046487659216\n",
      "[step: 2507] loss: 18.941696166992188\n",
      "[step: 2507] loss: 0.006034284830093384\n",
      "[step: 2508] loss: 18.748937606811523\n",
      "[step: 2508] loss: 0.00600849837064743\n",
      "[step: 2509] loss: 18.592796325683594\n",
      "[step: 2509] loss: 0.006007223855704069\n",
      "[step: 2510] loss: 18.510587692260742\n",
      "[step: 2510] loss: 0.0059800902381539345\n",
      "[step: 2511] loss: 18.51349639892578\n",
      "[step: 2511] loss: 0.005922683514654636\n",
      "[step: 2512] loss: 18.567337036132812\n",
      "[step: 2512] loss: 0.005966246128082275\n",
      "[step: 2513] loss: 18.650848388671875\n",
      "[step: 2513] loss: 0.005973463878035545\n",
      "[step: 2514] loss: 18.719833374023438\n",
      "[step: 2514] loss: 0.005944112315773964\n",
      "[step: 2515] loss: 18.688587188720703\n",
      "[step: 2515] loss: 0.005950345192104578\n",
      "[step: 2516] loss: 18.59282684326172\n",
      "[step: 2516] loss: 0.005928205791860819\n",
      "[step: 2517] loss: 18.514549255371094\n",
      "[step: 2517] loss: 0.005927698686718941\n",
      "[step: 2518] loss: 18.515701293945312\n",
      "[step: 2518] loss: 0.005970583762973547\n",
      "[step: 2519] loss: 18.626033782958984\n",
      "[step: 2519] loss: 0.0059286002069711685\n",
      "[step: 2520] loss: 18.87087631225586\n",
      "[step: 2520] loss: 0.0059313648380339146\n",
      "[step: 2521] loss: 19.27169418334961\n",
      "[step: 2521] loss: 0.0059995027258992195\n",
      "[step: 2522] loss: 20.030242919921875\n",
      "[step: 2522] loss: 0.005963602103292942\n",
      "[step: 2523] loss: 21.127708435058594\n",
      "[step: 2523] loss: 0.005934431683272123\n",
      "[step: 2524] loss: 23.317604064941406\n",
      "[step: 2524] loss: 0.005948257632553577\n",
      "[step: 2525] loss: 25.975299835205078\n",
      "[step: 2525] loss: 0.00593761820346117\n",
      "[step: 2526] loss: 30.682811737060547\n",
      "[step: 2526] loss: 0.0059721809811890125\n",
      "[step: 2527] loss: 33.529083251953125\n",
      "[step: 2527] loss: 0.005952110514044762\n",
      "[step: 2528] loss: 35.643341064453125\n",
      "[step: 2528] loss: 0.0059438967145979404\n",
      "[step: 2529] loss: 34.29145812988281\n",
      "[step: 2529] loss: 0.0059694573283195496\n",
      "[step: 2530] loss: 32.998451232910156\n",
      "[step: 2530] loss: 0.005955687258392572\n",
      "[step: 2531] loss: 31.705228805541992\n",
      "[step: 2531] loss: 0.005938188172876835\n",
      "[step: 2532] loss: 30.093952178955078\n",
      "[step: 2532] loss: 0.0059381406754255295\n",
      "[step: 2533] loss: 25.385316848754883\n",
      "[step: 2533] loss: 0.005945500452071428\n",
      "[step: 2534] loss: 27.219097137451172\n",
      "[step: 2534] loss: 0.0059528229758143425\n",
      "[step: 2535] loss: 28.27954864501953\n",
      "[step: 2535] loss: 0.005937866400927305\n",
      "[step: 2536] loss: 23.449312210083008\n",
      "[step: 2536] loss: 0.005924595985561609\n",
      "[step: 2537] loss: 20.05404281616211\n",
      "[step: 2537] loss: 0.005941595416516066\n",
      "[step: 2538] loss: 23.01605987548828\n",
      "[step: 2538] loss: 0.005923192482441664\n",
      "[step: 2539] loss: 23.907793045043945\n",
      "[step: 2539] loss: 0.005926358513534069\n",
      "[step: 2540] loss: 22.760324478149414\n",
      "[step: 2540] loss: 0.005920506548136473\n",
      "[step: 2541] loss: 22.791778564453125\n",
      "[step: 2541] loss: 0.005946929566562176\n",
      "[step: 2542] loss: 19.985916137695312\n",
      "[step: 2542] loss: 0.005949125159531832\n",
      "[step: 2543] loss: 20.374937057495117\n",
      "[step: 2543] loss: 0.005927509628236294\n",
      "[step: 2544] loss: 22.469181060791016\n",
      "[step: 2544] loss: 0.005929762031883001\n",
      "[step: 2545] loss: 20.876506805419922\n",
      "[step: 2545] loss: 0.005960218142718077\n",
      "[step: 2546] loss: 20.23537826538086\n",
      "[step: 2546] loss: 0.005925195291638374\n",
      "[step: 2547] loss: 19.983083724975586\n",
      "[step: 2547] loss: 0.00591276865452528\n",
      "[step: 2548] loss: 19.68815803527832\n",
      "[step: 2548] loss: 0.005916757974773645\n",
      "[step: 2549] loss: 20.904857635498047\n",
      "[step: 2549] loss: 0.005937863606959581\n",
      "[step: 2550] loss: 20.028980255126953\n",
      "[step: 2550] loss: 0.0059428745880723\n",
      "[step: 2551] loss: 19.047801971435547\n",
      "[step: 2551] loss: 0.005926887504756451\n",
      "[step: 2552] loss: 20.015159606933594\n",
      "[step: 2552] loss: 0.005931637715548277\n",
      "[step: 2553] loss: 20.517349243164062\n",
      "[step: 2553] loss: 0.005959664471447468\n",
      "[step: 2554] loss: 21.19304847717285\n",
      "[step: 2554] loss: 0.005925342906266451\n",
      "[step: 2555] loss: 23.490741729736328\n",
      "[step: 2555] loss: 0.005923200864344835\n",
      "[step: 2556] loss: 26.315664291381836\n",
      "[step: 2556] loss: 0.005939490627497435\n",
      "[step: 2557] loss: 32.39971160888672\n",
      "[step: 2557] loss: 0.0059546600095927715\n",
      "[step: 2558] loss: 38.05286407470703\n",
      "[step: 2558] loss: 0.005967369303107262\n",
      "[step: 2559] loss: 41.98088073730469\n",
      "[step: 2559] loss: 0.005965827498584986\n",
      "[step: 2560] loss: 33.18169021606445\n",
      "[step: 2560] loss: 0.005980934016406536\n",
      "[step: 2561] loss: 23.824485778808594\n",
      "[step: 2561] loss: 0.0059799388982355595\n",
      "[step: 2562] loss: 20.750595092773438\n",
      "[step: 2562] loss: 0.0059286137111485004\n",
      "[step: 2563] loss: 24.51982879638672\n",
      "[step: 2563] loss: 0.0059254891239106655\n",
      "[step: 2564] loss: 28.457979202270508\n",
      "[step: 2564] loss: 0.00592770054936409\n",
      "[step: 2565] loss: 27.58506965637207\n",
      "[step: 2565] loss: 0.005914227571338415\n",
      "[step: 2566] loss: 23.654388427734375\n",
      "[step: 2566] loss: 0.005907482001930475\n",
      "[step: 2567] loss: 20.24346160888672\n",
      "[step: 2567] loss: 0.00590902753174305\n",
      "[step: 2568] loss: 20.620901107788086\n",
      "[step: 2568] loss: 0.005912374705076218\n",
      "[step: 2569] loss: 23.80743408203125\n",
      "[step: 2569] loss: 0.005903691053390503\n",
      "[step: 2570] loss: 23.791894912719727\n",
      "[step: 2570] loss: 0.0058846683241426945\n",
      "[step: 2571] loss: 20.94136619567871\n",
      "[step: 2571] loss: 0.005892484914511442\n",
      "[step: 2572] loss: 19.748441696166992\n",
      "[step: 2572] loss: 0.005902493372559547\n",
      "[step: 2573] loss: 20.202239990234375\n",
      "[step: 2573] loss: 0.005897433031350374\n",
      "[step: 2574] loss: 20.752567291259766\n",
      "[step: 2574] loss: 0.005885823629796505\n",
      "[step: 2575] loss: 20.821971893310547\n",
      "[step: 2575] loss: 0.0058932011015713215\n",
      "[step: 2576] loss: 20.109439849853516\n",
      "[step: 2576] loss: 0.005898191127926111\n",
      "[step: 2577] loss: 19.183414459228516\n",
      "[step: 2577] loss: 0.0059002055786550045\n",
      "[step: 2578] loss: 19.522686004638672\n",
      "[step: 2578] loss: 0.005903647281229496\n",
      "[step: 2579] loss: 19.706470489501953\n",
      "[step: 2579] loss: 0.005921165458858013\n",
      "[step: 2580] loss: 19.093116760253906\n",
      "[step: 2580] loss: 0.005948275327682495\n",
      "[step: 2581] loss: 19.095476150512695\n",
      "[step: 2581] loss: 0.005988716147840023\n",
      "[step: 2582] loss: 19.362010955810547\n",
      "[step: 2582] loss: 0.006029920186847448\n",
      "[step: 2583] loss: 19.13787078857422\n",
      "[step: 2583] loss: 0.006116104312241077\n",
      "[step: 2584] loss: 18.819129943847656\n",
      "[step: 2584] loss: 0.006097765173763037\n",
      "[step: 2585] loss: 18.60854721069336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2585] loss: 0.0060838558711111546\n",
      "[step: 2586] loss: 18.4986629486084\n",
      "[step: 2586] loss: 0.006030685268342495\n",
      "[step: 2587] loss: 18.62810516357422\n",
      "[step: 2587] loss: 0.005945692770183086\n",
      "[step: 2588] loss: 18.844139099121094\n",
      "[step: 2588] loss: 0.00593799352645874\n",
      "[step: 2589] loss: 18.743709564208984\n",
      "[step: 2589] loss: 0.005932244937866926\n",
      "[step: 2590] loss: 18.4302978515625\n",
      "[step: 2590] loss: 0.0059617371298372746\n",
      "[step: 2591] loss: 18.27147102355957\n",
      "[step: 2591] loss: 0.005984014365822077\n",
      "[step: 2592] loss: 18.391714096069336\n",
      "[step: 2592] loss: 0.005980844143778086\n",
      "[step: 2593] loss: 18.402503967285156\n",
      "[step: 2593] loss: 0.0059308554045856\n",
      "[step: 2594] loss: 18.196727752685547\n",
      "[step: 2594] loss: 0.005910607986152172\n",
      "[step: 2595] loss: 18.111631393432617\n",
      "[step: 2595] loss: 0.005892997141927481\n",
      "[step: 2596] loss: 18.214710235595703\n",
      "[step: 2596] loss: 0.005901416298002005\n",
      "[step: 2597] loss: 18.251014709472656\n",
      "[step: 2597] loss: 0.005912284832447767\n",
      "[step: 2598] loss: 18.23826789855957\n",
      "[step: 2598] loss: 0.0059194001369178295\n",
      "[step: 2599] loss: 18.256690979003906\n",
      "[step: 2599] loss: 0.005897814873605967\n",
      "[step: 2600] loss: 18.184494018554688\n",
      "[step: 2600] loss: 0.005879310425370932\n",
      "[step: 2601] loss: 18.06340789794922\n",
      "[step: 2601] loss: 0.005874800030142069\n",
      "[step: 2602] loss: 18.05139923095703\n",
      "[step: 2602] loss: 0.005888347048312426\n",
      "[step: 2603] loss: 18.094337463378906\n",
      "[step: 2603] loss: 0.005904457997530699\n",
      "[step: 2604] loss: 18.052181243896484\n",
      "[step: 2604] loss: 0.005899113602936268\n",
      "[step: 2605] loss: 17.945833206176758\n",
      "[step: 2605] loss: 0.005878861993551254\n",
      "[step: 2606] loss: 17.895954132080078\n",
      "[step: 2606] loss: 0.005872306879609823\n",
      "[step: 2607] loss: 17.89879608154297\n",
      "[step: 2607] loss: 0.00587221747264266\n",
      "[step: 2608] loss: 17.900226593017578\n",
      "[step: 2608] loss: 0.0058843884617090225\n",
      "[step: 2609] loss: 17.89242935180664\n",
      "[step: 2609] loss: 0.005888899322599173\n",
      "[step: 2610] loss: 17.880407333374023\n",
      "[step: 2610] loss: 0.005890164989978075\n",
      "[step: 2611] loss: 17.848697662353516\n",
      "[step: 2611] loss: 0.005889505613595247\n",
      "[step: 2612] loss: 17.81041717529297\n",
      "[step: 2612] loss: 0.005902052856981754\n",
      "[step: 2613] loss: 17.79608917236328\n",
      "[step: 2613] loss: 0.005930090788751841\n",
      "[step: 2614] loss: 17.805513381958008\n",
      "[step: 2614] loss: 0.0059423670172691345\n",
      "[step: 2615] loss: 17.811960220336914\n",
      "[step: 2615] loss: 0.005974872503429651\n",
      "[step: 2616] loss: 17.80140495300293\n",
      "[step: 2616] loss: 0.005942957475781441\n",
      "[step: 2617] loss: 17.776973724365234\n",
      "[step: 2617] loss: 0.005934546701610088\n",
      "[step: 2618] loss: 17.758575439453125\n",
      "[step: 2618] loss: 0.0059345499612390995\n",
      "[step: 2619] loss: 17.761791229248047\n",
      "[step: 2619] loss: 0.005907058250159025\n",
      "[step: 2620] loss: 17.786062240600586\n",
      "[step: 2620] loss: 0.0058932011015713215\n",
      "[step: 2621] loss: 17.82647705078125\n",
      "[step: 2621] loss: 0.005881956312805414\n",
      "[step: 2622] loss: 17.912906646728516\n",
      "[step: 2622] loss: 0.005895351059734821\n",
      "[step: 2623] loss: 18.103614807128906\n",
      "[step: 2623] loss: 0.005910305771976709\n",
      "[step: 2624] loss: 18.516666412353516\n",
      "[step: 2624] loss: 0.005916878115385771\n",
      "[step: 2625] loss: 19.39678955078125\n",
      "[step: 2625] loss: 0.005900382064282894\n",
      "[step: 2626] loss: 21.35951042175293\n",
      "[step: 2626] loss: 0.005896261427551508\n",
      "[step: 2627] loss: 25.282764434814453\n",
      "[step: 2627] loss: 0.005893378052860498\n",
      "[step: 2628] loss: 33.27438735961914\n",
      "[step: 2628] loss: 0.005912858061492443\n",
      "[step: 2629] loss: 44.16450500488281\n",
      "[step: 2629] loss: 0.0058944192714989185\n",
      "[step: 2630] loss: 55.76047134399414\n",
      "[step: 2630] loss: 0.005890592932701111\n",
      "[step: 2631] loss: 58.31715774536133\n",
      "[step: 2631] loss: 0.00589010352268815\n",
      "[step: 2632] loss: 52.973968505859375\n",
      "[step: 2632] loss: 0.005898122675716877\n",
      "[step: 2633] loss: 40.54756546020508\n",
      "[step: 2633] loss: 0.005884512793272734\n",
      "[step: 2634] loss: 30.42212677001953\n",
      "[step: 2634] loss: 0.005878759548068047\n",
      "[step: 2635] loss: 32.643821716308594\n",
      "[step: 2635] loss: 0.0058797369711101055\n",
      "[step: 2636] loss: 42.14386749267578\n",
      "[step: 2636] loss: 0.005904363002628088\n",
      "[step: 2637] loss: 33.075096130371094\n",
      "[step: 2637] loss: 0.005905529018491507\n",
      "[step: 2638] loss: 24.36919403076172\n",
      "[step: 2638] loss: 0.00591407623142004\n",
      "[step: 2639] loss: 27.923805236816406\n",
      "[step: 2639] loss: 0.005924667231738567\n",
      "[step: 2640] loss: 27.89800453186035\n",
      "[step: 2640] loss: 0.005966352764517069\n",
      "[step: 2641] loss: 27.96155548095703\n",
      "[step: 2641] loss: 0.005978969391435385\n",
      "[step: 2642] loss: 26.02608871459961\n",
      "[step: 2642] loss: 0.005988067016005516\n",
      "[step: 2643] loss: 23.81755256652832\n",
      "[step: 2643] loss: 0.005970320198684931\n",
      "[step: 2644] loss: 25.265933990478516\n",
      "[step: 2644] loss: 0.005943750496953726\n",
      "[step: 2645] loss: 23.832908630371094\n",
      "[step: 2645] loss: 0.005916038993746042\n",
      "[step: 2646] loss: 22.86832046508789\n",
      "[step: 2646] loss: 0.005894929636269808\n",
      "[step: 2647] loss: 22.33066749572754\n",
      "[step: 2647] loss: 0.0058732787147164345\n",
      "[step: 2648] loss: 23.73569107055664\n",
      "[step: 2648] loss: 0.005867503117769957\n",
      "[step: 2649] loss: 24.00845718383789\n",
      "[step: 2649] loss: 0.005856445524841547\n",
      "[step: 2650] loss: 22.040287017822266\n",
      "[step: 2650] loss: 0.005866044666618109\n",
      "[step: 2651] loss: 21.260387420654297\n",
      "[step: 2651] loss: 0.0058864401653409\n",
      "[step: 2652] loss: 20.61138916015625\n",
      "[step: 2652] loss: 0.00589825538918376\n",
      "[step: 2653] loss: 21.62146759033203\n",
      "[step: 2653] loss: 0.005905419122427702\n",
      "[step: 2654] loss: 20.228160858154297\n",
      "[step: 2654] loss: 0.005909570027142763\n",
      "[step: 2655] loss: 20.575519561767578\n",
      "[step: 2655] loss: 0.005922382697463036\n",
      "[step: 2656] loss: 21.215330123901367\n",
      "[step: 2656] loss: 0.0059147425927221775\n",
      "[step: 2657] loss: 20.193771362304688\n",
      "[step: 2657] loss: 0.005888936575502157\n",
      "[step: 2658] loss: 19.751506805419922\n",
      "[step: 2658] loss: 0.0058751292526721954\n",
      "[step: 2659] loss: 18.889728546142578\n",
      "[step: 2659] loss: 0.0058743455447256565\n",
      "[step: 2660] loss: 19.46738052368164\n",
      "[step: 2660] loss: 0.005868200678378344\n",
      "[step: 2661] loss: 19.609230041503906\n",
      "[step: 2661] loss: 0.0058636972680687904\n",
      "[step: 2662] loss: 19.185853958129883\n",
      "[step: 2662] loss: 0.0058562541380524635\n",
      "[step: 2663] loss: 19.145858764648438\n",
      "[step: 2663] loss: 0.00585203105583787\n",
      "[step: 2664] loss: 19.07980728149414\n",
      "[step: 2664] loss: 0.005851622670888901\n",
      "[step: 2665] loss: 18.981416702270508\n",
      "[step: 2665] loss: 0.00585384713485837\n",
      "[step: 2666] loss: 18.62190055847168\n",
      "[step: 2666] loss: 0.005855560302734375\n",
      "[step: 2667] loss: 18.23885726928711\n",
      "[step: 2667] loss: 0.005851190537214279\n",
      "[step: 2668] loss: 18.387832641601562\n",
      "[step: 2668] loss: 0.005847162567079067\n",
      "[step: 2669] loss: 18.493526458740234\n",
      "[step: 2669] loss: 0.005849482491612434\n",
      "[step: 2670] loss: 18.50486183166504\n",
      "[step: 2670] loss: 0.0058565386570990086\n",
      "[step: 2671] loss: 18.36427116394043\n",
      "[step: 2671] loss: 0.005859043914824724\n",
      "[step: 2672] loss: 18.298561096191406\n",
      "[step: 2672] loss: 0.005862977355718613\n",
      "[step: 2673] loss: 18.4976863861084\n",
      "[step: 2673] loss: 0.005873052403330803\n",
      "[step: 2674] loss: 18.321609497070312\n",
      "[step: 2674] loss: 0.005910766776651144\n",
      "[step: 2675] loss: 18.176239013671875\n",
      "[step: 2675] loss: 0.005947956815361977\n",
      "[step: 2676] loss: 18.05157470703125\n",
      "[step: 2676] loss: 0.006023888010531664\n",
      "[step: 2677] loss: 17.918621063232422\n",
      "[step: 2677] loss: 0.006046530790627003\n",
      "[step: 2678] loss: 17.989551544189453\n",
      "[step: 2678] loss: 0.006079195998609066\n",
      "[step: 2679] loss: 17.85801124572754\n",
      "[step: 2679] loss: 0.006063144188374281\n",
      "[step: 2680] loss: 17.775440216064453\n",
      "[step: 2680] loss: 0.006008405704051256\n",
      "[step: 2681] loss: 17.75996208190918\n",
      "[step: 2681] loss: 0.005967525765299797\n",
      "[step: 2682] loss: 17.765932083129883\n",
      "[step: 2682] loss: 0.005958158057183027\n",
      "[step: 2683] loss: 17.814437866210938\n",
      "[step: 2683] loss: 0.005952376406639814\n",
      "[step: 2684] loss: 17.760082244873047\n",
      "[step: 2684] loss: 0.00594338309019804\n",
      "[step: 2685] loss: 17.758981704711914\n",
      "[step: 2685] loss: 0.005906735546886921\n",
      "[step: 2686] loss: 17.76047134399414\n",
      "[step: 2686] loss: 0.005880309734493494\n",
      "[step: 2687] loss: 17.80048370361328\n",
      "[step: 2687] loss: 0.005922588985413313\n",
      "[step: 2688] loss: 17.87788963317871\n",
      "[step: 2688] loss: 0.005932019092142582\n",
      "[step: 2689] loss: 17.926088333129883\n",
      "[step: 2689] loss: 0.005910507403314114\n",
      "[step: 2690] loss: 18.024089813232422\n",
      "[step: 2690] loss: 0.005896799266338348\n",
      "[step: 2691] loss: 18.220653533935547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2691] loss: 0.005891059525310993\n",
      "[step: 2692] loss: 18.526086807250977\n",
      "[step: 2692] loss: 0.005904137622565031\n",
      "[step: 2693] loss: 19.067394256591797\n",
      "[step: 2693] loss: 0.0058785430155694485\n",
      "[step: 2694] loss: 19.766643524169922\n",
      "[step: 2694] loss: 0.005892210174351931\n",
      "[step: 2695] loss: 20.971393585205078\n",
      "[step: 2695] loss: 0.005925584584474564\n",
      "[step: 2696] loss: 22.43857192993164\n",
      "[step: 2696] loss: 0.005914885085076094\n",
      "[step: 2697] loss: 24.524620056152344\n",
      "[step: 2697] loss: 0.00588027760386467\n",
      "[step: 2698] loss: 25.787017822265625\n",
      "[step: 2698] loss: 0.00589085603132844\n",
      "[step: 2699] loss: 26.690263748168945\n",
      "[step: 2699] loss: 0.005883891135454178\n",
      "[step: 2700] loss: 24.58928108215332\n",
      "[step: 2700] loss: 0.005899596493691206\n",
      "[step: 2701] loss: 21.543148040771484\n",
      "[step: 2701] loss: 0.005887006409466267\n",
      "[step: 2702] loss: 18.636478424072266\n",
      "[step: 2702] loss: 0.005904932972043753\n",
      "[step: 2703] loss: 17.742591857910156\n",
      "[step: 2703] loss: 0.005887510254979134\n",
      "[step: 2704] loss: 18.881906509399414\n",
      "[step: 2704] loss: 0.005870342720299959\n",
      "[step: 2705] loss: 20.720748901367188\n",
      "[step: 2705] loss: 0.005879386328160763\n",
      "[step: 2706] loss: 21.820762634277344\n",
      "[step: 2706] loss: 0.005921248812228441\n",
      "[step: 2707] loss: 21.2912654876709\n",
      "[step: 2707] loss: 0.005898633040487766\n",
      "[step: 2708] loss: 19.81119728088379\n",
      "[step: 2708] loss: 0.0058886329643428326\n",
      "[step: 2709] loss: 18.175050735473633\n",
      "[step: 2709] loss: 0.005910162348300219\n",
      "[step: 2710] loss: 17.63631820678711\n",
      "[step: 2710] loss: 0.005921576637774706\n",
      "[step: 2711] loss: 18.263896942138672\n",
      "[step: 2711] loss: 0.005908187013119459\n",
      "[step: 2712] loss: 19.242324829101562\n",
      "[step: 2712] loss: 0.005903229117393494\n",
      "[step: 2713] loss: 19.77011489868164\n",
      "[step: 2713] loss: 0.0059129102155566216\n",
      "[step: 2714] loss: 19.339397430419922\n",
      "[step: 2714] loss: 0.0058913701213896275\n",
      "[step: 2715] loss: 18.448436737060547\n",
      "[step: 2715] loss: 0.005856189876794815\n",
      "[step: 2716] loss: 17.666187286376953\n",
      "[step: 2716] loss: 0.005846396554261446\n",
      "[step: 2717] loss: 17.489940643310547\n",
      "[step: 2717] loss: 0.005849691107869148\n",
      "[step: 2718] loss: 17.86029052734375\n",
      "[step: 2718] loss: 0.0058457558043301105\n",
      "[step: 2719] loss: 18.327861785888672\n",
      "[step: 2719] loss: 0.005850108806043863\n",
      "[step: 2720] loss: 18.540264129638672\n",
      "[step: 2720] loss: 0.005852708127349615\n",
      "[step: 2721] loss: 18.340538024902344\n",
      "[step: 2721] loss: 0.005855776835232973\n",
      "[step: 2722] loss: 17.951658248901367\n",
      "[step: 2722] loss: 0.005852143745869398\n",
      "[step: 2723] loss: 17.62551498413086\n",
      "[step: 2723] loss: 0.005843850784003735\n",
      "[step: 2724] loss: 17.497318267822266\n",
      "[step: 2724] loss: 0.005845851730555296\n",
      "[step: 2725] loss: 17.517240524291992\n",
      "[step: 2725] loss: 0.005854601506143808\n",
      "[step: 2726] loss: 17.599533081054688\n",
      "[step: 2726] loss: 0.005867134314030409\n",
      "[step: 2727] loss: 17.6840763092041\n",
      "[step: 2727] loss: 0.0058770752511918545\n",
      "[step: 2728] loss: 17.73589515686035\n",
      "[step: 2728] loss: 0.005900873802602291\n",
      "[step: 2729] loss: 17.779577255249023\n",
      "[step: 2729] loss: 0.005890355445444584\n",
      "[step: 2730] loss: 17.803863525390625\n",
      "[step: 2730] loss: 0.005892045330256224\n",
      "[step: 2731] loss: 17.797447204589844\n",
      "[step: 2731] loss: 0.005904440302401781\n",
      "[step: 2732] loss: 17.72098159790039\n",
      "[step: 2732] loss: 0.005911308340728283\n",
      "[step: 2733] loss: 17.597131729125977\n",
      "[step: 2733] loss: 0.005903786513954401\n",
      "[step: 2734] loss: 17.444469451904297\n",
      "[step: 2734] loss: 0.005902606062591076\n",
      "[step: 2735] loss: 17.32111358642578\n",
      "[step: 2735] loss: 0.005893434397876263\n",
      "[step: 2736] loss: 17.251869201660156\n",
      "[step: 2736] loss: 0.005899977870285511\n",
      "[step: 2737] loss: 17.238121032714844\n",
      "[step: 2737] loss: 0.0058881198056042194\n",
      "[step: 2738] loss: 17.261281967163086\n",
      "[step: 2738] loss: 0.0058931936509907246\n",
      "[step: 2739] loss: 17.291582107543945\n",
      "[step: 2739] loss: 0.005872092675417662\n",
      "[step: 2740] loss: 17.31173324584961\n",
      "[step: 2740] loss: 0.0058583407662808895\n",
      "[step: 2741] loss: 17.321210861206055\n",
      "[step: 2741] loss: 0.005845497362315655\n",
      "[step: 2742] loss: 17.338726043701172\n",
      "[step: 2742] loss: 0.005834185518324375\n",
      "[step: 2743] loss: 17.378612518310547\n",
      "[step: 2743] loss: 0.0058303577825427055\n",
      "[step: 2744] loss: 17.46271514892578\n",
      "[step: 2744] loss: 0.005830904468894005\n",
      "[step: 2745] loss: 17.60698890686035\n",
      "[step: 2745] loss: 0.005831662565469742\n",
      "[step: 2746] loss: 17.839679718017578\n",
      "[step: 2746] loss: 0.005829806439578533\n",
      "[step: 2747] loss: 18.19286346435547\n",
      "[step: 2747] loss: 0.005829804111272097\n",
      "[step: 2748] loss: 18.767841339111328\n",
      "[step: 2748] loss: 0.0058303154073655605\n",
      "[step: 2749] loss: 19.613943099975586\n",
      "[step: 2749] loss: 0.00583628099411726\n",
      "[step: 2750] loss: 20.988475799560547\n",
      "[step: 2750] loss: 0.00584237277507782\n",
      "[step: 2751] loss: 22.81294059753418\n",
      "[step: 2751] loss: 0.005865762010216713\n",
      "[step: 2752] loss: 25.38406753540039\n",
      "[step: 2752] loss: 0.005878971423953772\n",
      "[step: 2753] loss: 27.634906768798828\n",
      "[step: 2753] loss: 0.0058995746076107025\n",
      "[step: 2754] loss: 29.229869842529297\n",
      "[step: 2754] loss: 0.005917080212384462\n",
      "[step: 2755] loss: 28.23136329650879\n",
      "[step: 2755] loss: 0.005940302275121212\n",
      "[step: 2756] loss: 25.078346252441406\n",
      "[step: 2756] loss: 0.005951999220997095\n",
      "[step: 2757] loss: 21.082191467285156\n",
      "[step: 2757] loss: 0.005965281277894974\n",
      "[step: 2758] loss: 18.332443237304688\n",
      "[step: 2758] loss: 0.0059651341289281845\n",
      "[step: 2759] loss: 17.97040367126465\n",
      "[step: 2759] loss: 0.0059548793360590935\n",
      "[step: 2760] loss: 19.577411651611328\n",
      "[step: 2760] loss: 0.005921995732933283\n",
      "[step: 2761] loss: 21.60415267944336\n",
      "[step: 2761] loss: 0.00588717358186841\n",
      "[step: 2762] loss: 22.43686294555664\n",
      "[step: 2762] loss: 0.005850477144122124\n",
      "[step: 2763] loss: 21.495582580566406\n",
      "[step: 2763] loss: 0.0058324141427874565\n",
      "[step: 2764] loss: 19.6212158203125\n",
      "[step: 2764] loss: 0.005827243439853191\n",
      "[step: 2765] loss: 18.3503360748291\n",
      "[step: 2765] loss: 0.005842996761202812\n",
      "[step: 2766] loss: 18.244876861572266\n",
      "[step: 2766] loss: 0.00586211122572422\n",
      "[step: 2767] loss: 18.862672805786133\n",
      "[step: 2767] loss: 0.005885744467377663\n",
      "[step: 2768] loss: 19.17375946044922\n",
      "[step: 2768] loss: 0.005909225903451443\n",
      "[step: 2769] loss: 19.07301139831543\n",
      "[step: 2769] loss: 0.005900209303945303\n",
      "[step: 2770] loss: 18.84006118774414\n",
      "[step: 2770] loss: 0.005884946323931217\n",
      "[step: 2771] loss: 18.552715301513672\n",
      "[step: 2771] loss: 0.005847101099789143\n",
      "[step: 2772] loss: 18.26311683654785\n",
      "[step: 2772] loss: 0.005824723746627569\n",
      "[step: 2773] loss: 17.98332977294922\n",
      "[step: 2773] loss: 0.0058249481953680515\n",
      "[step: 2774] loss: 17.953086853027344\n",
      "[step: 2774] loss: 0.005835311021655798\n",
      "[step: 2775] loss: 18.086017608642578\n",
      "[step: 2775] loss: 0.0058545442298054695\n",
      "[step: 2776] loss: 18.130460739135742\n",
      "[step: 2776] loss: 0.005864408798515797\n",
      "[step: 2777] loss: 17.94251823425293\n",
      "[step: 2777] loss: 0.005873392336070538\n",
      "[step: 2778] loss: 17.603775024414062\n",
      "[step: 2778] loss: 0.005871042143553495\n",
      "[step: 2779] loss: 17.41075897216797\n",
      "[step: 2779] loss: 0.0058645932003855705\n",
      "[step: 2780] loss: 17.457855224609375\n",
      "[step: 2780] loss: 0.005849071778357029\n",
      "[step: 2781] loss: 17.621978759765625\n",
      "[step: 2781] loss: 0.005842600017786026\n",
      "[step: 2782] loss: 17.72521209716797\n",
      "[step: 2782] loss: 0.005841233301907778\n",
      "[step: 2783] loss: 17.78048324584961\n",
      "[step: 2783] loss: 0.005854982882738113\n",
      "[step: 2784] loss: 17.795406341552734\n",
      "[step: 2784] loss: 0.005862049292773008\n",
      "[step: 2785] loss: 17.80233383178711\n",
      "[step: 2785] loss: 0.005869715940207243\n",
      "[step: 2786] loss: 17.72771453857422\n",
      "[step: 2786] loss: 0.005858727265149355\n",
      "[step: 2787] loss: 17.621854782104492\n",
      "[step: 2787] loss: 0.0058417050167918205\n",
      "[step: 2788] loss: 17.512399673461914\n",
      "[step: 2788] loss: 0.005823909770697355\n",
      "[step: 2789] loss: 17.505752563476562\n",
      "[step: 2789] loss: 0.005818699020892382\n",
      "[step: 2790] loss: 17.585466384887695\n",
      "[step: 2790] loss: 0.0058277323842048645\n",
      "[step: 2791] loss: 17.733924865722656\n",
      "[step: 2791] loss: 0.005842574872076511\n",
      "[step: 2792] loss: 17.903566360473633\n",
      "[step: 2792] loss: 0.00586558086797595\n",
      "[step: 2793] loss: 18.171419143676758\n",
      "[step: 2793] loss: 0.005871666129678488\n",
      "[step: 2794] loss: 18.5489444732666\n",
      "[step: 2794] loss: 0.005879091564565897\n",
      "[step: 2795] loss: 19.187685012817383\n",
      "[step: 2795] loss: 0.0058854324743151665\n",
      "[step: 2796] loss: 20.006181716918945\n",
      "[step: 2796] loss: 0.005890898406505585\n",
      "[step: 2797] loss: 21.134689331054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2797] loss: 0.005902512464672327\n",
      "[step: 2798] loss: 22.356693267822266\n",
      "[step: 2798] loss: 0.005904570687562227\n",
      "[step: 2799] loss: 23.801589965820312\n",
      "[step: 2799] loss: 0.005906680598855019\n",
      "[step: 2800] loss: 24.77621841430664\n",
      "[step: 2800] loss: 0.005903659388422966\n",
      "[step: 2801] loss: 25.345680236816406\n",
      "[step: 2801] loss: 0.005900850053876638\n",
      "[step: 2802] loss: 24.262840270996094\n",
      "[step: 2802] loss: 0.005895656067878008\n",
      "[step: 2803] loss: 22.157493591308594\n",
      "[step: 2803] loss: 0.005906614940613508\n",
      "[step: 2804] loss: 19.2922306060791\n",
      "[step: 2804] loss: 0.0059241377748548985\n",
      "[step: 2805] loss: 17.451683044433594\n",
      "[step: 2805] loss: 0.005986528471112251\n",
      "[step: 2806] loss: 17.36790657043457\n",
      "[step: 2806] loss: 0.0059132203459739685\n",
      "[step: 2807] loss: 18.602554321289062\n",
      "[step: 2807] loss: 0.005876069888472557\n",
      "[step: 2808] loss: 20.089982986450195\n",
      "[step: 2808] loss: 0.0058936006389558315\n",
      "[step: 2809] loss: 21.08340835571289\n",
      "[step: 2809] loss: 0.005874213296920061\n",
      "[step: 2810] loss: 21.485538482666016\n",
      "[step: 2810] loss: 0.005881285294890404\n",
      "[step: 2811] loss: 21.135536193847656\n",
      "[step: 2811] loss: 0.005903435871005058\n",
      "[step: 2812] loss: 20.417177200317383\n",
      "[step: 2812] loss: 0.005879461299628019\n",
      "[step: 2813] loss: 19.175230026245117\n",
      "[step: 2813] loss: 0.005887497216463089\n",
      "[step: 2814] loss: 18.301414489746094\n",
      "[step: 2814] loss: 0.00589307164773345\n",
      "[step: 2815] loss: 18.122861862182617\n",
      "[step: 2815] loss: 0.005889230873435736\n",
      "[step: 2816] loss: 18.609773635864258\n",
      "[step: 2816] loss: 0.005874409340322018\n",
      "[step: 2817] loss: 19.18368911743164\n",
      "[step: 2817] loss: 0.005855837371200323\n",
      "[step: 2818] loss: 19.134010314941406\n",
      "[step: 2818] loss: 0.005851415451616049\n",
      "[step: 2819] loss: 18.687496185302734\n",
      "[step: 2819] loss: 0.005869104992598295\n",
      "[step: 2820] loss: 18.10797882080078\n",
      "[step: 2820] loss: 0.00584791274741292\n",
      "[step: 2821] loss: 17.855533599853516\n",
      "[step: 2821] loss: 0.005835182499140501\n",
      "[step: 2822] loss: 17.872995376586914\n",
      "[step: 2822] loss: 0.00582209462299943\n",
      "[step: 2823] loss: 17.906875610351562\n",
      "[step: 2823] loss: 0.005836795549839735\n",
      "[step: 2824] loss: 17.94443130493164\n",
      "[step: 2824] loss: 0.005841287784278393\n",
      "[step: 2825] loss: 17.992265701293945\n",
      "[step: 2825] loss: 0.005830024369060993\n",
      "[step: 2826] loss: 18.121688842773438\n",
      "[step: 2826] loss: 0.005814576055854559\n",
      "[step: 2827] loss: 18.245620727539062\n",
      "[step: 2827] loss: 0.005831270013004541\n",
      "[step: 2828] loss: 18.180147171020508\n",
      "[step: 2828] loss: 0.00584341399371624\n",
      "[step: 2829] loss: 17.882373809814453\n",
      "[step: 2829] loss: 0.005837153643369675\n",
      "[step: 2830] loss: 17.47951316833496\n",
      "[step: 2830] loss: 0.005836756434291601\n",
      "[step: 2831] loss: 17.198976516723633\n",
      "[step: 2831] loss: 0.005856609437614679\n",
      "[step: 2832] loss: 17.162052154541016\n",
      "[step: 2832] loss: 0.005873523186892271\n",
      "[step: 2833] loss: 17.336013793945312\n",
      "[step: 2833] loss: 0.005887732375413179\n",
      "[step: 2834] loss: 17.56967544555664\n",
      "[step: 2834] loss: 0.005891885608434677\n",
      "[step: 2835] loss: 17.76321792602539\n",
      "[step: 2835] loss: 0.005903848446905613\n",
      "[step: 2836] loss: 17.844371795654297\n",
      "[step: 2836] loss: 0.005895682144910097\n",
      "[step: 2837] loss: 17.924030303955078\n",
      "[step: 2837] loss: 0.005869291722774506\n",
      "[step: 2838] loss: 18.097103118896484\n",
      "[step: 2838] loss: 0.00584522495046258\n",
      "[step: 2839] loss: 18.472705841064453\n",
      "[step: 2839] loss: 0.005826240871101618\n",
      "[step: 2840] loss: 19.16504669189453\n",
      "[step: 2840] loss: 0.005807437933981419\n",
      "[step: 2841] loss: 20.355751037597656\n",
      "[step: 2841] loss: 0.0057941037230193615\n",
      "[step: 2842] loss: 22.029468536376953\n",
      "[step: 2842] loss: 0.005798764061182737\n",
      "[step: 2843] loss: 24.717981338500977\n",
      "[step: 2843] loss: 0.005804694723337889\n",
      "[step: 2844] loss: 27.317859649658203\n",
      "[step: 2844] loss: 0.005816594231873751\n",
      "[step: 2845] loss: 30.46479034423828\n",
      "[step: 2845] loss: 0.00583068048581481\n",
      "[step: 2846] loss: 31.886611938476562\n",
      "[step: 2846] loss: 0.005847760941833258\n",
      "[step: 2847] loss: 32.05243682861328\n",
      "[step: 2847] loss: 0.0058579836040735245\n",
      "[step: 2848] loss: 31.18452262878418\n",
      "[step: 2848] loss: 0.005885510239750147\n",
      "[step: 2849] loss: 27.65058135986328\n",
      "[step: 2849] loss: 0.005865967832505703\n",
      "[step: 2850] loss: 22.10775375366211\n",
      "[step: 2850] loss: 0.005846469663083553\n",
      "[step: 2851] loss: 21.607131958007812\n",
      "[step: 2851] loss: 0.005828789435327053\n",
      "[step: 2852] loss: 26.356529235839844\n",
      "[step: 2852] loss: 0.0058178068138659\n",
      "[step: 2853] loss: 28.802793502807617\n",
      "[step: 2853] loss: 0.005809856113046408\n",
      "[step: 2854] loss: 25.29220199584961\n",
      "[step: 2854] loss: 0.005808360408991575\n",
      "[step: 2855] loss: 22.374862670898438\n",
      "[step: 2855] loss: 0.005810853559523821\n",
      "[step: 2856] loss: 24.20925521850586\n",
      "[step: 2856] loss: 0.005814066622406244\n",
      "[step: 2857] loss: 24.40029525756836\n",
      "[step: 2857] loss: 0.005823072511702776\n",
      "[step: 2858] loss: 23.690343856811523\n",
      "[step: 2858] loss: 0.0058337305672466755\n",
      "[step: 2859] loss: 22.651935577392578\n",
      "[step: 2859] loss: 0.005848635919392109\n",
      "[step: 2860] loss: 20.949787139892578\n",
      "[step: 2860] loss: 0.005852601490914822\n",
      "[step: 2861] loss: 18.356956481933594\n",
      "[step: 2861] loss: 0.005859517026692629\n",
      "[step: 2862] loss: 18.587841033935547\n",
      "[step: 2862] loss: 0.005862922873347998\n",
      "[step: 2863] loss: 21.043058395385742\n",
      "[step: 2863] loss: 0.005860739853233099\n",
      "[step: 2864] loss: 20.862476348876953\n",
      "[step: 2864] loss: 0.0058500193990767\n",
      "[step: 2865] loss: 18.746091842651367\n",
      "[step: 2865] loss: 0.005839937832206488\n",
      "[step: 2866] loss: 18.71509552001953\n",
      "[step: 2866] loss: 0.005828396882861853\n",
      "[step: 2867] loss: 20.013174057006836\n",
      "[step: 2867] loss: 0.0058242082595825195\n",
      "[step: 2868] loss: 19.39417266845703\n",
      "[step: 2868] loss: 0.005813400261104107\n",
      "[step: 2869] loss: 18.18536376953125\n",
      "[step: 2869] loss: 0.005813363008201122\n",
      "[step: 2870] loss: 18.04378890991211\n",
      "[step: 2870] loss: 0.0058159842155873775\n",
      "[step: 2871] loss: 18.052597045898438\n",
      "[step: 2871] loss: 0.005830289330333471\n",
      "[step: 2872] loss: 17.721317291259766\n",
      "[step: 2872] loss: 0.005837614648044109\n",
      "[step: 2873] loss: 17.772430419921875\n",
      "[step: 2873] loss: 0.005845105275511742\n",
      "[step: 2874] loss: 17.78738784790039\n",
      "[step: 2874] loss: 0.005845425184816122\n",
      "[step: 2875] loss: 17.43584442138672\n",
      "[step: 2875] loss: 0.005842564161866903\n",
      "[step: 2876] loss: 17.603313446044922\n",
      "[step: 2876] loss: 0.005825999658554792\n",
      "[step: 2877] loss: 18.103179931640625\n",
      "[step: 2877] loss: 0.00581016531214118\n",
      "[step: 2878] loss: 17.892602920532227\n",
      "[step: 2878] loss: 0.005800402257591486\n",
      "[step: 2879] loss: 17.288225173950195\n",
      "[step: 2879] loss: 0.0057973712682724\n",
      "[step: 2880] loss: 17.30791473388672\n",
      "[step: 2880] loss: 0.005799239967018366\n",
      "[step: 2881] loss: 17.559856414794922\n",
      "[step: 2881] loss: 0.0058030192740261555\n",
      "[step: 2882] loss: 17.37077522277832\n",
      "[step: 2882] loss: 0.005818570964038372\n",
      "[step: 2883] loss: 17.122821807861328\n",
      "[step: 2883] loss: 0.005811326205730438\n",
      "[step: 2884] loss: 17.15298080444336\n",
      "[step: 2884] loss: 0.005811191163957119\n",
      "[step: 2885] loss: 17.146509170532227\n",
      "[step: 2885] loss: 0.005816653836518526\n",
      "[step: 2886] loss: 17.063640594482422\n",
      "[step: 2886] loss: 0.005832781083881855\n",
      "[step: 2887] loss: 17.052648544311523\n",
      "[step: 2887] loss: 0.0058557866141200066\n",
      "[step: 2888] loss: 17.08480453491211\n",
      "[step: 2888] loss: 0.005890930537134409\n",
      "[step: 2889] loss: 17.0194091796875\n",
      "[step: 2889] loss: 0.005925926845520735\n",
      "[step: 2890] loss: 16.98457145690918\n",
      "[step: 2890] loss: 0.005977574735879898\n",
      "[step: 2891] loss: 17.16971206665039\n",
      "[step: 2891] loss: 0.006007691379636526\n",
      "[step: 2892] loss: 17.498878479003906\n",
      "[step: 2892] loss: 0.00602464796975255\n",
      "[step: 2893] loss: 17.734519958496094\n",
      "[step: 2893] loss: 0.00596034387126565\n",
      "[step: 2894] loss: 18.121950149536133\n",
      "[step: 2894] loss: 0.005877312738448381\n",
      "[step: 2895] loss: 18.905317306518555\n",
      "[step: 2895] loss: 0.005818255245685577\n",
      "[step: 2896] loss: 20.186071395874023\n",
      "[step: 2896] loss: 0.005812399089336395\n",
      "[step: 2897] loss: 21.90353012084961\n",
      "[step: 2897] loss: 0.005845669191330671\n",
      "[step: 2898] loss: 24.36996841430664\n",
      "[step: 2898] loss: 0.005866072606295347\n",
      "[step: 2899] loss: 26.666149139404297\n",
      "[step: 2899] loss: 0.0058821155689656734\n",
      "[step: 2900] loss: 28.870525360107422\n",
      "[step: 2900] loss: 0.005865717306733131\n",
      "[step: 2901] loss: 27.9644775390625\n",
      "[step: 2901] loss: 0.005851286463439465\n",
      "[step: 2902] loss: 25.16805648803711\n",
      "[step: 2902] loss: 0.005836141761392355\n",
      "[step: 2903] loss: 20.729923248291016\n",
      "[step: 2903] loss: 0.005812359042465687\n",
      "[step: 2904] loss: 17.76142120361328\n",
      "[step: 2904] loss: 0.005798348691314459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2905] loss: 17.368419647216797\n",
      "[step: 2905] loss: 0.005799403414130211\n",
      "[step: 2906] loss: 18.77370834350586\n",
      "[step: 2906] loss: 0.005817350931465626\n",
      "[step: 2907] loss: 20.77255630493164\n",
      "[step: 2907] loss: 0.005833464674651623\n",
      "[step: 2908] loss: 22.177932739257812\n",
      "[step: 2908] loss: 0.00582664692774415\n",
      "[step: 2909] loss: 22.09467124938965\n",
      "[step: 2909] loss: 0.0057980772107839584\n",
      "[step: 2910] loss: 20.207895278930664\n",
      "[step: 2910] loss: 0.005781455431133509\n",
      "[step: 2911] loss: 18.28018569946289\n",
      "[step: 2911] loss: 0.005778901278972626\n",
      "[step: 2912] loss: 17.784637451171875\n",
      "[step: 2912] loss: 0.005787230096757412\n",
      "[step: 2913] loss: 18.443313598632812\n",
      "[step: 2913] loss: 0.00578593323007226\n",
      "[step: 2914] loss: 19.009441375732422\n",
      "[step: 2914] loss: 0.00578090688213706\n",
      "[step: 2915] loss: 19.03689193725586\n",
      "[step: 2915] loss: 0.005785410758107901\n",
      "[step: 2916] loss: 18.65650177001953\n",
      "[step: 2916] loss: 0.005787935107946396\n",
      "[step: 2917] loss: 17.674251556396484\n",
      "[step: 2917] loss: 0.0057947332970798016\n",
      "[step: 2918] loss: 16.879535675048828\n",
      "[step: 2918] loss: 0.005783281289041042\n",
      "[step: 2919] loss: 16.967201232910156\n",
      "[step: 2919] loss: 0.005778438411653042\n",
      "[step: 2920] loss: 17.66016387939453\n",
      "[step: 2920] loss: 0.005782203748822212\n",
      "[step: 2921] loss: 18.084760665893555\n",
      "[step: 2921] loss: 0.0057898880913853645\n",
      "[step: 2922] loss: 17.876976013183594\n",
      "[step: 2922] loss: 0.005800190847367048\n",
      "[step: 2923] loss: 17.488597869873047\n",
      "[step: 2923] loss: 0.005817793775349855\n",
      "[step: 2924] loss: 17.083816528320312\n",
      "[step: 2924] loss: 0.005852330941706896\n",
      "[step: 2925] loss: 16.74312973022461\n",
      "[step: 2925] loss: 0.005914128851145506\n",
      "[step: 2926] loss: 16.62395477294922\n",
      "[step: 2926] loss: 0.005982249043881893\n",
      "[step: 2927] loss: 16.83062744140625\n",
      "[step: 2927] loss: 0.006045895628631115\n",
      "[step: 2928] loss: 17.139671325683594\n",
      "[step: 2928] loss: 0.006061925087124109\n",
      "[step: 2929] loss: 17.24373435974121\n",
      "[step: 2929] loss: 0.006058063358068466\n",
      "[step: 2930] loss: 17.093875885009766\n",
      "[step: 2930] loss: 0.006028546951711178\n",
      "[step: 2931] loss: 16.867481231689453\n",
      "[step: 2931] loss: 0.006045148707926273\n",
      "[step: 2932] loss: 16.759090423583984\n",
      "[step: 2932] loss: 0.005967009346932173\n",
      "[step: 2933] loss: 16.724578857421875\n",
      "[step: 2933] loss: 0.005873014684766531\n",
      "[step: 2934] loss: 16.706680297851562\n",
      "[step: 2934] loss: 0.005864949431270361\n",
      "[step: 2935] loss: 16.717493057250977\n",
      "[step: 2935] loss: 0.00589594803750515\n",
      "[step: 2936] loss: 16.83289337158203\n",
      "[step: 2936] loss: 0.005897594150155783\n",
      "[step: 2937] loss: 17.042564392089844\n",
      "[step: 2937] loss: 0.005884694866836071\n",
      "[step: 2938] loss: 17.28097152709961\n",
      "[step: 2938] loss: 0.005887188483029604\n",
      "[step: 2939] loss: 17.534473419189453\n",
      "[step: 2939] loss: 0.005851568654179573\n",
      "[step: 2940] loss: 17.90007972717285\n",
      "[step: 2940] loss: 0.005816886201500893\n",
      "[step: 2941] loss: 18.561725616455078\n",
      "[step: 2941] loss: 0.005835187155753374\n",
      "[step: 2942] loss: 19.650150299072266\n",
      "[step: 2942] loss: 0.00584563659504056\n",
      "[step: 2943] loss: 21.37380599975586\n",
      "[step: 2943] loss: 0.005831975024193525\n",
      "[step: 2944] loss: 24.028457641601562\n",
      "[step: 2944] loss: 0.0058081489987671375\n",
      "[step: 2945] loss: 27.706510543823242\n",
      "[step: 2945] loss: 0.0058244564570486546\n",
      "[step: 2946] loss: 31.740951538085938\n",
      "[step: 2946] loss: 0.005822974257171154\n",
      "[step: 2947] loss: 34.37118911743164\n",
      "[step: 2947] loss: 0.005786954890936613\n",
      "[step: 2948] loss: 33.13005828857422\n",
      "[step: 2948] loss: 0.005793828051537275\n",
      "[step: 2949] loss: 27.567516326904297\n",
      "[step: 2949] loss: 0.005813797935843468\n",
      "[step: 2950] loss: 21.104082107543945\n",
      "[step: 2950] loss: 0.005800553131848574\n",
      "[step: 2951] loss: 18.238300323486328\n",
      "[step: 2951] loss: 0.005798033904284239\n",
      "[step: 2952] loss: 19.92053985595703\n",
      "[step: 2952] loss: 0.005788086447864771\n",
      "[step: 2953] loss: 23.025341033935547\n",
      "[step: 2953] loss: 0.005779350642114878\n",
      "[step: 2954] loss: 23.477035522460938\n",
      "[step: 2954] loss: 0.005782841704785824\n",
      "[step: 2955] loss: 21.659021377563477\n",
      "[step: 2955] loss: 0.005791599862277508\n",
      "[step: 2956] loss: 20.20528221130371\n",
      "[step: 2956] loss: 0.0057790400460362434\n",
      "[step: 2957] loss: 20.618017196655273\n",
      "[step: 2957] loss: 0.005766562186181545\n",
      "[step: 2958] loss: 21.44590187072754\n",
      "[step: 2958] loss: 0.005773297976702452\n",
      "[step: 2959] loss: 21.36993980407715\n",
      "[step: 2959] loss: 0.00577849755063653\n",
      "[step: 2960] loss: 19.85171127319336\n",
      "[step: 2960] loss: 0.005770599003881216\n",
      "[step: 2961] loss: 18.28980255126953\n",
      "[step: 2961] loss: 0.00577063811942935\n",
      "[step: 2962] loss: 17.46848487854004\n",
      "[step: 2962] loss: 0.005780963692814112\n",
      "[step: 2963] loss: 18.047664642333984\n",
      "[step: 2963] loss: 0.0057829562574625015\n",
      "[step: 2964] loss: 18.885093688964844\n",
      "[step: 2964] loss: 0.005792116746306419\n",
      "[step: 2965] loss: 18.630311965942383\n",
      "[step: 2965] loss: 0.005785070825368166\n",
      "[step: 2966] loss: 17.695480346679688\n",
      "[step: 2966] loss: 0.00578659400343895\n",
      "[step: 2967] loss: 17.20511245727539\n",
      "[step: 2967] loss: 0.0057793790474534035\n",
      "[step: 2968] loss: 17.845561981201172\n",
      "[step: 2968] loss: 0.005769829265773296\n",
      "[step: 2969] loss: 18.512521743774414\n",
      "[step: 2969] loss: 0.00576303293928504\n",
      "[step: 2970] loss: 18.07058334350586\n",
      "[step: 2970] loss: 0.005760048050433397\n",
      "[step: 2971] loss: 17.092437744140625\n",
      "[step: 2971] loss: 0.005757891107350588\n",
      "[step: 2972] loss: 16.818058013916016\n",
      "[step: 2972] loss: 0.005757331848144531\n",
      "[step: 2973] loss: 17.344152450561523\n",
      "[step: 2973] loss: 0.005754949990659952\n",
      "[step: 2974] loss: 17.70406723022461\n",
      "[step: 2974] loss: 0.005752685945481062\n",
      "[step: 2975] loss: 17.196887969970703\n",
      "[step: 2975] loss: 0.00575271574780345\n",
      "[step: 2976] loss: 16.585552215576172\n",
      "[step: 2976] loss: 0.0057556768879294395\n",
      "[step: 2977] loss: 16.62008285522461\n",
      "[step: 2977] loss: 0.0057577332481741905\n",
      "[step: 2978] loss: 17.040603637695312\n",
      "[step: 2978] loss: 0.005760454572737217\n",
      "[step: 2979] loss: 17.166738510131836\n",
      "[step: 2979] loss: 0.0057709356769919395\n",
      "[step: 2980] loss: 16.89075469970703\n",
      "[step: 2980] loss: 0.00578094320371747\n",
      "[step: 2981] loss: 16.705720901489258\n",
      "[step: 2981] loss: 0.005809890106320381\n",
      "[step: 2982] loss: 16.932018280029297\n",
      "[step: 2982] loss: 0.005810524802654982\n",
      "[step: 2983] loss: 17.32379913330078\n",
      "[step: 2983] loss: 0.005819234997034073\n",
      "[step: 2984] loss: 17.7333927154541\n",
      "[step: 2984] loss: 0.005821015685796738\n",
      "[step: 2985] loss: 18.1341552734375\n",
      "[step: 2985] loss: 0.005815090611577034\n",
      "[step: 2986] loss: 19.09921646118164\n",
      "[step: 2986] loss: 0.005802013445645571\n",
      "[step: 2987] loss: 20.55048370361328\n",
      "[step: 2987] loss: 0.005786710884422064\n",
      "[step: 2988] loss: 22.87200355529785\n",
      "[step: 2988] loss: 0.00577820697799325\n",
      "[step: 2989] loss: 24.99996566772461\n",
      "[step: 2989] loss: 0.00578496977686882\n",
      "[step: 2990] loss: 27.509313583374023\n",
      "[step: 2990] loss: 0.005800690967589617\n",
      "[step: 2991] loss: 27.252582550048828\n",
      "[step: 2991] loss: 0.005824841093271971\n",
      "[step: 2992] loss: 25.156291961669922\n",
      "[step: 2992] loss: 0.005847929045557976\n",
      "[step: 2993] loss: 20.536258697509766\n",
      "[step: 2993] loss: 0.005884270183742046\n",
      "[step: 2994] loss: 17.519296646118164\n",
      "[step: 2994] loss: 0.0059220758266747\n",
      "[step: 2995] loss: 17.72026252746582\n",
      "[step: 2995] loss: 0.005971381440758705\n",
      "[step: 2996] loss: 19.357013702392578\n",
      "[step: 2996] loss: 0.005996718071401119\n",
      "[step: 2997] loss: 20.042190551757812\n",
      "[step: 2997] loss: 0.00598988588899374\n",
      "[step: 2998] loss: 18.967544555664062\n",
      "[step: 2998] loss: 0.005926094017922878\n",
      "[step: 2999] loss: 18.645606994628906\n",
      "[step: 2999] loss: 0.00584203889593482\n",
      "[step: 3000] loss: 20.047805786132812\n",
      "[step: 3000] loss: 0.005764651577919722\n",
      "[step: 3001] loss: 21.720287322998047\n",
      "[step: 3001] loss: 0.005771168973296881\n",
      "[step: 3002] loss: 21.217206954956055\n",
      "[step: 3002] loss: 0.005828055553138256\n",
      "[step: 3003] loss: 19.58767318725586\n",
      "[step: 3003] loss: 0.005861029028892517\n",
      "[step: 3004] loss: 18.36458396911621\n",
      "[step: 3004] loss: 0.005863375496119261\n",
      "[step: 3005] loss: 18.26256561279297\n",
      "[step: 3005] loss: 0.005814804695546627\n",
      "[step: 3006] loss: 17.968393325805664\n",
      "[step: 3006] loss: 0.005792216397821903\n",
      "[step: 3007] loss: 17.23988914489746\n",
      "[step: 3007] loss: 0.005778624676167965\n",
      "[step: 3008] loss: 17.09444808959961\n",
      "[step: 3008] loss: 0.005777055397629738\n",
      "[step: 3009] loss: 17.678686141967773\n",
      "[step: 3009] loss: 0.005789000075310469\n",
      "[step: 3010] loss: 17.844858169555664\n",
      "[step: 3010] loss: 0.005798369646072388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3011] loss: 17.217731475830078\n",
      "[step: 3011] loss: 0.005807880777865648\n",
      "[step: 3012] loss: 16.978836059570312\n",
      "[step: 3012] loss: 0.005788236856460571\n",
      "[step: 3013] loss: 17.46580696105957\n",
      "[step: 3013] loss: 0.005776969250291586\n",
      "[step: 3014] loss: 17.514114379882812\n",
      "[step: 3014] loss: 0.005767997354269028\n",
      "[step: 3015] loss: 16.82329559326172\n",
      "[step: 3015] loss: 0.005761943757534027\n",
      "[step: 3016] loss: 16.216415405273438\n",
      "[step: 3016] loss: 0.0057568661868572235\n",
      "[step: 3017] loss: 16.400087356567383\n",
      "[step: 3017] loss: 0.005757789593189955\n",
      "[step: 3018] loss: 16.81228256225586\n",
      "[step: 3018] loss: 0.005783667787909508\n",
      "[step: 3019] loss: 16.749948501586914\n",
      "[step: 3019] loss: 0.005779766943305731\n",
      "[step: 3020] loss: 16.44671630859375\n",
      "[step: 3020] loss: 0.005765958223491907\n",
      "[step: 3021] loss: 16.391202926635742\n",
      "[step: 3021] loss: 0.005748555529862642\n",
      "[step: 3022] loss: 16.574264526367188\n",
      "[step: 3022] loss: 0.005737904459238052\n",
      "[step: 3023] loss: 16.658687591552734\n",
      "[step: 3023] loss: 0.005741376895457506\n",
      "[step: 3024] loss: 16.504295349121094\n",
      "[step: 3024] loss: 0.005751703400164843\n",
      "[step: 3025] loss: 16.4066162109375\n",
      "[step: 3025] loss: 0.005766610149294138\n",
      "[step: 3026] loss: 16.516765594482422\n",
      "[step: 3026] loss: 0.0057686446234583855\n",
      "[step: 3027] loss: 16.67992401123047\n",
      "[step: 3027] loss: 0.005773801822215319\n",
      "[step: 3028] loss: 16.66288948059082\n",
      "[step: 3028] loss: 0.00575704500079155\n",
      "[step: 3029] loss: 16.525026321411133\n",
      "[step: 3029] loss: 0.005745971109718084\n",
      "[step: 3030] loss: 16.469755172729492\n",
      "[step: 3030] loss: 0.005740134511142969\n",
      "[step: 3031] loss: 16.620954513549805\n",
      "[step: 3031] loss: 0.0057394737377762794\n",
      "[step: 3032] loss: 16.877464294433594\n",
      "[step: 3032] loss: 0.005742150358855724\n",
      "[step: 3033] loss: 17.15487289428711\n",
      "[step: 3033] loss: 0.005744371563196182\n",
      "[step: 3034] loss: 17.381595611572266\n",
      "[step: 3034] loss: 0.005749502219259739\n",
      "[step: 3035] loss: 17.802024841308594\n",
      "[step: 3035] loss: 0.005756812170147896\n",
      "[step: 3036] loss: 18.51629638671875\n",
      "[step: 3036] loss: 0.005777490325272083\n",
      "[step: 3037] loss: 19.679489135742188\n",
      "[step: 3037] loss: 0.005785154644399881\n",
      "[step: 3038] loss: 21.2338924407959\n",
      "[step: 3038] loss: 0.005800711922347546\n",
      "[step: 3039] loss: 23.14828109741211\n",
      "[step: 3039] loss: 0.005802659317851067\n",
      "[step: 3040] loss: 24.84217071533203\n",
      "[step: 3040] loss: 0.005805552005767822\n",
      "[step: 3041] loss: 25.996719360351562\n",
      "[step: 3041] loss: 0.005804019048810005\n",
      "[step: 3042] loss: 25.72842025756836\n",
      "[step: 3042] loss: 0.005812464747577906\n",
      "[step: 3043] loss: 24.383115768432617\n",
      "[step: 3043] loss: 0.005831835325807333\n",
      "[step: 3044] loss: 21.613697052001953\n",
      "[step: 3044] loss: 0.005880904849618673\n",
      "[step: 3045] loss: 18.94059181213379\n",
      "[step: 3045] loss: 0.005936155095696449\n",
      "[step: 3046] loss: 17.18717384338379\n",
      "[step: 3046] loss: 0.006010020151734352\n",
      "[step: 3047] loss: 17.41179656982422\n",
      "[step: 3047] loss: 0.006021738052368164\n",
      "[step: 3048] loss: 18.928295135498047\n",
      "[step: 3048] loss: 0.005928827449679375\n",
      "[step: 3049] loss: 20.372116088867188\n",
      "[step: 3049] loss: 0.005799242295324802\n",
      "[step: 3050] loss: 21.441490173339844\n",
      "[step: 3050] loss: 0.005776872858405113\n",
      "[step: 3051] loss: 22.324573516845703\n",
      "[step: 3051] loss: 0.005835996009409428\n",
      "[step: 3052] loss: 22.583759307861328\n",
      "[step: 3052] loss: 0.005881542805582285\n",
      "[step: 3053] loss: 21.254886627197266\n",
      "[step: 3053] loss: 0.005788442213088274\n",
      "[step: 3054] loss: 18.401865005493164\n",
      "[step: 3054] loss: 0.005809226538985968\n",
      "[step: 3055] loss: 16.441499710083008\n",
      "[step: 3055] loss: 0.005953059531748295\n",
      "[step: 3056] loss: 16.9512996673584\n",
      "[step: 3056] loss: 0.005877632647752762\n",
      "[step: 3057] loss: 18.65728759765625\n",
      "[step: 3057] loss: 0.005819032434374094\n",
      "[step: 3058] loss: 19.357986450195312\n",
      "[step: 3058] loss: 0.005895910784602165\n",
      "[step: 3059] loss: 18.7032527923584\n",
      "[step: 3059] loss: 0.0058350954204797745\n",
      "[step: 3060] loss: 18.217201232910156\n",
      "[step: 3060] loss: 0.0057990229688584805\n",
      "[step: 3061] loss: 18.759838104248047\n",
      "[step: 3061] loss: 0.005828044842928648\n",
      "[step: 3062] loss: 18.82956886291504\n",
      "[step: 3062] loss: 0.005831540562212467\n",
      "[step: 3063] loss: 17.910518646240234\n",
      "[step: 3063] loss: 0.00579699594527483\n",
      "[step: 3064] loss: 16.690227508544922\n",
      "[step: 3064] loss: 0.005781133659183979\n",
      "[step: 3065] loss: 16.563472747802734\n",
      "[step: 3065] loss: 0.0058096046559512615\n",
      "[step: 3066] loss: 17.00162124633789\n",
      "[step: 3066] loss: 0.00579551188275218\n",
      "[step: 3067] loss: 16.889205932617188\n",
      "[step: 3067] loss: 0.0057587712071835995\n",
      "[step: 3068] loss: 16.373672485351562\n",
      "[step: 3068] loss: 0.005781910847872496\n",
      "[step: 3069] loss: 16.311534881591797\n",
      "[step: 3069] loss: 0.0057792156003415585\n",
      "[step: 3070] loss: 16.834508895874023\n",
      "[step: 3070] loss: 0.005761576816439629\n",
      "[step: 3071] loss: 17.09764862060547\n",
      "[step: 3071] loss: 0.0057553634978830814\n",
      "[step: 3072] loss: 16.709779739379883\n",
      "[step: 3072] loss: 0.0057571204379200935\n",
      "[step: 3073] loss: 16.26556396484375\n",
      "[step: 3073] loss: 0.005753282457590103\n",
      "[step: 3074] loss: 16.262718200683594\n",
      "[step: 3074] loss: 0.005746244452893734\n",
      "[step: 3075] loss: 16.493667602539062\n",
      "[step: 3075] loss: 0.005739198997616768\n",
      "[step: 3076] loss: 16.524925231933594\n",
      "[step: 3076] loss: 0.005751156248152256\n",
      "[step: 3077] loss: 16.296310424804688\n",
      "[step: 3077] loss: 0.005750000011175871\n",
      "[step: 3078] loss: 16.185955047607422\n",
      "[step: 3078] loss: 0.005744595546275377\n",
      "[step: 3079] loss: 16.291358947753906\n",
      "[step: 3079] loss: 0.005753892473876476\n",
      "[step: 3080] loss: 16.414947509765625\n",
      "[step: 3080] loss: 0.005756941623985767\n",
      "[step: 3081] loss: 16.34273910522461\n",
      "[step: 3081] loss: 0.005761872977018356\n",
      "[step: 3082] loss: 16.159168243408203\n",
      "[step: 3082] loss: 0.005772521253675222\n",
      "[step: 3083] loss: 16.086708068847656\n",
      "[step: 3083] loss: 0.005781854502856731\n",
      "[step: 3084] loss: 16.223262786865234\n",
      "[step: 3084] loss: 0.005797034129500389\n",
      "[step: 3085] loss: 16.437618255615234\n",
      "[step: 3085] loss: 0.00582217238843441\n",
      "[step: 3086] loss: 16.69870376586914\n",
      "[step: 3086] loss: 0.00585409440100193\n",
      "[step: 3087] loss: 17.00673484802246\n",
      "[step: 3087] loss: 0.005867037922143936\n",
      "[step: 3088] loss: 17.67969512939453\n",
      "[step: 3088] loss: 0.005881251767277718\n",
      "[step: 3089] loss: 18.76573944091797\n",
      "[step: 3089] loss: 0.005869991146028042\n",
      "[step: 3090] loss: 20.63766098022461\n",
      "[step: 3090] loss: 0.005852268543094397\n",
      "[step: 3091] loss: 23.041067123413086\n",
      "[step: 3091] loss: 0.005832437891513109\n",
      "[step: 3092] loss: 26.40771484375\n",
      "[step: 3092] loss: 0.005815659649670124\n",
      "[step: 3093] loss: 28.91061019897461\n",
      "[step: 3093] loss: 0.00580167630687356\n",
      "[step: 3094] loss: 30.637134552001953\n",
      "[step: 3094] loss: 0.005777583923190832\n",
      "[step: 3095] loss: 28.049854278564453\n",
      "[step: 3095] loss: 0.0057825446128845215\n",
      "[step: 3096] loss: 23.515636444091797\n",
      "[step: 3096] loss: 0.005797784775495529\n",
      "[step: 3097] loss: 19.539348602294922\n",
      "[step: 3097] loss: 0.0058059836737811565\n",
      "[step: 3098] loss: 18.830432891845703\n",
      "[step: 3098] loss: 0.005778660997748375\n",
      "[step: 3099] loss: 20.799280166625977\n",
      "[step: 3099] loss: 0.0057400488294661045\n",
      "[step: 3100] loss: 21.829833984375\n",
      "[step: 3100] loss: 0.005731937475502491\n",
      "[step: 3101] loss: 21.086647033691406\n",
      "[step: 3101] loss: 0.00574694387614727\n",
      "[step: 3102] loss: 19.182842254638672\n",
      "[step: 3102] loss: 0.005747081711888313\n",
      "[step: 3103] loss: 18.985336303710938\n",
      "[step: 3103] loss: 0.005738791078329086\n",
      "[step: 3104] loss: 20.03131866455078\n",
      "[step: 3104] loss: 0.0057352688163518906\n",
      "[step: 3105] loss: 20.352630615234375\n",
      "[step: 3105] loss: 0.005737627856433392\n",
      "[step: 3106] loss: 19.177146911621094\n",
      "[step: 3106] loss: 0.005751927383244038\n",
      "[step: 3107] loss: 18.007240295410156\n",
      "[step: 3107] loss: 0.005744569003582001\n",
      "[step: 3108] loss: 17.867233276367188\n",
      "[step: 3108] loss: 0.005734033416956663\n",
      "[step: 3109] loss: 18.050716400146484\n",
      "[step: 3109] loss: 0.005724266171455383\n",
      "[step: 3110] loss: 17.729124069213867\n",
      "[step: 3110] loss: 0.005722871050238609\n",
      "[step: 3111] loss: 17.381309509277344\n",
      "[step: 3111] loss: 0.005728103220462799\n",
      "[step: 3112] loss: 17.407428741455078\n",
      "[step: 3112] loss: 0.005723766982555389\n",
      "[step: 3113] loss: 17.280914306640625\n",
      "[step: 3113] loss: 0.005712080746889114\n",
      "[step: 3114] loss: 16.737483978271484\n",
      "[step: 3114] loss: 0.005704529583454132\n",
      "[step: 3115] loss: 16.250003814697266\n",
      "[step: 3115] loss: 0.005707907024770975\n",
      "[step: 3116] loss: 16.429641723632812\n",
      "[step: 3116] loss: 0.0057136500254273415\n",
      "[step: 3117] loss: 17.016361236572266\n",
      "[step: 3117] loss: 0.005713070742785931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3118] loss: 17.03812599182129\n",
      "[step: 3118] loss: 0.005709577817469835\n",
      "[step: 3119] loss: 16.458240509033203\n",
      "[step: 3119] loss: 0.00570515776053071\n",
      "[step: 3120] loss: 16.01696014404297\n",
      "[step: 3120] loss: 0.005703634116798639\n",
      "[step: 3121] loss: 16.1673641204834\n",
      "[step: 3121] loss: 0.005705066956579685\n",
      "[step: 3122] loss: 16.43474006652832\n",
      "[step: 3122] loss: 0.00570880388841033\n",
      "[step: 3123] loss: 16.314571380615234\n",
      "[step: 3123] loss: 0.005714360158890486\n",
      "[step: 3124] loss: 16.08612823486328\n",
      "[step: 3124] loss: 0.005722224246710539\n",
      "[step: 3125] loss: 16.123390197753906\n",
      "[step: 3125] loss: 0.005736458580940962\n",
      "[step: 3126] loss: 16.342226028442383\n",
      "[step: 3126] loss: 0.005774281919002533\n",
      "[step: 3127] loss: 16.418500900268555\n",
      "[step: 3127] loss: 0.0058084349147975445\n",
      "[step: 3128] loss: 16.291080474853516\n",
      "[step: 3128] loss: 0.005883703473955393\n",
      "[step: 3129] loss: 16.224079132080078\n",
      "[step: 3129] loss: 0.005904724821448326\n",
      "[step: 3130] loss: 16.410091400146484\n",
      "[step: 3130] loss: 0.005929061211645603\n",
      "[step: 3131] loss: 16.753795623779297\n",
      "[step: 3131] loss: 0.005924040451645851\n",
      "[step: 3132] loss: 17.181049346923828\n",
      "[step: 3132] loss: 0.005883750505745411\n",
      "[step: 3133] loss: 17.596996307373047\n",
      "[step: 3133] loss: 0.005885284394025803\n",
      "[step: 3134] loss: 18.282630920410156\n",
      "[step: 3134] loss: 0.005952096544206142\n",
      "[step: 3135] loss: 19.46424102783203\n",
      "[step: 3135] loss: 0.005975381471216679\n",
      "[step: 3136] loss: 21.261554718017578\n",
      "[step: 3136] loss: 0.005903364159166813\n",
      "[step: 3137] loss: 23.79922866821289\n",
      "[step: 3137] loss: 0.005808695685118437\n",
      "[step: 3138] loss: 26.307327270507812\n",
      "[step: 3138] loss: 0.005813038907945156\n",
      "[step: 3139] loss: 28.557235717773438\n",
      "[step: 3139] loss: 0.0058657098561525345\n",
      "[step: 3140] loss: 29.1509952545166\n",
      "[step: 3140] loss: 0.0058896844275295734\n",
      "[step: 3141] loss: 27.683719635009766\n",
      "[step: 3141] loss: 0.005840734113007784\n",
      "[step: 3142] loss: 24.330982208251953\n",
      "[step: 3142] loss: 0.0057921637780964375\n",
      "[step: 3143] loss: 19.566774368286133\n",
      "[step: 3143] loss: 0.005778378341346979\n",
      "[step: 3144] loss: 16.814008712768555\n",
      "[step: 3144] loss: 0.005767686292529106\n",
      "[step: 3145] loss: 17.28881072998047\n",
      "[step: 3145] loss: 0.005755596328526735\n",
      "[step: 3146] loss: 19.535446166992188\n",
      "[step: 3146] loss: 0.005735327489674091\n",
      "[step: 3147] loss: 20.726848602294922\n",
      "[step: 3147] loss: 0.0057324618101119995\n",
      "[step: 3148] loss: 19.75835418701172\n",
      "[step: 3148] loss: 0.005747657269239426\n",
      "[step: 3149] loss: 18.825775146484375\n",
      "[step: 3149] loss: 0.005754258017987013\n",
      "[step: 3150] loss: 19.595531463623047\n",
      "[step: 3150] loss: 0.005744573660194874\n",
      "[step: 3151] loss: 21.557842254638672\n",
      "[step: 3151] loss: 0.005752763710916042\n",
      "[step: 3152] loss: 22.93310546875\n",
      "[step: 3152] loss: 0.005758314859122038\n",
      "[step: 3153] loss: 23.577739715576172\n",
      "[step: 3153] loss: 0.005757384933531284\n",
      "[step: 3154] loss: 22.67208480834961\n",
      "[step: 3154] loss: 0.005746905226260424\n",
      "[step: 3155] loss: 21.541439056396484\n",
      "[step: 3155] loss: 0.005748734809458256\n",
      "[step: 3156] loss: 19.320833206176758\n",
      "[step: 3156] loss: 0.005725705996155739\n",
      "[step: 3157] loss: 17.904430389404297\n",
      "[step: 3157] loss: 0.0057107978500425816\n",
      "[step: 3158] loss: 17.724489212036133\n",
      "[step: 3158] loss: 0.0057107568718492985\n",
      "[step: 3159] loss: 18.243389129638672\n",
      "[step: 3159] loss: 0.00570924486964941\n",
      "[step: 3160] loss: 18.075939178466797\n",
      "[step: 3160] loss: 0.005698599386960268\n",
      "[step: 3161] loss: 17.227798461914062\n",
      "[step: 3161] loss: 0.005698699038475752\n",
      "[step: 3162] loss: 16.88931655883789\n",
      "[step: 3162] loss: 0.005709062796086073\n",
      "[step: 3163] loss: 17.444766998291016\n",
      "[step: 3163] loss: 0.005717901512980461\n",
      "[step: 3164] loss: 17.968955993652344\n",
      "[step: 3164] loss: 0.005731374491006136\n",
      "[step: 3165] loss: 17.805444717407227\n",
      "[step: 3165] loss: 0.005744231399148703\n",
      "[step: 3166] loss: 17.183223724365234\n",
      "[step: 3166] loss: 0.005771468859165907\n",
      "[step: 3167] loss: 16.74920654296875\n",
      "[step: 3167] loss: 0.0057786027900874615\n",
      "[step: 3168] loss: 16.591089248657227\n",
      "[step: 3168] loss: 0.005787623580545187\n",
      "[step: 3169] loss: 16.396631240844727\n",
      "[step: 3169] loss: 0.005790479015558958\n",
      "[step: 3170] loss: 16.171825408935547\n",
      "[step: 3170] loss: 0.005790157709270716\n",
      "[step: 3171] loss: 16.12555694580078\n",
      "[step: 3171] loss: 0.005774373188614845\n",
      "[step: 3172] loss: 16.23531150817871\n",
      "[step: 3172] loss: 0.00575368432328105\n",
      "[step: 3173] loss: 16.26092529296875\n",
      "[step: 3173] loss: 0.005726825445890427\n",
      "[step: 3174] loss: 16.218936920166016\n",
      "[step: 3174] loss: 0.005704232957214117\n",
      "[step: 3175] loss: 16.296838760375977\n",
      "[step: 3175] loss: 0.005691199097782373\n",
      "[step: 3176] loss: 16.434986114501953\n",
      "[step: 3176] loss: 0.005690635181963444\n",
      "[step: 3177] loss: 16.472475051879883\n",
      "[step: 3177] loss: 0.005696613807231188\n",
      "[step: 3178] loss: 16.320674896240234\n",
      "[step: 3178] loss: 0.005706120748072863\n",
      "[step: 3179] loss: 16.163982391357422\n",
      "[step: 3179] loss: 0.005712061654776335\n",
      "[step: 3180] loss: 16.210935592651367\n",
      "[step: 3180] loss: 0.005717173218727112\n",
      "[step: 3181] loss: 16.399023056030273\n",
      "[step: 3181] loss: 0.0057121869176626205\n",
      "[step: 3182] loss: 16.51618766784668\n",
      "[step: 3182] loss: 0.005704591516405344\n",
      "[step: 3183] loss: 16.479473114013672\n",
      "[step: 3183] loss: 0.005699720233678818\n",
      "[step: 3184] loss: 16.40648651123047\n",
      "[step: 3184] loss: 0.0056987605057656765\n",
      "[step: 3185] loss: 16.502735137939453\n",
      "[step: 3185] loss: 0.005703172646462917\n",
      "[step: 3186] loss: 16.78406524658203\n",
      "[step: 3186] loss: 0.005706573836505413\n",
      "[step: 3187] loss: 17.195571899414062\n",
      "[step: 3187] loss: 0.005712202284485102\n",
      "[step: 3188] loss: 17.596914291381836\n",
      "[step: 3188] loss: 0.005709596909582615\n",
      "[step: 3189] loss: 18.140472412109375\n",
      "[step: 3189] loss: 0.005710806231945753\n",
      "[step: 3190] loss: 18.651840209960938\n",
      "[step: 3190] loss: 0.005701031070202589\n",
      "[step: 3191] loss: 19.512130737304688\n",
      "[step: 3191] loss: 0.005703463219106197\n",
      "[step: 3192] loss: 20.12186050415039\n",
      "[step: 3192] loss: 0.005705690011382103\n",
      "[step: 3193] loss: 20.894020080566406\n",
      "[step: 3193] loss: 0.005724925547838211\n",
      "[step: 3194] loss: 21.143352508544922\n",
      "[step: 3194] loss: 0.005754075478762388\n",
      "[step: 3195] loss: 21.153831481933594\n",
      "[step: 3195] loss: 0.005801412742584944\n",
      "[step: 3196] loss: 20.78403091430664\n",
      "[step: 3196] loss: 0.005857672076672316\n",
      "[step: 3197] loss: 19.936691284179688\n",
      "[step: 3197] loss: 0.005931034218519926\n",
      "[step: 3198] loss: 18.85007095336914\n",
      "[step: 3198] loss: 0.00596339488402009\n",
      "[step: 3199] loss: 17.663570404052734\n",
      "[step: 3199] loss: 0.005964051466435194\n",
      "[step: 3200] loss: 16.79210662841797\n",
      "[step: 3200] loss: 0.00589405233040452\n",
      "[step: 3201] loss: 16.529294967651367\n",
      "[step: 3201] loss: 0.00580515805631876\n",
      "[step: 3202] loss: 16.8414306640625\n",
      "[step: 3202] loss: 0.0057427845895290375\n",
      "[step: 3203] loss: 17.428726196289062\n",
      "[step: 3203] loss: 0.005741123575717211\n",
      "[step: 3204] loss: 18.23153305053711\n",
      "[step: 3204] loss: 0.00577193358913064\n",
      "[step: 3205] loss: 19.01291275024414\n",
      "[step: 3205] loss: 0.0057913437485694885\n",
      "[step: 3206] loss: 19.649572372436523\n",
      "[step: 3206] loss: 0.005803894717246294\n",
      "[step: 3207] loss: 19.648319244384766\n",
      "[step: 3207] loss: 0.005776631645858288\n",
      "[step: 3208] loss: 18.961910247802734\n",
      "[step: 3208] loss: 0.005739443004131317\n",
      "[step: 3209] loss: 17.711139678955078\n",
      "[step: 3209] loss: 0.0057067060843110085\n",
      "[step: 3210] loss: 16.511194229125977\n",
      "[step: 3210] loss: 0.005703954491764307\n",
      "[step: 3211] loss: 15.782448768615723\n",
      "[step: 3211] loss: 0.005731167271733284\n",
      "[step: 3212] loss: 15.573543548583984\n",
      "[step: 3212] loss: 0.005743795074522495\n",
      "[step: 3213] loss: 15.732587814331055\n",
      "[step: 3213] loss: 0.005739456973969936\n",
      "[step: 3214] loss: 16.09979820251465\n",
      "[step: 3214] loss: 0.0057113440707325935\n",
      "[step: 3215] loss: 16.588497161865234\n",
      "[step: 3215] loss: 0.005686324555426836\n",
      "[step: 3216] loss: 17.12071418762207\n",
      "[step: 3216] loss: 0.005681472364813089\n",
      "[step: 3217] loss: 17.51963233947754\n",
      "[step: 3217] loss: 0.005695406347513199\n",
      "[step: 3218] loss: 17.699504852294922\n",
      "[step: 3218] loss: 0.005719475448131561\n",
      "[step: 3219] loss: 17.63477897644043\n",
      "[step: 3219] loss: 0.005724698770791292\n",
      "[step: 3220] loss: 17.39736557006836\n",
      "[step: 3220] loss: 0.005723604932427406\n",
      "[step: 3221] loss: 17.234899520874023\n",
      "[step: 3221] loss: 0.0057011875323951244\n",
      "[step: 3222] loss: 17.00925064086914\n",
      "[step: 3222] loss: 0.005688907578587532\n",
      "[step: 3223] loss: 16.87943458557129\n",
      "[step: 3223] loss: 0.005680827423930168\n",
      "[step: 3224] loss: 16.601572036743164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3224] loss: 0.005680006463080645\n",
      "[step: 3225] loss: 16.400501251220703\n",
      "[step: 3225] loss: 0.0056837755255401134\n",
      "[step: 3226] loss: 16.178401947021484\n",
      "[step: 3226] loss: 0.005688007455319166\n",
      "[step: 3227] loss: 16.04995346069336\n",
      "[step: 3227] loss: 0.0056929695419967175\n",
      "[step: 3228] loss: 15.969209671020508\n",
      "[step: 3228] loss: 0.005694087594747543\n",
      "[step: 3229] loss: 15.957571029663086\n",
      "[step: 3229] loss: 0.0057033696211874485\n",
      "[step: 3230] loss: 16.08699607849121\n",
      "[step: 3230] loss: 0.005694070365279913\n",
      "[step: 3231] loss: 16.392332077026367\n",
      "[step: 3231] loss: 0.005690278019756079\n",
      "[step: 3232] loss: 16.95067596435547\n",
      "[step: 3232] loss: 0.005685809534043074\n",
      "[step: 3233] loss: 17.684017181396484\n",
      "[step: 3233] loss: 0.005682516843080521\n",
      "[step: 3234] loss: 18.531484603881836\n",
      "[step: 3234] loss: 0.005683823022991419\n",
      "[step: 3235] loss: 19.362014770507812\n",
      "[step: 3235] loss: 0.005693237297236919\n",
      "[step: 3236] loss: 20.048879623413086\n",
      "[step: 3236] loss: 0.0057185073383152485\n",
      "[step: 3237] loss: 20.445850372314453\n",
      "[step: 3237] loss: 0.005759730003774166\n",
      "[step: 3238] loss: 20.653076171875\n",
      "[step: 3238] loss: 0.005831702146679163\n",
      "[step: 3239] loss: 20.388154983520508\n",
      "[step: 3239] loss: 0.005902433302253485\n",
      "[step: 3240] loss: 19.953022003173828\n",
      "[step: 3240] loss: 0.0059622968547046185\n",
      "[step: 3241] loss: 18.959712982177734\n",
      "[step: 3241] loss: 0.00592012470588088\n",
      "[step: 3242] loss: 17.967288970947266\n",
      "[step: 3242] loss: 0.005837072618305683\n",
      "[step: 3243] loss: 17.044078826904297\n",
      "[step: 3243] loss: 0.005795601289719343\n",
      "[step: 3244] loss: 16.566932678222656\n",
      "[step: 3244] loss: 0.005841020494699478\n",
      "[step: 3245] loss: 16.471370697021484\n",
      "[step: 3245] loss: 0.005874781869351864\n",
      "[step: 3246] loss: 16.464689254760742\n",
      "[step: 3246] loss: 0.0058462671004235744\n",
      "[step: 3247] loss: 16.450538635253906\n",
      "[step: 3247] loss: 0.00581487687304616\n",
      "[step: 3248] loss: 16.31247329711914\n",
      "[step: 3248] loss: 0.005783379077911377\n",
      "[step: 3249] loss: 16.25267219543457\n",
      "[step: 3249] loss: 0.005752066615968943\n",
      "[step: 3250] loss: 16.262422561645508\n",
      "[step: 3250] loss: 0.005719461478292942\n",
      "[step: 3251] loss: 16.419567108154297\n",
      "[step: 3251] loss: 0.005721228197216988\n",
      "[step: 3252] loss: 16.79751968383789\n",
      "[step: 3252] loss: 0.005736811086535454\n",
      "[step: 3253] loss: 17.531143188476562\n",
      "[step: 3253] loss: 0.005751113872975111\n",
      "[step: 3254] loss: 19.020776748657227\n",
      "[step: 3254] loss: 0.005733236204832792\n",
      "[step: 3255] loss: 21.534908294677734\n",
      "[step: 3255] loss: 0.00571136549115181\n",
      "[step: 3256] loss: 24.627201080322266\n",
      "[step: 3256] loss: 0.005703000817447901\n",
      "[step: 3257] loss: 26.90998649597168\n",
      "[step: 3257] loss: 0.005693231243640184\n",
      "[step: 3258] loss: 24.875751495361328\n",
      "[step: 3258] loss: 0.00568385049700737\n",
      "[step: 3259] loss: 20.63890838623047\n",
      "[step: 3259] loss: 0.005688605830073357\n",
      "[step: 3260] loss: 19.449798583984375\n",
      "[step: 3260] loss: 0.005695487838238478\n",
      "[step: 3261] loss: 21.744430541992188\n",
      "[step: 3261] loss: 0.005691023077815771\n",
      "[step: 3262] loss: 20.58544921875\n",
      "[step: 3262] loss: 0.005686281248927116\n",
      "[step: 3263] loss: 18.532554626464844\n",
      "[step: 3263] loss: 0.005688263103365898\n",
      "[step: 3264] loss: 20.223133087158203\n",
      "[step: 3264] loss: 0.005695666186511517\n",
      "[step: 3265] loss: 24.095903396606445\n",
      "[step: 3265] loss: 0.0056833927519619465\n",
      "[step: 3266] loss: 26.363954544067383\n",
      "[step: 3266] loss: 0.005680042318999767\n",
      "[step: 3267] loss: 28.258506774902344\n",
      "[step: 3267] loss: 0.005677732173353434\n",
      "[step: 3268] loss: 27.56633758544922\n",
      "[step: 3268] loss: 0.005680652800947428\n",
      "[step: 3269] loss: 27.81622314453125\n",
      "[step: 3269] loss: 0.0056904759258031845\n",
      "[step: 3270] loss: 26.902334213256836\n",
      "[step: 3270] loss: 0.005711009260267019\n",
      "[step: 3271] loss: 23.91986846923828\n",
      "[step: 3271] loss: 0.005731530953198671\n",
      "[step: 3272] loss: 22.636568069458008\n",
      "[step: 3272] loss: 0.005769174080342054\n",
      "[step: 3273] loss: 19.839698791503906\n",
      "[step: 3273] loss: 0.005799426231533289\n",
      "[step: 3274] loss: 18.406314849853516\n",
      "[step: 3274] loss: 0.0058575524017214775\n",
      "[step: 3275] loss: 20.59585189819336\n",
      "[step: 3275] loss: 0.005807720590382814\n",
      "[step: 3276] loss: 20.94695281982422\n",
      "[step: 3276] loss: 0.005773106589913368\n",
      "[step: 3277] loss: 19.559890747070312\n",
      "[step: 3277] loss: 0.0057632396928966045\n",
      "[step: 3278] loss: 17.71454620361328\n",
      "[step: 3278] loss: 0.005740674678236246\n",
      "[step: 3279] loss: 17.983436584472656\n",
      "[step: 3279] loss: 0.0057213930413126945\n",
      "[step: 3280] loss: 18.290433883666992\n",
      "[step: 3280] loss: 0.005707894451916218\n",
      "[step: 3281] loss: 18.419265747070312\n",
      "[step: 3281] loss: 0.005709427874535322\n",
      "[step: 3282] loss: 17.807994842529297\n",
      "[step: 3282] loss: 0.005721468478441238\n",
      "[step: 3283] loss: 16.470232009887695\n",
      "[step: 3283] loss: 0.005740492604672909\n",
      "[step: 3284] loss: 16.340606689453125\n",
      "[step: 3284] loss: 0.005746976938098669\n",
      "[step: 3285] loss: 17.104068756103516\n",
      "[step: 3285] loss: 0.005718691740185022\n",
      "[step: 3286] loss: 17.154647827148438\n",
      "[step: 3286] loss: 0.005692498292773962\n",
      "[step: 3287] loss: 16.524375915527344\n",
      "[step: 3287] loss: 0.00567650655284524\n",
      "[step: 3288] loss: 16.26226806640625\n",
      "[step: 3288] loss: 0.0056771934032440186\n",
      "[step: 3289] loss: 16.581798553466797\n",
      "[step: 3289] loss: 0.005688400007784367\n",
      "[step: 3290] loss: 16.683124542236328\n",
      "[step: 3290] loss: 0.0056937080807983875\n",
      "[step: 3291] loss: 16.090137481689453\n",
      "[step: 3291] loss: 0.005696864798665047\n",
      "[step: 3292] loss: 15.751413345336914\n",
      "[step: 3292] loss: 0.005684060975909233\n",
      "[step: 3293] loss: 16.06085205078125\n",
      "[step: 3293] loss: 0.0056731016375124454\n",
      "[step: 3294] loss: 16.075302124023438\n",
      "[step: 3294] loss: 0.00566439051181078\n",
      "[step: 3295] loss: 15.84766960144043\n",
      "[step: 3295] loss: 0.005664198659360409\n",
      "[step: 3296] loss: 15.773123741149902\n",
      "[step: 3296] loss: 0.005672362167388201\n",
      "[step: 3297] loss: 15.626595497131348\n",
      "[step: 3297] loss: 0.005674454383552074\n",
      "[step: 3298] loss: 15.509016990661621\n",
      "[step: 3298] loss: 0.005668718367815018\n",
      "[step: 3299] loss: 15.528457641601562\n",
      "[step: 3299] loss: 0.005660527851432562\n",
      "[step: 3300] loss: 15.555829048156738\n",
      "[step: 3300] loss: 0.005657851230353117\n",
      "[step: 3301] loss: 15.569779396057129\n",
      "[step: 3301] loss: 0.005662969779223204\n",
      "[step: 3302] loss: 15.491464614868164\n",
      "[step: 3302] loss: 0.005670048762112856\n",
      "[step: 3303] loss: 15.337894439697266\n",
      "[step: 3303] loss: 0.005676659755408764\n",
      "[step: 3304] loss: 15.336106300354004\n",
      "[step: 3304] loss: 0.005679716821759939\n",
      "[step: 3305] loss: 15.429603576660156\n",
      "[step: 3305] loss: 0.005701852962374687\n",
      "[step: 3306] loss: 15.412732124328613\n",
      "[step: 3306] loss: 0.005713403690606356\n",
      "[step: 3307] loss: 15.355369567871094\n",
      "[step: 3307] loss: 0.005745958536863327\n",
      "[step: 3308] loss: 15.415764808654785\n",
      "[step: 3308] loss: 0.005775034427642822\n",
      "[step: 3309] loss: 15.568841934204102\n",
      "[step: 3309] loss: 0.005821052007377148\n",
      "[step: 3310] loss: 15.78940486907959\n",
      "[step: 3310] loss: 0.0058396668173372746\n",
      "[step: 3311] loss: 16.228683471679688\n",
      "[step: 3311] loss: 0.005845319014042616\n",
      "[step: 3312] loss: 16.945011138916016\n",
      "[step: 3312] loss: 0.005803942680358887\n",
      "[step: 3313] loss: 18.35983657836914\n",
      "[step: 3313] loss: 0.005746732000261545\n",
      "[step: 3314] loss: 20.544212341308594\n",
      "[step: 3314] loss: 0.005699771456420422\n",
      "[step: 3315] loss: 24.722862243652344\n",
      "[step: 3315] loss: 0.005683873780071735\n",
      "[step: 3316] loss: 28.652992248535156\n",
      "[step: 3316] loss: 0.005690387450158596\n",
      "[step: 3317] loss: 34.02411651611328\n",
      "[step: 3317] loss: 0.005708315409719944\n",
      "[step: 3318] loss: 31.841808319091797\n",
      "[step: 3318] loss: 0.005715831182897091\n",
      "[step: 3319] loss: 25.763809204101562\n",
      "[step: 3319] loss: 0.005712626967579126\n",
      "[step: 3320] loss: 18.401527404785156\n",
      "[step: 3320] loss: 0.0057182274758815765\n",
      "[step: 3321] loss: 16.761974334716797\n",
      "[step: 3321] loss: 0.005701005924493074\n",
      "[step: 3322] loss: 20.25409698486328\n",
      "[step: 3322] loss: 0.00569545803591609\n",
      "[step: 3323] loss: 23.07693099975586\n",
      "[step: 3323] loss: 0.0056876507587730885\n",
      "[step: 3324] loss: 22.87529182434082\n",
      "[step: 3324] loss: 0.005678842309862375\n",
      "[step: 3325] loss: 20.78689193725586\n",
      "[step: 3325] loss: 0.005670113023370504\n",
      "[step: 3326] loss: 18.97810173034668\n",
      "[step: 3326] loss: 0.00567259918898344\n",
      "[step: 3327] loss: 19.590200424194336\n",
      "[step: 3327] loss: 0.005685257259756327\n",
      "[step: 3328] loss: 20.36276626586914\n",
      "[step: 3328] loss: 0.005695433355867863\n",
      "[step: 3329] loss: 20.510379791259766\n",
      "[step: 3329] loss: 0.005709282122552395\n",
      "[step: 3330] loss: 19.885101318359375\n",
      "[step: 3330] loss: 0.005686044692993164\n",
      "[step: 3331] loss: 18.755298614501953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3331] loss: 0.005665321834385395\n",
      "[step: 3332] loss: 18.060611724853516\n",
      "[step: 3332] loss: 0.005647014360874891\n",
      "[step: 3333] loss: 18.048046112060547\n",
      "[step: 3333] loss: 0.005645327270030975\n",
      "[step: 3334] loss: 18.761730194091797\n",
      "[step: 3334] loss: 0.005654876120388508\n",
      "[step: 3335] loss: 18.503704071044922\n",
      "[step: 3335] loss: 0.005659619811922312\n",
      "[step: 3336] loss: 17.282678604125977\n",
      "[step: 3336] loss: 0.005666871089488268\n",
      "[step: 3337] loss: 16.240039825439453\n",
      "[step: 3337] loss: 0.005655631422996521\n",
      "[step: 3338] loss: 16.38906478881836\n",
      "[step: 3338] loss: 0.005652230232954025\n",
      "[step: 3339] loss: 17.029823303222656\n",
      "[step: 3339] loss: 0.005654225125908852\n",
      "[step: 3340] loss: 16.721324920654297\n",
      "[step: 3340] loss: 0.0056597162038087845\n",
      "[step: 3341] loss: 15.800379753112793\n",
      "[step: 3341] loss: 0.005661299917846918\n",
      "[step: 3342] loss: 15.655234336853027\n",
      "[step: 3342] loss: 0.005659388843923807\n",
      "[step: 3343] loss: 16.32767105102539\n",
      "[step: 3343] loss: 0.0056573208421468735\n",
      "[step: 3344] loss: 16.361845016479492\n",
      "[step: 3344] loss: 0.005664943251758814\n",
      "[step: 3345] loss: 15.609037399291992\n",
      "[step: 3345] loss: 0.005682578310370445\n",
      "[step: 3346] loss: 15.452585220336914\n",
      "[step: 3346] loss: 0.00574048375710845\n",
      "[step: 3347] loss: 15.931435585021973\n",
      "[step: 3347] loss: 0.005747511517256498\n",
      "[step: 3348] loss: 15.98200511932373\n",
      "[step: 3348] loss: 0.005765909794718027\n",
      "[step: 3349] loss: 15.563211441040039\n",
      "[step: 3349] loss: 0.005800479557365179\n",
      "[step: 3350] loss: 15.445777893066406\n",
      "[step: 3350] loss: 0.00582114327698946\n",
      "[step: 3351] loss: 15.747461318969727\n",
      "[step: 3351] loss: 0.005814169067889452\n",
      "[step: 3352] loss: 15.812251091003418\n",
      "[step: 3352] loss: 0.005766744259744883\n",
      "[step: 3353] loss: 15.650726318359375\n",
      "[step: 3353] loss: 0.0057023572735488415\n",
      "[step: 3354] loss: 15.597408294677734\n",
      "[step: 3354] loss: 0.005685477051883936\n",
      "[step: 3355] loss: 15.820565223693848\n",
      "[step: 3355] loss: 0.005691694561392069\n",
      "[step: 3356] loss: 16.092681884765625\n",
      "[step: 3356] loss: 0.0057188523933291435\n",
      "[step: 3357] loss: 16.22116470336914\n",
      "[step: 3357] loss: 0.005730724893510342\n",
      "[step: 3358] loss: 16.389488220214844\n",
      "[step: 3358] loss: 0.005708725657314062\n",
      "[step: 3359] loss: 16.889629364013672\n",
      "[step: 3359] loss: 0.005711723584681749\n",
      "[step: 3360] loss: 17.747079849243164\n",
      "[step: 3360] loss: 0.005695962347090244\n",
      "[step: 3361] loss: 18.969661712646484\n",
      "[step: 3361] loss: 0.005696057341992855\n",
      "[step: 3362] loss: 20.0655574798584\n",
      "[step: 3362] loss: 0.005698374006897211\n",
      "[step: 3363] loss: 21.53378677368164\n",
      "[step: 3363] loss: 0.005697291344404221\n",
      "[step: 3364] loss: 23.086870193481445\n",
      "[step: 3364] loss: 0.005702354945242405\n",
      "[step: 3365] loss: 24.353404998779297\n",
      "[step: 3365] loss: 0.005677759647369385\n",
      "[step: 3366] loss: 24.82345199584961\n",
      "[step: 3366] loss: 0.005671563558280468\n",
      "[step: 3367] loss: 23.234115600585938\n",
      "[step: 3367] loss: 0.005680561996996403\n",
      "[step: 3368] loss: 20.36395263671875\n",
      "[step: 3368] loss: 0.00567656010389328\n",
      "[step: 3369] loss: 17.334003448486328\n",
      "[step: 3369] loss: 0.005662995390594006\n",
      "[step: 3370] loss: 15.749281883239746\n",
      "[step: 3370] loss: 0.0056512667797505856\n",
      "[step: 3371] loss: 15.863897323608398\n",
      "[step: 3371] loss: 0.005660880357027054\n",
      "[step: 3372] loss: 16.647274017333984\n",
      "[step: 3372] loss: 0.0056670657359063625\n",
      "[step: 3373] loss: 17.433971405029297\n",
      "[step: 3373] loss: 0.005654254462569952\n",
      "[step: 3374] loss: 18.134519577026367\n",
      "[step: 3374] loss: 0.0056456043384969234\n",
      "[step: 3375] loss: 18.53487777709961\n",
      "[step: 3375] loss: 0.005650152452290058\n",
      "[step: 3376] loss: 18.485145568847656\n",
      "[step: 3376] loss: 0.0056588370352983475\n",
      "[step: 3377] loss: 17.287202835083008\n",
      "[step: 3377] loss: 0.005661836825311184\n",
      "[step: 3378] loss: 16.230764389038086\n",
      "[step: 3378] loss: 0.00566477095708251\n",
      "[step: 3379] loss: 15.787420272827148\n",
      "[step: 3379] loss: 0.005686499644070864\n",
      "[step: 3380] loss: 15.894166946411133\n",
      "[step: 3380] loss: 0.005720923189073801\n",
      "[step: 3381] loss: 15.896469116210938\n",
      "[step: 3381] loss: 0.005764812231063843\n",
      "[step: 3382] loss: 15.556917190551758\n",
      "[step: 3382] loss: 0.005804692395031452\n",
      "[step: 3383] loss: 15.364048957824707\n",
      "[step: 3383] loss: 0.005860207136720419\n",
      "[step: 3384] loss: 15.664376258850098\n",
      "[step: 3384] loss: 0.005891459062695503\n",
      "[step: 3385] loss: 16.214466094970703\n",
      "[step: 3385] loss: 0.005887738894671202\n",
      "[step: 3386] loss: 16.49018096923828\n",
      "[step: 3386] loss: 0.005808212794363499\n",
      "[step: 3387] loss: 16.31251335144043\n",
      "[step: 3387] loss: 0.005716721527278423\n",
      "[step: 3388] loss: 15.945897102355957\n",
      "[step: 3388] loss: 0.00566926971077919\n",
      "[step: 3389] loss: 15.784402847290039\n",
      "[step: 3389] loss: 0.005683607421815395\n",
      "[step: 3390] loss: 15.750734329223633\n",
      "[step: 3390] loss: 0.005739146843552589\n",
      "[step: 3391] loss: 15.70370864868164\n",
      "[step: 3391] loss: 0.005750239826738834\n",
      "[step: 3392] loss: 15.47104549407959\n",
      "[step: 3392] loss: 0.005722493398934603\n",
      "[step: 3393] loss: 15.256938934326172\n",
      "[step: 3393] loss: 0.00566480029374361\n",
      "[step: 3394] loss: 15.164127349853516\n",
      "[step: 3394] loss: 0.0056371488608419895\n",
      "[step: 3395] loss: 15.183612823486328\n",
      "[step: 3395] loss: 0.0056571802124381065\n",
      "[step: 3396] loss: 15.172233581542969\n",
      "[step: 3396] loss: 0.005693037062883377\n",
      "[step: 3397] loss: 15.045774459838867\n",
      "[step: 3397] loss: 0.005721877794712782\n",
      "[step: 3398] loss: 14.890324592590332\n",
      "[step: 3398] loss: 0.0056843226775527\n",
      "[step: 3399] loss: 14.834124565124512\n",
      "[step: 3399] loss: 0.005652187392115593\n",
      "[step: 3400] loss: 14.899188995361328\n",
      "[step: 3400] loss: 0.005649447441101074\n",
      "[step: 3401] loss: 14.992599487304688\n",
      "[step: 3401] loss: 0.005669017322361469\n",
      "[step: 3402] loss: 15.013509750366211\n",
      "[step: 3402] loss: 0.005695550702512264\n",
      "[step: 3403] loss: 14.954108238220215\n",
      "[step: 3403] loss: 0.005695648491382599\n",
      "[step: 3404] loss: 14.880355834960938\n",
      "[step: 3404] loss: 0.00568747241050005\n",
      "[step: 3405] loss: 14.856327056884766\n",
      "[step: 3405] loss: 0.005651870276778936\n",
      "[step: 3406] loss: 14.912945747375488\n",
      "[step: 3406] loss: 0.005635484587401152\n",
      "[step: 3407] loss: 15.030803680419922\n",
      "[step: 3407] loss: 0.005636976100504398\n",
      "[step: 3408] loss: 15.205772399902344\n",
      "[step: 3408] loss: 0.005648807156831026\n",
      "[step: 3409] loss: 15.447209358215332\n",
      "[step: 3409] loss: 0.005662280134856701\n",
      "[step: 3410] loss: 15.918985366821289\n",
      "[step: 3410] loss: 0.005667695309966803\n",
      "[step: 3411] loss: 16.757421493530273\n",
      "[step: 3411] loss: 0.00568451127037406\n",
      "[step: 3412] loss: 18.405845642089844\n",
      "[step: 3412] loss: 0.005670328624546528\n",
      "[step: 3413] loss: 21.12519073486328\n",
      "[step: 3413] loss: 0.005673519801348448\n",
      "[step: 3414] loss: 25.725500106811523\n",
      "[step: 3414] loss: 0.005694407969713211\n",
      "[step: 3415] loss: 31.126625061035156\n",
      "[step: 3415] loss: 0.005708626005798578\n",
      "[step: 3416] loss: 37.20021438598633\n",
      "[step: 3416] loss: 0.005723066162317991\n",
      "[step: 3417] loss: 37.321754455566406\n",
      "[step: 3417] loss: 0.005682320799678564\n",
      "[step: 3418] loss: 32.30615234375\n",
      "[step: 3418] loss: 0.0056535000912845135\n",
      "[step: 3419] loss: 24.89853286743164\n",
      "[step: 3419] loss: 0.005643265321850777\n",
      "[step: 3420] loss: 20.72995376586914\n",
      "[step: 3420] loss: 0.005646507255733013\n",
      "[step: 3421] loss: 22.451583862304688\n",
      "[step: 3421] loss: 0.0056624338030815125\n",
      "[step: 3422] loss: 23.425525665283203\n",
      "[step: 3422] loss: 0.005660735536366701\n",
      "[step: 3423] loss: 22.33037567138672\n",
      "[step: 3423] loss: 0.005653614643961191\n",
      "[step: 3424] loss: 23.30492401123047\n",
      "[step: 3424] loss: 0.005639386363327503\n",
      "[step: 3425] loss: 25.686599731445312\n",
      "[step: 3425] loss: 0.005628041923046112\n",
      "[step: 3426] loss: 25.847797393798828\n",
      "[step: 3426] loss: 0.005626389756798744\n",
      "[step: 3427] loss: 21.276134490966797\n",
      "[step: 3427] loss: 0.005626921076327562\n",
      "[step: 3428] loss: 18.834217071533203\n",
      "[step: 3428] loss: 0.005631690379232168\n",
      "[step: 3429] loss: 19.360759735107422\n",
      "[step: 3429] loss: 0.005633004941046238\n",
      "[step: 3430] loss: 19.983444213867188\n",
      "[step: 3430] loss: 0.005644925404340029\n",
      "[step: 3431] loss: 18.52916717529297\n",
      "[step: 3431] loss: 0.005631601437926292\n",
      "[step: 3432] loss: 17.204004287719727\n",
      "[step: 3432] loss: 0.005622884724289179\n",
      "[step: 3433] loss: 17.809459686279297\n",
      "[step: 3433] loss: 0.005621346645057201\n",
      "[step: 3434] loss: 18.0357608795166\n",
      "[step: 3434] loss: 0.0056250388734042645\n",
      "[step: 3435] loss: 17.259384155273438\n",
      "[step: 3435] loss: 0.005631750915199518\n",
      "[step: 3436] loss: 16.714599609375\n",
      "[step: 3436] loss: 0.005635911598801613\n",
      "[step: 3437] loss: 16.83647918701172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3437] loss: 0.00564477639272809\n",
      "[step: 3438] loss: 16.496177673339844\n",
      "[step: 3438] loss: 0.0056538209319114685\n",
      "[step: 3439] loss: 16.016254425048828\n",
      "[step: 3439] loss: 0.00566475186496973\n",
      "[step: 3440] loss: 16.448490142822266\n",
      "[step: 3440] loss: 0.005671835970133543\n",
      "[step: 3441] loss: 16.603633880615234\n",
      "[step: 3441] loss: 0.005677652545273304\n",
      "[step: 3442] loss: 15.743986129760742\n",
      "[step: 3442] loss: 0.005683616269379854\n",
      "[step: 3443] loss: 15.509824752807617\n",
      "[step: 3443] loss: 0.005704913754016161\n",
      "[step: 3444] loss: 16.060827255249023\n",
      "[step: 3444] loss: 0.005736692808568478\n",
      "[step: 3445] loss: 15.630912780761719\n",
      "[step: 3445] loss: 0.0058029633946716785\n",
      "[step: 3446] loss: 15.272470474243164\n",
      "[step: 3446] loss: 0.005863423924893141\n",
      "[step: 3447] loss: 15.816786766052246\n",
      "[step: 3447] loss: 0.005935755558311939\n",
      "[step: 3448] loss: 15.558958053588867\n",
      "[step: 3448] loss: 0.005918869748711586\n",
      "[step: 3449] loss: 15.069171905517578\n",
      "[step: 3449] loss: 0.005889365449547768\n",
      "[step: 3450] loss: 15.217411994934082\n",
      "[step: 3450] loss: 0.0058203814551234245\n",
      "[step: 3451] loss: 15.34805965423584\n",
      "[step: 3451] loss: 0.005748720373958349\n",
      "[step: 3452] loss: 15.166322708129883\n",
      "[step: 3452] loss: 0.005667924880981445\n",
      "[step: 3453] loss: 15.114005088806152\n",
      "[step: 3453] loss: 0.005672797095030546\n",
      "[step: 3454] loss: 15.126367568969727\n",
      "[step: 3454] loss: 0.0057155373506248\n",
      "[step: 3455] loss: 15.021124839782715\n",
      "[step: 3455] loss: 0.005727450828999281\n",
      "[step: 3456] loss: 14.970090866088867\n",
      "[step: 3456] loss: 0.005754736717790365\n",
      "[step: 3457] loss: 14.934144973754883\n",
      "[step: 3457] loss: 0.005716380197554827\n",
      "[step: 3458] loss: 14.87916374206543\n",
      "[step: 3458] loss: 0.005734601989388466\n",
      "[step: 3459] loss: 14.944208145141602\n",
      "[step: 3459] loss: 0.005742392037063837\n",
      "[step: 3460] loss: 14.971363067626953\n",
      "[step: 3460] loss: 0.005692348815500736\n",
      "[step: 3461] loss: 14.845667839050293\n",
      "[step: 3461] loss: 0.005694038234651089\n",
      "[step: 3462] loss: 14.789436340332031\n",
      "[step: 3462] loss: 0.005724157672375441\n",
      "[step: 3463] loss: 14.855321884155273\n",
      "[step: 3463] loss: 0.005681963171809912\n",
      "[step: 3464] loss: 14.809937477111816\n",
      "[step: 3464] loss: 0.0056819310411810875\n",
      "[step: 3465] loss: 14.713956832885742\n",
      "[step: 3465] loss: 0.0056760297156870365\n",
      "[step: 3466] loss: 14.742910385131836\n",
      "[step: 3466] loss: 0.005680403672158718\n",
      "[step: 3467] loss: 14.78215217590332\n",
      "[step: 3467] loss: 0.005737925413995981\n",
      "[step: 3468] loss: 14.739402770996094\n",
      "[step: 3468] loss: 0.005711372476071119\n",
      "[step: 3469] loss: 14.726078033447266\n",
      "[step: 3469] loss: 0.005699098110198975\n",
      "[step: 3470] loss: 14.725736618041992\n",
      "[step: 3470] loss: 0.005694508086889982\n",
      "[step: 3471] loss: 14.712451934814453\n",
      "[step: 3471] loss: 0.005671245511621237\n",
      "[step: 3472] loss: 14.711543083190918\n",
      "[step: 3472] loss: 0.00567034212872386\n",
      "[step: 3473] loss: 14.716039657592773\n",
      "[step: 3473] loss: 0.005663603078573942\n",
      "[step: 3474] loss: 14.73183822631836\n",
      "[step: 3474] loss: 0.005659847985953093\n",
      "[step: 3475] loss: 14.792240142822266\n",
      "[step: 3475] loss: 0.0056411465629935265\n",
      "[step: 3476] loss: 14.913239479064941\n",
      "[step: 3476] loss: 0.005646806675940752\n",
      "[step: 3477] loss: 15.12681770324707\n",
      "[step: 3477] loss: 0.005647167097777128\n",
      "[step: 3478] loss: 15.489387512207031\n",
      "[step: 3478] loss: 0.005636701360344887\n",
      "[step: 3479] loss: 16.233049392700195\n",
      "[step: 3479] loss: 0.005620576906949282\n",
      "[step: 3480] loss: 17.5477237701416\n",
      "[step: 3480] loss: 0.005627396982163191\n",
      "[step: 3481] loss: 20.073389053344727\n",
      "[step: 3481] loss: 0.005632954649627209\n",
      "[step: 3482] loss: 23.811695098876953\n",
      "[step: 3482] loss: 0.005633344873785973\n",
      "[step: 3483] loss: 30.10717010498047\n",
      "[step: 3483] loss: 0.005627298727631569\n",
      "[step: 3484] loss: 34.71796417236328\n",
      "[step: 3484] loss: 0.005648070480674505\n",
      "[step: 3485] loss: 37.03288269042969\n",
      "[step: 3485] loss: 0.0056440625339746475\n",
      "[step: 3486] loss: 30.502750396728516\n",
      "[step: 3486] loss: 0.005650052800774574\n",
      "[step: 3487] loss: 22.525136947631836\n",
      "[step: 3487] loss: 0.005649199243634939\n",
      "[step: 3488] loss: 19.531646728515625\n",
      "[step: 3488] loss: 0.005663374904543161\n",
      "[step: 3489] loss: 19.800212860107422\n",
      "[step: 3489] loss: 0.0056707351468503475\n",
      "[step: 3490] loss: 20.188053131103516\n",
      "[step: 3490] loss: 0.0056749871000647545\n",
      "[step: 3491] loss: 20.634746551513672\n",
      "[step: 3491] loss: 0.005686432123184204\n",
      "[step: 3492] loss: 22.644527435302734\n",
      "[step: 3492] loss: 0.0057304236106574535\n",
      "[step: 3493] loss: 24.616798400878906\n",
      "[step: 3493] loss: 0.005709308665245771\n",
      "[step: 3494] loss: 22.28605842590332\n",
      "[step: 3494] loss: 0.005682741291821003\n",
      "[step: 3495] loss: 18.377809524536133\n",
      "[step: 3495] loss: 0.005655728280544281\n",
      "[step: 3496] loss: 18.231164932250977\n",
      "[step: 3496] loss: 0.005635799840092659\n",
      "[step: 3497] loss: 19.028919219970703\n",
      "[step: 3497] loss: 0.005618182476609945\n",
      "[step: 3498] loss: 18.477073669433594\n",
      "[step: 3498] loss: 0.005621755961328745\n",
      "[step: 3499] loss: 17.91241455078125\n",
      "[step: 3499] loss: 0.005631992127746344\n",
      "[step: 3500] loss: 18.360626220703125\n",
      "[step: 3500] loss: 0.005651289131492376\n",
      "[step: 3501] loss: 17.339570999145508\n",
      "[step: 3501] loss: 0.005663033574819565\n",
      "[step: 3502] loss: 16.30081558227539\n",
      "[step: 3502] loss: 0.005672241561114788\n",
      "[step: 3503] loss: 16.97001075744629\n",
      "[step: 3503] loss: 0.005668568890541792\n",
      "[step: 3504] loss: 17.1431827545166\n",
      "[step: 3504] loss: 0.005651435814797878\n",
      "[step: 3505] loss: 16.24850082397461\n",
      "[step: 3505] loss: 0.0056238011457026005\n",
      "[step: 3506] loss: 16.015365600585938\n",
      "[step: 3506] loss: 0.005606960970908403\n",
      "[step: 3507] loss: 16.53740692138672\n",
      "[step: 3507] loss: 0.005601746961474419\n",
      "[step: 3508] loss: 16.14775848388672\n",
      "[step: 3508] loss: 0.0056031192652881145\n",
      "[step: 3509] loss: 15.470057487487793\n",
      "[step: 3509] loss: 0.005611692555248737\n",
      "[step: 3510] loss: 15.83627986907959\n",
      "[step: 3510] loss: 0.005625251214951277\n",
      "[step: 3511] loss: 15.999540328979492\n",
      "[step: 3511] loss: 0.005640395916998386\n",
      "[step: 3512] loss: 15.274930000305176\n",
      "[step: 3512] loss: 0.005647609941661358\n",
      "[step: 3513] loss: 15.036941528320312\n",
      "[step: 3513] loss: 0.0056405686773359776\n",
      "[step: 3514] loss: 15.488300323486328\n",
      "[step: 3514] loss: 0.005631481297314167\n",
      "[step: 3515] loss: 15.485227584838867\n",
      "[step: 3515] loss: 0.005613778717815876\n",
      "[step: 3516] loss: 15.042139053344727\n",
      "[step: 3516] loss: 0.005600016564130783\n",
      "[step: 3517] loss: 14.998003005981445\n",
      "[step: 3517] loss: 0.00559202441945672\n",
      "[step: 3518] loss: 15.208521842956543\n",
      "[step: 3518] loss: 0.0055924528278410435\n",
      "[step: 3519] loss: 15.078685760498047\n",
      "[step: 3519] loss: 0.005592669826000929\n",
      "[step: 3520] loss: 14.833053588867188\n",
      "[step: 3520] loss: 0.005597627721726894\n",
      "[step: 3521] loss: 14.991039276123047\n",
      "[step: 3521] loss: 0.005608930718153715\n",
      "[step: 3522] loss: 15.080676078796387\n",
      "[step: 3522] loss: 0.005612408276647329\n",
      "[step: 3523] loss: 14.811483383178711\n",
      "[step: 3523] loss: 0.005617401096969843\n",
      "[step: 3524] loss: 14.672121047973633\n",
      "[step: 3524] loss: 0.0056117987260222435\n",
      "[step: 3525] loss: 14.749796867370605\n",
      "[step: 3525] loss: 0.005611386615782976\n",
      "[step: 3526] loss: 14.718870162963867\n",
      "[step: 3526] loss: 0.005605226848274469\n",
      "[step: 3527] loss: 14.648185729980469\n",
      "[step: 3527] loss: 0.005599995609372854\n",
      "[step: 3528] loss: 14.669429779052734\n",
      "[step: 3528] loss: 0.005595895927399397\n",
      "[step: 3529] loss: 14.693990707397461\n",
      "[step: 3529] loss: 0.0055974568240344524\n",
      "[step: 3530] loss: 14.663032531738281\n",
      "[step: 3530] loss: 0.005605601705610752\n",
      "[step: 3531] loss: 14.602149963378906\n",
      "[step: 3531] loss: 0.0056246439926326275\n",
      "[step: 3532] loss: 14.550206184387207\n",
      "[step: 3532] loss: 0.0056684864684939384\n",
      "[step: 3533] loss: 14.531880378723145\n",
      "[step: 3533] loss: 0.005748553667217493\n",
      "[step: 3534] loss: 14.558490753173828\n",
      "[step: 3534] loss: 0.005874733440577984\n",
      "[step: 3535] loss: 14.573138236999512\n",
      "[step: 3535] loss: 0.005933597683906555\n",
      "[step: 3536] loss: 14.550471305847168\n",
      "[step: 3536] loss: 0.005856039468199015\n",
      "[step: 3537] loss: 14.537683486938477\n",
      "[step: 3537] loss: 0.005689661949872971\n",
      "[step: 3538] loss: 14.58039665222168\n",
      "[step: 3538] loss: 0.0056809536181390285\n",
      "[step: 3539] loss: 14.649312019348145\n",
      "[step: 3539] loss: 0.005764995701611042\n",
      "[step: 3540] loss: 14.707564353942871\n",
      "[step: 3540] loss: 0.0057564894668757915\n",
      "[step: 3541] loss: 14.829158782958984\n",
      "[step: 3541] loss: 0.005650253966450691\n",
      "[step: 3542] loss: 15.146230697631836\n",
      "[step: 3542] loss: 0.005633283872157335\n",
      "[step: 3543] loss: 15.719196319580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3543] loss: 0.005677876062691212\n",
      "[step: 3544] loss: 16.72994613647461\n",
      "[step: 3544] loss: 0.005646871868520975\n",
      "[step: 3545] loss: 18.36733627319336\n",
      "[step: 3545] loss: 0.005632712040096521\n",
      "[step: 3546] loss: 21.23508644104004\n",
      "[step: 3546] loss: 0.005643394775688648\n",
      "[step: 3547] loss: 24.57786750793457\n",
      "[step: 3547] loss: 0.005632130894809961\n",
      "[step: 3548] loss: 28.620079040527344\n",
      "[step: 3548] loss: 0.005619531497359276\n",
      "[step: 3549] loss: 28.48638916015625\n",
      "[step: 3549] loss: 0.00561210373416543\n",
      "[step: 3550] loss: 25.939117431640625\n",
      "[step: 3550] loss: 0.005599317606538534\n",
      "[step: 3551] loss: 20.677799224853516\n",
      "[step: 3551] loss: 0.005611491855233908\n",
      "[step: 3552] loss: 18.915119171142578\n",
      "[step: 3552] loss: 0.005616057198494673\n",
      "[step: 3553] loss: 20.990781784057617\n",
      "[step: 3553] loss: 0.00559227354824543\n",
      "[step: 3554] loss: 25.36131477355957\n",
      "[step: 3554] loss: 0.005598797462880611\n",
      "[step: 3555] loss: 26.648778915405273\n",
      "[step: 3555] loss: 0.005600437521934509\n",
      "[step: 3556] loss: 22.793102264404297\n",
      "[step: 3556] loss: 0.005584793630987406\n",
      "[step: 3557] loss: 18.269155502319336\n",
      "[step: 3557] loss: 0.0055880858562886715\n",
      "[step: 3558] loss: 16.212495803833008\n",
      "[step: 3558] loss: 0.00559807987883687\n",
      "[step: 3559] loss: 17.483142852783203\n",
      "[step: 3559] loss: 0.0055866604670882225\n",
      "[step: 3560] loss: 19.124839782714844\n",
      "[step: 3560] loss: 0.005584577098488808\n",
      "[step: 3561] loss: 18.395343780517578\n",
      "[step: 3561] loss: 0.005604113452136517\n",
      "[step: 3562] loss: 16.427444458007812\n",
      "[step: 3562] loss: 0.005610019434243441\n",
      "[step: 3563] loss: 15.607572555541992\n",
      "[step: 3563] loss: 0.005657280795276165\n",
      "[step: 3564] loss: 16.806943893432617\n",
      "[step: 3564] loss: 0.005708344280719757\n",
      "[step: 3565] loss: 17.771230697631836\n",
      "[step: 3565] loss: 0.0057848477736115456\n",
      "[step: 3566] loss: 17.005592346191406\n",
      "[step: 3566] loss: 0.00585215725004673\n",
      "[step: 3567] loss: 15.635295867919922\n",
      "[step: 3567] loss: 0.0059498753398656845\n",
      "[step: 3568] loss: 15.83068561553955\n",
      "[step: 3568] loss: 0.005947646219283342\n",
      "[step: 3569] loss: 16.756349563598633\n",
      "[step: 3569] loss: 0.005884706974029541\n",
      "[step: 3570] loss: 16.730388641357422\n",
      "[step: 3570] loss: 0.005791361443698406\n",
      "[step: 3571] loss: 15.727887153625488\n",
      "[step: 3571] loss: 0.0057442975230515\n",
      "[step: 3572] loss: 15.259185791015625\n",
      "[step: 3572] loss: 0.0057050068862736225\n",
      "[step: 3573] loss: 15.787216186523438\n",
      "[step: 3573] loss: 0.005694578401744366\n",
      "[step: 3574] loss: 16.036588668823242\n",
      "[step: 3574] loss: 0.005711598787456751\n",
      "[step: 3575] loss: 15.469647407531738\n",
      "[step: 3575] loss: 0.005704490467905998\n",
      "[step: 3576] loss: 15.089361190795898\n",
      "[step: 3576] loss: 0.005658019334077835\n",
      "[step: 3577] loss: 15.299079895019531\n",
      "[step: 3577] loss: 0.005615998059511185\n",
      "[step: 3578] loss: 15.558923721313477\n",
      "[step: 3578] loss: 0.005629753693938255\n",
      "[step: 3579] loss: 15.296270370483398\n",
      "[step: 3579] loss: 0.005642341449856758\n",
      "[step: 3580] loss: 15.03784465789795\n",
      "[step: 3580] loss: 0.005626267287880182\n",
      "[step: 3581] loss: 15.174654960632324\n",
      "[step: 3581] loss: 0.005610006395727396\n",
      "[step: 3582] loss: 15.45199966430664\n",
      "[step: 3582] loss: 0.005609224084764719\n",
      "[step: 3583] loss: 15.525341033935547\n",
      "[step: 3583] loss: 0.005610295105725527\n",
      "[step: 3584] loss: 15.530521392822266\n",
      "[step: 3584] loss: 0.005604926031082869\n",
      "[step: 3585] loss: 15.689411163330078\n",
      "[step: 3585] loss: 0.005611010827124119\n",
      "[step: 3586] loss: 16.145572662353516\n",
      "[step: 3586] loss: 0.005588243715465069\n",
      "[step: 3587] loss: 16.747394561767578\n",
      "[step: 3587] loss: 0.005579899996519089\n",
      "[step: 3588] loss: 17.486358642578125\n",
      "[step: 3588] loss: 0.005591563880443573\n",
      "[step: 3589] loss: 18.324111938476562\n",
      "[step: 3589] loss: 0.00559367798268795\n",
      "[step: 3590] loss: 19.39647102355957\n",
      "[step: 3590] loss: 0.005598973948508501\n",
      "[step: 3591] loss: 20.918184280395508\n",
      "[step: 3591] loss: 0.005580309312790632\n",
      "[step: 3592] loss: 22.343292236328125\n",
      "[step: 3592] loss: 0.005579615943133831\n",
      "[step: 3593] loss: 23.139175415039062\n",
      "[step: 3593] loss: 0.005589877720922232\n",
      "[step: 3594] loss: 22.53396224975586\n",
      "[step: 3594] loss: 0.005593026056885719\n",
      "[step: 3595] loss: 21.032485961914062\n",
      "[step: 3595] loss: 0.005603523924946785\n",
      "[step: 3596] loss: 18.815399169921875\n",
      "[step: 3596] loss: 0.005615326575934887\n",
      "[step: 3597] loss: 16.84740447998047\n",
      "[step: 3597] loss: 0.005645190365612507\n",
      "[step: 3598] loss: 15.6378755569458\n",
      "[step: 3598] loss: 0.00569944316521287\n",
      "[step: 3599] loss: 15.656633377075195\n",
      "[step: 3599] loss: 0.00579668115824461\n",
      "[step: 3600] loss: 16.47652244567871\n",
      "[step: 3600] loss: 0.0059624966233968735\n",
      "[step: 3601] loss: 17.25710678100586\n",
      "[step: 3601] loss: 0.006061672233045101\n",
      "[step: 3602] loss: 17.742881774902344\n",
      "[step: 3602] loss: 0.005996424239128828\n",
      "[step: 3603] loss: 18.212709426879883\n",
      "[step: 3603] loss: 0.005763731896877289\n",
      "[step: 3604] loss: 18.79932403564453\n",
      "[step: 3604] loss: 0.005664281081408262\n",
      "[step: 3605] loss: 19.317562103271484\n",
      "[step: 3605] loss: 0.005762422922998667\n",
      "[step: 3606] loss: 19.194433212280273\n",
      "[step: 3606] loss: 0.005730837117880583\n",
      "[step: 3607] loss: 17.786067962646484\n",
      "[step: 3607] loss: 0.005626770202070475\n",
      "[step: 3608] loss: 15.888211250305176\n",
      "[step: 3608] loss: 0.005678512621670961\n",
      "[step: 3609] loss: 14.717193603515625\n",
      "[step: 3609] loss: 0.0057351416908204556\n",
      "[step: 3610] loss: 14.901227951049805\n",
      "[step: 3610] loss: 0.00567066203802824\n",
      "[step: 3611] loss: 15.837149620056152\n",
      "[step: 3611] loss: 0.005604610312730074\n",
      "[step: 3612] loss: 16.324329376220703\n",
      "[step: 3612] loss: 0.0056376890279352665\n",
      "[step: 3613] loss: 15.978254318237305\n",
      "[step: 3613] loss: 0.005644327495247126\n",
      "[step: 3614] loss: 15.363624572753906\n",
      "[step: 3614] loss: 0.005604105070233345\n",
      "[step: 3615] loss: 15.306266784667969\n",
      "[step: 3615] loss: 0.005623865872621536\n",
      "[step: 3616] loss: 15.762712478637695\n",
      "[step: 3616] loss: 0.0056278398260474205\n",
      "[step: 3617] loss: 16.09789276123047\n",
      "[step: 3617] loss: 0.0055943881161510944\n",
      "[step: 3618] loss: 15.688173294067383\n",
      "[step: 3618] loss: 0.0055990093387663364\n",
      "[step: 3619] loss: 15.080324172973633\n",
      "[step: 3619] loss: 0.00559910899028182\n",
      "[step: 3620] loss: 14.856977462768555\n",
      "[step: 3620] loss: 0.005584371741861105\n",
      "[step: 3621] loss: 15.102766990661621\n",
      "[step: 3621] loss: 0.0055818576365709305\n",
      "[step: 3622] loss: 15.329554557800293\n",
      "[step: 3622] loss: 0.005590035580098629\n",
      "[step: 3623] loss: 15.215791702270508\n",
      "[step: 3623] loss: 0.005587457679212093\n",
      "[step: 3624] loss: 14.868573188781738\n",
      "[step: 3624] loss: 0.0055872248485684395\n",
      "[step: 3625] loss: 14.702154159545898\n",
      "[step: 3625] loss: 0.005585807375609875\n",
      "[step: 3626] loss: 14.789843559265137\n",
      "[step: 3626] loss: 0.005579269025474787\n",
      "[step: 3627] loss: 14.944814682006836\n",
      "[step: 3627] loss: 0.0055717406794428825\n",
      "[step: 3628] loss: 14.911819458007812\n",
      "[step: 3628] loss: 0.005568007472902536\n",
      "[step: 3629] loss: 14.789337158203125\n",
      "[step: 3629] loss: 0.005565217230468988\n",
      "[step: 3630] loss: 14.731714248657227\n",
      "[step: 3630] loss: 0.005568825174123049\n",
      "[step: 3631] loss: 14.880868911743164\n",
      "[step: 3631] loss: 0.005560850724577904\n",
      "[step: 3632] loss: 15.150529861450195\n",
      "[step: 3632] loss: 0.005558277480304241\n",
      "[step: 3633] loss: 15.46252155303955\n",
      "[step: 3633] loss: 0.0055573186837136745\n",
      "[step: 3634] loss: 15.868368148803711\n",
      "[step: 3634] loss: 0.0055509586818516254\n",
      "[step: 3635] loss: 16.47177505493164\n",
      "[step: 3635] loss: 0.0055484892800450325\n",
      "[step: 3636] loss: 17.457481384277344\n",
      "[step: 3636] loss: 0.005551681388169527\n",
      "[step: 3637] loss: 19.00045394897461\n",
      "[step: 3637] loss: 0.0055494047701358795\n",
      "[step: 3638] loss: 20.75347900390625\n",
      "[step: 3638] loss: 0.005548672750592232\n",
      "[step: 3639] loss: 23.124958038330078\n",
      "[step: 3639] loss: 0.005557930562645197\n",
      "[step: 3640] loss: 24.051753997802734\n",
      "[step: 3640] loss: 0.005557981785386801\n",
      "[step: 3641] loss: 24.413381576538086\n",
      "[step: 3641] loss: 0.005564054939895868\n",
      "[step: 3642] loss: 23.05449867248535\n",
      "[step: 3642] loss: 0.0055648586712777615\n",
      "[step: 3643] loss: 21.431716918945312\n",
      "[step: 3643] loss: 0.005569241475313902\n",
      "[step: 3644] loss: 20.07696533203125\n",
      "[step: 3644] loss: 0.0055696554481983185\n",
      "[step: 3645] loss: 18.41974639892578\n",
      "[step: 3645] loss: 0.005581242498010397\n",
      "[step: 3646] loss: 16.548309326171875\n",
      "[step: 3646] loss: 0.005595686379820108\n",
      "[step: 3647] loss: 15.88461685180664\n",
      "[step: 3647] loss: 0.0056142364628612995\n",
      "[step: 3648] loss: 17.355281829833984\n",
      "[step: 3648] loss: 0.005638891365379095\n",
      "[step: 3649] loss: 19.345355987548828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3649] loss: 0.005670422688126564\n",
      "[step: 3650] loss: 19.447696685791016\n",
      "[step: 3650] loss: 0.00569453090429306\n",
      "[step: 3651] loss: 18.08542251586914\n",
      "[step: 3651] loss: 0.0057113440707325935\n",
      "[step: 3652] loss: 16.74899673461914\n",
      "[step: 3652] loss: 0.005691110156476498\n",
      "[step: 3653] loss: 16.773517608642578\n",
      "[step: 3653] loss: 0.005656072404235601\n",
      "[step: 3654] loss: 16.792884826660156\n",
      "[step: 3654] loss: 0.005604610312730074\n",
      "[step: 3655] loss: 15.880636215209961\n",
      "[step: 3655] loss: 0.005579749122262001\n",
      "[step: 3656] loss: 15.15652084350586\n",
      "[step: 3656] loss: 0.005588062573224306\n",
      "[step: 3657] loss: 15.386617660522461\n",
      "[step: 3657] loss: 0.0056173717603087425\n",
      "[step: 3658] loss: 15.713668823242188\n",
      "[step: 3658] loss: 0.005644959397614002\n",
      "[step: 3659] loss: 15.548325538635254\n",
      "[step: 3659] loss: 0.0056258756667375565\n",
      "[step: 3660] loss: 15.188520431518555\n",
      "[step: 3660] loss: 0.00558497104793787\n",
      "[step: 3661] loss: 15.407510757446289\n",
      "[step: 3661] loss: 0.005547625944018364\n",
      "[step: 3662] loss: 15.769801139831543\n",
      "[step: 3662] loss: 0.0055437469854950905\n",
      "[step: 3663] loss: 15.533096313476562\n",
      "[step: 3663] loss: 0.00556426215916872\n",
      "[step: 3664] loss: 14.968223571777344\n",
      "[step: 3664] loss: 0.005581787321716547\n",
      "[step: 3665] loss: 14.665807723999023\n",
      "[step: 3665] loss: 0.005586978513747454\n",
      "[step: 3666] loss: 14.729917526245117\n",
      "[step: 3666] loss: 0.005557607393711805\n",
      "[step: 3667] loss: 14.649628639221191\n",
      "[step: 3667] loss: 0.005538704805076122\n",
      "[step: 3668] loss: 14.371695518493652\n",
      "[step: 3668] loss: 0.005542630795389414\n",
      "[step: 3669] loss: 14.310047149658203\n",
      "[step: 3669] loss: 0.005558912176638842\n",
      "[step: 3670] loss: 14.49693775177002\n",
      "[step: 3670] loss: 0.005572166293859482\n",
      "[step: 3671] loss: 14.646328926086426\n",
      "[step: 3671] loss: 0.005562874488532543\n",
      "[step: 3672] loss: 14.611772537231445\n",
      "[step: 3672] loss: 0.005555353127419949\n",
      "[step: 3673] loss: 14.492422103881836\n",
      "[step: 3673] loss: 0.00554469833150506\n",
      "[step: 3674] loss: 14.48221206665039\n",
      "[step: 3674] loss: 0.005547988228499889\n",
      "[step: 3675] loss: 14.547998428344727\n",
      "[step: 3675] loss: 0.00555772241204977\n",
      "[step: 3676] loss: 14.557415008544922\n",
      "[step: 3676] loss: 0.00556526705622673\n",
      "[step: 3677] loss: 14.547189712524414\n",
      "[step: 3677] loss: 0.005565207451581955\n",
      "[step: 3678] loss: 14.631258010864258\n",
      "[step: 3678] loss: 0.00556696904823184\n",
      "[step: 3679] loss: 14.896441459655762\n",
      "[step: 3679] loss: 0.0055756475776433945\n",
      "[step: 3680] loss: 15.35192584991455\n",
      "[step: 3680] loss: 0.00560026103630662\n",
      "[step: 3681] loss: 15.891698837280273\n",
      "[step: 3681] loss: 0.005640930961817503\n",
      "[step: 3682] loss: 16.86736297607422\n",
      "[step: 3682] loss: 0.005704046227037907\n",
      "[step: 3683] loss: 18.08514404296875\n",
      "[step: 3683] loss: 0.005758700426667929\n",
      "[step: 3684] loss: 20.240676879882812\n",
      "[step: 3684] loss: 0.00581091595813632\n",
      "[step: 3685] loss: 22.24945831298828\n",
      "[step: 3685] loss: 0.005809123162180185\n",
      "[step: 3686] loss: 24.575979232788086\n",
      "[step: 3686] loss: 0.00576500641182065\n",
      "[step: 3687] loss: 24.886232376098633\n",
      "[step: 3687] loss: 0.00570059847086668\n",
      "[step: 3688] loss: 23.681766510009766\n",
      "[step: 3688] loss: 0.00565291615203023\n",
      "[step: 3689] loss: 21.271793365478516\n",
      "[step: 3689] loss: 0.005629835184663534\n",
      "[step: 3690] loss: 18.583650588989258\n",
      "[step: 3690] loss: 0.0056203329004347324\n",
      "[step: 3691] loss: 17.172815322875977\n",
      "[step: 3691] loss: 0.005639045964926481\n",
      "[step: 3692] loss: 17.17413902282715\n",
      "[step: 3692] loss: 0.0056291501969099045\n",
      "[step: 3693] loss: 18.014263153076172\n",
      "[step: 3693] loss: 0.0055903485044837\n",
      "[step: 3694] loss: 18.510147094726562\n",
      "[step: 3694] loss: 0.0055615780875086784\n",
      "[step: 3695] loss: 18.48819351196289\n",
      "[step: 3695] loss: 0.005565213039517403\n",
      "[step: 3696] loss: 17.829761505126953\n",
      "[step: 3696] loss: 0.005582235287874937\n",
      "[step: 3697] loss: 17.282997131347656\n",
      "[step: 3697] loss: 0.005581854376941919\n",
      "[step: 3698] loss: 16.96375274658203\n",
      "[step: 3698] loss: 0.005568333435803652\n",
      "[step: 3699] loss: 16.899688720703125\n",
      "[step: 3699] loss: 0.00555795431137085\n",
      "[step: 3700] loss: 16.793577194213867\n",
      "[step: 3700] loss: 0.005557155702263117\n",
      "[step: 3701] loss: 16.440319061279297\n",
      "[step: 3701] loss: 0.005558254197239876\n",
      "[step: 3702] loss: 15.852973937988281\n",
      "[step: 3702] loss: 0.005554948467761278\n",
      "[step: 3703] loss: 15.351144790649414\n",
      "[step: 3703] loss: 0.005548927001655102\n",
      "[step: 3704] loss: 15.068960189819336\n",
      "[step: 3704] loss: 0.005556494928896427\n",
      "[step: 3705] loss: 15.02889633178711\n",
      "[step: 3705] loss: 0.005551500711590052\n",
      "[step: 3706] loss: 15.174972534179688\n",
      "[step: 3706] loss: 0.0055479747243225574\n",
      "[step: 3707] loss: 15.332427978515625\n",
      "[step: 3707] loss: 0.005543579813092947\n",
      "[step: 3708] loss: 15.43043327331543\n",
      "[step: 3708] loss: 0.005540682468563318\n",
      "[step: 3709] loss: 15.343761444091797\n",
      "[step: 3709] loss: 0.005537695717066526\n",
      "[step: 3710] loss: 15.226930618286133\n",
      "[step: 3710] loss: 0.005534660071134567\n",
      "[step: 3711] loss: 14.968000411987305\n",
      "[step: 3711] loss: 0.005531045142561197\n",
      "[step: 3712] loss: 14.706232070922852\n",
      "[step: 3712] loss: 0.005529807880520821\n",
      "[step: 3713] loss: 14.606356620788574\n",
      "[step: 3713] loss: 0.005528552457690239\n",
      "[step: 3714] loss: 14.72662353515625\n",
      "[step: 3714] loss: 0.005530837457627058\n",
      "[step: 3715] loss: 14.876667022705078\n",
      "[step: 3715] loss: 0.005540133453905582\n",
      "[step: 3716] loss: 14.837875366210938\n",
      "[step: 3716] loss: 0.005540181417018175\n",
      "[step: 3717] loss: 14.546751976013184\n",
      "[step: 3717] loss: 0.00554868346080184\n",
      "[step: 3718] loss: 14.254558563232422\n",
      "[step: 3718] loss: 0.005537135060876608\n",
      "[step: 3719] loss: 14.160036087036133\n",
      "[step: 3719] loss: 0.0055365064181387424\n",
      "[step: 3720] loss: 14.285669326782227\n",
      "[step: 3720] loss: 0.005541509483009577\n",
      "[step: 3721] loss: 14.459098815917969\n",
      "[step: 3721] loss: 0.005543861072510481\n",
      "[step: 3722] loss: 14.50041675567627\n",
      "[step: 3722] loss: 0.00554229412227869\n",
      "[step: 3723] loss: 14.377326965332031\n",
      "[step: 3723] loss: 0.005544998683035374\n",
      "[step: 3724] loss: 14.218029022216797\n",
      "[step: 3724] loss: 0.005551080219447613\n",
      "[step: 3725] loss: 14.156333923339844\n",
      "[step: 3725] loss: 0.005562080070376396\n",
      "[step: 3726] loss: 14.236120223999023\n",
      "[step: 3726] loss: 0.005562760401517153\n",
      "[step: 3727] loss: 14.413932800292969\n",
      "[step: 3727] loss: 0.0055680968798696995\n",
      "[step: 3728] loss: 14.67007064819336\n",
      "[step: 3728] loss: 0.005575262475758791\n",
      "[step: 3729] loss: 15.028867721557617\n",
      "[step: 3729] loss: 0.005587623920291662\n",
      "[step: 3730] loss: 15.62982177734375\n",
      "[step: 3730] loss: 0.005601278506219387\n",
      "[step: 3731] loss: 16.55933380126953\n",
      "[step: 3731] loss: 0.005609189625829458\n",
      "[step: 3732] loss: 18.230911254882812\n",
      "[step: 3732] loss: 0.005612977780401707\n",
      "[step: 3733] loss: 20.594486236572266\n",
      "[step: 3733] loss: 0.005612567532807589\n",
      "[step: 3734] loss: 24.414087295532227\n",
      "[step: 3734] loss: 0.005592553410679102\n",
      "[step: 3735] loss: 27.936302185058594\n",
      "[step: 3735] loss: 0.005578602664172649\n",
      "[step: 3736] loss: 30.718730926513672\n",
      "[step: 3736] loss: 0.005586366169154644\n",
      "[step: 3737] loss: 29.710430145263672\n",
      "[step: 3737] loss: 0.005600611213594675\n",
      "[step: 3738] loss: 27.32026481628418\n",
      "[step: 3738] loss: 0.005617305636405945\n",
      "[step: 3739] loss: 27.297239303588867\n",
      "[step: 3739] loss: 0.005603657569736242\n",
      "[step: 3740] loss: 28.515308380126953\n",
      "[step: 3740] loss: 0.005581114441156387\n",
      "[step: 3741] loss: 25.477142333984375\n",
      "[step: 3741] loss: 0.00555487722158432\n",
      "[step: 3742] loss: 18.92139434814453\n",
      "[step: 3742] loss: 0.005575362127274275\n",
      "[step: 3743] loss: 19.366899490356445\n",
      "[step: 3743] loss: 0.005582698155194521\n",
      "[step: 3744] loss: 24.112171173095703\n",
      "[step: 3744] loss: 0.005598146468400955\n",
      "[step: 3745] loss: 20.942333221435547\n",
      "[step: 3745] loss: 0.005597449839115143\n",
      "[step: 3746] loss: 17.303890228271484\n",
      "[step: 3746] loss: 0.0055808862671256065\n",
      "[step: 3747] loss: 19.222400665283203\n",
      "[step: 3747] loss: 0.005563683807849884\n",
      "[step: 3748] loss: 17.205331802368164\n",
      "[step: 3748] loss: 0.005565374158322811\n",
      "[step: 3749] loss: 15.923933982849121\n",
      "[step: 3749] loss: 0.005574028939008713\n",
      "[step: 3750] loss: 17.977970123291016\n",
      "[step: 3750] loss: 0.005583483725786209\n",
      "[step: 3751] loss: 17.241470336914062\n",
      "[step: 3751] loss: 0.005568551830947399\n",
      "[step: 3752] loss: 17.214794158935547\n",
      "[step: 3752] loss: 0.005557599477469921\n",
      "[step: 3753] loss: 17.003326416015625\n",
      "[step: 3753] loss: 0.005545346532016993\n",
      "[step: 3754] loss: 15.851142883300781\n",
      "[step: 3754] loss: 0.005546508356928825\n",
      "[step: 3755] loss: 16.114355087280273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3755] loss: 0.005538650788366795\n",
      "[step: 3756] loss: 15.66850471496582\n",
      "[step: 3756] loss: 0.005524809472262859\n",
      "[step: 3757] loss: 15.203057289123535\n",
      "[step: 3757] loss: 0.005510491319000721\n",
      "[step: 3758] loss: 15.719350814819336\n",
      "[step: 3758] loss: 0.005508920177817345\n",
      "[step: 3759] loss: 15.196540832519531\n",
      "[step: 3759] loss: 0.005516176111996174\n",
      "[step: 3760] loss: 15.167658805847168\n",
      "[step: 3760] loss: 0.005523924250155687\n",
      "[step: 3761] loss: 15.356773376464844\n",
      "[step: 3761] loss: 0.005534266121685505\n",
      "[step: 3762] loss: 14.772713661193848\n",
      "[step: 3762] loss: 0.005522832740098238\n",
      "[step: 3763] loss: 15.105670928955078\n",
      "[step: 3763] loss: 0.005523110739886761\n",
      "[step: 3764] loss: 15.234699249267578\n",
      "[step: 3764] loss: 0.005529994610697031\n",
      "[step: 3765] loss: 14.954063415527344\n",
      "[step: 3765] loss: 0.005540015641599894\n",
      "[step: 3766] loss: 15.234687805175781\n",
      "[step: 3766] loss: 0.005545578896999359\n",
      "[step: 3767] loss: 14.948566436767578\n",
      "[step: 3767] loss: 0.005551168229430914\n",
      "[step: 3768] loss: 14.70144271850586\n",
      "[step: 3768] loss: 0.00556115061044693\n",
      "[step: 3769] loss: 14.85008716583252\n",
      "[step: 3769] loss: 0.005587444640696049\n",
      "[step: 3770] loss: 14.63952922821045\n",
      "[step: 3770] loss: 0.0056031192652881145\n",
      "[step: 3771] loss: 14.698631286621094\n",
      "[step: 3771] loss: 0.005615990608930588\n",
      "[step: 3772] loss: 14.90861701965332\n",
      "[step: 3772] loss: 0.005610870197415352\n",
      "[step: 3773] loss: 14.793843269348145\n",
      "[step: 3773] loss: 0.005593905691057444\n",
      "[step: 3774] loss: 15.01579475402832\n",
      "[step: 3774] loss: 0.005563776008784771\n",
      "[step: 3775] loss: 15.1380615234375\n",
      "[step: 3775] loss: 0.005530802067369223\n",
      "[step: 3776] loss: 15.143887519836426\n",
      "[step: 3776] loss: 0.005510904360562563\n",
      "[step: 3777] loss: 15.498981475830078\n",
      "[step: 3777] loss: 0.005514007993042469\n",
      "[step: 3778] loss: 15.844330787658691\n",
      "[step: 3778] loss: 0.005533463787287474\n",
      "[step: 3779] loss: 16.310226440429688\n",
      "[step: 3779] loss: 0.005558682139962912\n",
      "[step: 3780] loss: 16.886098861694336\n",
      "[step: 3780] loss: 0.005582076963037252\n",
      "[step: 3781] loss: 17.525318145751953\n",
      "[step: 3781] loss: 0.00559647660702467\n",
      "[step: 3782] loss: 17.820701599121094\n",
      "[step: 3782] loss: 0.005617209244519472\n",
      "[step: 3783] loss: 17.942773818969727\n",
      "[step: 3783] loss: 0.005619997624307871\n",
      "[step: 3784] loss: 17.44547462463379\n",
      "[step: 3784] loss: 0.0056300293654203415\n",
      "[step: 3785] loss: 16.745588302612305\n",
      "[step: 3785] loss: 0.00566791882738471\n",
      "[step: 3786] loss: 15.672832489013672\n",
      "[step: 3786] loss: 0.0057052187621593475\n",
      "[step: 3787] loss: 14.89883804321289\n",
      "[step: 3787] loss: 0.005779304541647434\n",
      "[step: 3788] loss: 14.40091323852539\n",
      "[step: 3788] loss: 0.005788284819573164\n",
      "[step: 3789] loss: 14.288741111755371\n",
      "[step: 3789] loss: 0.005767705384641886\n",
      "[step: 3790] loss: 14.471392631530762\n",
      "[step: 3790] loss: 0.0057844179682433605\n",
      "[step: 3791] loss: 14.67620849609375\n",
      "[step: 3791] loss: 0.005808967165648937\n",
      "[step: 3792] loss: 14.85835075378418\n",
      "[step: 3792] loss: 0.005808747839182615\n",
      "[step: 3793] loss: 14.85909652709961\n",
      "[step: 3793] loss: 0.0056601776741445065\n",
      "[step: 3794] loss: 14.64301872253418\n",
      "[step: 3794] loss: 0.005631449166685343\n",
      "[step: 3795] loss: 14.365208625793457\n",
      "[step: 3795] loss: 0.00571007514372468\n",
      "[step: 3796] loss: 14.168988227844238\n",
      "[step: 3796] loss: 0.005691210273653269\n",
      "[step: 3797] loss: 14.075417518615723\n",
      "[step: 3797] loss: 0.005731998942792416\n",
      "[step: 3798] loss: 14.164607048034668\n",
      "[step: 3798] loss: 0.005684295203536749\n",
      "[step: 3799] loss: 14.320247650146484\n",
      "[step: 3799] loss: 0.005596202332526445\n",
      "[step: 3800] loss: 14.463947296142578\n",
      "[step: 3800] loss: 0.005627909675240517\n",
      "[step: 3801] loss: 14.592435836791992\n",
      "[step: 3801] loss: 0.005638279020786285\n",
      "[step: 3802] loss: 14.722593307495117\n",
      "[step: 3802] loss: 0.005587955936789513\n",
      "[step: 3803] loss: 14.909950256347656\n",
      "[step: 3803] loss: 0.005610153079032898\n",
      "[step: 3804] loss: 15.265846252441406\n",
      "[step: 3804] loss: 0.005686457268893719\n",
      "[step: 3805] loss: 15.87632942199707\n",
      "[step: 3805] loss: 0.005606770049780607\n",
      "[step: 3806] loss: 16.972631454467773\n",
      "[step: 3806] loss: 0.005619688890874386\n",
      "[step: 3807] loss: 18.58837890625\n",
      "[step: 3807] loss: 0.005620893556624651\n",
      "[step: 3808] loss: 21.081403732299805\n",
      "[step: 3808] loss: 0.005552006419748068\n",
      "[step: 3809] loss: 23.92784881591797\n",
      "[step: 3809] loss: 0.005599610973149538\n",
      "[step: 3810] loss: 26.971477508544922\n",
      "[step: 3810] loss: 0.005577414762228727\n",
      "[step: 3811] loss: 27.968521118164062\n",
      "[step: 3811] loss: 0.005535535980015993\n",
      "[step: 3812] loss: 26.37771224975586\n",
      "[step: 3812] loss: 0.005568026099354029\n",
      "[step: 3813] loss: 22.100566864013672\n",
      "[step: 3813] loss: 0.005557797383517027\n",
      "[step: 3814] loss: 17.513168334960938\n",
      "[step: 3814] loss: 0.005523109342902899\n",
      "[step: 3815] loss: 15.558759689331055\n",
      "[step: 3815] loss: 0.005555091425776482\n",
      "[step: 3816] loss: 16.486160278320312\n",
      "[step: 3816] loss: 0.00554291857406497\n",
      "[step: 3817] loss: 18.7288761138916\n",
      "[step: 3817] loss: 0.005530698224902153\n",
      "[step: 3818] loss: 20.348703384399414\n",
      "[step: 3818] loss: 0.0055320304818451405\n",
      "[step: 3819] loss: 19.920894622802734\n",
      "[step: 3819] loss: 0.005533079616725445\n",
      "[step: 3820] loss: 18.452552795410156\n",
      "[step: 3820] loss: 0.005522455554455519\n",
      "[step: 3821] loss: 16.801612854003906\n",
      "[step: 3821] loss: 0.005520731210708618\n",
      "[step: 3822] loss: 16.434528350830078\n",
      "[step: 3822] loss: 0.005513281561434269\n",
      "[step: 3823] loss: 17.169641494750977\n",
      "[step: 3823] loss: 0.005510958377271891\n",
      "[step: 3824] loss: 17.788799285888672\n",
      "[step: 3824] loss: 0.005513487849384546\n",
      "[step: 3825] loss: 17.261064529418945\n",
      "[step: 3825] loss: 0.005515018943697214\n",
      "[step: 3826] loss: 15.79171371459961\n",
      "[step: 3826] loss: 0.005503631196916103\n",
      "[step: 3827] loss: 14.762445449829102\n",
      "[step: 3827] loss: 0.00549757806584239\n",
      "[step: 3828] loss: 15.267694473266602\n",
      "[step: 3828] loss: 0.005509370006620884\n",
      "[step: 3829] loss: 16.337575912475586\n",
      "[step: 3829] loss: 0.005497208796441555\n",
      "[step: 3830] loss: 16.82208824157715\n",
      "[step: 3830] loss: 0.005492622032761574\n",
      "[step: 3831] loss: 16.358543395996094\n",
      "[step: 3831] loss: 0.005498649552464485\n",
      "[step: 3832] loss: 15.555981636047363\n",
      "[step: 3832] loss: 0.005496606230735779\n",
      "[step: 3833] loss: 15.014859199523926\n",
      "[step: 3833] loss: 0.005491660907864571\n",
      "[step: 3834] loss: 14.902069091796875\n",
      "[step: 3834] loss: 0.005493387579917908\n",
      "[step: 3835] loss: 14.905777931213379\n",
      "[step: 3835] loss: 0.005490501876920462\n",
      "[step: 3836] loss: 14.750144958496094\n",
      "[step: 3836] loss: 0.005490387789905071\n",
      "[step: 3837] loss: 14.548439025878906\n",
      "[step: 3837] loss: 0.005489158444106579\n",
      "[step: 3838] loss: 14.465841293334961\n",
      "[step: 3838] loss: 0.00549276964738965\n",
      "[step: 3839] loss: 14.447324752807617\n",
      "[step: 3839] loss: 0.005498518701642752\n",
      "[step: 3840] loss: 14.310481071472168\n",
      "[step: 3840] loss: 0.005508215632289648\n",
      "[step: 3841] loss: 14.073923110961914\n",
      "[step: 3841] loss: 0.005532552488148212\n",
      "[step: 3842] loss: 14.028480529785156\n",
      "[step: 3842] loss: 0.00557279959321022\n",
      "[step: 3843] loss: 14.286540985107422\n",
      "[step: 3843] loss: 0.005635764915496111\n",
      "[step: 3844] loss: 14.58018970489502\n",
      "[step: 3844] loss: 0.005726201925426722\n",
      "[step: 3845] loss: 14.698749542236328\n",
      "[step: 3845] loss: 0.005845222622156143\n",
      "[step: 3846] loss: 14.554146766662598\n",
      "[step: 3846] loss: 0.005907973740249872\n",
      "[step: 3847] loss: 14.45208740234375\n",
      "[step: 3847] loss: 0.005869388580322266\n",
      "[step: 3848] loss: 14.51005744934082\n",
      "[step: 3848] loss: 0.00570208765566349\n",
      "[step: 3849] loss: 14.836830139160156\n",
      "[step: 3849] loss: 0.005550248548388481\n",
      "[step: 3850] loss: 15.286733627319336\n",
      "[step: 3850] loss: 0.005545022897422314\n",
      "[step: 3851] loss: 15.775336265563965\n",
      "[step: 3851] loss: 0.005620434880256653\n",
      "[step: 3852] loss: 16.142578125\n",
      "[step: 3852] loss: 0.0056275418028235435\n",
      "[step: 3853] loss: 16.57163429260254\n",
      "[step: 3853] loss: 0.005560996476560831\n",
      "[step: 3854] loss: 16.89500617980957\n",
      "[step: 3854] loss: 0.00554922828450799\n",
      "[step: 3855] loss: 17.625104904174805\n",
      "[step: 3855] loss: 0.0055896020494401455\n",
      "[step: 3856] loss: 18.092241287231445\n",
      "[step: 3856] loss: 0.005590181332081556\n",
      "[step: 3857] loss: 18.746679306030273\n",
      "[step: 3857] loss: 0.005534786265343428\n",
      "[step: 3858] loss: 18.353675842285156\n",
      "[step: 3858] loss: 0.005507695488631725\n",
      "[step: 3859] loss: 17.842517852783203\n",
      "[step: 3859] loss: 0.005540240090340376\n",
      "[step: 3860] loss: 17.01299476623535\n",
      "[step: 3860] loss: 0.005541015882045031\n",
      "[step: 3861] loss: 16.349842071533203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3861] loss: 0.005500744096934795\n",
      "[step: 3862] loss: 16.002086639404297\n",
      "[step: 3862] loss: 0.00550256809219718\n",
      "[step: 3863] loss: 15.344406127929688\n",
      "[step: 3863] loss: 0.005528823472559452\n",
      "[step: 3864] loss: 14.608285903930664\n",
      "[step: 3864] loss: 0.005518642719835043\n",
      "[step: 3865] loss: 14.282346725463867\n",
      "[step: 3865] loss: 0.005507588852196932\n",
      "[step: 3866] loss: 14.584135055541992\n",
      "[step: 3866] loss: 0.005527373403310776\n",
      "[step: 3867] loss: 15.05991268157959\n",
      "[step: 3867] loss: 0.005534475669264793\n",
      "[step: 3868] loss: 15.138580322265625\n",
      "[step: 3868] loss: 0.005518215708434582\n",
      "[step: 3869] loss: 14.831401824951172\n",
      "[step: 3869] loss: 0.005519455298781395\n",
      "[step: 3870] loss: 14.463781356811523\n",
      "[step: 3870] loss: 0.005517842713743448\n",
      "[step: 3871] loss: 14.394832611083984\n",
      "[step: 3871] loss: 0.005505444947630167\n",
      "[step: 3872] loss: 14.628190994262695\n",
      "[step: 3872] loss: 0.005486515816301107\n",
      "[step: 3873] loss: 14.806992530822754\n",
      "[step: 3873] loss: 0.00548947649076581\n",
      "[step: 3874] loss: 14.809085845947266\n",
      "[step: 3874] loss: 0.005497465841472149\n",
      "[step: 3875] loss: 14.603761672973633\n",
      "[step: 3875] loss: 0.005493030417710543\n",
      "[step: 3876] loss: 14.530197143554688\n",
      "[step: 3876] loss: 0.005490656942129135\n",
      "[step: 3877] loss: 14.669425964355469\n",
      "[step: 3877] loss: 0.005495155695825815\n",
      "[step: 3878] loss: 14.958513259887695\n",
      "[step: 3878] loss: 0.00549481762573123\n",
      "[step: 3879] loss: 15.26921272277832\n",
      "[step: 3879] loss: 0.005481025669723749\n",
      "[step: 3880] loss: 15.451446533203125\n",
      "[step: 3880] loss: 0.005477686878293753\n",
      "[step: 3881] loss: 15.49860668182373\n",
      "[step: 3881] loss: 0.005479790735989809\n",
      "[step: 3882] loss: 15.555511474609375\n",
      "[step: 3882] loss: 0.005481948144733906\n",
      "[step: 3883] loss: 15.627347946166992\n",
      "[step: 3883] loss: 0.005486566573381424\n",
      "[step: 3884] loss: 15.884129524230957\n",
      "[step: 3884] loss: 0.005501010455191135\n",
      "[step: 3885] loss: 15.867770195007324\n",
      "[step: 3885] loss: 0.0055243102833628654\n",
      "[step: 3886] loss: 15.815715789794922\n",
      "[step: 3886] loss: 0.0055491249077022076\n",
      "[step: 3887] loss: 15.403146743774414\n",
      "[step: 3887] loss: 0.005585494451224804\n",
      "[step: 3888] loss: 15.124399185180664\n",
      "[step: 3888] loss: 0.005622376222163439\n",
      "[step: 3889] loss: 15.113344192504883\n",
      "[step: 3889] loss: 0.005655870772898197\n",
      "[step: 3890] loss: 15.544684410095215\n",
      "[step: 3890] loss: 0.005655855871737003\n",
      "[step: 3891] loss: 16.587074279785156\n",
      "[step: 3891] loss: 0.0056433165445923805\n",
      "[step: 3892] loss: 17.62776756286621\n",
      "[step: 3892] loss: 0.005646509118378162\n",
      "[step: 3893] loss: 19.28695297241211\n",
      "[step: 3893] loss: 0.0056411223486065865\n",
      "[step: 3894] loss: 20.349225997924805\n",
      "[step: 3894] loss: 0.005604363977909088\n",
      "[step: 3895] loss: 21.626724243164062\n",
      "[step: 3895] loss: 0.005545858759433031\n",
      "[step: 3896] loss: 22.100452423095703\n",
      "[step: 3896] loss: 0.0055457609705626965\n",
      "[step: 3897] loss: 21.29286766052246\n",
      "[step: 3897] loss: 0.005565998610109091\n",
      "[step: 3898] loss: 20.068603515625\n",
      "[step: 3898] loss: 0.005543511360883713\n",
      "[step: 3899] loss: 18.29905891418457\n",
      "[step: 3899] loss: 0.005516421049833298\n",
      "[step: 3900] loss: 17.88446044921875\n",
      "[step: 3900] loss: 0.0055244192481040955\n",
      "[step: 3901] loss: 18.589725494384766\n",
      "[step: 3901] loss: 0.005523653235286474\n",
      "[step: 3902] loss: 19.133657455444336\n",
      "[step: 3902] loss: 0.0054984004236757755\n",
      "[step: 3903] loss: 18.72690200805664\n",
      "[step: 3903] loss: 0.0054976982064545155\n",
      "[step: 3904] loss: 16.63246726989746\n",
      "[step: 3904] loss: 0.005492696072906256\n",
      "[step: 3905] loss: 14.83399772644043\n",
      "[step: 3905] loss: 0.005479658953845501\n",
      "[step: 3906] loss: 14.574464797973633\n",
      "[step: 3906] loss: 0.005491197109222412\n",
      "[step: 3907] loss: 15.468986511230469\n",
      "[step: 3907] loss: 0.005498054437339306\n",
      "[step: 3908] loss: 16.203144073486328\n",
      "[step: 3908] loss: 0.005484243854880333\n",
      "[step: 3909] loss: 15.867900848388672\n",
      "[step: 3909] loss: 0.005492320284247398\n",
      "[step: 3910] loss: 15.006755828857422\n",
      "[step: 3910] loss: 0.005506427958607674\n",
      "[step: 3911] loss: 14.412059783935547\n",
      "[step: 3911] loss: 0.005507840774953365\n",
      "[step: 3912] loss: 14.46218490600586\n",
      "[step: 3912] loss: 0.005512203089892864\n",
      "[step: 3913] loss: 14.801291465759277\n",
      "[step: 3913] loss: 0.005534319672733545\n",
      "[step: 3914] loss: 14.843832015991211\n",
      "[step: 3914] loss: 0.005563424900174141\n",
      "[step: 3915] loss: 14.696139335632324\n",
      "[step: 3915] loss: 0.005618489813059568\n",
      "[step: 3916] loss: 14.671239852905273\n",
      "[step: 3916] loss: 0.0056245713494718075\n",
      "[step: 3917] loss: 14.845743179321289\n",
      "[step: 3917] loss: 0.005634435452520847\n",
      "[step: 3918] loss: 14.96718978881836\n",
      "[step: 3918] loss: 0.00560802360996604\n",
      "[step: 3919] loss: 15.042398452758789\n",
      "[step: 3919] loss: 0.005567824933677912\n",
      "[step: 3920] loss: 15.010685920715332\n",
      "[step: 3920] loss: 0.005517551209777594\n",
      "[step: 3921] loss: 14.985588073730469\n",
      "[step: 3921] loss: 0.0054927729070186615\n",
      "[step: 3922] loss: 14.881158828735352\n",
      "[step: 3922] loss: 0.0055035934783518314\n",
      "[step: 3923] loss: 14.804990768432617\n",
      "[step: 3923] loss: 0.005524733103811741\n",
      "[step: 3924] loss: 14.579587936401367\n",
      "[step: 3924] loss: 0.005541589576750994\n",
      "[step: 3925] loss: 14.396525382995605\n",
      "[step: 3925] loss: 0.005555561278015375\n",
      "[step: 3926] loss: 14.34426212310791\n",
      "[step: 3926] loss: 0.005595639813691378\n",
      "[step: 3927] loss: 14.492746353149414\n",
      "[step: 3927] loss: 0.005540819838643074\n",
      "[step: 3928] loss: 14.874528884887695\n",
      "[step: 3928] loss: 0.005572331603616476\n",
      "[step: 3929] loss: 15.318756103515625\n",
      "[step: 3929] loss: 0.005663419142365456\n",
      "[step: 3930] loss: 16.002288818359375\n",
      "[step: 3930] loss: 0.005627453327178955\n",
      "[step: 3931] loss: 16.966806411743164\n",
      "[step: 3931] loss: 0.005663939751684666\n",
      "[step: 3932] loss: 18.267986297607422\n",
      "[step: 3932] loss: 0.005692257545888424\n",
      "[step: 3933] loss: 20.123903274536133\n",
      "[step: 3933] loss: 0.005618314724415541\n",
      "[step: 3934] loss: 21.00018310546875\n",
      "[step: 3934] loss: 0.005629041697829962\n",
      "[step: 3935] loss: 22.406965255737305\n",
      "[step: 3935] loss: 0.005628836341202259\n",
      "[step: 3936] loss: 22.05286979675293\n",
      "[step: 3936] loss: 0.005545152351260185\n",
      "[step: 3937] loss: 23.473567962646484\n",
      "[step: 3937] loss: 0.005521178711205721\n",
      "[step: 3938] loss: 22.919572830200195\n",
      "[step: 3938] loss: 0.005550687201321125\n",
      "[step: 3939] loss: 22.246706008911133\n",
      "[step: 3939] loss: 0.005527307279407978\n",
      "[step: 3940] loss: 20.803722381591797\n",
      "[step: 3940] loss: 0.005518087651580572\n",
      "[step: 3941] loss: 20.559545516967773\n",
      "[step: 3941] loss: 0.005541061982512474\n",
      "[step: 3942] loss: 20.93341827392578\n",
      "[step: 3942] loss: 0.005511945113539696\n",
      "[step: 3943] loss: 19.69210433959961\n",
      "[step: 3943] loss: 0.0054953922517597675\n",
      "[step: 3944] loss: 18.289520263671875\n",
      "[step: 3944] loss: 0.005495985969901085\n",
      "[step: 3945] loss: 17.325002670288086\n",
      "[step: 3945] loss: 0.005490575917065144\n",
      "[step: 3946] loss: 17.352136611938477\n",
      "[step: 3946] loss: 0.005487003363668919\n",
      "[step: 3947] loss: 18.285018920898438\n",
      "[step: 3947] loss: 0.005486819427460432\n",
      "[step: 3948] loss: 18.083831787109375\n",
      "[step: 3948] loss: 0.0054880050010979176\n",
      "[step: 3949] loss: 17.68410873413086\n",
      "[step: 3949] loss: 0.005484946072101593\n",
      "[step: 3950] loss: 17.343345642089844\n",
      "[step: 3950] loss: 0.005474884528666735\n",
      "[step: 3951] loss: 15.779873847961426\n",
      "[step: 3951] loss: 0.005482470151036978\n",
      "[step: 3952] loss: 14.260941505432129\n",
      "[step: 3952] loss: 0.005472357384860516\n",
      "[step: 3953] loss: 14.799769401550293\n",
      "[step: 3953] loss: 0.005467722658067942\n",
      "[step: 3954] loss: 15.83875846862793\n",
      "[step: 3954] loss: 0.005464443936944008\n",
      "[step: 3955] loss: 15.279712677001953\n",
      "[step: 3955] loss: 0.005461249966174364\n",
      "[step: 3956] loss: 14.546076774597168\n",
      "[step: 3956] loss: 0.005455367267131805\n",
      "[step: 3957] loss: 14.880281448364258\n",
      "[step: 3957] loss: 0.005457629915326834\n",
      "[step: 3958] loss: 14.823638916015625\n",
      "[step: 3958] loss: 0.005459093023091555\n",
      "[step: 3959] loss: 14.044737815856934\n",
      "[step: 3959] loss: 0.005462707486003637\n",
      "[step: 3960] loss: 14.048847198486328\n",
      "[step: 3960] loss: 0.005470393691211939\n",
      "[step: 3961] loss: 14.628783226013184\n",
      "[step: 3961] loss: 0.005474258679896593\n",
      "[step: 3962] loss: 14.614275932312012\n",
      "[step: 3962] loss: 0.005485865753144026\n",
      "[step: 3963] loss: 14.097187042236328\n",
      "[step: 3963] loss: 0.005504451226443052\n",
      "[step: 3964] loss: 13.74925422668457\n",
      "[step: 3964] loss: 0.00553062092512846\n",
      "[step: 3965] loss: 13.812731742858887\n",
      "[step: 3965] loss: 0.005555830430239439\n",
      "[step: 3966] loss: 13.99566650390625\n",
      "[step: 3966] loss: 0.005596555303782225\n",
      "[step: 3967] loss: 13.953392028808594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3967] loss: 0.005574430804699659\n",
      "[step: 3968] loss: 13.770173072814941\n",
      "[step: 3968] loss: 0.00555117754265666\n",
      "[step: 3969] loss: 13.769681930541992\n",
      "[step: 3969] loss: 0.005518828518688679\n",
      "[step: 3970] loss: 13.863072395324707\n",
      "[step: 3970] loss: 0.005494656041264534\n",
      "[step: 3971] loss: 13.780372619628906\n",
      "[step: 3971] loss: 0.005494119133800268\n",
      "[step: 3972] loss: 13.568085670471191\n",
      "[step: 3972] loss: 0.005500619765371084\n",
      "[step: 3973] loss: 13.524681091308594\n",
      "[step: 3973] loss: 0.00549348583444953\n",
      "[step: 3974] loss: 13.615342140197754\n",
      "[step: 3974] loss: 0.0054750884883105755\n",
      "[step: 3975] loss: 13.642311096191406\n",
      "[step: 3975] loss: 0.0054648215882480145\n",
      "[step: 3976] loss: 13.567206382751465\n",
      "[step: 3976] loss: 0.0054694609716534615\n",
      "[step: 3977] loss: 13.51565933227539\n",
      "[step: 3977] loss: 0.005482683423906565\n",
      "[step: 3978] loss: 13.568077087402344\n",
      "[step: 3978] loss: 0.005494528915733099\n",
      "[step: 3979] loss: 13.645122528076172\n",
      "[step: 3979] loss: 0.005501844920217991\n",
      "[step: 3980] loss: 13.65787124633789\n",
      "[step: 3980] loss: 0.005511893425136805\n",
      "[step: 3981] loss: 13.678292274475098\n",
      "[step: 3981] loss: 0.005529613699764013\n",
      "[step: 3982] loss: 13.836688041687012\n",
      "[step: 3982] loss: 0.005553490947932005\n",
      "[step: 3983] loss: 14.179891586303711\n",
      "[step: 3983] loss: 0.005606707651168108\n",
      "[step: 3984] loss: 14.79123306274414\n",
      "[step: 3984] loss: 0.005565119441598654\n",
      "[step: 3985] loss: 15.84663200378418\n",
      "[step: 3985] loss: 0.005553386639803648\n",
      "[step: 3986] loss: 17.768177032470703\n",
      "[step: 3986] loss: 0.0055660041980445385\n",
      "[step: 3987] loss: 21.104955673217773\n",
      "[step: 3987] loss: 0.005547449924051762\n",
      "[step: 3988] loss: 25.8833065032959\n",
      "[step: 3988] loss: 0.005533934570848942\n",
      "[step: 3989] loss: 32.946563720703125\n",
      "[step: 3989] loss: 0.005514790304005146\n",
      "[step: 3990] loss: 37.87754440307617\n",
      "[step: 3990] loss: 0.005483295302838087\n",
      "[step: 3991] loss: 39.9854736328125\n",
      "[step: 3991] loss: 0.005494004115462303\n",
      "[step: 3992] loss: 34.25959396362305\n",
      "[step: 3992] loss: 0.005521095357835293\n",
      "[step: 3993] loss: 24.358177185058594\n",
      "[step: 3993] loss: 0.005541306454688311\n",
      "[step: 3994] loss: 24.95197296142578\n",
      "[step: 3994] loss: 0.0055190217681229115\n",
      "[step: 3995] loss: 22.513633728027344\n",
      "[step: 3995] loss: 0.005503818858414888\n",
      "[step: 3996] loss: 22.863128662109375\n",
      "[step: 3996] loss: 0.0054716682061553\n",
      "[step: 3997] loss: 28.6884765625\n",
      "[step: 3997] loss: 0.005478894803673029\n",
      "[step: 3998] loss: 28.784683227539062\n",
      "[step: 3998] loss: 0.0055114589631557465\n",
      "[step: 3999] loss: 26.987293243408203\n",
      "[step: 3999] loss: 0.0054970066994428635\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [800,25] vs. [398,25]\n\t [[node sub (defined at <ipython-input-1-6d038b9841fd>:108)  = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_4_0_0, _arg_Placeholder_5_0_1)]]\n\nCaused by op 'sub', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-6d038b9841fd>\", line 108, in <module>\n    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 866, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 8318, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [800,25] vs. [398,25]\n\t [[node sub (defined at <ipython-input-1-6d038b9841fd>:108)  = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_4_0_0, _arg_Placeholder_5_0_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [800,25] vs. [398,25]\n\t [[{{node sub}} = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_4_0_0, _arg_Placeholder_5_0_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d038b9841fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mrmse_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidationY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m    \u001b[0;31m# print(\"pred: {}\".format(test_predict[-1,:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [800,25] vs. [398,25]\n\t [[node sub (defined at <ipython-input-1-6d038b9841fd>:108)  = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_4_0_0, _arg_Placeholder_5_0_1)]]\n\nCaused by op 'sub', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-6d038b9841fd>\", line 108, in <module>\n    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 866, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 8318, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [800,25] vs. [398,25]\n\t [[node sub (defined at <ipython-input-1-6d038b9841fd>:108)  = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_4_0_0, _arg_Placeholder_5_0_1)]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0)-np.min(data,0)\n",
    "    return numerator / (denominator+1e-7)\n",
    "\n",
    "xy = np.genfromtxt('/Users/yeseo/Desktop/City_Counted_TaxiMach_Link_Dataset_Full_201501 - 12.txt',delimiter = ',',dtype = None)\n",
    "xy_with_noise = np.genfromtxt('/Users/yeseo/Desktop/2015eliminated_1.txt',delimiter = ',',dtype = None)\n",
    "\n",
    "#data_preprocessing\n",
    "xy= xy[:,:27]\n",
    "a = xy[:,:2]\n",
    "b = xy[:,2:]\n",
    "b= MinMaxScaler(b)\n",
    "xy = np.hstack((a,b))\n",
    "\n",
    "xy_with_noise = xy_with_noise[:,:27]\n",
    "a_with_noise = xy_with_noise[:,:2]\n",
    "b_with_noise = xy_with_noise[:,2:]\n",
    "b_with_noise = MinMaxScaler(b_with_noise)\n",
    "xy_with_noise = np.hstack((a_with_noise,b_with_noise))\n",
    "\n",
    "\n",
    "#parameters\n",
    "seq_length =6\n",
    "data_dim =27\n",
    "hidden_dim = 54\n",
    "output_dim = 25\n",
    "learning_rate = 0.01\n",
    "iterations = 4000\n",
    "\n",
    "train_size = int(len(xy)*0.7)\n",
    "validation_size = int(len(xy)*0.2)\n",
    "\n",
    "#divide data set to train,validation and test set\n",
    "train_set = xy[:train_size]\n",
    "validation_set = xy[train_size:train_size+validation_size]\n",
    "test_set = xy[train_size+validation_size:]\n",
    "\n",
    "train_set_with_noise = xy_with_noise[:train_size]\n",
    "validation_set_with_noise = xy_with_noise[train_size:train_size+validation_size]\n",
    "test_set_with_noise = xy_with_noise[train_size+validation_size:]\n",
    "\n",
    "# build data set for rnn\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range (0,len(time_series)-seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i+seq_length,2:]\n",
    "     \n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#train_set, test_set 만들기\n",
    "trainX, trainY = build_dataset(train_set,seq_length)\n",
    "validationX, validationY = build_dataset(validation_set,seq_length)\n",
    "testX,testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "trainX_with_noise, trainY_with_noise = build_dataset(train_set_with_noise,seq_length)\n",
    "validationX_with_noise, validationY_with_noise = build_dataset(validation_set_with_noise,seq_length)\n",
    "testX_with_noise,testY_with_noise = build_dataset(test_set_with_noise, seq_length)\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y1 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "X2 = tf.placeholder(tf.float32,[None, seq_length,data_dim])\n",
    "Y2 = tf.placeholder(tf.float32,[None,25])\n",
    "\n",
    "#LSTM CELL만들기\n",
    "\n",
    "with tf.variable_scope(\"rnn1\"):\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs1,_states1 = tf.nn.dynamic_rnn(cell1,X1,dtype = tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs1[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss1 =tf.reduce_sum(tf.square(Y_pred-Y1))\n",
    "    train1 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss1)\n",
    "\n",
    "with tf.variable_scope(\"rnn2\"):\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple = True, activation=tf.tanh)\n",
    "    outputs2,_states2 = tf.nn.dynamic_rnn(cell2, X2, dtype = tf.float32)\n",
    "    Y_pred_with_noise = tf.contrib.layers.fully_connected(outputs2[:,-1], output_dim,activation_fn = None)\n",
    "\n",
    "    loss2 =tf.reduce_mean(tf.square(Y_pred_with_noise-Y2))\n",
    "    train2 = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss2)\n",
    "\n",
    "\n",
    "#RMSE 측정\n",
    "targets = tf.placeholder(tf.float32,[None,25])\n",
    "predictions = tf.placeholder(tf.float32,[None,25])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions)))\n",
    "\n",
    "x1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "x2 = x1+0.3\n",
    "x3 = x2+0.3\n",
    "loss_for_graph = np.zeros(iterations)\n",
    "x4 = np.array(range(0,iterations))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss1 = sess.run([train1,loss1],feed_dict={X1:trainX, Y1:trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss1))\n",
    "        loss_for_graph[i] = step_loss1\n",
    "        _, step_loss2 = sess.run([train2,loss2],feed_dict={X2:trainX_with_noise, Y2:trainY_with_noise})\n",
    "        print(\"[step: {}] loss: {}\".format(i,step_loss2))\n",
    "        \n",
    "    validation_predict = sess.run(Y_pred, feed_dict = {X1:validationX})\n",
    "    validation_predict_with_noise = sess.run(Y_pred_with_noise, feed_dict = {X2:validationX_with_noise})\n",
    "    test_predict = sess.run(Y_pred, feed_dict = {X1:testX})\n",
    "    \n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validationY,predictions: validation_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "   # print(\"pred: {}\".format(test_predict[-1,:]))\n",
    "    #print(\"real: {}\".format(testY[-1,:]))\n",
    "    #print(\"noise: {}\".format(eliminate_noise_pred[-1,:]))\n",
    "    \n",
    "#    plt.bar(x1,test_predict[-1,:],label = 'predict',color ='b',width = 0.1)\n",
    "  #  plt.bar(x2,testY[-1,:],label = 'real',color ='g',width = 0.1)\n",
    "    #plt.bar(x3,eliminate_noise_pred[-1,:],label = 'noise',color ='g',width = 0.1)\n",
    "    plt.plot(x4,loss_for_graph)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
